入力してください: 「https://zenn.dev/asap」この執筆者のもっともいいねがついている記事を調査して、その記事の中身を取得し、その記事がなぜ評価されているのかを分析してください。
response:  {'messages': [HumanMessage(content=[{'type': 'text', 'text': '「https://zenn.dev/asap」この執筆者のもっともいいねがついている記事を調査して、その記事の中身を取得し、その記事がなぜ評価されているのかを分析してください。'}], additional_kwargs={}, response_metadata={}), AIMessage(content='zenn.devのasap氏の最もいいねが多い記事を調べ、その内容を取得し、評価されている理由を分析する、というご要望ですね。\n\nまず、zenn.dev/asapにアクセスして、最もいいねが多い記事のタイトルとURLを特定する必要があります。その後、そのURLにアクセスして記事の内容を取得します。最後に、記事の内容から評価されている理由を推測します。\n\nでは、始めましょう。\n\n1.  zenn.dev/asapにアクセスし、最もいいねが多い記事の情報を取得します。', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-989fe5ca-2637-4757-878f-db26e13526b8-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap'}, 'id': '58fad86e-0b01-4fc6-a8cc-57deb5c48b41', 'type': 'tool_call'}], usage_metadata={'input_tokens': 778, 'output_tokens': 128, 'total_tokens': 906, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap\n\n- Page URL: https://zenn.dev/asap\n- Page Title: asapさんの記事一覧 | Zenn\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- banner [ref=s1e32]:\n    - img "asap" [ref=s1e36]\n    - heading "asap" [level=1] [ref=s1e39]\n    - button "フォロー" [ref=s1e41]\n    - paragraph [ref=s1e43]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n    - button "2,214 Likes" [ref=s1e46]\n    - button "310 Followers" [ref=s1e48]\n    - link "@personabb" [ref=s1e51]:\n        - /url: https://github.com/personabb\n        - tooltip "@personabb" [ref=s1e52]:\n            - img "GitHub" [ref=s1e53]\n            - text: "@personabb"\n    - link "@asap2650" [ref=s1e55]:\n        - /url: https://twitter.com/asap2650\n        - tooltip "@asap2650" [ref=s1e56]:\n            - img "X(Twitter)" [ref=s1e57]\n            - text: "@asap2650"\n    - link "yard.tips" [ref=s1e63]:\n        - /url: https://yard.tips/asap2650\n        - tooltip "yard.tips" [ref=s1e64]:\n            - img [ref=s1e65]\n            - text: yard.tips\n    - link "RSS" [ref=s1e67]:\n        - /url: /asap/feed\n        - tooltip "RSS" [ref=s1e68]:\n            - img [ref=s1e69]\n            - text: RSS\n- link "Articles 72" [ref=s1e80]:\n    - /url: /asap\n- link "Scraps 5" [ref=s1e82]:\n    - /url: /asap?tab=scraps\n- link "Comments" [ref=s1e84]:\n    - /url: /asap?tab=comments\n- article [ref=s1e89]:\n    - link "tech" [ref=s1e90]:\n        - /url: /tech-or-idea\n    - link "❄️ LangChainでGemini-2.0 APIの画像編集を実行する方法 6日前 いいねされた数 18" [ref=s1e91]:\n        - /url: /asap/articles/1d5e6ec06c380f\n        - text: ❄️\n        - heading "LangChainでGemini-2.0 APIの画像編集を実行する方法" [level=3] [ref=s1e95]\n        - time [ref=s1e98]: 6日前\n        - img "いいねされた数" [ref=s1e100]\n        - text: "18"\n- article [ref=s1e102]:\n    - link "tech" [ref=s1e103]:\n        - /url: /tech-or-idea\n    - link "🙌 完全自律型AIエージェントOSS【LangManus】完全に理解した！ 13日前 いいねされた数 48" [ref=s1e104]:\n        - /url: /asap/articles/b27e832c2a49b7\n        - text: 🙌\n        - heading "完全自律型AIエージェントOSS【LangManus】完全に理解した！" [level=3] [ref=s1e108]\n        - time [ref=s1e111]: 13日前\n        - img "いいねされた数" [ref=s1e113]\n        - text: "48"\n- article [ref=s1e115]:\n    - link "tech" [ref=s1e116]:\n        - /url: /tech-or-idea\n    - link "💫 LangChainでOpenAI gpt-4o / o1にPDFを入力する方法 20日前 いいねされた数 9" [ref=s1e117]:\n        - /url: /asap/articles/74885fab863c9e\n        - text: 💫\n        - heading "LangChainでOpenAI gpt-4o / o1にPDFを入力する方法" [level=3]\n          [ref=s1e121]\n        - time [ref=s1e124]: 20日前\n        - img "いいねされた数" [ref=s1e126]\n        - text: "9"\n- article [ref=s1e128]:\n    - link "tech" [ref=s1e129]:\n        - /url: /tech-or-idea\n    - link "👣 Ollama + LangChainで画像を入力する方法【Gemma3 27B】 27日前 いいねされた数 14" [ref=s1e130]:\n        - /url: /asap/articles/18b110d551b811\n        - text: 👣\n        - heading "Ollama + LangChainで画像を入力する方法【Gemma3 27B】" [level=3]\n          [ref=s1e134]\n        - time [ref=s1e137]: 27日前\n        - img "いいねされた数" [ref=s1e139]\n        - text: "14"\n- article [ref=s1e141]:\n    - link "tech" [ref=s1e142]:\n        - /url: /tech-or-idea\n    - link "🦄 Google CloudのVector StoreでRAG Agentsを構築する【Big Querty編】 28日前 いいねされた数 12" [ref=s1e143]:\n        - /url: /asap/articles/3d258861c6c528\n        - text: 🦄\n        - heading "Google CloudのVector StoreでRAG Agentsを構築する【Big Querty編】"\n          [level=3] [ref=s1e147]\n        - time [ref=s1e150]: 28日前\n        - img "いいねされた数" [ref=s1e152]\n        - text: "12"\n- article [ref=s1e154]:\n    - link "tech" [ref=s1e155]:\n        - /url: /tech-or-idea\n    - link "🎶 最新音楽生成AI 【DiffRhythm】をGoogle Colabで動かせるようにしました！（簡易版） 1ヶ月前 いいねされた数 39" [ref=s1e156]:\n        - /url: /asap/articles/4edc4a71de8961\n        - text: 🎶\n        - heading "最新音楽生成AI 【DiffRhythm】をGoogle Colabで動かせるようにしました！（簡易版）"\n          [level=3] [ref=s1e160]\n        - time [ref=s1e163]: 1ヶ月前\n        - img "いいねされた数" [ref=s1e165]\n        - text: "39"\n- article [ref=s1e167]:\n    - link "tech" [ref=s1e168]:\n        - /url: /tech-or-idea\n    - link "🐤 LangGraphのTool Callingを利用して、RAG Agentsを構築する（後編） 1ヶ月前 いいねされた数 12" [ref=s1e169]:\n        - /url: /asap/articles/c58cfab9c1362a\n        - text: 🐤\n        - heading "LangGraphのTool Callingを利用して、RAG Agentsを構築する（後編）" [level=3]\n          [ref=s1e173]\n        - time [ref=s1e176]: 1ヶ月前\n        - img "いいねされた数" [ref=s1e178]\n        - text: "12"\n- article [ref=s1e180]:\n    - link "idea" [ref=s1e181]:\n        - /url: /tech-or-idea\n    - link "🏆 イベントに登壇してきた話【資料もこちら】 1ヶ月前 いいねされた数 20" [ref=s1e182]:\n        - /url: /asap/articles/89d2a13bdc75a3\n        - text: 🏆\n        - heading "イベントに登壇してきた話【資料もこちら】" [level=3] [ref=s1e186]\n        - time [ref=s1e189]: 1ヶ月前\n        - img "いいねされた数" [ref=s1e191]\n        - text: "20"\n- article [ref=s1e193]:\n    - link "tech" [ref=s1e194]:\n        - /url: /tech-or-idea\n    - link "👌 Grok API（Grok 2）で遊んでみた！【Elyza Tasks 100】 2ヶ月前 いいねされた数 11" [ref=s1e195]:\n        - /url: /asap/articles/6aa01bde32163d\n        - text: 👌\n        - heading "Grok API（Grok 2）で遊んでみた！【Elyza Tasks 100】" [level=3]\n          [ref=s1e199]\n        - time [ref=s1e202]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e204]\n        - text: "11"\n- article [ref=s1e206]:\n    - link "tech" [ref=s1e207]:\n        - /url: /tech-or-idea\n    - link "📑 Multi-head Latent Attention (MLA)の詳細【be absorbedとは？】 2ヶ月前 いいねされた数 22" [ref=s1e208]:\n        - /url: /asap/articles/be3d4b60d8ac92\n        - text: 📑\n        - heading "Multi-head Latent Attention (MLA)の詳細【be absorbedとは？】"\n          [level=3] [ref=s1e212]\n        - time [ref=s1e215]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e217]\n        - text: "22"\n- article [ref=s1e219]:\n    - link "tech" [ref=s1e220]:\n        - /url: /tech-or-idea\n    - link "⛳ LangGraphのTool Callingを利用して、RAG Agentsを構築する（前編） 2ヶ月前 いいねされた数 46" [ref=s1e221]:\n        - /url: /asap/articles/2ad7eb5e7560cf\n        - text: ⛳\n        - heading "LangGraphのTool Callingを利用して、RAG Agentsを構築する（前編）" [level=3]\n          [ref=s1e225]\n        - time [ref=s1e228]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e230]\n        - text: "46"\n- article [ref=s1e232]:\n    - link "tech" [ref=s1e233]:\n        - /url: /tech-or-idea\n    - link "🫠 【LangChain】で【Transformer】を利用する場合に【chat_template】を設定する方法【エラー対処記事】 2ヶ月前 いいねされた数 13" [ref=s1e234]:\n        - /url: /asap/articles/985f3580ea476d\n        - text: 🫠\n        - heading\n          "【LangChain】で【Transformer】を利用する場合に【chat_template】を設定する方法【エラー対処記事】"\n          [level=3] [ref=s1e238]\n        - time [ref=s1e241]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e243]\n        - text: "13"\n- article [ref=s1e245]:\n    - link "tech" [ref=s1e246]:\n        - /url: /tech-or-idea\n    - link "🌊 採点理由を出力しながらローカルLLMを評価する【Elyza Tasks 100】 2ヶ月前 いいねされた数 26" [ref=s1e247]:\n        - /url: /asap/articles/3a99c34fc1d4cb\n        - text: 🌊\n        - heading "採点理由を出力しながらローカルLLMを評価する【Elyza Tasks 100】" [level=3]\n          [ref=s1e251]\n        - time [ref=s1e254]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e256]\n        - text: "26"\n- article [ref=s1e258]:\n    - link "tech" [ref=s1e259]:\n        - /url: /tech-or-idea\n    - link "📑 DeepSeek-R1の論文読んだ？【勉強になるよ】 2ヶ月前 いいねされた数 921" [ref=s1e260]:\n        - /url: /asap/articles/34237ad87f8511\n        - text: 📑\n        - heading "DeepSeek-R1の論文読んだ？【勉強になるよ】" [level=3] [ref=s1e264]\n        - time [ref=s1e267]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e269]\n        - text: "921"\n- article [ref=s1e271]:\n    - link "tech" [ref=s1e272]:\n        - /url: /tech-or-idea\n    - link "📖 VAEって結局何者なの？ 3ヶ月前 いいねされた数 104" [ref=s1e273]:\n        - /url: /asap/articles/6caa9043276424\n        - text: 📖\n        - heading "VAEって結局何者なの？" [level=3] [ref=s1e277]\n        - time [ref=s1e280]: 3ヶ月前\n        - img "いいねされた数" [ref=s1e282]\n        - text: "104"\n- article [ref=s1e284]:\n    - link "tech" [ref=s1e285]:\n        - /url: /tech-or-idea\n    - link "🦔 初めてのSageMaker AIでSFTTrainerによるLLMのフルパラメータファインチューニングを試す 3ヶ月前 いいねされた数 9" [ref=s1e286]:\n        - /url: /asap/articles/3f31b77010624c\n        - text: 🦔\n        - heading "初めてのSageMaker AIでSFTTrainerによるLLMのフルパラメータファインチューニングを試す"\n          [level=3] [ref=s1e290]\n        - time [ref=s1e293]: 3ヶ月前\n        - img "いいねされた数" [ref=s1e295]\n        - text: "9"\n- article [ref=s1e297]:\n    - link "tech" [ref=s1e298]:\n        - /url: /tech-or-idea\n    - link "😺 【動画翻訳】OpenAIの研究者によるo1やLLMに対するディスカッションの翻訳 3ヶ月前 いいねされた数 12" [ref=s1e299]:\n        - /url: /asap/articles/2d2832bf3288aa\n        - text: 😺\n        - heading "【動画翻訳】OpenAIの研究者によるo1やLLMに対するディスカッションの翻訳" [level=3]\n          [ref=s1e303]\n        - time [ref=s1e306]: 3ヶ月前\n        - img "いいねされた数" [ref=s1e308]\n        - text: "12"\n- article [ref=s1e310]:\n    - link "tech" [ref=s1e311]:\n        - /url: /tech-or-idea\n    - link "🧚\u200d♀️ AIでオリジナル楽曲を作り、自作のかわいい声で歌ってもらう方法 3ヶ月前 いいねされた数 28" [ref=s1e312]:\n        - /url: /asap/articles/886af0fe48dda3\n        - text: 🧚\u200d♀️\n        - heading "AIでオリジナル楽曲を作り、自作のかわいい声で歌ってもらう方法" [level=3] [ref=s1e316]\n        - time [ref=s1e319]: 3ヶ月前\n        - img "いいねされた数" [ref=s1e321]\n        - text: "28"\n- article [ref=s1e323]:\n    - link "tech" [ref=s1e324]:\n        - /url: /tech-or-idea\n    - link "🎮 Google ColabでSeed-VCによる声色変換を試す 4ヶ月前 いいねされた数 27" [ref=s1e325]:\n        - /url: /asap/articles/9c54aef739a6ed\n        - text: 🎮\n        - heading "Google ColabでSeed-VCによる声色変換を試す" [level=3] [ref=s1e329]\n        - time [ref=s1e332]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e334]\n        - text: "27"\n- article [ref=s1e336]:\n    - link "tech" [ref=s1e337]:\n        - /url: /tech-or-idea\n    - link "🐡 LLMの性能における重要補題【Johnson–Lindenstrauss lemma】のお気持ち〜Attentionの数式理解を添えて〜 4ヶ月前 いいねされた数 21" [ref=s1e338]:\n        - /url: /asap/articles/6bdf89d8e964ec\n        - text: 🐡\n        - heading "LLMの性能における重要補題【Johnson–Lindenstrauss\n          lemma】のお気持ち〜Attentionの数式理解を添えて〜" [level=3] [ref=s1e342]\n        - time [ref=s1e345]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e347]\n        - text: "21"\n- article [ref=s1e349]:\n    - link "tech" [ref=s1e350]:\n        - /url: /tech-or-idea\n    - link "📏 【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】 4ヶ月前 いいねされた数 50" [ref=s1e351]:\n        - /url: /asap/articles/ea9f42edff6573\n        - text: 📏\n        - heading "【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】"\n          [level=3] [ref=s1e355]\n        - time [ref=s1e358]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e360]\n        - text: "50"\n- article [ref=s1e362]:\n    - link "tech" [ref=s1e363]:\n        - /url: /tech-or-idea\n    - link "🎙️ 高品質合成音声モデル【Style-Bert-VITS2】をGoogle Colabで学習し、【Aivis Speech】で利用する方法 4ヶ月前 いいねされた数 24" [ref=s1e364]:\n        - /url: /asap/articles/5c437ce2943ed7\n        - text: 🎙️\n        - heading "高品質合成音声モデル【Style-Bert-VITS2】をGoogle Colabで学習し、【Aivis\n          Speech】で利用する方法" [level=3] [ref=s1e368]\n        - time [ref=s1e371]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e373]\n        - text: "24"\n- article [ref=s1e375]:\n    - link "tech" [ref=s1e376]:\n        - /url: /tech-or-idea\n    - link "🎄 【LangChain】LCEL ステップバイステップ ハンズオン 〜なろう短編小説の自動生成＆評価・改善を行う「AIエージェント」を添えて〜 4ヶ月前 いいねされた数 19" [ref=s1e377]:\n        - /url: /asap/articles/aa587ad8d76105\n        - text: 🎄\n        - heading "【LangChain】LCEL ステップバイステップ ハンズオン\n          〜なろう短編小説の自動生成＆評価・改善を行う「AIエージェント」を添えて〜" [level=3] [ref=s1e381]\n        - time [ref=s1e384]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e386]\n        - text: "19"\n- article [ref=s1e388]:\n    - link "tech" [ref=s1e389]:\n        - /url: /tech-or-idea\n    - link "💻 LangGraphによる「AIエージェントWebアプリ」を作成する【Next.js】 4ヶ月前 いいねされた数 45" [ref=s1e390]:\n        - /url: /asap/articles/a350fe4ac00343\n        - text: 💻\n        - heading "LangGraphによる「AIエージェントWebアプリ」を作成する【Next.js】" [level=3]\n          [ref=s1e394]\n        - time [ref=s1e397]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e399]\n        - text: "45"\n- article [ref=s1e401]:\n    - link "tech" [ref=s1e402]:\n        - /url: /tech-or-idea\n    - link "🤔 LangGraphにて入力情報が不十分な場合に、ユーザへ追加質問をするAIエージェントを作る方法 5ヶ月前 いいねされた数 41" [ref=s1e403]:\n        - /url: /asap/articles/03548f844efe6f\n        - text: 🤔\n        - heading "LangGraphにて入力情報が不十分な場合に、ユーザへ追加質問をするAIエージェントを作る方法" [level=3]\n          [ref=s1e407]\n        - time [ref=s1e410]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e412]\n        - text: "41"\n- article [ref=s1e414]:\n    - link "tech" [ref=s1e415]:\n        - /url: /tech-or-idea\n    - link "🤖 高品質動画生成AI【Allegro】をローカルで実行する【VRAM12GB】 5ヶ月前 いいねされた数 10" [ref=s1e416]:\n        - /url: /asap/articles/dac57655eb0314\n        - text: 🤖\n        - heading "高品質動画生成AI【Allegro】をローカルで実行する【VRAM12GB】" [level=3] [ref=s1e420]\n        - time [ref=s1e423]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e425]\n        - text: "10"\n- article [ref=s1e427]:\n    - link "tech" [ref=s1e428]:\n        - /url: /tech-or-idea\n    - link "🌊 フレーム補間技術【RIFE】を利用してCogVideoXのFPSを改善しよう 5ヶ月前 いいねされた数 2" [ref=s1e429]:\n        - /url: /asap/articles/dbeef72b7bfdc3\n        - text: 🌊\n        - heading "フレーム補間技術【RIFE】を利用してCogVideoXのFPSを改善しよう" [level=3] [ref=s1e433]\n        - time [ref=s1e436]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e438]\n        - text: "2"\n- article [ref=s1e440]:\n    - link "tech" [ref=s1e441]:\n        - /url: /tech-or-idea\n    - link "🔥 LangGraphを使ってAIエージェントを実装する方法 5ヶ月前 いいねされた数 34" [ref=s1e442]:\n        - /url: /asap/articles/5da9cf01703a47\n        - text: 🔥\n        - heading "LangGraphを使ってAIエージェントを実装する方法" [level=3] [ref=s1e446]\n        - time [ref=s1e449]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e451]\n        - text: "34"\n- article [ref=s1e453]:\n    - link "tech" [ref=s1e454]:\n        - /url: /tech-or-idea\n    - link "🌈 動画生成AI【CogVideoX】をローカルで実行する 5ヶ月前 いいねされた数 9" [ref=s1e455]:\n        - /url: /asap/articles/a4077e8b9ddf1a\n        - text: 🌈\n        - heading "動画生成AI【CogVideoX】をローカルで実行する" [level=3] [ref=s1e459]\n        - time [ref=s1e462]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e464]\n        - text: "9"\n- article [ref=s1e466]:\n    - link "tech" [ref=s1e467]:\n        - /url: /tech-or-idea\n    - link "🛎️ ローカルでDIfyを使ってAIエージェントを実装する方法 5ヶ月前 いいねされた数 55" [ref=s1e468]:\n        - /url: /asap/articles/a35254020fbbae\n        - text: 🛎️\n        - heading "ローカルでDIfyを使ってAIエージェントを実装する方法" [level=3] [ref=s1e472]\n        - time [ref=s1e475]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e477]\n        - text: "55"\n- article [ref=s1e479]:\n    - link "tech" [ref=s1e480]:\n        - /url: /tech-or-idea\n    - link "💫 高品質動画生成AI【Pyramid Flow】をローカルで実行する 6ヶ月前 いいねされた数 12" [ref=s1e481]:\n        - /url: /asap/articles/e1880903625cb5\n        - text: 💫\n        - heading "高品質動画生成AI【Pyramid Flow】をローカルで実行する" [level=3] [ref=s1e485]\n        - time [ref=s1e488]: 6ヶ月前\n        - img "いいねされた数" [ref=s1e490]\n        - text: "12"\n- article [ref=s1e492]:\n    - link "tech" [ref=s1e493]:\n        - /url: /tech-or-idea\n    - link "📖 不偏分散で N-1 で割る理由 を自力で導出する 6ヶ月前 いいねされた数 5" [ref=s1e494]:\n        - /url: /asap/articles/f0cba34c2ac2ac\n        - text: 📖\n        - heading "不偏分散で N-1 で割る理由 を自力で導出する" [level=3] [ref=s1e498]\n        - time [ref=s1e501]: 6ヶ月前\n        - img "いいねされた数" [ref=s1e503]\n        - text: "5"\n- article [ref=s1e505]:\n    - link "tech" [ref=s1e506]:\n        - /url: /tech-or-idea\n    - link "✍️ 勾配降下法 ・ 誤差逆伝播法 を自力で導出する 6ヶ月前 いいねされた数 25" [ref=s1e507]:\n        - /url: /asap/articles/643f604d444813\n        - text: ✍️\n        - heading "勾配降下法 ・ 誤差逆伝播法 を自力で導出する" [level=3] [ref=s1e511]\n        - time [ref=s1e514]: 6ヶ月前\n        - img "いいねされた数" [ref=s1e516]\n        - text: "25"\n- article [ref=s1e518]:\n    - link "tech" [ref=s1e519]:\n        - /url: /tech-or-idea\n    - link "💰 Open AI Realtime APIで、会話履歴を削除して高額請求を回避したい！【Pythonサンプルコード】 2024/10/07 いいねされた数 25" [ref=s1e520]:\n        - /url: /asap/articles/af07fcedbbef61\n        - text: 💰\n        - heading "Open AI Realtime APIで、会話履歴を削除して高額請求を回避したい！【Pythonサンプルコード】"\n          [level=3] [ref=s1e524]\n        - time [ref=s1e527]: 2024/10/07\n        - img "いいねされた数" [ref=s1e529]\n        - text: "25"\n- article [ref=s1e531]:\n    - link "tech" [ref=s1e532]:\n        - /url: /tech-or-idea\n    - link "✈️ Open AI Realtime APIのPythonサンプルコードで、AI発話にカットイン（割り込み）する方法 2024/10/06 いいねされた数 24" [ref=s1e533]:\n        - /url: /asap/articles/563500af4649da\n        - text: ✈️\n        - heading "Open AI Realtime APIのPythonサンプルコードで、AI発話にカットイン（割り込み）する方法"\n          [level=3] [ref=s1e537]\n        - time [ref=s1e540]: 2024/10/06\n        - img "いいねされた数" [ref=s1e542]\n        - text: "24"\n- article [ref=s1e544]:\n    - link "tech" [ref=s1e545]:\n        - /url: /tech-or-idea\n    - link "🥰 Open AI Realtime APIのPythonサンプルコードを作成 2024/10/04 いいねされた数 79" [ref=s1e546]:\n        - /url: /asap/articles/4368fd306b592a\n        - text: 🥰\n        - heading "Open AI Realtime APIのPythonサンプルコードを作成" [level=3] [ref=s1e550]\n        - time [ref=s1e553]: 2024/10/04\n        - img "いいねされた数" [ref=s1e555]\n        - text: "79"\n- article [ref=s1e557]:\n    - link "tech" [ref=s1e558]:\n        - /url: /tech-or-idea\n    - link "📗 主成分分析を自力で導出する 2024/10/03 いいねされた数 6" [ref=s1e559]:\n        - /url: /asap/articles/ff8f34d19ca6a4\n        - text: 📗\n        - heading "主成分分析を自力で導出する" [level=3] [ref=s1e563]\n        - time [ref=s1e566]: 2024/10/03\n        - img "いいねされた数" [ref=s1e568]\n        - text: "6"\n- article [ref=s1e570]:\n    - link "tech" [ref=s1e571]:\n        - /url: /tech-or-idea\n    - link "😍 FLUX.1-dev で ControlNet-depth と LoRA【QTCanimatedFeeling-FLUX】 を試す 2024/10/02 いいねされた数 3" [ref=s1e572]:\n        - /url: /asap/articles/9192c2363dd96a\n        - text: 😍\n        - heading "FLUX.1-dev で ControlNet-depth と LoRA【QTCanimatedFeeling-FLUX】\n          を試す" [level=3] [ref=s1e576]\n        - time [ref=s1e579]: 2024/10/02\n        - img "いいねされた数" [ref=s1e581]\n        - text: "3"\n- article [ref=s1e583]:\n    - link "tech" [ref=s1e584]:\n        - /url: /tech-or-idea\n    - link "📗 拡散モデルの理論について（拡散過程の条件付き確率・逆拡散過程のスケジューラ） 2024/09/23 いいねされた数 8" [ref=s1e585]:\n        - /url: /asap/articles/8aaa896a02f168\n        - text: 📗\n        - heading "拡散モデルの理論について（拡散過程の条件付き確率・逆拡散過程のスケジューラ）" [level=3] [ref=s1e589]\n        - time [ref=s1e592]: 2024/09/23\n        - img "いいねされた数" [ref=s1e594]\n        - text: "8"\n- article [ref=s1e596]:\n    - link "tech" [ref=s1e597]:\n        - /url: /tech-or-idea\n    - link "📖 分散共分散行列の魅力 2024/09/20 いいねされた数 14" [ref=s1e598]:\n        - /url: /asap/articles/2269cd74c5ddc2\n        - text: 📖\n        - heading "分散共分散行列の魅力" [level=3] [ref=s1e602]\n        - time [ref=s1e605]: 2024/09/20\n        - img "いいねされた数" [ref=s1e607]\n        - text: "14"\n- article [ref=s1e609]:\n    - link "tech" [ref=s1e610]:\n        - /url: /tech-or-idea\n    - link "😍 超強力汎用ControlNet「AnyTest v4」を利用する方法【diffusers】 2024/09/17 いいねされた数 9" [ref=s1e611]:\n        - /url: /asap/articles/c08dffa41bec03\n        - text: 😍\n        - heading "超強力汎用ControlNet「AnyTest v4」を利用する方法【diffusers】" [level=3]\n          [ref=s1e615]\n        - time [ref=s1e618]: 2024/09/17\n        - img "いいねされた数" [ref=s1e620]\n        - text: "9"\n- article [ref=s1e622]:\n    - link "tech" [ref=s1e623]:\n        - /url: /tech-or-idea\n    - link "🌍 FLUX.1-devでControlNetを利用したら画像の質が下がったのでImg2Imgを使って改善してみた【Diffusers】 2024/09/13 いいねされた数 2" [ref=s1e624]:\n        - /url: /asap/articles/912d6de879d006\n        - text: 🌍\n        - heading\n          "FLUX.1-devでControlNetを利用したら画像の質が下がったのでImg2Imgを使って改善してみた【Diffusers】"\n          [level=3] [ref=s1e628]\n        - time [ref=s1e631]: 2024/09/13\n        - img "いいねされた数" [ref=s1e633]\n        - text: "2"\n- article [ref=s1e635]:\n    - link "tech" [ref=s1e636]:\n        - /url: /tech-or-idea\n    - link "🎮 【Next.js】ポートレートの表情を編集する自作UIを作ってみた【ComfyUI-AdvancedLivePortrait】 2024/09/11 いいねされた数 8" [ref=s1e637]:\n        - /url: /asap/articles/c49551ff14d5e9\n        - text: 🎮\n        - heading\n          "【Next.js】ポートレートの表情を編集する自作UIを作ってみた【ComfyUI-AdvancedLivePortrait】"\n          [level=3] [ref=s1e641]\n        - time [ref=s1e644]: 2024/09/11\n        - img "いいねされた数" [ref=s1e646]\n        - text: "8"\n- article [ref=s1e648]:\n    - link "tech" [ref=s1e649]:\n        - /url: /tech-or-idea\n    - link "📖 ComfyUI-AdvancedLivePortraitを使ってみる（No WebUI）（Google Colab） 2024/09/10 いいねされた数 7" [ref=s1e650]:\n        - /url: /asap/articles/c0a1b82a10f0e2\n        - text: 📖\n        - heading "ComfyUI-AdvancedLivePortraitを使ってみる（No WebUI）（Google Colab）"\n          [level=3] [ref=s1e654]\n        - time [ref=s1e657]: 2024/09/10\n        - img "いいねされた数" [ref=s1e659]\n        - text: "7"\n- article [ref=s1e661]:\n    - link "tech" [ref=s1e662]:\n        - /url: /tech-or-idea\n    - link "⛳ websocketsでリアルタイム音声処理 2024/09/06 いいねされた数 9" [ref=s1e663]:\n        - /url: /asap/articles/47f839550e5535\n        - text: ⛳\n        - heading "websocketsでリアルタイム音声処理" [level=3] [ref=s1e667]\n        - time [ref=s1e670]: 2024/09/06\n        - img "いいねされた数" [ref=s1e672]\n        - text: "9"\n- article [ref=s1e674]:\n    - link "tech" [ref=s1e675]:\n        - /url: /tech-or-idea\n    - link "😍 生成AIをGoogle Colabで簡単に 【FLUX.1-dev + ControlNet + LoRA】【Diffusers】 2024/08/31 いいねされた数 9" [ref=s1e676]:\n        - /url: /asap/articles/e4c199cecf1836\n        - text: 😍\n        - heading "生成AIをGoogle Colabで簡単に 【FLUX.1-dev + ControlNet +\n          LoRA】【Diffusers】" [level=3] [ref=s1e680]\n        - time [ref=s1e683]: 2024/08/31\n        - img "いいねされた数" [ref=s1e685]\n        - text: "9"\n- article [ref=s1e687]:\n    - link "tech" [ref=s1e688]:\n        - /url: /tech-or-idea\n    - link "👋 Style-Bert-VITS2のAPI化 2024/08/28 いいねされた数 6" [ref=s1e689]:\n        - /url: /asap/articles/12b0f31625a90f\n        - text: 👋\n        - heading "Style-Bert-VITS2のAPI化" [level=3] [ref=s1e693]\n        - time [ref=s1e696]: 2024/08/28\n        - img "いいねされた数" [ref=s1e698]\n        - text: "6"\n- article [ref=s1e700]:\n    - link "tech" [ref=s1e701]:\n        - /url: /tech-or-idea\n    - link "😊 【超簡単】1枚の画像と音声から口パク動画を作りたい！【EchoMimic】 2024/08/25 いいねされた数 6" [ref=s1e702]:\n        - /url: /asap/articles/db1ed88f291b98\n        - text: 😊\n        - heading "【超簡単】1枚の画像と音声から口パク動画を作りたい！【EchoMimic】" [level=3] [ref=s1e706]\n        - time [ref=s1e709]: 2024/08/25\n        - img "いいねされた数" [ref=s1e711]\n        - text: "6"\n- article [ref=s1e713]:\n    - link "tech" [ref=s1e714]:\n        - /url: /tech-or-idea\n    - link "✍️ 【Next.js】生成AIによる「お絵描きサポートWebアプリ」を作りたい！【全くわからない人】 2024/08/20 いいねされた数 8" [ref=s1e715]:\n        - /url: /asap/articles/8f51ffd4fc29b8\n        - text: ✍️\n        - heading "【Next.js】生成AIによる「お絵描きサポートWebアプリ」を作りたい！【全くわからない人】" [level=3]\n          [ref=s1e719]\n        - time [ref=s1e722]: 2024/08/20\n        - img "いいねされた数" [ref=s1e724]\n        - text: "8"\n- article [ref=s1e726]:\n    - link "tech" [ref=s1e727]:\n        - /url: /tech-or-idea\n    - link "🔥 GPT-SoVITS ver2を学習する【WebUI使わない】 2024/08/15 いいねされた数 8" [ref=s1e728]:\n        - /url: /asap/articles/e64c6fce28e0f0\n        - text: 🔥\n        - heading "GPT-SoVITS ver2を学習する【WebUI使わない】" [level=3] [ref=s1e732]\n        - time [ref=s1e735]: 2024/08/15\n        - img "いいねされた数" [ref=s1e737]\n        - text: "8"\n- article [ref=s1e739]:\n    - link "tech" [ref=s1e740]:\n        - /url: /tech-or-idea\n    - link "🔥 GPT-soVITS ver2をモジュールとして動かす 2024/08/08 いいねされた数 9" [ref=s1e741]:\n        - /url: /asap/articles/964a7d863d8896\n        - text: 🔥\n        - heading "GPT-soVITS ver2をモジュールとして動かす" [level=3] [ref=s1e745]\n        - time [ref=s1e748]: 2024/08/08\n        - img "いいねされた数" [ref=s1e750]\n        - text: "9"\n- article [ref=s1e752]:\n    - link "tech" [ref=s1e753]:\n        - /url: /tech-or-idea\n    - link "💯 拡散モデルの理論って面白いよね 2024/08/07 いいねされた数 87" [ref=s1e754]:\n        - /url: /asap/articles/4092ab60570b05\n        - text: 💯\n        - heading "拡散モデルの理論って面白いよね" [level=3] [ref=s1e758]\n        - time [ref=s1e761]: 2024/08/07\n        - img "いいねされた数" [ref=s1e763]\n        - text: "87"\n- article [ref=s1e765]:\n    - link "tech" [ref=s1e766]:\n        - /url: /tech-or-idea\n    - link "🙆\u200d♀️ 画像生成AIの生成途中経過を可視化したい！【Diffusers】 2024/08/06 いいねされた数 4" [ref=s1e767]:\n        - /url: /asap/articles/7940b17be86da7\n        - text: 🙆\u200d♀️\n        - heading "画像生成AIの生成途中経過を可視化したい！【Diffusers】" [level=3] [ref=s1e771]\n        - time [ref=s1e774]: 2024/08/06\n        - img "いいねされた数" [ref=s1e776]\n        - text: "4"\n- article [ref=s1e778]:\n    - link "tech" [ref=s1e779]:\n        - /url: /tech-or-idea\n    - link "🚀 生成AIをGoogle Colabで簡単に 【Part10 SDXL+ANIMAGINE +ControlNet+LoRA編（全部載せ）】 2024/08/03 いいねされた数 3" [ref=s1e780]:\n        - /url: /asap/articles/aa856819e2a722\n        - text: 🚀\n        - heading "生成AIをGoogle Colabで簡単に 【Part10 SDXL+ANIMAGINE\n          +ControlNet+LoRA編（全部載せ）】" [level=3] [ref=s1e784]\n        - time [ref=s1e787]: 2024/08/03\n        - img "いいねされた数" [ref=s1e789]\n        - text: "3"\n- article [ref=s1e791]:\n    - link "tech" [ref=s1e792]:\n        - /url: /tech-or-idea\n    - link "😸 生成AIをGoogle Colaboratoryで簡単に 【Part6.5 画像生成AI SDXL+ANIMAGINE XL 3.1編】 2024/07/31 いいねされた数 3" [ref=s1e793]:\n        - /url: /asap/articles/f9ac2e83c820ff\n        - text: 😸\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part6.5 画像生成AI SDXL+ANIMAGINE\n          XL 3.1編】" [level=3] [ref=s1e797]\n        - time [ref=s1e800]: 2024/07/31\n        - img "いいねされた数" [ref=s1e802]\n        - text: "3"\n- article [ref=s1e804]:\n    - link "tech" [ref=s1e805]:\n        - /url: /tech-or-idea\n    - link "🎃 生成AIをGoogle Colaboratoryで簡単に 【Part9 画像生成AI SD3+ControlNet編】 2024/07/29 いいねされた数 3" [ref=s1e806]:\n        - /url: /asap/articles/53560859d273e4\n        - text: 🎃\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part9 画像生成AI SD3+ControlNet編】"\n          [level=3] [ref=s1e810]\n        - time [ref=s1e813]: 2024/07/29\n        - img "いいねされた数" [ref=s1e815]\n        - text: "3"\n- article [ref=s1e817]:\n    - link "tech" [ref=s1e818]:\n        - /url: /tech-or-idea\n    - link "✨ 生成AIをGoogle Colaboratoryで簡単に 【Part8 画像生成AI Stable Diffusion 3 Medium編】 2024/07/25 いいねされた数 4" [ref=s1e819]:\n        - /url: /asap/articles/6f6ca9aa0513d8\n        - text: ✨\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part8 画像生成AI Stable Diffusion 3\n          Medium編】" [level=3] [ref=s1e823]\n        - time [ref=s1e826]: 2024/07/25\n        - img "いいねされた数" [ref=s1e828]\n        - text: "4"\n- article [ref=s1e830]:\n    - link "tech" [ref=s1e831]:\n        - /url: /tech-or-idea\n    - link "🌟 生成AIをGoogle Colaboratoryで簡単に 【Part7 画像生成AI SDXL Turbo編】 2024/07/23 いいねされた数 2" [ref=s1e832]:\n        - /url: /asap/articles/ed58a6899c1c26\n        - text: 🌟\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part7 画像生成AI SDXL Turbo編】"\n          [level=3] [ref=s1e836]\n        - time [ref=s1e839]: 2024/07/23\n        - img "いいねされた数" [ref=s1e841]\n        - text: "2"\n- article [ref=s1e843]:\n    - link "tech" [ref=s1e844]:\n        - /url: /tech-or-idea\n    - link "🌟 生成AIをGoogle Colaboratoryで簡単に 【Part6 画像生成AI Stable Diffusion XL編】 2024/07/19 いいねされた数 4" [ref=s1e845]:\n        - /url: /asap/articles/e4791be54f90ce\n        - text: 🌟\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part6 画像生成AI Stable Diffusion\n          XL編】" [level=3] [ref=s1e849]\n        - time [ref=s1e852]: 2024/07/19\n        - img "いいねされた数" [ref=s1e854]\n        - text: "4"\n- article [ref=s1e856]:\n    - link "tech" [ref=s1e857]:\n        - /url: /tech-or-idea\n    - link "🎤 生成AIをローカルで簡単に 【Part5.5 faster-whisper+マイク録音編】 2024/07/16 いいねされた数 7" [ref=s1e858]:\n        - /url: /asap/articles/2c0d421e68ef16\n        - text: 🎤\n        - heading "生成AIをローカルで簡単に 【Part5.5 faster-whisper+マイク録音編】" [level=3]\n          [ref=s1e862]\n        - time [ref=s1e865]: 2024/07/16\n        - img "いいねされた数" [ref=s1e867]\n        - text: "7"\n- article [ref=s1e869]:\n    - link "tech" [ref=s1e870]:\n        - /url: /tech-or-idea\n    - link "📖 生成AIをGoogle Colaboratoryで簡単に 【Part5 音声認識AI faster-whisper編】 2024/07/13 いいねされた数 2" [ref=s1e871]:\n        - /url: /asap/articles/ba8fcb1880165e\n        - text: 📖\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part5 音声認識AI faster-whisper編】"\n          [level=3] [ref=s1e875]\n        - time [ref=s1e878]: 2024/07/13\n        - img "いいねされた数" [ref=s1e880]\n        - text: "2"\n- article [ref=s1e882]:\n    - link "tech" [ref=s1e883]:\n        - /url: /tech-or-idea\n    - link "😸 生成AIをGoogle Colaboratoryで簡単に 【Part4 音声認識AI Whisper API編】 2024/07/11 いいねされた数 2" [ref=s1e884]:\n        - /url: /asap/articles/ecaa5631e42195\n        - text: 😸\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part4 音声認識AI Whisper API編】"\n          [level=3] [ref=s1e888]\n        - time [ref=s1e891]: 2024/07/11\n        - img "いいねされた数" [ref=s1e893]\n        - text: "2"\n- article [ref=s1e895]:\n    - link "tech" [ref=s1e896]:\n        - /url: /tech-or-idea\n    - link "😊 生成AIをGoogle Colaboratoryで簡単に 【Part3 テキスト生成 ローカルLLM編】 2024/07/08 いいねされた数 4" [ref=s1e897]:\n        - /url: /asap/articles/8c3d5164d14f6e\n        - text: 😊\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part3 テキスト生成 ローカルLLM編】"\n          [level=3] [ref=s1e901]\n        - time [ref=s1e904]: 2024/07/08\n        - img "いいねされた数" [ref=s1e906]\n        - text: "4"\n- article [ref=s1e908]:\n    - link "tech" [ref=s1e909]:\n        - /url: /tech-or-idea\n    - link "🐷 生成AIをGoogle Colaboratoryで簡単に 【Part2 テキスト生成ChatGPT API編】 2024/07/05 いいねされた数 2" [ref=s1e910]:\n        - /url: /asap/articles/f410dd5d407652\n        - text: 🐷\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part2 テキスト生成ChatGPT API編】"\n          [level=3] [ref=s1e914]\n        - time [ref=s1e917]: 2024/07/05\n        - img "いいねされた数" [ref=s1e919]\n        - text: "2"\n- article [ref=s1e921]:\n    - link "tech" [ref=s1e922]:\n        - /url: /tech-or-idea\n    - link "🗂 生成AIをGoogle Colaboratoryで簡単に 【Part1 合成音声Style-Bert-VITS2編】 2024/07/02 いいねされた数 3" [ref=s1e923]:\n        - /url: /asap/articles/7bdb20dcb5cc56\n        - text: 🗂\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part1 合成音声Style-Bert-VITS2編】"\n          [level=3] [ref=s1e927]\n        - time [ref=s1e930]: 2024/07/02\n        - img "いいねされた数" [ref=s1e932]\n        - text: "3"\n- article [ref=s1e934]:\n    - link "tech" [ref=s1e935]:\n        - /url: /tech-or-idea\n    - link "👏 Remdisの環境構築で沼った話（＋使い方の紹介） 2024/06/26 いいねされた数 14" [ref=s1e936]:\n        - /url: /asap/articles/101379d8951b08\n        - text: 👏\n        - heading "Remdisの環境構築で沼った話（＋使い方の紹介）" [level=3] [ref=s1e940]\n        - time [ref=s1e943]: 2024/06/26\n        - img "いいねされた数" [ref=s1e945]\n        - text: "14"\n- article [ref=s1e947]:\n    - link "tech" [ref=s1e948]:\n        - /url: /tech-or-idea\n    - link "🚀 Style-Bert-VITS2の使い方メモ 2024/06/17 いいねされた数 34" [ref=s1e949]:\n        - /url: /asap/articles/f8c0621cdd74cc\n        - text: 🚀\n        - heading "Style-Bert-VITS2の使い方メモ" [level=3] [ref=s1e953]\n        - time [ref=s1e956]: 2024/06/17\n        - img "いいねされた数" [ref=s1e958]\n        - text: "34"\n- article [ref=s1e960]:\n    - link "tech" [ref=s1e961]:\n        - /url: /tech-or-idea\n    - link "💭 Style-Bert-VITS2のVoice Conversion機能の復活に挑戦してみた（前編） 2024/06/09 いいねされた数 9" [ref=s1e962]:\n        - /url: /asap/articles/df2c610d1389e4\n        - text: 💭\n        - heading "Style-Bert-VITS2のVoice Conversion機能の復活に挑戦してみた（前編）" [level=3]\n          [ref=s1e966]\n        - time [ref=s1e969]: 2024/06/09\n        - img "いいねされた数" [ref=s1e971]\n        - text: "9"\n- article [ref=s1e973]:\n    - link "tech" [ref=s1e974]:\n        - /url: /tech-or-idea\n    - link "😸 OpenAI Assistants API v2を色々いじってみた（使い方）（File Search編） 2024/05/28 いいねされた数 10" [ref=s1e975]:\n        - /url: /asap/articles/5c2f8ffe2a46ce\n        - text: 😸\n        - heading "OpenAI Assistants API v2を色々いじってみた（使い方）（File Search編）"\n          [level=3] [ref=s1e979]\n        - time [ref=s1e982]: 2024/05/28\n        - img "いいねされた数" [ref=s1e984]\n        - text: "10"\n- article [ref=s1e986]:\n    - link "tech" [ref=s1e987]:\n        - /url: /tech-or-idea\n    - link "👻 OpenAI Assistants API v2を色々いじってみた（使い方）（code interpreter編） 2024/05/27 いいねされた数 5" [ref=s1e988]:\n        - /url: /asap/articles/b0ce16ea1832ac\n        - text: 👻\n        - heading "OpenAI Assistants API v2を色々いじってみた（使い方）（code interpreter編）"\n          [level=3] [ref=s1e992]\n        - time [ref=s1e995]: 2024/05/27\n        - img "いいねされた数" [ref=s1e997]\n        - text: "5"\n- article [ref=s1e999]:\n    - link "tech" [ref=s1e1000]:\n        - /url: /tech-or-idea\n    - link "😑 【自分用】「AI後輩ちゃん」の個人的つまづきポイントまとめ 2024/05/09 いいねされた数 3" [ref=s1e1001]:\n        - /url: /asap/articles/98b04c5ea82bd9\n        - text: 😑\n        - heading "【自分用】「AI後輩ちゃん」の個人的つまづきポイントまとめ" [level=3] [ref=s1e1005]\n        - time [ref=s1e1008]: 2024/05/09\n        - img "いいねされた数" [ref=s1e1010]\n        - text: "3"\n- article [ref=s1e1012]:\n    - link "tech" [ref=s1e1013]:\n        - /url: /tech-or-idea\n    - link "😍 音声対話システム「AI後輩ちゃん」を作ってみた 2024/05/04 いいねされた数 17" [ref=s1e1014]:\n        - /url: /asap/articles/5b1b7553fcaa76\n        - text: 😍\n        - heading "音声対話システム「AI後輩ちゃん」を作ってみた" [level=3] [ref=s1e1018]\n        - time [ref=s1e1021]: 2024/05/04\n        - img "いいねされた数" [ref=s1e1023]\n        - text: "17"\n- contentinfo [ref=s1e1026]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1030]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1031]\n    - paragraph [ref=s1e1039]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e1041]:\n        - heading "About" [level=4] [ref=s1e1042]\n        - list [ref=s1e1043]:\n            - listitem [ref=s1e1044]:\n                - link "Zennについて" [ref=s1e1046]:\n                    - /url: /about\n            - listitem [ref=s1e1047]:\n                - link "運営会社" [ref=s1e1048]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e1049]:\n                - link "お知らせ・リリース" [ref=s1e1050]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e1051]:\n                - link "イベント" [ref=s1e1053]:\n                    - /url: /events\n    - navigation [ref=s1e1054]:\n        - heading "Guides" [level=4] [ref=s1e1055]\n        - list [ref=s1e1056]:\n            - listitem [ref=s1e1057]:\n                - link "使い方" [ref=s1e1059]:\n                    - /url: /manual\n            - listitem [ref=s1e1060]:\n                - link "法人向けメニュー" [ref=s1e1062]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e1064]:\n                - link "Publication / Pro" [ref=s1e1066]:\n                    - /url: /publications\n            - listitem [ref=s1e1067]:\n                - link "よくある質問" [ref=s1e1069]:\n                    - /url: /faq\n    - navigation [ref=s1e1070]:\n        - heading "Links" [level=4] [ref=s1e1071]\n        - list [ref=s1e1072]:\n            - listitem [ref=s1e1073]:\n                - link "X(Twitter)" [ref=s1e1074]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e1075]:\n                - link "GitHub" [ref=s1e1076]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e1077]:\n                - link "メディアキット" [ref=s1e1079]:\n                    - /url: /mediakit\n    - navigation [ref=s1e1080]:\n        - heading "Legal" [level=4] [ref=s1e1081]\n        - list [ref=s1e1082]:\n            - listitem [ref=s1e1083]:\n                - link "利用規約" [ref=s1e1085]:\n                    - /url: /terms\n            - listitem [ref=s1e1086]:\n                - link "プライバシーポリシー" [ref=s1e1088]:\n                    - /url: /privacy\n            - listitem [ref=s1e1089]:\n                - link "特商法表記" [ref=s1e1091]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e1094]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e1095]\n- alert [ref=s1e1114]\n```\n', name='browser_navigate', tool_call_id='58fad86e-0b01-4fc6-a8cc-57deb5c48b41'), AIMessage(content='`https://zenn.dev/asap`にアクセスしました。\n\n執筆者の記事一覧から、いいねの数が多い記事を探します。\n\n- 「DeepSeek-R1の論文読んだ？【勉強になるよ】」 が921いいね\n- 「拡散モデルの理論って面白いよね」 が87いいね\n- 「Open AI Realtime APIのPythonサンプルコードを作成」 が79いいね\n- 「📏 【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】」 が50いいね\n- 「🙌 完全自律型AIエージェントOSS【LangManus】完全に理解した！」 が48いいね\n\n最もいいねが多い記事は「DeepSeek-R1の論文読んだ？【勉強になるよ】」です。\n記事の中身を取得します。', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap/articles/34237ad87f8511"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-27104099-cd7e-4de1-8c89-51060ed9c6f1-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap/articles/34237ad87f8511'}, 'id': 'd85f8505-9c2a-4960-b17f-9ef88d983d9d', 'type': 'tool_call'}], usage_metadata={'input_tokens': 17995, 'output_tokens': 216, 'total_tokens': 18211, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap/articles/34237ad87f8511\n\n- Page URL: https://zenn.dev/asap/articles/34237ad87f8511\n- Page Title: DeepSeek-R1の論文読んだ？【勉強になるよ】\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- article [ref=s1e32]:\n    - text: 📑\n    - heading "DeepSeek-R1の論文読んだ？【勉強になるよ】" [level=1] [ref=s1e38]\n    - text: 2025/01/27に公開\n    - tooltip "更新日" [ref=s1e45]:\n        - img [ref=s1e46]\n        - text: 2025/02/22 更新日\n    - link "12件" [ref=s1e51]:\n        - /url: "#discuss"\n        - img [ref=s1e52]\n        - text: 12件\n    - link "強化学習" [ref=s1e63]:\n        - /url: /topics/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92\n        - img [ref=s1e65]\n        - text: 強化学習\n    - link "蒸留" [ref=s1e67]:\n        - /url: /topics/%E8%92%B8%E7%95%99\n        - img [ref=s1e69]\n        - text: 蒸留\n    - link "LLM" [ref=s1e71]:\n        - /url: /topics/llm\n        - img [ref=s1e73]\n        - text: LLM\n    - link "生成 AI" [ref=s1e75]:\n        - /url: /topics/%E7%94%9F%E6%88%90ai\n        - img [ref=s1e77]\n        - text: 生成 AI\n    - link "DeepSeek" [ref=s1e79]:\n        - /url: /topics/deepseek\n        - img [ref=s1e81]\n        - text: DeepSeek\n    - link "tech" [ref=s1e83]:\n        - /url: /tech-or-idea\n        - img [ref=s1e85]\n        - text: tech\n    - heading "はじめに" [level=1] [ref=s1e89]\n    - paragraph [ref=s1e90]: タイトル通りDeepSeek-R1の論文を読んだので、少しでもわかりやすく説明できればと思います。\n        まだ論文を読んでいない方は、ぜひ原文も読んでみてください！\n    - paragraph [ref=s1e92]:\n        - text: 難しい数式や概念がほとんど使われていないため、論文の中では全然読みやすい方だと思います。読み物として読めるレベルです。\n        - iframe [ref=s1e94]:\n            - text: Loading...\n            - alert [ref=f1s1e8]\n    - paragraph [ref=s1e95]: とはいえ、本記事では細かい部分など間違っている部分もあるかもしれないので、ぜひ誤りがあったら、こそっと教えてください！\n    - complementary [ref=s1e96]:\n        - text: "!"\n        - paragraph [ref=s1e99]: 実は、本論文を解説する上で、著者らが公開しているもう一つの論文が大きく関連しています。 そちらの論文の内容も若干解説を入れております。\n        - paragraph [ref=s1e101]:\n            - text: 「\n            - \'link "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models" [ref=s1e102]\':\n                - /url: https://arxiv.org/html/2402.03300v3\n            - text: 」\n        - paragraph [ref=s1e103]: 本記事でも少し触れますが、上記の論文も非常に勉強になるため、ぜひ読んでみてください。\n    - complementary [ref=s1e104]:\n        - text: "!"\n        - paragraph [ref=s1e107]:\n            - text: 本記事は、\n            - link "DeepSeek-R1の論文" [ref=s1e108]:\n                - /url: https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\n            - text: と\n            - link "DeepSeekMathの論文" [ref=s1e109]:\n                - /url: https://arxiv.org/html/2402.03300v3\n            - text: を読んだ私の理解をもとに記載しています。\n        - paragraph [ref=s1e110]: 本論文で使われている技術に関しては、ある程度の知識を持っているので、大きくは外していないとは思いますが、私の主観も入っている部分もありますので、ご了承ください。\n    - paragraph [ref=s1e111]:\n        - text: また、DeepSeek-R1の論文が公開される前に、小型モデルに対して同様の実験（強化学習）をしていたグループがあるようです。\n            そちらのレポートは下記になります。\n        - iframe [ref=s1e114]:\n            - \'link "7B Model and 8K Examples: Emerging Reasoning with Reinforcement Learning is Both Effective and Effic A replication of DeepSeek-R1 training on small models with limited data hkust-nlp.notion.site favicon image hkust-nlp.notion.site hkust-nlp.notion.site thumbnail image" [ref=f2s1e6]\':\n                - /url: https://hkust-nlp.notion.site/simplerl-reason\n                - \'heading "7B Model and 8K Examples: Emerging Reasoning with\n                  Reinforcement Learning is Both Effective and Effic" [level=1]\n                  [ref=f2s1e8]\'\n                - text: A replication of DeepSeek-R1 training on small models with limited data\n                - img "hkust-nlp.notion.site favicon image" [ref=f2s1e11]\n                - text: hkust-nlp.notion.site\n                - img "hkust-nlp.notion.site thumbnail image" [ref=f2s1e13]\n            - alert [ref=f2s1e15]\n    - paragraph [ref=s1e115]: 意図せず、DeepSeek-R1-Zeroの再現実験のようなレポートになっていますが、レポートの著者はDeepSeek-R1論文の公開前から実験していると主張しています。\n        こちらも非常に興味深かったため紹介です。\n    - heading "本論文の興味深いところ" [level=1] [ref=s1e117]\n    - paragraph [ref=s1e118]: 本論文は、大きく分けて3つの構成でできています\n    - list [ref=s1e119]:\n        - listitem [ref=s1e120]:\n            - strong [ref=s1e121]: 強化学習による思考能力の強化\n            - list [ref=s1e122]:\n                - listitem [ref=s1e123]: LLM（DeepSeek-V3-Base）に対して「強化学習のみ」を適用させたところ、強力な思考能力を獲得した「DeepSeek-R1-Zero」が作成されました。\n                - listitem [ref=s1e124]: 本論文のメインストリームであり、非常に興味深い示唆が得られる部分です。\n        - listitem [ref=s1e125]:\n            - strong [ref=s1e126]: SFTによるさらなる強化\n            - list [ref=s1e127]:\n                - listitem [ref=s1e128]: DeepSeek-V3-Baseに対して、コールドスタート問題を解消するために、CoT（Chain of\n                    Thought）データセットによるSFT（教師付きファインチューニング）を実施\n                - listitem [ref=s1e129]: その後、強化学習を適用させることで、o1レベルの性能を獲得した「DeepSeek-R1」が作成されました。\n                - listitem [ref=s1e130]: 非常に強力なLLMを獲得するまでの反復学習などの試行錯誤などを学ぶことができます。\n        - listitem [ref=s1e131]:\n            - strong [ref=s1e132]: 蒸留による小型モデルの性能強化\n            - list [ref=s1e133]:\n                - listitem [ref=s1e134]: 上記で作成されたDeepSeek-R1により、他社の小型オープンソースLLMを蒸留しました。\n                - listitem [ref=s1e135]: その結果、元の性能を大幅に上回る、非常に高い性能の小型LLMが得られました。\n    - paragraph [ref=s1e136]: その中でも私が特に面白いなと思った要素は下記の要素になります\n    - list [ref=s1e137]:\n        - listitem [ref=s1e138]:\n            - strong [ref=s1e139]: 強化学習手法「GRPO」すごい！\n        - listitem [ref=s1e140]:\n            - strong [ref=s1e141]: ルールベース報酬で思考能力が向上するのがすごい！\n        - listitem [ref=s1e142]:\n            - strong [ref=s1e143]: aha moment（ハッとする瞬間）面白い！\n        - listitem [ref=s1e144]:\n            - strong [ref=s1e145]: 蒸留vs強化学習の比較実験から重要な示唆が得られた！\n    - heading "本論文の流れ" [level=1] [ref=s1e146]\n    - heading "前提のモデル構造" [level=3] [ref=s1e147]\n    - paragraph [ref=s1e148]: そもそも、大前提として、この論文は「DeepSeek-V3-Base」という強力なLLMの性能を改善する論文です。\n        これ単体でも、GPT-4oレベルの能力を持っているオープンソースLLMになります。\n    - paragraph [ref=s1e150]: 本論文のモデルアーキテクチャは「DeepSeek-V3-Base」と完全に同じものを利用します。\n        （つまりモデルアーキテクチャをかえずに、学習方法を工夫してLLMの性能を上げる挑戦になります）\n    - heading "DeepSeek-R1-Zeroを強化学習を用いて作る" [level=3] [ref=s1e152]\n    - paragraph [ref=s1e153]: その上で、まずは、「DeepSeek-V3-Base」に対して、大規模な強化学習のみを適応させることで、強化学習の可能性について探究する話が始まります。\n        結果として、「DeepSeek-R1-Zero」ができます。\n        「DeepSeek-R1-Zero」は、SFT（教師付きファインチューニング）をしなくても、強化学習のみで圧倒的な思考能力を取得できることを示す興味深いモデルです。\n    - heading "DeepSeek-R1をSFT+強化学習を用いて作る" [level=3] [ref=s1e156]\n    - paragraph [ref=s1e157]: その後、あまりにも「DeepSeek-R1-Zero」の性能が良かったことから、さらなる探究が進められました。\n    - paragraph [ref=s1e158]: この時点で、「DeepSeek-R1-Zero」には下記の課題がありました。\n    - list [ref=s1e159]:\n        - listitem [ref=s1e160]: 学習初期段階にて、強化学習の学習が不安定であり、収束に時間がかかる（コールドスタート問題）\n        - listitem [ref=s1e161]:\n            - text: 思考能力は高いが、言語が混在（日本語で話していたら急に中国語になるなど）し、人間が読みにくい\n            - list [ref=s1e162]:\n                - listitem [ref=s1e163]: 結果、高度な思考が必要なタスクには強いが、一般的タスク（ロールプレイやライティングなど）には使いにくいという評価\n    - paragraph [ref=s1e164]: そこで、「DeepSeek-V3-Base」に対して、少量のCoT（Chain of\n        Thought）データセットを用いて、SFT（教師付き微調整）を行います。このデータは、著者らが収集・構築したものになります。\n        その後、SFT後のモデルに対して、大規模強化学習を行います。\n        このとき、「DeepSeek-R1-Zero」を作成した際の報酬に加え、「言語一貫性報酬」を導入して、上述した言語の不一致の問題を緩和させます。\n    - paragraph [ref=s1e167]: その後、得られたモデル（チェックポイント）を利用して、さらなるSFT（教師付き微調整）学習データを収集し、追加SFTを行います。\n        さらに、その後、改めて２段階目の大規模強化学習を実施することで、最終的に「DeepSeek-R1」が得られます。\n    - heading "小型モデルへの蒸留を試す" [level=3] [ref=s1e169]\n    - paragraph [ref=s1e170]: 最後に、小型モデルに対する「DeepSeek-R1」による蒸留が試されています。\n        その中で、小型モデルに対して、「DeepSeek-R1」による蒸留を実施するのと、「DeepSeek-R1-Zero」と同様の強化学習手法で思考能力を強化するのと、どちらが性能向上に寄与するのかを調査してくれています。\n    - paragraph [ref=s1e172]: これが非常に興味深いです。\n    - heading "DeepSeek-R1-Zeroを作るまで" [level=1] [ref=s1e173]\n    - paragraph [ref=s1e174]: では、ここから論文の詳細な解説に入っていきます。\n    - paragraph [ref=s1e175]: まずは、「DeepSeek-R1」の前段階である「DeepSeek-R1-Zero」を作るまでの手法を解説します。\n    - paragraph [ref=s1e176]:\n        - strong [ref=s1e177]: 前段階と侮るなかれ、「DeepSeek-R1-Zero」を作るまでの手法と示唆にこそ、本論文の貴重な示唆が含まれています。\n    - heading "元々のアーキテクチャ" [level=2] [ref=s1e178]\n    - paragraph [ref=s1e179]: 今回作る「DeepSeek-R1-Zero」や「DeepSeek-R1」は、新しいモデル構造を採用して、性能を向上させたわけではありません。\n    - paragraph [ref=s1e180]: 元々のアーキテクチャは「DeepSeek-V3-Base」というアーキテクチャになります。\n        「DeepSeek-V3-Base」は、681Bのモデルパラメータを持つ大規模言語モデルで、性能はGPT-4oに匹敵する非常に高性能モデルありながら、モデル重みが完全に公開されているオープンソースモデルです。\n    - paragraph [ref=s1e182]: 「DeepSeek-V3-Base」に対して、Instructionチューニングを施したchatモデルに関しては、APIでも公開されています。\n        非常に安価に使えるにも関わらず、GPT-4oに匹敵する性能を誇ることから、人気のモデルになっております。\n    - complementary [ref=s1e184]:\n        - text: "!"\n        - paragraph [ref=s1e187]: 安価なモデルではありますが、APIの提供元は中国の企業ですので、データの取り扱いなどは十分注意してください。\n            基本的には、API利用の際には、収集されて困るような情報は入力しないようにするのが無難です。\n            （オープンソースモデルをローカルで動かす場合は、おそらく心配ありません）\n    - paragraph [ref=s1e190]: 本論文のすごいところは、モデル構造を変えずに、「DeepSeek-V3-Base」に対する学習方法を工夫しただけで、OpenAI\n        o1に匹敵する性能のLLMを開発していることです。\n        具体的には、大規模強化学習をLLMに対して適用することで、「DeepSeek-R1-Zero」を開発します。\n    - heading "すごいぞ！強化学習" [level=2] [ref=s1e192]\n    - heading "強化学習って何？" [level=3] [ref=s1e193]\n    - paragraph [ref=s1e194]: そもそも「強化学習って何？」という方もいらっしゃると思うので、簡単に説明します。\n        （以前、スイカゲームを強化学習でプレイしたこともあるので、いずれ詳しい解説記事を作る予定ですが、ここではざっくり説明します。）\n    - paragraph [ref=s1e196]: 強化学習とは、一言で言えば「一連の行動の流れ（シークエンス）によって得られる、累積報酬の期待値を最大化する」ことを目指す学習手法です。\n        ある行動をとった結果がすぐにはわからず、時間が経ってから影響が現れるような問題に適しています。\n        専門的に言うと、「マルコフ決定過程（MDP）」が成立する問題で活用されます。\n    - heading "テトリスで考える" [level=3] [ref=s1e199]\n    - paragraph [ref=s1e200]: 例えば、「テトリス」を考えてみましょう。\n    - paragraph [ref=s1e201]: テトリスは、上から「ミノ」と呼ばれる、さまざまな形のブロックを落として、横一列を綺麗に埋めてブロックを消去するゲームです。\n    - paragraph [ref=s1e202]: 横一列がすべて揃うと、その列が消えて、「ポイント」を獲得できます。\n        逆に、列を消せずに、ブロックが一定の高さまで積み上がると「ゲームオーバ」になります。\n    - paragraph [ref=s1e204]: テトリスを強化学習で解こうとすると、「どうすれば最も多くのポイントを得られるか」をAIに学習させることが重要になります。\n        単純に列を揃えて消すだけではなく、「次の手」や「その次の手」を見据えて、より良い結果につながるようにミノを配置することも必要です。\n        しかし、通常の学習方法(よくある入出力を近似させる学習方法)では、目の前の結果だけに注目してしまい、将来的な高得点につながる手を学習させることは難しいです。\n    - paragraph [ref=s1e207]:\n        - strong [ref=s1e208]: そこで役立つのが「強化学習」です。\n    - paragraph [ref=s1e209]: 強化学習では、ゲームの最初から最後までの流れを通して、「どう行動すれば最終的に得られる得点を最大化できるか」を学習できます。\n        このように、途中の行動の良し悪しがすぐにはわからなくても、最終的なゴール（得点の最大化）が設定できる問題に対して、強化学習は非常に効果的です。\n    - paragraph [ref=s1e211]:\n        - img [ref=s1e212]\n    - group [ref=s1e213]: 補足\n    - heading "強化学習の事例「AlphaGo Zero」" [level=3] [ref=s1e215]\n    - paragraph [ref=s1e216]: さらに、強化学習の威力をお伝えする事例を記載します。\n    - paragraph [ref=s1e217]: 2015年にアメリカのDeepMind社（Googleの傘下）が「AlphaGo」というモデルを開発しました。これはAIが囲碁をプレイするモデルで、当時の世界王者をハンディキャップなしで破るという、非常に衝撃的なモデルでした。\n        （当時としては、非常にセンセーショナルな出来事だったため、記憶にある方もいらっしゃるかと思います）\n    - paragraph [ref=s1e219]: この「AlphaGo」は、深層学習と強化学習により学習されています。\n        プロ棋士の棋譜データで深層学習で学習させた上で、強化学習による自己対局（AI同士の対局）を実施したモデルです。\n    - paragraph [ref=s1e221]:\n        - text: さて、ここまでも十分すごいですが、\n        - strong [ref=s1e223]: さらにすごいのはここからです。\n    - paragraph [ref=s1e224]: DeepMind社は後継モデルとして「AlphaGo Zero」というモデルを開発しました。\n        このモデルは、プロ棋士の棋譜データを「使わず」に、完全に「強化学習のみ」で学習されたモデルです。\n        つまり、囲碁のルールだけを与え、あとは最初から最後までAI同士の自己対局のみで学習されています。\n        （この場合、対局に勝利したら報酬、敗北したら罰を与える形で強化学習をし、対局に勝利した時に取っていた手（行動）の選択確率を上げるように学習しています）\n    - paragraph [ref=s1e228]:\n        - strong [ref=s1e229]: そして、その「AlphaGo Zero」は「AlphaGo」に対して、100戦全勝で圧勝しました。\n    - paragraph [ref=s1e230]: AIが囲碁の学習をする際に、人間が持つ定石の情報などは不要だったわけです。\n    - paragraph [ref=s1e231]: これが強化学習の威力です。\n        余談ですが、今回の論文で作成する「DeepSeek-R1-Zero」も（「DeepSeek-V3-Base」をベースにはしますが）強化学習のみで学習されます。\n        名前に入っている「Zero」は同じく強化学習のみで開発された「AlphaGo Zero」のオマージュかなと思います。\n    - heading "そもそもLLMで強化学習とは？" [level=2] [ref=s1e234]\n    - paragraph [ref=s1e235]: 「なんかゲームみたいなものに、強化学習を使うとうまくいきそう」というのは理解していただけたかなと思います。\n    - paragraph [ref=s1e236]:\n        - text: では、LLMに対してはどうか、というのを解説していきます。\n        - strong [ref=s1e238]: 実は、LLMと強化学習は非常に相性がいいです。\n    - paragraph [ref=s1e239]: なぜなら、LLMは一度にすべての文章を出力するAIではなく、1tokenずつ出力するモデルであり、その文章が良い文章か悪い文章かは、すべてのtokenを出力してみないとわからないからです。\n    - paragraph [ref=s1e240]:\n        - text: テトリスでは、ミノを一つずつ落とし、一列が消えたタイミングで、ポイントを取得するという形で評価されます。\n        - img [ref=s1e242]\n    - paragraph [ref=s1e243]:\n        - text: LLMでも、同様にtokenを1tokenずつ出力し、一文全部が出力されたタイミングで、その文章の良し悪しを評価することができれば、強化学習と同じ枠組みを使うことができるというわけです。\n        - img [ref=s1e245]\n    - heading "報酬モデルを作成し、累積報酬を最大化" [level=3] [ref=s1e246]\n    - paragraph [ref=s1e247]:\n        - text: テトリスでは、ゲームシステムが勝手に点数をつけてくれますが、LLMにおいて文章の良し悪しを勝手に判断してくれる便利な機能はありません。そこで我々が、その判断モデルを作ってあげる必要があります。\n            そこで、ニューラルネットワークを利用して、LLMに入力されたプロンプト\n        - math [ref=s1e252]: x\n        - text: に対して、LLMが生成した文章\n        - math [ref=s1e259]: "y"\n        - text: の点数\n        - math [ref=s1e266]: r\n        - text: を出力するようなネットワークを「報酬モデル\n        - math [ref=s1e273]: r θ ( r ∣ y , x )\n        - text: 」として作成することを考えます。\n    - paragraph [ref=s1e286]:\n        - text: まず初めに、データセットとして、「プロンプト\n        - math [ref=s1e290]: x\n        - text: 」「好ましい回答\n        - math [ref=s1e297]: y +\n        - text: 」「好ましくない回答\n        - math [ref=s1e306]: y −\n        - text: 」の3つが組となっているデータセットを用意します。 報酬モデル\n        - math [ref=s1e316]: r θ ( r ∣ y , x )\n        - text: は、「プロンプト\n        - math [ref=s1e332]: x\n        - text: 」と「出力\n        - math [ref=s1e339]: "y"\n        - text: 」の両方を入力することで、報酬（点数）を出力するモデルです。\n    - paragraph [ref=s1e343]:\n        - text: そこで、好ましい回答の報酬推定値\n        - math [ref=s1e347]: r θ ( r ∣ y + , x )\n        - text: と、好ましくない回答の報酬推定値\n        - math [ref=s1e365]: r θ ( r ∣ y − , x )\n        - text: の差を一定以上にするような損失関数を用いて学習を行います。 すると、報酬モデル\n        - math [ref=s1e384]: r θ ( r ∣ y , x )\n        - text: は適切に、好ましい回答\n        - math [ref=s1e400]: y +\n        - text: には高い点数を、好ましくない回答\n        - math [ref=s1e409]: y −\n        - text: には低い点数をつけるようになります。\n    - paragraph [ref=s1e415]:\n        - text: 報酬モデル\n        - math [ref=s1e419]: r θ ( r ∣ y , x )\n        - text: か完成したら、強化学習をするための要素は整います。\n    - paragraph [ref=s1e432]:\n        - text: 状態\n        - math [ref=s1e436]: x\n        - text: は、LLMに入力される「プロンプト」で、LLMの出力が\n        - math [ref=s1e443]: "y"\n        - text: とします。 報酬\n        - math [ref=s1e451]: r\n        - text: は、報酬モデル\n        - math [ref=s1e458]: r θ ( r ∣ y , x )\n        - text: の出力値です。 LLMが全文出力したら、プロンプト\n        - math [ref=s1e475]: x\n        - text: と、LLMの出力\n        - math [ref=s1e482]: "y"\n        - text: を報酬モデルに与えることで、取得します。 方策は、LLMが出力する各tokenごとの確率分布自体です。 このLLM自体を「方策モデル\n        - math [ref=s1e491]: π ϕ ( y t ∣ x , y < t )\n        - text: 」と呼びます。\n    - complementary [ref=s1e510]:\n        - text: "!"\n        - paragraph [ref=s1e513]:\n            - text: LLM（\n            - math [ref=s1e517]: π ϕ ( y t ∣ x , y < t )\n            - text: ）はユーザが入力したプロンプト\n            - math [ref=s1e539]: x\n            - text: と、それまでにLLMが出力した内容\n            - math [ref=s1e546]: y < t\n            - text: をもとに、\n            - math [ref=s1e557]: t\n            - text: ステップ目のtoken\n            - math [ref=s1e564]: y t\n            - text: を出力します。\n        - paragraph [ref=s1e570]: これが、LLMは、「文章の次に出てくる単語を一つずつ予測するモデル」と言われる所以です。\n    - paragraph [ref=s1e571]: これにより、累積報酬を最大化するように方策モデル（LLM）を学習することで、LLMは好ましい回答を出力するようにネットワークを更新できます。\n    - paragraph [ref=s1e572]: これが、LLMを強化学習で学習するということです。 このような手法をRLHF（Reinforcement\n        learning from human feedback）と総称します。\n    - group [ref=s1e574]: 細かい話\n    - heading "つよつよ手法「PPO（Proximal Policy Optimization）」" [level=3] [ref=s1e576]\n    - paragraph [ref=s1e577]: ここまでで、「LLMと強化学習は相性が良い」こと、「強化学習では累積報酬の期待値（状態価値）を最大化することが重要」であることを話しました。\n    - paragraph [ref=s1e578]: さて、実際にLLMに強化学習を組み込む場合、しばしばPPO（Proximal Policy\n        Optimization）という手法が利用されます。\n        （なんなら、LLM以外で強化学習を実用する上でも、最初に選択肢になるベースライン的に強力な手法でもあります）\n    - paragraph [ref=s1e580]: 今回はPPOが主題ではないので、簡単に説明します。\n    - heading "PPOの簡易的な説明" [level=4] [ref=s1e581]\n    - paragraph [ref=s1e582]:\n        - text: PPOでは、行動を確率的に出力する方策モデル（LLM）と、現在の状態価値を推定する状態価値モデルを用意します。 状態価値\n        - math [ref=s1e587]: V t\n        - text: とは、「累積報酬の期待値」として定義できます。 したがって、現時点から、方策に従って行動をした場合の、将来的な報酬総和の期待値が「状態価値\n        - math [ref=s1e597]: V t\n        - text: 」です。\n    - paragraph [ref=s1e603]:\n        - text: 強化学習の枠組みで累積報酬\n        - math [ref=s1e607]: G t\n        - text: が計算されるため、その期待値である状態価値\n        - math [ref=s1e616]: V t\n        - text: も学習が可能です。 一方で、方策モデルを学習する際に、Advantage\n        - math [ref=s1e626]: A t\n        - text: というものを利用します。\n    - paragraph [ref=s1e632]:\n        - text: Advantage\n        - math [ref=s1e636]: A t\n        - text: というのは下記の式で表されます。\n    - math [ref=s1e648]: A t = G t − V t ( v t ∣ y < t , x )\n    - paragraph [ref=s1e675]:\n        - text: 状態価値\n        - math [ref=s1e679]: V t ( v t ∣ y < t , x )\n        - text: は、累積報酬\n        - math [ref=s1e701]: G t\n        - text: の期待値（大雑把にいうと平均）になるように学習されているという前提があるため、Advantage\n        - math [ref=s1e710]: A t\n        - text: が正になるということは、今回LLMが出力したtoken\n        - math [ref=s1e719]: y t\n        - text: は、将来的に高い報酬を得ることができる「良い出力であった」と判断できます。\n    - paragraph [ref=s1e725]:\n        - text: したがって、方策モデル\n        - math [ref=s1e729]: π ϕ ( y t ∣ x , y < t )\n        - text: はAdvantage\n        - math [ref=s1e751]: A t\n        - text: を最大化するように学習します。\n    - paragraph [ref=s1e757]:\n        - text: Advantage\n        - math [ref=s1e761]: A t\n        - text: を考えずに、単純に累積報酬\n        - math [ref=s1e770]: G t\n        - text: を最大化するように考えることもできます。（むしろそれが初期の強化学習です）\n    - complementary [ref=s1e776]:\n        - text: "!"\n        - paragraph [ref=s1e779]: （2025年2月15日 追記） この辺りの表現は、わかりやすくするために若干誤解を招く表現になっています。\n            下に「細かい話」として補足説明を書きました。\n    - paragraph [ref=s1e782]: しかしその場合、そもそもどんな行動を取っても報酬が高くなる状態と、どんな行動を取っても報酬が低くなってしまう状態において、勾配のスケールがばらつくため、学習の安定性が低くなります。\n    - paragraph [ref=s1e783]:\n        - text: PPOでは、Advantage\n        - math [ref=s1e787]: A t\n        - text: を考えることにより、出力したtokenが平均よりも良いか悪いかだけで学習をすることができ、状態ごとの報酬の差をモデルが考慮する必要がなくなります。\n    - complementary [ref=s1e793]:\n        - text: "!"\n        - paragraph [ref=s1e796]: 本来、PPOの貢献は、あるパラメータ更新時において、更新前と更新後の確率分布が大きく変わらないような制約を、少ない計算量で加えることです。\n            ただ、この話は、DeepSeek-R1-Zeroにおける強化学習を理解するためには、必ずしも重要ではありません。\n            したがって、今回の記事では、必要な部分だけに絞り説明をしました。\n    - group [ref=s1e799]: 細かい話（追記）\n    - heading "著者発案「GRPO（Group Relative Policy Optimization）」" [level=3]\n      [ref=s1e801]\n    - paragraph [ref=s1e802]: さて、PPOは強化学習の手法において、高い性能を発揮しますが、LLMに適用することを考えると、状態価値モデルを別途学習しないといけないという点において、計算量的に大変な手法です。\n    - paragraph [ref=s1e803]: "一方で、PPOの状態価値モデルがやっていたことは、Advantage A_t(a_t|y_t,\n        y_{<t},x) を計算し、今回出力したtokenが、平均よりも良いのか悪いのかを判断することです。"\n    - paragraph [ref=s1e806]: "そこで、LLMにおいては、わざわざ状態価値モデルを用意して平均を計算しなくても、実際に複数文章を生成して、そ\\\n        れぞれの報酬を平均することで、Advantage A_t(a_t|y_t, y_{<t},x) を計算しようと考えました。\n        こうすることで、状態価値モデルなしで、Advantage A_t(a_t|y_t, y_{<t},x)\n        を計算できるため、計算量を大幅に削減できることができます。"\n    - paragraph [ref=s1e812]:\n        - text: この手法は「GRPO（Group Relative Policy Optimization）」と呼びます。\n            本論文の著者が、過去に公開した論文で提案した手法です。\n        - iframe [ref=s1e815]: <could not take iframe snapshot>\n    - paragraph [ref=s1e816]:\n        - text: 下記が、論文中で提示されている「PPO」と「GRPO」の違いを説明する図です。\n        - img [ref=s1e818]\n        - text: "このように状態価値モデル（Value model）なしに、Advantage A_t(a_t|y_t, y_{<t},x)\n            が計算できていることがわかります。"\n    - complementary [ref=s1e821]:\n        - text: "!"\n        - paragraph [ref=s1e824]: 本記事でのアルファベットの付け方と、上記の図のアルファベットの付け方が異なることに注意してください。\n        - paragraph [ref=s1e825]: 上記の図（GRPO）では、 プロンプトを q 、LLMの各出力文章を o_i （ i\n            はそれぞれの文章単位でのインデックス）、各文章ごとの報酬モデルの出力を r_i 、報酬 r_i\n            の平均から計算された各文章ごとのAdvantageを A_i としています。\n        - paragraph [ref=s1e839]: 本記事での添え字は、token単位での添え字を採用しています。 例えば A_t なら t token目でのAdvantageです。\n        - paragraph [ref=s1e845]: 上記の図では、文章単位での添え字です。 例えば、 A_i なら、 i 文章目でのAdvantageとなります。\n            従って A_i はベクトルとなっており、中にtokenごとのAdvantageの値も格納されているはずです。\n    - heading "報酬モデルを作らずルールベースで実施" [level=3] [ref=s1e854]\n    - paragraph [ref=s1e855]:\n        - text: 改めて、LLMに強化学習を適用するために必要なものをまとめます。 （下記の図のGRPOの手法を利用します）\n        - img [ref=s1e858]\n    - paragraph [ref=s1e859]: "まず、大前提として方策モデル \\\\pi_\\\\phi(y_t|x,y_{<t}) （LLM）は必要です。"\n    - paragraph [ref=s1e862]: "上の図中における「Reference Model」とは、「強化学習をする前」の方策モデル\n        \\\\pi_{ref}(y_t|x,y_{<t}) （つまり、DeepSeek-V3-Base）です。 また、「Policy\n        Model」というのは、「強化学習中」の現時点での方策モデル \\\\pi_\\\\phi(y_t|x,y_{<t}) を指しています。"\n    - paragraph [ref=s1e868]: そして、「プロンプト」「良い回答」「悪い回答」の3つを組とするデータセットで学習された、報酬モデル\n        r_\\theta(r|y,x) （Reward Model）が必要です。 報酬モデル r_\\theta(r|y,x) は「プロンプト x\n        」と「LLMの出力回答 y 」を入力することで、「報酬 r 」を出力します。\n    - paragraph [ref=s1e880]: この二つがあれば、著者が提案したGRPOという手法で、強化学習が可能になります。 （GRPOでは、状態価値モデルは不要です）\n    - paragraph [ref=s1e882]:\n        - text: しかしながら著者らは、さらに驚くべきことに、「DeepSeek-R1-Zero」を学習する際に、\n        - strong [ref=s1e884]: 報酬モデルを使わずに、ルールベースで報酬を計算しています。\n        - text: それにより、圧倒的な計算効率を達成しています。\n    - paragraph [ref=s1e886]: 著者らが設定しているルールは下記の二つです。\n    - list [ref=s1e887]:\n        - listitem [ref=s1e888]:\n            - strong [ref=s1e889]: 精度報酬\n            - list [ref=s1e890]:\n                - listitem [ref=s1e891]: 応答が正しいかどうかを評価する\n                - listitem [ref=s1e892]: 例えば、数学の問題であれば、最終的な回答が答えと合っているかどうかで報酬を渡す。\n        - listitem [ref=s1e893]:\n            - strong [ref=s1e894]: フォーマット報酬\n            - list [ref=s1e895]:\n                - listitem [ref=s1e896]:\n                    - text: 思考プロセスを\n                    - code [ref=s1e897]: <think>\n                    - text: と\n                    - code [ref=s1e898]: </think>\n                    - text: タグの間に置くことをモデルに強制し、合っていない場合はペナルティを課す。\n    - paragraph [ref=s1e899]:\n        - text: このたった2つの報酬のみをルールベースで設定しており、\n        - strong [ref=s1e901]: ニューラルネットワークベースの報酬モデルは利用していません\n    - paragraph [ref=s1e902]: ニューラルネットワークの報酬モデルを利用せずに、ルールベース報酬だけで強化学習をすることにより、報酬モデル分の計算量を削減しています。\n    - heading "この報酬設計で良い理由" [level=4] [ref=s1e903]\n    - paragraph [ref=s1e904]: フォーマット報酬は、モデルに出力形式を強制するものであるので、思考能力を向上させる直接的な報酬ではありません。\n        であれば、「DeepSeek-R1-Zero」の圧倒的な思考能力を支えているのは、「精度報酬」になります。\n    - paragraph [ref=s1e906]: 本論文での、強化学習の段階で利用しているデータセットは、おそらく数学データセットです。\n    - paragraph [ref=s1e907]:\n        - text: 数学の問題は、基本的に回答が一意に定まるため、ルールベースであっても出力が正しいかどうかは判断できます。\n        - strong [ref=s1e909]: そして、数学の問題に正答するためには、思考能力を強化する必要があります。\n    - complementary [ref=s1e910]:\n        - text: "!"\n        - paragraph [ref=s1e913]: ここが本論文の非常に面白い部分です。\n            DeepSeek-R1-Zeroを学習する際に、より長い長考をさせるようなデータセットによる学習や、思考部分に対して報酬を与えるような仕組みは「全く無く」、ただ、数学の問題の回答が合っているか間違っているかのゴール報酬のみが与えられています。\n        - paragraph [ref=s1e915]: それだけにも関わらず、自動的に数学の問題を正答するために、思考能力を強化するように学習が進んでおり、強化学習の威力を感じます。\n    - paragraph [ref=s1e916]: 強化学習の枠組みにおいて、最終的に報酬を得る（問題に正解する）ために、今、何のtokenを出力しないといけないのか、が最適化されます。\n        結果として、問題の正当に近づくようなtokenを出力する能力、すなわち問題をステップバイステップで解く能力が磨かれることになります。\n    - paragraph [ref=s1e918]: このように、数学という問題の答えを合わせるように報酬設計をすることで、勝手に思考能力が強化される、というのは非常に面白いなと思います。\n    - heading "（補足）データセットの作り方（の考察）" [level=4] [ref=s1e919]\n    - paragraph [ref=s1e920]:\n        - text: 本論文での、強化学習に利用するデータセットは、おそらく\n        - link "上述した論文" [ref=s1e921]:\n            - /url: https://arxiv.org/html/2402.03300v3\n        - text: の前半に記載されている数学データセットです。\n    - complementary [ref=s1e922]:\n        - text: "!"\n        - paragraph [ref=s1e925]: 強化学習に利用したデータセットに関しては、本論文で言及されていなかったと思うので憶測です。間違っていたら教えてください。\n    - paragraph [ref=s1e926]: 本論文の主要部分ではないですが、興味深いため紹介します。\n    - paragraph [ref=s1e927]:\n        - img [ref=s1e928]\n        - text: 引用：\n        - \'link "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models" [ref=s1e929]\':\n            - /url: https://arxiv.org/html/2402.03300v3\n    - paragraph [ref=s1e930]:\n        - strong [ref=s1e931]: 1. Train a FastText Model\n        - text: まず初めに、「OpenWebMath」という高品質の数学のウェブテキストのデータセットを初期のシードコーパスとして利用します。\n            このコーパスから50万のデータを正例、 Common\n            Crawlデータセット（大規模なWebページコーパス）から取得した50万のデータを負例として、fastTextモデルを学習します。\n            こうすることにより、fastTextモデルを利用して、Webページから、それが数学コンテンツか、非数学コンテンツかを分類することができます。\n    - paragraph [ref=s1e935]:\n        - strong [ref=s1e936]: 2. Recall Math-Related Webpages From Common Crawl\n    - paragraph [ref=s1e937]: 学習した、fastTextモデルを利用して、Common\n        Crawlデータセットから数学のWebページを取得します。 このとき、低品質な数学コンテンツを除外するために、 fastText\n        モデルによって予測されたスコアをもとにランク付けした、上位のスコアのページのみを取得します。\n    - paragraph [ref=s1e939]:\n        - strong [ref=s1e940]: 3. Discover Math-Related Domains\n    - paragraph [ref=s1e941]: この方法で取得されたWebページは、数学の問題ではありますが、偏りがあります。\n        なぜなら、初期の学習に利用された「OpenWebMath」データセットには、少数のデータしかなく、さらに上位のスコアのWebコーパス（類似度の高いWebコーパス）しか収集していないため、「OpenWebMath」データセットと似たような数学Webページしか、収集できていないからです。\n    - paragraph [ref=s1e943]: そこで、上記の方法で収集したWebページの親ドメインに着目します。\n        そして、親ドメイン内のWebページのうち、10%以上のWebページが、今回の方法で収集されている場合、親ドメイン自体が数学に関連するドメインであると、考えることができます。\n    - complementary [ref=s1e945]:\n        - text: "!"\n        - paragraph [ref=s1e948]: 例えば、「mathhoge.com」の配下のページの10%以上がfastText\n            モデルによって収集されていた場合は、「mathhoge.com」自体を数学コンテンツであると分類します。\n    - paragraph [ref=s1e949]: あとは、この親ドメイン内の、まだ収集されていないWebページから、高品質な数学Webページを収集することで、幅広い数学のコンテンツを収集できそうです。\n    - paragraph [ref=s1e950]:\n        - strong [ref=s1e951]: 4. Annotate Math-Related URL Path From Labelers\n    - paragraph [ref=s1e952]: 分類されたドメイン内の数学コンテンツに対して、「手動」でURLに注釈をつけ、注釈をつけたもののうち、またシードコーパスに収集されていないWebページを、シードコーパスに追加します。\n    - paragraph [ref=s1e953]: この方法を繰り返すことで、高品質かつ多様な数学データセットを収集することができます。\n        著者らはこのデータ収集を4回繰り返したのちに、3,550万の数学Webページ、合計1200億トークンのコーパスを完成させました。\n    - paragraph [ref=s1e955]: 4回の繰り返しの段階で、データのうち98%が3回目までの繰り返しですでに収集されていたことがわかったため、4回で繰り返し作業を終了したとのことです。\n    - heading "（補足）DPO（Direct Preference Optimization）との違い" [level=3]\n      [ref=s1e956]\n    - paragraph [ref=s1e957]: DPO（Direct Preference Optimization）という手法があります。\n        こちらは、強化学習を利用して、LLMに好ましい回答を学習させるRLHF（Reinforcement learning from human\n        feedback）と同様の効果を、強化学習を使わずに学習させることができる手法です。\n    - paragraph [ref=s1e959]: "強化学習をしないため、報酬モデルも状態価値モデルも必要なく、単純に「プロンプト x 」と「好ましい回答 y^+\n        」「好ましくない回答 y^- 」の3つが組となっているデータセット D を用意すれば、あとは \\\\pi_\\\\phi(y^+|x) 、\n        \\\\pi_\\\\phi(y^-|x) 、 \\\\pi_{ref}(y^+|x) 、 \\\\pi_{ref}(y^+|x)\n        を計算して、損失関数に落とし込むことで、学習が可能になります。"\n    - paragraph [ref=s1e976]: このように、「報酬モデルも状態価値モデルも必要ない」の部分が、本論文で提案している「GRPO（Group\n        Relative Policy Optimization）+ルールベース報酬」の手法の利点と近いです。\n    - paragraph [ref=s1e977]: ただし、DPOでは、上述した通り、「プロンプト x 」と「好ましい回答 y^+ 」「好ましくない回答 y^-\n        」の3つが組となっているデータセットが必要です。 逆に言えば、このデータセットの範囲内でしか学習ができません。\n    - paragraph [ref=s1e985]: 一方で、著者らが提案する「GRPO（Group Relative Policy\n        Optimization）+ルールベース報酬」の手法では、数学などの答えが一意に決まり、ルールベースで正解不正解を判断できるデータセット（つまり、数学データセット）であれば、「プロンプト\n        x 」と「回答 y 」の組だけで、学習が可能です。\n    - paragraph [ref=s1e990]:\n        - strong [ref=s1e991]: したがって、DPOよりもはるかに多くのデータを用いて学習ができます。\n        - text: これが、今回の提案手法の利点になりそうです。\n    - heading "著者らの執念" [level=3] [ref=s1e993]\n    - paragraph [ref=s1e994]: 著者らは、LLMを強化学習に適用する上で、計算量的に障害になる報酬モデルと状態価値モデルを利用しない斬新な手法を提案しました。\n        これにより、圧倒的な計算効率にて、強化学習を適用させました。\n    - paragraph [ref=s1e996]: また、似た効果を持つ手法としてDPOという手法があります。 ただし、DPOでは、利用できるデータセットが制限されるため、大規模な学習は困難です。\n    - paragraph [ref=s1e998]: しかしながら著者らの手法は、ただ数学のように一定の推論の結果、答えが一意に定まるような問題を解かせ続ければ、ルールベース報酬による強化学習で、思考機能を強化させることができてしまっています。\n    - paragraph [ref=s1e999]:\n        - strong [ref=s1e1000]: 絶対に、学習データの量を減らさずに、計算量を徹底的に削減してやろうという著者らの執念を感じるとともに、こういう工夫に積み重ねが、高性能と圧倒的に格安なAPI料金の両立を達成しているのだろうなと感じました。\n    - heading "ZeroのSystemプロンプト" [level=2] [ref=s1e1001]\n    - paragraph [ref=s1e1002]: 続いては、少し話が変わり、「DeepSeek-R1-Zero」に与えているsystemプロンプトについて紹介します。\n        このように、systemプロンプトが公開されているというのは、非常に勉強になりますね。とても嬉しいです\n    - paragraph [ref=s1e1004]: systemプロンプトは下記です。\n    - paragraph [ref=s1e1005]: "A conversation between User and Assistant. The user\n        asks a question, and the Assistant solves it. The assistant first thinks\n        about the reasoning process in the mind and then provides the user with\n        the answer. The reasoning process and answer are enclosed within <think>\n        </think> and <answer> </answer> tags, respectively, i.e., <think>\n        reasoning process here </think> <answer> answer here </answer>. User:\n        prompt. Assistant:"\n    - paragraph [ref=s1e1006]: 訳）\n        ユーザーとアシスタントの会話。ユーザーが質問し、アシスタントがそれを解決する。アシスタントは、まず頭の中で推論プロセスを考え、それからユーザに答えを提供します。推論過程と答えはそれぞれ<think>\n        </think>タグと<answer> </answer>タグで囲まれます。つまり、<think> 推論過程はここ</think>\n        <answer>答えはここ</answer>です。ユーザー：プロンプト アシスタント：\n    - paragraph [ref=s1e1008]: 上記部分の「prompt」の部分は、ユーザが入力したプロンプトの内容に書き換えられます。\n        このように、systemプロンプトにおいて<think> </think>タグを利用して、その中で思考をさせる形になっています。\n    - heading "Zeroの「Aha Moment」" [level=2] [ref=s1e1010]\n    - paragraph [ref=s1e1011]: 「Aha Moment」とは、辞書で調べると「〔問題の答えが突然ひらめいたときなどの〕なるほど！と思う瞬間」とのことです。\n    - paragraph [ref=s1e1012]: 実は、DeepSeek-R1-Zeroの出力において、それが発生したそうです。 数学の問題を解く途中で、下記のような出力が見られました。\n    - paragraph [ref=s1e1014]: "Wait, wait. Wait. That’s an aha moment I can flag\n        here. Let’s reevaluate this step-by-step to identify if the correct sum\n        can be · · · We started with the equation:"\n    - paragraph [ref=s1e1017]: 訳） 待て、待て。待ってくれ。これはハッとさせられる瞬間だ。 正しい和が - - - -\n        になるかどうかを確認するために、ステップ・バイ・ステップで再評価してみよう。\n    - paragraph [ref=s1e1020]: これは恐ろしいことです。\n        モデルは途中まで推論を実施しながら、途中で自身の出力結果を見直し、さらにもう一度ステップバイステップで検討をするという、高度な問題解決能力を獲得しています。\n        （自分の考えを見直して、さらに良い戦略を発見する。これができる人間がどれくらいいるのだろうか）\n    - paragraph [ref=s1e1023]: 何度も記載していますが、DeepSeek-R1-Zeroを学習する際に、問題の解き方を明示的にモデルに教えることや、思考部分を強化させるような報酬は与えていません。\n        ただ、数学の問題に正解することで得られる報酬を与えるだけで、モデルが自律的に高度な問題解決戦略を獲得しています。\n        （もちろん、推論の途中で、検算を行うなどのデータセットで学習もさせていません。もしかしたら、DeepSeek-V3-Baseを学習させるときのSFTデータセットには含まれているかもですが。）\n    - paragraph [ref=s1e1026]: これこそが、本論文で何度も述べている、「強化学習の可能性」です。\n        強化学習は（そのほかの学習手法とは異なり）（人間にとって）予想外で、かつ洗練された結果を導き、人工知能が新たなレベルの知性を手にいれるための核となる技術になりそうです。\n    - heading "DeepSeek-R1-Zeroのまとめ" [level=2] [ref=s1e1028]\n    - paragraph [ref=s1e1029]: DeepSeek-R1-ZeroはDeepSeek-V3-Baseに対して、「GRPO+ルールベース報酬」の強化学習「のみ」を適用し、圧倒的な思考能力を獲得したモデルになります。\n    - paragraph [ref=s1e1030]: DeepSeek-R1-Zeroを学習する際に、問題の解き方を明示的にモデルに教えることや、思考部分を強化させるような報酬は与えていません。\n        ただ、数学の問題に正解することで得られる報酬を与えるだけで、モデルが自律的に高度な問題解決戦略を獲得するという、とても興味深く、恐ろしい結果が示されています。\n    - heading "DeepSeek-R1への道" [level=1] [ref=s1e1032]\n    - paragraph [ref=s1e1033]: さて、ここまで、「DeepSeek-R1-Zero」について、たくさん語ってきましたが、とても魅力的でしたよね？\n        あくまで、「DeepSeek-R1-Zero」は「DeepSeek-V3-Base」に対して、強化学習のみを適用させて、強化学習の可能性を探ったモデルでした。\n    - paragraph [ref=s1e1035]:\n        - text: その結果、こんなにも魅力的なモデル（DeepSeek-R1-Zero）ができてしまったら、、、\n        - strong [ref=s1e1037]: SFTも入れて完全体のモデルを作りたくなりますよね？？？\n    - paragraph [ref=s1e1038]: というモチベーションで「DeepSeek-R1」は作られています。\n    - paragraph [ref=s1e1039]: もし、DeepSeek-R1-Zeroの性能がそんなに高くなかったら、今騒がれているDeepSeek-R1は作ろうという発想にすらならなかったと思います。\n    - heading "DeepSeek-R1-Zeroの課題" [level=2] [ref=s1e1040]\n    - paragraph [ref=s1e1041]: さて、非常に高性能で、示唆に富んだDeepSeek-R1-Zeroでしたが、課題はいくつかあります。 それは下記です。\n    - list [ref=s1e1043]:\n        - listitem [ref=s1e1044]: 学習初期は強化学習の学習が不安定で収束に時間がかかる（コールドスタート問題）\n        - listitem [ref=s1e1045]:\n            - text: 思考能力は高いが、言語が混在（日本語で話していたら急に中国語になるなど）し、人間が理解しにくい\n            - list [ref=s1e1046]:\n                - listitem [ref=s1e1047]: 結果、高度な思考タスクには強いが、（ロールプレイなどの）一般的なタスクには使えないという評価\n    - paragraph [ref=s1e1048]: そこで、上記の課題を解決する旅に出ます。\n    - heading "課題対処への道" [level=2] [ref=s1e1049]\n    - paragraph [ref=s1e1050]: DeepSeek-R1-Zeroでは、ベースモデルであるDeepSeek-V3-Baseに対して、そのまま上述した強化学習を用いて学習をしていました。\n    - paragraph [ref=s1e1051]:\n        - text: しかし、もともとDeepSeek-V3-Baseは長考モデルとして学習されたモデルではないため、いくらsystemプロンプトで長考をさせる制限（\n        - code [ref=s1e1052]: <think>\n        - text: タグ）を入れたとしても、強化学習の初期フェーズは不安定な学習になってしまいます。\n            （例えば、「そもそも適切な形式で思考をしない」、「思考形式は合っているが、意味のある思考をしていない」、その結果として「数学の問題になかなか正答できないまま、報酬が得られず、学習が不安定になる」が考えられます）\n    - paragraph [ref=s1e1054]: そこで、学習初期のコールドスタート問題を防ぐために、DeepSeek-V3-Baseに対して、SFT（Supervised\n        Fine-Tuning）を実施します。 そのために著者らは、少量のCoT（Chain-of-Thought）データセットを構築しています。\n    - heading "CoTデータセットの構築+SFT" [level=3] [ref=s1e1056]\n    - paragraph [ref=s1e1057]: データセットの構築方法は下記が一例として提示されております。\n    - list [ref=s1e1058]:\n        - listitem [ref=s1e1059]: いくつかの長いCoT解答を提示し、モデルに長いCoT形式の回答を生成させてデータセットに追加する\n        - listitem [ref=s1e1060]: モデルに対して、自己反省や検証を行いながら、詳細な回答生成を促すようなプロンプトを送信し、その出力をデータセットに追加する\n        - listitem [ref=s1e1061]: DeepSeek-R1-Zeroの出力を、読みやすい形式に整理してからデータセットに追加する\n        - listitem [ref=s1e1062]: モデルの出力結果を人間が精査して、より適切なCoTデータの形に整理してから、データセットに追加する\n    - paragraph [ref=s1e1063]: 上記の方法で、数千件のCoTデータを構築し、そのデータを用いて、DeepSeek-V3-BaseをSFTしています。\n    - heading "CoTデータセットを用いることの利点" [level=3] [ref=s1e1064]\n    - paragraph [ref=s1e1065]: 著者らは、下記の要素において、CoTデータセットを用いることに利点があると考えています。\n    - paragraph [ref=s1e1066]:\n        - strong [ref=s1e1067]: 読みやすさ\n        - text: DeepSeek-R1-Zeroの一番の課題は、その出力結果が人間が理解するのが難しいことです。\n            回答には、複数の言語が混在していたり、マークダウン形式などの読みやすい形式が適切に使われていないことで、重要な情報が何かわかりにくい状態でした。\n    - paragraph [ref=s1e1070]: そこで、今回のデータセットを作成する際には、各回答の末尾に「要約文」を入れるようにすることで、人間が読みやすい形に出力させるようにしています。\n        具体的には下記のような出力形式で、全てのCoTデータセットは作成されています。\n    - code [ref=s1e1075]: "|special_token|<reasoning_process>|special_token|<summary>"\n    - button "クリップボードにコピー" [ref=s1e1076]:\n        - img [ref=s1e1077]\n    - button "右端で折り返す" [ref=s1e1078]:\n        - img [ref=s1e1079]\n    - paragraph [ref=s1e1080]:\n        - code [ref=s1e1081]: <reasoning_process>\n        - text: の部分に、モデルの思考に該当する部分が、\n        - code [ref=s1e1082]: <summary>\n        - text: の部分に最終的な結論の要約が記載されます。\n    - paragraph [ref=s1e1083]: これにより、「DeepSeek-R1-Zero」にあった、重要な情報が何かわかりにくいという人間が理解するのが難しい問題を緩和します。\n        （言語の混在に関しての対処は後述します）\n    - paragraph [ref=s1e1085]:\n        - strong [ref=s1e1086]: ポテンシャル\n        - text: 今回、強化学習のみを実施するのではなく、人間の事前知識に基づいて作成したCoTデータセットでSFTを実施しています。\n            また、SFTと強化学習を複数回反復して、繰り返し学習をしています。\n            その結果、DeepSeek-R1はDeepSeek-R1-Zeroよりも良い結果を得ることができました。\n    - paragraph [ref=s1e1090]: 著者らは、このような反復的な学習こそが、長考モデル（reasoning models）を学習する上で良い方法であると主張しています。\n    - heading "推論思考の強化学習" [level=3] [ref=s1e1091]\n    - paragraph [ref=s1e1092]: CoTデータセットによりSFTしたDeepSeek-V3-Baseに対して、上述した（DeepSeek-R1-Zeroを学習した）強化学習手法を適用します。\n        これにより、モデルの思考能力が大幅に強化されます。\n    - paragraph [ref=s1e1094]: 一方で、「DeepSeek-R1-Zero」には、言語不一致の問題がありました。\n        そこで、DeepSeek-R1-Zeroのルールベース報酬に、「言語一貫性報酬」を追加で導入しています。\n    - paragraph [ref=s1e1096]: この報酬導入で、実は、モデルの思考能力が若干落ちてしまうことが実験的にわかっています。\n        しかしながら、この報酬があることで、ようやく人間が理解しやすい文章を生成してくれるようになります。\n    - heading "さらなる追加のSFT" [level=3] [ref=s1e1098]\n    - paragraph [ref=s1e1099]: 上記の強化学習が収束したら、さらにデータを追加してSFTをやり直します。\n        ここでは、初期段階の、CoTに重きを置いた、コールドスタートを解消するための学習データだけにとどまらず、ライティング、ロールプレイといった汎用タスクの性能を向上させるため、他ドメインからのデータも合わせて収集します。\n    - heading "Reasoning Data" [level=4] [ref=s1e1101]\n    - paragraph [ref=s1e1102]: まずは、前段階と同様に思考能力を向上させるために、CoT学習データ（Reasoning Data）を取得します。\n    - paragraph [ref=s1e1103]: 取得方法は以下です。\n    - list [ref=s1e1104]:\n        - listitem [ref=s1e1105]:\n            - text: 強化学習済みモデルの出力から、適切なものを収集して学習データに追加\n            - list [ref=s1e1106]:\n                - listitem [ref=s1e1107]: 前段の強化学習では、ルールベースの報酬モデルを利用していたので、それが使える問題（数学など）のみを学習データとしていたが、次の強化学習では報酬モデルを作成するため、もっと広い範囲でCoTができるデータも学習データに追加\n                - listitem [ref=s1e1108]: モデルの出力はまだ読みにくいところがあるため、さまざまフィルタリング\n    - paragraph [ref=s1e1109]: この結果、合計で600kのReasoning Data（学習データ）を収集しています。\n    - heading "Non-Reasoning Data" [level=4] [ref=s1e1110]\n    - paragraph [ref=s1e1111]: ライティングやQ＆A、翻訳などのNon-Reasoning\n        Dataについては、DeepSeek-V3のSFTデータセットの一部を再利用して、学習データを収集しています。 上記のReasoning\n        Data以外のタスク（Non-Reasoning\n        Data）に関しては、DeepSeek-V3によって簡易的なCoTを実施して、回答を生成し、学習データとします。\n        ただし、本当に簡単なタスク（あいさつとか）に関しては、CoTを学習データに追加しません。\n    - paragraph [ref=s1e1114]: こうすることで、合計200kのトレーニングサンプルを最終的に収集したそうです。\n    - paragraph [ref=s1e1115]: 上記の合計800kサンプルの学習データを利用して、DeepSeek-V3-Base（初回強化学習済み）を2epoch分SFTを実施しています。\n    - heading "最後の強化学習" [level=3] [ref=s1e1116]\n    - paragraph [ref=s1e1117]: 最後に、モデルの思考能力をさらに磨くと同時に、モデルの無害性などを向上させることを目的に、強化学習を実施します。\n    - paragraph [ref=s1e1118]: 思考タスクの強化学習に関しては、これまで説明したのと同様に、数学、コード、論理的推論の領域でルールベース報酬を利用して強化学習をします。\n        一方で、思考タスク以外の一般的なデータに対しては、ルールベースで回答の良し悪しを判断することはできないため、報酬モデルを別途DeepSeek-V3パイプラインをベースに作成しています。\n    - heading "DeepSeek-R1のまとめ" [level=2] [ref=s1e1120]\n    - paragraph [ref=s1e1121]: 以上より、DeepSeek-R1では、DeepSeek-V3-Baseに対して、SFTと強化学習を2回実施することにより構築されています。\n    - paragraph [ref=s1e1122]: SFTにより、強化学習のコールドスタート問題の解消と、モデル出力のユーザフレンドリー性の向上、一般的なタスクに対する性能向上に貢献しています。\n    - heading "結果" [level=1] [ref=s1e1123]\n    - paragraph [ref=s1e1124]: 結果に関しては、他の方がさまざま検証していたり、話題になっているので詳しくは書きません。\n        ただ、DeepSeek-R1は、現在のオープンソースモデルの中では、最も高い性能と言ってよく、また、Open\n        AIのo1モデルにも匹敵する性能だと言われています。\n    - heading "蒸留" [level=1] [ref=s1e1126]\n    - paragraph [ref=s1e1127]: 最後に、蒸留モデルについての解説します。（論文でも言及されています）\n        DeepSeek-R1は671Bものモデルパラメータを持つため、一般的な家庭用PCで動作させるのは難しいです。\n    - paragraph [ref=s1e1129]: そこで、より小さなモデルパラメータのモデルに、DeepSeek-R1のような思考能力を与えるために、蒸留を実施しています。\n        （公式が、個人の利用者のことも考えて、家庭用PCで動作する思考モデルを開発してくれるのは非常にありがたいですね）\n    - paragraph [ref=s1e1131]: 具体的には、Qwen2.5-Math-1.5B、Qwen2.5-Math-7B、Qwen2.5-14B、Qwen2.5-32B、Llama-3.1-8B、Llama-3.3-70B-Instructに対して、DeepSeek-R1から取得した800kのトレーニングサンプルを利用して、直接FineTuningする形で、蒸留を実施しています。\n    - paragraph [ref=s1e1132]: 蒸留モデルに対しては、SFTのみを実施し、後段の強化学習は実施しておらず、蒸留モデルに対する追加の強化学習の探究は、著者らが実施する予定はないそうです。\n        （著者らは、強化学習を追加で実施することにより性能が上がると思っているが、その探究は、別のコミュニティに任せると記述しています）\n    - heading "蒸留 vs 強化学習" [level=2] [ref=s1e1134]\n    - paragraph [ref=s1e1135]: 蒸留後の強化学習に関しては、著者らは実験していない、と記述していました。\n        では、小型モデルに対して、蒸留を実施する前に、強化学習を実施したらどうなるのでしょうか？\n        DeepSeek-V3-Baseのように、大幅に思考能力が強化されて、性能向上が期待できるのでしょうか？\n    - paragraph [ref=s1e1138]: それに関しては、著者らが実験してくれています。\n    - paragraph [ref=s1e1139]: ここでは、「Qwen-32B-Base」モデルを利用して、10kステップ以上の大規模強化学習を実施し、「DeepSeek-R1-Zero-Qwen-32B」を構築しました。\n        一方で、「Qwen-32B-Base」モデルに対して、蒸留を実施したモデルを、「DeepSeek-R1Distill-Qwen-32B」モデルとして、この二つのモデルを比較します。\n    - paragraph [ref=s1e1141]: その結果を比較すると、小型モデルに対する強化学習を適用した「DeepSeek-R1-Zero-Qwen-32B」よりも、小型モデルに蒸留を適用した「DeepSeek-R1Distill-Qwen-32B」モデルの方がはるかに高い性能でした。\n    - paragraph [ref=s1e1142]:\n        - strong [ref=s1e1143]: 以下は私の主観ですが、\n        - text: 以上の結果から、\n        - strong [ref=s1e1146]: 強化学習による思考能力の向上は、安易に小型モデルに試していい戦略でない\n        - text: ということが示唆されていると思います。\n    - paragraph [ref=s1e1148]: 強化学習による思考能力の大幅な強化は、おそらくですが「DeepSeek-V3」という元々大規模かつ性能の高いモデルであったからこそ、達成されたのだと思います。\n    - paragraph [ref=s1e1149]: DeepSeek-R1-Zeroの課題でも説明した通り、今回のルールベース報酬では、正しい数学の解答を生成できないと永遠に報酬が獲得できません。\n        小型モデルでは、このコールドスタートの問題が、より致命的なレベルで発生するのだと思います。\n    - paragraph [ref=s1e1151]: したがって、思考モデルを作りたいからといって、小型モデルに対して、安易にルールベース強化学習を試すのはおすすめしません。\n        しかしながら、コールドスタート問題を解決できる戦略があるのであれば、有効な手段になると思われます。\n        （例えば、DeepSeek-R1で蒸留済みの小型モデルを利用する、最初から数学に強い小型モデルで強化学習を試すなどです。重要なのは強化学習のコールドスタートを解消できるかどうかだと思われます。）\n    - paragraph [ref=s1e1154]:\n        - text: （追記） また、同様の実験を小型モデルに対して試したグループもいるようです。 そちらの実験レポートが下記になります。\n        - iframe [ref=s1e1158]: <could not take iframe snapshot>\n        - text: （DeepSeek-R1-Zeroと同様に、「Aha Moment」が見られることもわかっており、非常に興味深いです）\n    - paragraph [ref=s1e1159]: ただ、こちらのレポートにおいても、ベースモデルとしては数学に特化したモデルを利用しています。\n        このことからも、ある程度の数学能力は必要なのだなと思います。\n    - heading "今後の展望" [level=1] [ref=s1e1161]\n    - paragraph [ref=s1e1162]: DeepSeek-R1は、さまざまなタスクにおいて、OpenAI\n        o1に匹敵する性能を達成した、驚異的なオープンソースモデルですが、苦手としている部分もあります。 それが今後の展望に記載されています。\n        （今後の展望を見れば、DeepSeek-R1が苦手な部分を知ることができます）\n    - paragraph [ref=s1e1165]: それは以下の項目です。\n    - list [ref=s1e1166]:\n        - listitem [ref=s1e1167]:\n            - strong [ref=s1e1168]: 一般的な能力\n            - list [ref=s1e1169]:\n                - listitem [ref=s1e1170]: 関数呼び出しやマルチターン対話、複雑なロールプレイング。、json出力において、DeepSeek-V3に及ばない\n        - listitem [ref=s1e1171]:\n            - strong [ref=s1e1172]: 言語の混在\n            - list [ref=s1e1173]:\n                - listitem [ref=s1e1174]: 現在、DeepSeek-R1は英語と中国語に最適化されているため、そのほかの言語を利用した場合、言語の混在が発生する可能性がある\n        - listitem [ref=s1e1175]:\n            - strong [ref=s1e1176]: プロンプトエンジニアリング\n            - list [ref=s1e1177]:\n                - listitem [ref=s1e1178]: DeepSeek-R1はプロンプトに非常に敏感で、特にFewーshotプロンプトを利用すると、パフォーマンスが低下する。\n                - listitem [ref=s1e1179]: 基本的には、問題を直接シンプルに記述し、例を提示しない（Zero-shotプロンプト）を利用することを推奨している\n        - listitem [ref=s1e1180]:\n            - strong [ref=s1e1181]: ソフトウェアエンジニアリングタスク\n            - list [ref=s1e1182]:\n                - listitem [ref=s1e1183]: コード生成やコード保管などのソフトウェアエンジニアリングタスクは、評価に時間がかかるため、強化学習の効率が悪くなることから、現時点では強化学習において、このようなタスクは学習していない。\n                    大々的には学習しておらず、一部のテスト可能な範囲でしか学習できていない。\n                - listitem [ref=s1e1185]: 結果として、このタスクの性能は、DeepSeek-V3からほとんど改善していない。\n    - complementary [ref=s1e1186]:\n        - text: "!"\n        - paragraph [ref=s1e1189]: （追記） masaishiさんのコメントに基づき修正させていただきました。 私の読み間違いで誤解を与えてしまい申し訳ございません。\n        - blockquote [ref=s1e1192]:\n            - paragraph [ref=s1e1193]: 2.2.2. Reward Modeling ・・・ ・Accuracy\n                rewards:・・・Similarly, for LeetCode problems, a compiler can be\n                used to generate feedback based on predefined test cases.\n        - paragraph [ref=s1e1196]: と記載のあるように、全くコード生成などを学習していないわけではなく、一部の、簡単にテストができる問題のみ学習しているようです。\n            ただ、そういう簡易的なタスクだけしか学習できていないため、性能向上は小さめということだと思われます。\n    - paragraph [ref=s1e1198]: 上記の内容が、現時点での「DeepSeek-R1」の課題です。\n        ですが、今後のアップデートで改善していくとも記載されているため、現時点で不満があったとしても、上記に記載されている項目であれば、今後改善する可能性が高いです。\n    - heading "まとめ" [level=1] [ref=s1e1200]\n    - paragraph [ref=s1e1201]: ここまで読んでくださった方、本当にお疲れ様でした。\n        （これでも説明はだいぶ短くした方なんです・・・本当は、もっと強化学習の部分とかを数式をもとに説明したかったです）\n    - paragraph [ref=s1e1203]: DeepSeek-R1の論文いかがでしたか？\n        私にとっては、非常に勉強になるような示唆や知見に溢れており、こんな論文を公開してくれて、かつオープンソースのモデルとしても公開してくれる「DeepSeek-AI」には感謝しかありません。\n    - paragraph [ref=s1e1205]: 本記事が、皆様の理解の助けになり、一人でも多くの方が原論文に興味を持って、読んでくれましたら幸いです。\n        ここまで読んでくださってありがとうございました！\n    - heading "参考文献" [level=1] [ref=s1e1207]\n    - paragraph [ref=s1e1208]: 本論文は、論文の中では読みやすい方だとは思いますが、強化学習に関しては、一定程度の基礎知識は要求されます。\n        そこで、本論文を読む上で必要な知識を収集できるおすすめの書籍に関して記載します。 （少なくとも私はこの書籍で学習しました）\n    - paragraph [ref=s1e1211]: （書籍のリンクはamazonアフィリエイトリンクになります）\n    - heading "強化学習" [level=4] [ref=s1e1212]\n    - paragraph [ref=s1e1213]: 強化学習に関しておすすめの本は、下記の本たちです。\n    - paragraph [ref=s1e1214]:\n        - link "ゼロから作るDeep Learning ❹ ―強化学習編" [ref=s1e1215]:\n            - /url: https://amzn.to/40qFuNg\n        - text: まずは、定番の本です。\n            強化学習に関して、本当に最初の部分（マルコフ決定過程とは何か）から、非常に詳細かつわかりやすく解説されており、深層学習を利用して、どのように強化学習をすることできるのかまで、しっかり書かれているためおすすめです。\n            また、コードと理論のバランスもちょうどよく、理系大学生とかであれば問題なく読み進められると思います。\n            特に、強化学習の基礎の部分が、詳細かつ分かりやすく書かれているため、一冊目におすすめです。\n    - paragraph [ref=s1e1220]:\n        - link "機械学習スタートアップシリーズ Ｐｙｔｈｏｎで学ぶ強化学習 ［改訂第２版］ 入門から実践まで (ＫＳ情報科学専門書)" [ref=s1e1221]:\n            - /url: https://amzn.to/4joKbjg\n    - paragraph [ref=s1e1223]: 普段からpythonを使っており、数式よりも実装に興味がある方にとっては、こちらの書籍もおすすめです。\n        こちらの書籍は、強化学習の基礎よりは、少し応用というか、実用を意識した書籍になっており、実務でよく使われるモデルなどに関しても、詳しく説明されています。\n        強化学習の基礎はわかっている人や、もっと実装をベースに学びたいというかたにおすすめです。\n    - paragraph [ref=s1e1226]:\n        - link "強化学習 (機械学習プロフェッショナルシリーズ)" [ref=s1e1227]:\n            - /url: https://amzn.to/40NX5QM\n        - link "強化学習から信頼できる意思決定へ (AI/データサイエンスライブラリ“基礎から応用へ” 5)" [ref=s1e1229]:\n            - /url: https://amzn.to/4jtFq8d\n    - paragraph [ref=s1e1230]: 名著です。\n        どちらも、理論説明の部分が非常に面白く、正直難易度は高いですが、強化学習の理論をちゃんと理解したいという方におすすめの書籍です。\n        下の本に関しては、2024年の本ということもあり、近年の強化学習の実世界での応用と、理論的な定式化の架け橋になっているような本になっており、非常に面白いです。\n    - heading "AlphaGO Zero" [level=4] [ref=s1e1233]\n    - paragraph [ref=s1e1234]:\n        - link "AlphaZero 深層学習・強化学習・探索 人工知能プログラミング実践入門" [ref=s1e1235]:\n            - /url: https://amzn.to/3Cdnuy1\n    - paragraph [ref=s1e1236]: だいぶ昔に購入させていただいた本ですが、まさか2025年に活躍することがあろうとは。\n        という感じですが、こちらの書籍も参考にさせていただきました。 Alpha\n        Zeroを作る上で核となる技術を余すところなく解説されており、私は非常に勉強になりました。 Alpha\n        Zeroに興味がある方は、ぜひ一度ご一読ください。\n    - heading "LLM、RLHF, DPO" [level=4] [ref=s1e1240]\n    - paragraph [ref=s1e1241]:\n        - link "大規模言語モデル入門Ⅱ〜生成型LLMの実装と評価" [ref=s1e1242]:\n            - /url: https://amzn.to/3Cs3Uhn\n        - text: よく紹介させていただいておりますが、こちらの書籍は、LLMのファインチューニングから、RLHF、RAG、分散学習にかけて、本当に幅広く解説されており、いつも参考にさせていただいております。\n            今回のDPOやRLHFの解説部分でも、とても活躍してくれました。\n            LLMを取り扱っている方は、とりあえず買っておいても損はないと思います。\n            さまざまな章、ページで読めば読むほど新しい発見が得られる、スルメのような本だなといつも思っております。\n    - button "いいね" [ref=s1e1249]:\n        - img [ref=s1e1250]\n    - text: "921"\n    - button "ブックマーク" [ref=s1e1266]:\n        - img [ref=s1e1267]\n    - text: "344"\n    - button "その他の操作" [ref=s1e1272]:\n        - img [ref=s1e1273]\n    - tooltip "X(Twitter)にポスト" [ref=s1e1276]:\n        - img [ref=s1e1277]\n        - text: X(Twitter)にポスト\n    - tooltip "Facebookに投稿" [ref=s1e1283]:\n        - img [ref=s1e1284]\n        - text: Facebookに投稿\n    - tooltip "はてなブックマークに登録" [ref=s1e1290]:\n        - img [ref=s1e1291]\n        - text: はてなブックマークに登録\n    - complementary [ref=s1e1294]:\n        - link "asap" [ref=s1e1296]:\n            - /url: /asap\n            - img "asap" [ref=s1e1297]\n        - link "asap" [ref=s1e1299]:\n            - /url: /asap\n        - paragraph [ref=s1e1301]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n        - button "フォロー" [ref=s1e1305]\n        - tooltip "@personabb" [ref=s1e1306]:\n            - img "GitHub" [ref=s1e1307]\n            - text: "@personabb"\n        - tooltip "@asap2650" [ref=s1e1309]:\n            - img "X(Twitter)" [ref=s1e1310]\n            - text: "@asap2650"\n        - tooltip "yard.tips" [ref=s1e1316]:\n            - img "リンク" [ref=s1e1317]\n            - text: yard.tips\n    - complementary [ref=s1e1320]:\n        - text: バッジを贈って著者を応援しよう\n        - paragraph [ref=s1e1324]: バッジを受け取った著者にはZennから現金やAmazonギフトカードが還元されます。\n        - button "バッジを贈る" [ref=s1e1326]\n        - img [ref=s1e1327]\n    - text: この記事に贈られたバッジ\n    - img "Thank you" [ref=s1e1338]\n    - img [ref=s1e1340]\n    - img "参考になった" [ref=s1e1352]\n    - img [ref=s1e1354]\n    - img "もっと読みたい" [ref=s1e1363]\n    - img [ref=s1e1365]\n    - img "参考になった" [ref=s1e1375]\n    - img [ref=s1e1377]\n    - img "もっと読みたい" [ref=s1e1386]\n    - img [ref=s1e1388]\n    - heading "Discussion" [level=3] [ref=s1e1400]\n    - article [ref=s1e1403]:\n        - link "はまなすなぎさ" [ref=s1e1405]:\n            - /url: /nhamanasu\n            - img "はまなすなぎさ" [ref=s1e1406]\n        - link "はまなすなぎさ" [ref=s1e1407]:\n            - /url: /nhamanasu\n        - link "2025年1月28日 8時33分" [ref=s1e1408]:\n            - /url: "#comment-f230c9d295b3f5"\n            - tooltip "2025年1月28日 8時33分" [ref=s1e1409]: 2ヶ月前 2025年1月28日 8時33分\n        - button "メニューを開く" [ref=s1e1411]:\n            - img [ref=s1e1412]\n        - paragraph [ref=s1e1417]: 非常にためになる骨太な記事、ありがとうございます！とても勉強になります。\n        - paragraph [ref=s1e1418]: ちょっとした誤字かも？の箇所なのですが、冒頭の「強化学習手法「DRPO」すごい！」の部分は「GRPO」ではないかなと思いました。ご報告まで。\n        - button "いいね" [ref=s1e1421]:\n            - img [ref=s1e1422]\n        - text: "1"\n        - button "コメント 1" [ref=s1e1437]:\n            - img "コメント" [ref=s1e1438]\n            - text: "1"\n        - article [ref=s1e1442]:\n            - link "asap" [ref=s1e1444]:\n                - /url: /asap\n                - img "asap" [ref=s1e1445]\n            - link "asap" [ref=s1e1446]:\n                - /url: /asap\n            - link "2025年1月28日 10時23分" [ref=s1e1447]:\n                - /url: "#comment-41cdebd92d8d44"\n                - tooltip "2025年1月28日 10時23分" [ref=s1e1448]: 2ヶ月前 2025年1月28日 10時23分\n            - button "メニューを開く" [ref=s1e1450]:\n                - img [ref=s1e1451]\n            - paragraph [ref=s1e1456]: 読んでくださりありがとうございます！ おっしゃる通りです！修正させていただきました！\n            - button "いいね" [ref=s1e1460]:\n                - img [ref=s1e1461]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1477]\n    - article [ref=s1e1479]:\n        - link "bang" [ref=s1e1481]:\n            - /url: /zenmnm\n            - img "bang" [ref=s1e1482]\n        - link "bang" [ref=s1e1483]:\n            - /url: /zenmnm\n        - link "2025年1月28日 13時46分" [ref=s1e1484]:\n            - /url: "#comment-d0aac9375fbbf7"\n            - tooltip "2025年1月28日 13時46分" [ref=s1e1485]: 2ヶ月前 2025年1月28日 13時46分\n        - button "メニューを開く" [ref=s1e1487]:\n            - img [ref=s1e1488]\n        - paragraph [ref=s1e1493]: AIについてはまだ詳しくありませんが、興味はあります。同じように古いモデルを使って新しいモデルを作るのですが、蒸留方法の代わりにモデルを組み合わせることは可能でしょうか?sakana\n            aiのようにです\n        - button "いいね" [ref=s1e1496]:\n            - img [ref=s1e1497]\n        - text: "1"\n        - button "コメント 2" [ref=s1e1512]:\n            - img "コメント" [ref=s1e1513]\n            - text: "2"\n        - article [ref=s1e1517]:\n            - link "asap" [ref=s1e1519]:\n                - /url: /asap\n                - img "asap" [ref=s1e1520]\n            - link "asap" [ref=s1e1521]:\n                - /url: /asap\n            - link "2025年1月28日 15時33分" [ref=s1e1522]:\n                - /url: "#comment-27e79c6ee51f2a"\n                - tooltip "2025年1月28日 15時33分" [ref=s1e1523]: 2ヶ月前 2025年1月28日 15時33分\n            - button "メニューを開く" [ref=s1e1525]:\n                - img [ref=s1e1526]\n            - paragraph [ref=s1e1531]: モデルマージに関してはあまり詳しくないですが、基本的には同じアーキテクチャのモデルを利用することになると思います。したがってDeepSeek-R1によって蒸留されたLlamaモデルとかであれば、他のLlamaモデル（同じモデルアーキテクチャ）とモデルマージすることで、推論機能を付け加えたり、日本語機能を強化したりすることが期待できると思います。\n            - button "いいね" [ref=s1e1534]:\n                - img [ref=s1e1535]\n        - article [ref=s1e1549]:\n            - link "bang" [ref=s1e1551]:\n                - /url: /zenmnm\n                - img "bang" [ref=s1e1552]\n            - link "bang" [ref=s1e1553]:\n                - /url: /zenmnm\n            - link "2025年1月28日 16時42分" [ref=s1e1554]:\n                - /url: "#comment-0221309240798f"\n                - tooltip "2025年1月28日 16時42分" [ref=s1e1555]: 2ヶ月前 2025年1月28日 16時42分\n            - button "メニューを開く" [ref=s1e1557]:\n                - img [ref=s1e1558]\n            - paragraph [ref=s1e1563]: お返事ありがとうございます。私はまだ初心者なので、これは私の軽い考えです。今では研究のプロセスごとにいくつかの方法があり、それらを組み合わせたり微調整したりすれば、たくさんのモデルができるようになっているでしょう。\n            - button "いいね" [ref=s1e1566]:\n                - img [ref=s1e1567]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1583]\n    - article [ref=s1e1585]:\n        - link "HWSeungwoo" [ref=s1e1587]:\n            - /url: /syoyuu96\n            - img "HWSeungwoo" [ref=s1e1588]\n        - link "HWSeungwoo" [ref=s1e1589]:\n            - /url: /syoyuu96\n        - link "2025年1月28日 19時28分" [ref=s1e1590]:\n            - /url: "#comment-a07230ddbdaebe"\n            - tooltip "2025年1月28日 19時28分" [ref=s1e1591]: 2ヶ月前 2025年1月28日 19時28分\n        - button "メニューを開く" [ref=s1e1593]:\n            - img [ref=s1e1594]\n        - paragraph [ref=s1e1599]: Deep\n            Seekに関する記事やニュースはたくさん目にしますが、ここまで深掘りされた内容には驚きました。とても勉強になりました。ありがとうございます！\n        - button "いいね" [ref=s1e1602]:\n            - img [ref=s1e1603]\n        - text: "1"\n        - button "コメント 1" [ref=s1e1618]:\n            - img "コメント" [ref=s1e1619]\n            - text: "1"\n        - article [ref=s1e1623]:\n            - link "asap" [ref=s1e1625]:\n                - /url: /asap\n                - img "asap" [ref=s1e1626]\n            - link "asap" [ref=s1e1627]:\n                - /url: /asap\n            - link "2025年1月28日 19時32分" [ref=s1e1628]:\n                - /url: "#comment-1e1e3abc86df00"\n                - tooltip "2025年1月28日 19時32分" [ref=s1e1629]: 2ヶ月前 2025年1月28日 19時32分\n            - button "メニューを開く" [ref=s1e1631]:\n                - img [ref=s1e1632]\n            - paragraph [ref=s1e1637]: 読んでくださってありがとうございます！ コメント嬉しいです！ありがとうございます！\n            - button "いいね" [ref=s1e1641]:\n                - img [ref=s1e1642]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1658]\n    - article [ref=s1e1660]:\n        - link "masaishi" [ref=s1e1662]:\n            - /url: /masaishi\n            - img "masaishi" [ref=s1e1663]\n        - link "masaishi" [ref=s1e1664]:\n            - /url: /masaishi\n        - link "2025年1月29日 16時14分" [ref=s1e1665]:\n            - /url: "#comment-a0d75bd96cadfc"\n            - tooltip "2025年1月29日 16時14分" [ref=s1e1666]: 2ヶ月前 2025年1月29日 16時14分\n        - button "メニューを開く" [ref=s1e1668]:\n            - img [ref=s1e1669]\n        - paragraph [ref=s1e1674]: 素晴らしい記事をありがとうございます！\n            GRPOなどの強化学習手法に関する解説も大変勉強になりました。この記事をきっかけに、DeepSeek-R1の論文にも興味を持ち、読んでみようと思えました。\n        - paragraph [ref=s1e1676]: 一点だけ気になったことがあり、もし私の理解が誤っていたら申し訳ありません。\n            "コード生成やコード保管などのソフトウェアエンジニアリングタスク"の強化学習に用いられていないとありますが、論文の\n            2.2.2（Reward Modeling）の\n        - blockquote [ref=s1e1678]:\n            - paragraph [ref=s1e1679]: For LeetCode problems, a compiler can be used to\n                generate feedback based on predefined test cases.\n        - paragraph [ref=s1e1680]: という記述を見ると、少なくともアルゴリズム問題のコード生成には強化学習が用いられているのかなと思います。\n        - paragraph [ref=s1e1681]: ソフトウェアエンジニアリングタスクは強化学習していませんが、コード生成自体は行っているような気がするので、注釈などで少し補足していただけると、読者としては誤解が減って助かるかもしれません。しかし、かなり細かい点ですので、最終的には著者さまの判断にお任せいたします。\n        - button "いいね" [ref=s1e1684]:\n            - img [ref=s1e1685]\n        - text: "2"\n        - button "コメント 2" [ref=s1e1700]:\n            - img "コメント" [ref=s1e1701]\n            - text: "2"\n        - article [ref=s1e1705]:\n            - link "asap" [ref=s1e1707]:\n                - /url: /asap\n                - img "asap" [ref=s1e1708]\n            - link "asap" [ref=s1e1709]:\n                - /url: /asap\n            - link "2025年1月29日 16時37分" [ref=s1e1710]:\n                - /url: "#comment-afb254c15106b7"\n                - tooltip "2025年1月29日 16時37分" [ref=s1e1711]: 2ヶ月前 2025年1月29日 16時37分\n            - button "メニューを開く" [ref=s1e1713]:\n                - img [ref=s1e1714]\n            - paragraph [ref=s1e1719]: コメントありがとうございます！ おっしゃる通りでした！読み間違いです。ご指摘助かります。\n            - blockquote [ref=s1e1721]:\n                - paragraph [ref=s1e1722]: Due to the long evaluation times, which impact the\n                    efficiency of the RL process, large-scale RL has not been\n                    applied extensively in software engineering tasks.\n            - paragraph [ref=s1e1723]: この部分の文章をもとに記載していましたが、大々的には適用されていないだけで、一部は学習に入っているっぽいですね。\n            - paragraph [ref=s1e1724]: 助かります！修正もこれからさせていただきます\n            - button "いいね" [ref=s1e1727]:\n                - img [ref=s1e1728]\n            - text: "2"\n        - article [ref=s1e1743]:\n            - link "masaishi" [ref=s1e1745]:\n                - /url: /masaishi\n                - img "masaishi" [ref=s1e1746]\n            - link "masaishi" [ref=s1e1747]:\n                - /url: /masaishi\n            - link "2025年1月29日 19時5分" [ref=s1e1748]:\n                - /url: "#comment-debff7de1aad74"\n                - tooltip "2025年1月29日 19時5分" [ref=s1e1749]: 2ヶ月前 2025年1月29日 19時5分\n            - button "メニューを開く" [ref=s1e1751]:\n                - img [ref=s1e1752]\n            - paragraph [ref=s1e1757]: ご丁寧にありがとうございます！\n            - button "いいね" [ref=s1e1760]:\n                - img [ref=s1e1761]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1777]\n    - article [ref=s1e1779]:\n        - link "RyotoTanaka" [ref=s1e1781]:\n            - /url: /ryototanaka\n            - img "RyotoTanaka" [ref=s1e1782]\n        - link "RyotoTanaka" [ref=s1e1783]:\n            - /url: /ryototanaka\n        - link "2025年1月30日 8時0分" [ref=s1e1784]:\n            - /url: "#comment-4ae639fe8d1892"\n            - tooltip "2025年1月30日 8時0分" [ref=s1e1785]: 2ヶ月前 2025年1月30日 8時0分\n        - button "メニューを開く" [ref=s1e1787]:\n            - img [ref=s1e1788]\n        - paragraph [ref=s1e1793]: 読み応えがありました。素晴らしい記事をありがとうございます！ 些細な事なのですが、DPOは Direct\n            Preference Optimization ではないでしょうか。\n        - button "いいね" [ref=s1e1797]:\n            - img [ref=s1e1798]\n        - text: "1"\n        - button "コメント 1" [ref=s1e1813]:\n            - img "コメント" [ref=s1e1814]\n            - text: "1"\n        - article [ref=s1e1818]:\n            - link "asap" [ref=s1e1820]:\n                - /url: /asap\n                - img "asap" [ref=s1e1821]\n            - link "asap" [ref=s1e1822]:\n                - /url: /asap\n            - link "2025年1月30日 13時15分" [ref=s1e1823]:\n                - /url: "#comment-ec7e829404f7bc"\n                - tooltip "2025年1月30日 13時15分" [ref=s1e1824]: 2ヶ月前 2025年1月30日 13時15分\n            - button "メニューを開く" [ref=s1e1826]:\n                - img [ref=s1e1827]\n            - paragraph [ref=s1e1832]: ほんとですね！間違えて入力してました ありがとうございます！\n            - button "いいね" [ref=s1e1836]:\n                - img [ref=s1e1837]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1853]\n    - text: ログインするとコメントできます\n    - button "Login" [ref=s1e1857]\n    - complementary [ref=s1e1858]:\n        - link "asap" [ref=s1e1863]:\n            - /url: /asap\n            - img "asap" [ref=s1e1864]\n        - link "asap" [ref=s1e1866]:\n            - /url: /asap\n        - button "フォロー" [ref=s1e1868]\n        - link "@personabb" [ref=s1e1869]:\n            - /url: https://github.com/personabb\n            - tooltip "@personabb" [ref=s1e1870]:\n                - img "GitHub" [ref=s1e1871]\n                - text: "@personabb"\n        - link "@asap2650" [ref=s1e1873]:\n            - /url: https://twitter.com/asap2650\n            - tooltip "@asap2650" [ref=s1e1874]:\n                - img "X(Twitter)" [ref=s1e1875]\n                - text: "@asap2650"\n        - link "yard.tips" [ref=s1e1881]:\n            - /url: https://yard.tips/asap2650\n            - tooltip "yard.tips" [ref=s1e1882]:\n                - img "リンク" [ref=s1e1883]\n                - text: yard.tips\n        - paragraph [ref=s1e1886]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n        - button "バッジを贈る" [ref=s1e1889]\n        - link "バッジを贈るとは" [ref=s1e1891]:\n            - /url: /faq#badges\n            - text: バッジを贈るとは\n            - img [ref=s1e1892]\n        - text: 目次\n        - list [ref=s1e1901]:\n            - listitem [ref=s1e1902]:\n                - link "はじめに" [ref=s1e1903]:\n                    - /url: "#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB"\n            - listitem [ref=s1e1904]:\n                - link "本論文の興味深いところ" [ref=s1e1905]:\n                    - /url: "#%E6%9C%AC%E8%AB%96%E6%96%87%E3%81%AE%E8%88%88%E5%91%B3%E6%B7%B1%E3%81%8\\\n                        4%E3%81%A8%E3%81%93%E3%82%8D"\n            - listitem [ref=s1e1906]:\n                - link "本論文の流れ" [ref=s1e1907]:\n                    - /url: "#%E6%9C%AC%E8%AB%96%E6%96%87%E3%81%AE%E6%B5%81%E3%82%8C"\n                - list [ref=s1e1908]:\n                    - listitem [ref=s1e1909]:\n                        - link "前提のモデル構造" [ref=s1e1910]:\n                            - /url: "#%E5%89%8D%E6%8F%90%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E6%A7%8B%E9%80%A\\\n                                0"\n                    - listitem [ref=s1e1911]:\n                        - link "DeepSeek-R1-Zeroを強化学習を用いて作る" [ref=s1e1912]:\n                            - /url: "#deepseek-r1-zero%E3%82%92%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E3%82%92%\\\n                                E7%94%A8%E3%81%84%E3%81%A6%E4%BD%9C%E3%82%8B"\n                    - listitem [ref=s1e1913]:\n                        - link "DeepSeek-R1をSFT+強化学習を用いて作る" [ref=s1e1914]:\n                            - /url: "#deepseek-r1%E3%82%92sft%2B%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E3%82%92\\\n                                %E7%94%A8%E3%81%84%E3%81%A6%E4%BD%9C%E3%82%8B"\n                    - listitem [ref=s1e1915]:\n                        - link "小型モデルへの蒸留を試す" [ref=s1e1916]:\n                            - /url: "#%E5%B0%8F%E5%9E%8B%E3%83%A2%E3%83%87%E3%83%AB%E3%81%B8%E3%81%AE%E8%92%B\\\n                                8%E7%95%99%E3%82%92%E8%A9%A6%E3%81%99"\n            - listitem [ref=s1e1917]:\n                - link "DeepSeek-R1-Zeroを作るまで" [ref=s1e1918]:\n                    - /url: "#deepseek-r1-zero%E3%82%92%E4%BD%9C%E3%82%8B%E3%81%BE%E3%81%A7"\n                - list [ref=s1e1919]:\n                    - listitem [ref=s1e1920]:\n                        - link "元々のアーキテクチャ" [ref=s1e1921]:\n                            - /url: "#%E5%85%83%E3%80%85%E3%81%AE%E3%82%A2%E3%83%BC%E3%82%AD%E3%83%86%E3%82%A\\\n                                F%E3%83%81%E3%83%A3"\n                    - listitem [ref=s1e1922]:\n                        - link "すごいぞ！強化学習" [ref=s1e1923]:\n                            - /url: "#%E3%81%99%E3%81%94%E3%81%84%E3%81%9E%EF%BC%81%E5%BC%B7%E5%8C%96%E5%AD%A\\\n                                6%E7%BF%92"\n                    - listitem [ref=s1e1924]:\n                        - link "そもそもLLMで強化学習とは？" [ref=s1e1925]:\n                            - /url: "#%E3%81%9D%E3%82%82%E3%81%9D%E3%82%82llm%E3%81%A7%E5%BC%B7%E5%8C%96%E5%A\\\n                                D%A6%E7%BF%92%E3%81%A8%E3%81%AF%EF%BC%9F"\n                    - listitem [ref=s1e1926]:\n                        - link "ZeroのSystemプロンプト" [ref=s1e1927]:\n                            - /url: "#zero%E3%81%AEsystem%E3%83%97%E3%83%AD%E3%83%B3%E3%83%97%E3%83%88"\n                    - listitem [ref=s1e1928]:\n                        - link "Zeroの「Aha Moment」" [ref=s1e1929]:\n                            - /url: "#zero%E3%81%AE%E3%80%8Caha-moment%E3%80%8D"\n                    - listitem [ref=s1e1930]:\n                        - link "DeepSeek-R1-Zeroのまとめ" [ref=s1e1931]:\n                            - /url: "#deepseek-r1-zero%E3%81%AE%E3%81%BE%E3%81%A8%E3%82%81"\n            - listitem [ref=s1e1932]:\n                - link "DeepSeek-R1への道" [ref=s1e1933]:\n                    - /url: "#deepseek-r1%E3%81%B8%E3%81%AE%E9%81%93"\n                - list [ref=s1e1934]:\n                    - listitem [ref=s1e1935]:\n                        - link "DeepSeek-R1-Zeroの課題" [ref=s1e1936]:\n                            - /url: "#deepseek-r1-zero%E3%81%AE%E8%AA%B2%E9%A1%8C"\n                    - listitem [ref=s1e1937]:\n                        - link "課題対処への道" [ref=s1e1938]:\n                            - /url: "#%E8%AA%B2%E9%A1%8C%E5%AF%BE%E5%87%A6%E3%81%B8%E3%81%AE%E9%81%93"\n                    - listitem [ref=s1e1939]:\n                        - link "DeepSeek-R1のまとめ" [ref=s1e1940]:\n                            - /url: "#deepseek-r1%E3%81%AE%E3%81%BE%E3%81%A8%E3%82%81"\n            - listitem [ref=s1e1941]:\n                - link "結果" [ref=s1e1942]:\n                    - /url: "#%E7%B5%90%E6%9E%9C"\n            - listitem [ref=s1e1943]:\n                - link "蒸留" [ref=s1e1944]:\n                    - /url: "#%E8%92%B8%E7%95%99"\n                - list [ref=s1e1945]:\n                    - listitem [ref=s1e1946]:\n                        - link "蒸留 vs 強化学習" [ref=s1e1947]:\n                            - /url: "#%E8%92%B8%E7%95%99-vs-%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92"\n            - listitem [ref=s1e1948]:\n                - link "今後の展望" [ref=s1e1949]:\n                    - /url: "#%E4%BB%8A%E5%BE%8C%E3%81%AE%E5%B1%95%E6%9C%9B"\n            - listitem [ref=s1e1950]:\n                - link "まとめ" [ref=s1e1951]:\n                    - /url: "#%E3%81%BE%E3%81%A8%E3%82%81"\n            - listitem [ref=s1e1952]:\n                - link "参考文献" [ref=s1e1953]:\n                    - /url: "#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"\n        - paragraph [ref=s1e1955]: Zennからのお知らせ\n        - article [ref=s1e1958]:\n            - link "第２回 AI Agent Hackathon with Google Cloud 第２回 AI Agent Hackathon with Google Cloud 4/14登録受付開始！" [ref=s1e1959]:\n                - /url: /hackathons/google-cloud-japan-ai-hackathon-vol2\n                - img "第２回 AI Agent Hackathon with Google Cloud" [ref=s1e1961]\n                - paragraph [ref=s1e1962]: 第２回 AI Agent Hackathon with Google Cloud 4/14登録受付開始！\n        - article [ref=s1e1963]:\n            - link "Zenncafe日比谷#2 生成AIで変わる仕事 Zenncafe日比谷#2 生成AIで変わる仕事" [ref=s1e1964]:\n                - /url: https://zenn.connpass.com/event/351131/\n                - img "Zenncafe日比谷#2 生成AIで変わる仕事" [ref=s1e1966]\n                - paragraph [ref=s1e1967]: Zenncafe日比谷#2 生成AIで変わる仕事\n- contentinfo [ref=s1e1969]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1973]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1974]\n    - paragraph [ref=s1e1982]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e1984]:\n        - heading "About" [level=4] [ref=s1e1985]\n        - list [ref=s1e1986]:\n            - listitem [ref=s1e1987]:\n                - link "Zennについて" [ref=s1e1989]:\n                    - /url: /about\n            - listitem [ref=s1e1990]:\n                - link "運営会社" [ref=s1e1991]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e1992]:\n                - link "お知らせ・リリース" [ref=s1e1993]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e1994]:\n                - link "イベント" [ref=s1e1996]:\n                    - /url: /events\n    - navigation [ref=s1e1997]:\n        - heading "Guides" [level=4] [ref=s1e1998]\n        - list [ref=s1e1999]:\n            - listitem [ref=s1e2000]:\n                - link "使い方" [ref=s1e2002]:\n                    - /url: /manual\n            - listitem [ref=s1e2003]:\n                - link "法人向けメニュー" [ref=s1e2005]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e2007]:\n                - link "Publication / Pro" [ref=s1e2009]:\n                    - /url: /publications\n            - listitem [ref=s1e2010]:\n                - link "よくある質問" [ref=s1e2012]:\n                    - /url: /faq\n    - navigation [ref=s1e2013]:\n        - heading "Links" [level=4] [ref=s1e2014]\n        - list [ref=s1e2015]:\n            - listitem [ref=s1e2016]:\n                - link "X(Twitter)" [ref=s1e2017]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e2018]:\n                - link "GitHub" [ref=s1e2019]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e2020]:\n                - link "メディアキット" [ref=s1e2022]:\n                    - /url: /mediakit\n    - navigation [ref=s1e2023]:\n        - heading "Legal" [level=4] [ref=s1e2024]\n        - list [ref=s1e2025]:\n            - listitem [ref=s1e2026]:\n                - link "利用規約" [ref=s1e2028]:\n                    - /url: /terms\n            - listitem [ref=s1e2029]:\n                - link "プライバシーポリシー" [ref=s1e2031]:\n                    - /url: /privacy\n            - listitem [ref=s1e2032]:\n                - link "特商法表記" [ref=s1e2034]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e2037]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e2038]\n- alert [ref=s1e2057]\n```\n', name='browser_navigate', tool_call_id='d85f8505-9c2a-4960-b17f-9ef88d983d9d'), AIMessage(content='まず、指定されたZennの著者のページにアクセスし、最もいいねが多い記事を特定しました。その結果、「DeepSeek-R1の論文読んだ？【勉強になるよ】」という記事が最も評価されていることがわかりました。\n\n次に、その記事の内容を詳しく調べました。このブログ記事は、DeepSeek-R1という大規模言語モデルに関する論文を解説したものです。記事は、論文の内容をわかりやすく説明することに重点を置いており、特に強化学習の概念や、DeepSeek-R1を開発する上で用いられたGRPO（Group Relative Policy Optimization）という手法について詳しく解説しています。\n\n記事が評価されている理由として、以下の点が考えられます。\n\n1.  **技術的な深さと分かりやすさの両立:** 大規模言語モデルや強化学習といった複雑なトピックを扱いながらも、専門知識がない読者にも理解できるように、かみ砕いた説明がなされています。\n2.  **論文の内容の網羅性:** DeepSeek-R1の論文の重要なポイントを漏れなくカバーしており、論文を読むための良い入門となっています。\n3.  **著者自身の考察:** 論文の内容を解説するだけでなく、著者自身の考察や解釈が加えられており、読者に新たな視点を提供しています。\n4.  **読者への配慮:** 記事の冒頭で、誤りがあるかもしれないことを断り、コメントでの指摘を歓迎するなど、読者とのコミュニケーションを重視する姿勢が見られます。\n5.  **参考文献の紹介:** 記事の最後に、強化学習や関連技術を学ぶための参考文献が紹介されており、読者の学習をサポートする姿勢が評価されていると考えられます。\n\nこれらの要素が組み合わさることで、この記事はDeepSeek-R1に関心のあるエンジニアにとって非常に価値のある情報源となり、高い評価を得ていると考えられます。', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-d5ac6129-cca3-4759-ac6a-eafe68db505f-0', usage_metadata={'input_tokens': 47987, 'output_tokens': 387, 'total_tokens': 48374, 'input_token_details': {'cache_read': 0}})]}
=================================
まず、指定されたZennの著者のページにアクセスし、最もいいねが多い記事を特定しました。その結果、「DeepSeek-R1の論文読んだ？【勉強になるよ】」という記事が最も評価されていることがわかりました。

次に、その記事の内容を詳しく調べました。このブログ記事は、DeepSeek-R1という大規模言語モデルに関する論文を解説したものです。記事は、論文の内容をわかりやすく説明することに重点を置いており、特に強化学習の概念や、DeepSeek-R1を開発する上で用いられたGRPO（Group Relative Policy Optimization）という手法について詳しく解説しています。

記事が評価されている理由として、以下の点が考えられます。

1.  **技術的な深さと分かりやすさの両立:** 大規模言語モデルや強化学習といった複雑なトピックを扱いながらも、専門知識がない読者にも理解できるように、かみ砕いた説明がなされています。
2.  **論文の内容の網羅性:** DeepSeek-R1の論文の重要なポイントを漏れなくカバーしており、論文を読むための良い入門となっています。
3.  **著者自身の考察:** 論文の内容を解説するだけでなく、著者自身の考察や解釈が加えられており、読者に新たな視点を提供しています。
4.  **読者への配慮:** 記事の冒頭で、誤りがあるかもしれないことを断り、コメントでの指摘を歓迎するなど、読者とのコミュニケーションを重視する姿勢が見られます。
5.  **参考文献の紹介:** 記事の最後に、強化学習や関連技術を学ぶための参考文献が紹介されており、読者の学習をサポートする姿勢が評価されていると考えられます。

これらの要素が組み合わさることで、この記事はDeepSeek-R1に関心のあるエンジニアにとって非常に価値のある情報源となり、高い評価を得ていると考えられます。








入力してください: LLMに強化学習ってどういうこと？GRPOって何？
response:  {'messages': [HumanMessage(content=[{'type': 'text', 'text': '「https://zenn.dev/asap」この執筆者のもっともいいねがついている記事を調査して、その記事の中身を取得し、その記事がなぜ評価されているのかを分析してください。'}], additional_kwargs={}, response_metadata={}), AIMessage(content='zenn.devのasap氏の最もいいねが多い記事を調べ、その内容を取得し、評価されている理由を分析する、というご要望ですね。\n\nまず、zenn.dev/asapにアクセスして、最もいいねが多い記事のタイトルとURLを特定する必要があります。その後、そのURLにアクセスして記事の内容を取得します。最後に、記事の内容から評価されている理由を推測します。\n\nでは、始めましょう。\n\n1.  zenn.dev/asapにアクセスし、最もいいねが多い記事の情報を取得します。', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-989fe5ca-2637-4757-878f-db26e13526b8-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap'}, 'id': '58fad86e-0b01-4fc6-a8cc-57deb5c48b41', 'type': 'tool_call'}], usage_metadata={'input_tokens': 778, 'output_tokens': 128, 'total_tokens': 906, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap\n\n- Page URL: https://zenn.dev/asap\n- Page Title: asapさんの記事一覧 | Zenn\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- banner [ref=s1e32]:\n    - img "asap" [ref=s1e36]\n    - heading "asap" [level=1] [ref=s1e39]\n    - button "フォロー" [ref=s1e41]\n    - paragraph [ref=s1e43]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n    - button "2,214 Likes" [ref=s1e46]\n    - button "310 Followers" [ref=s1e48]\n    - link "@personabb" [ref=s1e51]:\n        - /url: https://github.com/personabb\n        - tooltip "@personabb" [ref=s1e52]:\n            - img "GitHub" [ref=s1e53]\n            - text: "@personabb"\n    - link "@asap2650" [ref=s1e55]:\n        - /url: https://twitter.com/asap2650\n        - tooltip "@asap2650" [ref=s1e56]:\n            - img "X(Twitter)" [ref=s1e57]\n            - text: "@asap2650"\n    - link "yard.tips" [ref=s1e63]:\n        - /url: https://yard.tips/asap2650\n        - tooltip "yard.tips" [ref=s1e64]:\n            - img [ref=s1e65]\n            - text: yard.tips\n    - link "RSS" [ref=s1e67]:\n        - /url: /asap/feed\n        - tooltip "RSS" [ref=s1e68]:\n            - img [ref=s1e69]\n            - text: RSS\n- link "Articles 72" [ref=s1e80]:\n    - /url: /asap\n- link "Scraps 5" [ref=s1e82]:\n    - /url: /asap?tab=scraps\n- link "Comments" [ref=s1e84]:\n    - /url: /asap?tab=comments\n- article [ref=s1e89]:\n    - link "tech" [ref=s1e90]:\n        - /url: /tech-or-idea\n    - link "❄️ LangChainでGemini-2.0 APIの画像編集を実行する方法 6日前 いいねされた数 18" [ref=s1e91]:\n        - /url: /asap/articles/1d5e6ec06c380f\n        - text: ❄️\n        - heading "LangChainでGemini-2.0 APIの画像編集を実行する方法" [level=3] [ref=s1e95]\n        - time [ref=s1e98]: 6日前\n        - img "いいねされた数" [ref=s1e100]\n        - text: "18"\n- article [ref=s1e102]:\n    - link "tech" [ref=s1e103]:\n        - /url: /tech-or-idea\n    - link "🙌 完全自律型AIエージェントOSS【LangManus】完全に理解した！ 13日前 いいねされた数 48" [ref=s1e104]:\n        - /url: /asap/articles/b27e832c2a49b7\n        - text: 🙌\n        - heading "完全自律型AIエージェントOSS【LangManus】完全に理解した！" [level=3] [ref=s1e108]\n        - time [ref=s1e111]: 13日前\n        - img "いいねされた数" [ref=s1e113]\n        - text: "48"\n- article [ref=s1e115]:\n    - link "tech" [ref=s1e116]:\n        - /url: /tech-or-idea\n    - link "💫 LangChainでOpenAI gpt-4o / o1にPDFを入力する方法 20日前 いいねされた数 9" [ref=s1e117]:\n        - /url: /asap/articles/74885fab863c9e\n        - text: 💫\n        - heading "LangChainでOpenAI gpt-4o / o1にPDFを入力する方法" [level=3]\n          [ref=s1e121]\n        - time [ref=s1e124]: 20日前\n        - img "いいねされた数" [ref=s1e126]\n        - text: "9"\n- article [ref=s1e128]:\n    - link "tech" [ref=s1e129]:\n        - /url: /tech-or-idea\n    - link "👣 Ollama + LangChainで画像を入力する方法【Gemma3 27B】 27日前 いいねされた数 14" [ref=s1e130]:\n        - /url: /asap/articles/18b110d551b811\n        - text: 👣\n        - heading "Ollama + LangChainで画像を入力する方法【Gemma3 27B】" [level=3]\n          [ref=s1e134]\n        - time [ref=s1e137]: 27日前\n        - img "いいねされた数" [ref=s1e139]\n        - text: "14"\n- article [ref=s1e141]:\n    - link "tech" [ref=s1e142]:\n        - /url: /tech-or-idea\n    - link "🦄 Google CloudのVector StoreでRAG Agentsを構築する【Big Querty編】 28日前 いいねされた数 12" [ref=s1e143]:\n        - /url: /asap/articles/3d258861c6c528\n        - text: 🦄\n        - heading "Google CloudのVector StoreでRAG Agentsを構築する【Big Querty編】"\n          [level=3] [ref=s1e147]\n        - time [ref=s1e150]: 28日前\n        - img "いいねされた数" [ref=s1e152]\n        - text: "12"\n- article [ref=s1e154]:\n    - link "tech" [ref=s1e155]:\n        - /url: /tech-or-idea\n    - link "🎶 最新音楽生成AI 【DiffRhythm】をGoogle Colabで動かせるようにしました！（簡易版） 1ヶ月前 いいねされた数 39" [ref=s1e156]:\n        - /url: /asap/articles/4edc4a71de8961\n        - text: 🎶\n        - heading "最新音楽生成AI 【DiffRhythm】をGoogle Colabで動かせるようにしました！（簡易版）"\n          [level=3] [ref=s1e160]\n        - time [ref=s1e163]: 1ヶ月前\n        - img "いいねされた数" [ref=s1e165]\n        - text: "39"\n- article [ref=s1e167]:\n    - link "tech" [ref=s1e168]:\n        - /url: /tech-or-idea\n    - link "🐤 LangGraphのTool Callingを利用して、RAG Agentsを構築する（後編） 1ヶ月前 いいねされた数 12" [ref=s1e169]:\n        - /url: /asap/articles/c58cfab9c1362a\n        - text: 🐤\n        - heading "LangGraphのTool Callingを利用して、RAG Agentsを構築する（後編）" [level=3]\n          [ref=s1e173]\n        - time [ref=s1e176]: 1ヶ月前\n        - img "いいねされた数" [ref=s1e178]\n        - text: "12"\n- article [ref=s1e180]:\n    - link "idea" [ref=s1e181]:\n        - /url: /tech-or-idea\n    - link "🏆 イベントに登壇してきた話【資料もこちら】 1ヶ月前 いいねされた数 20" [ref=s1e182]:\n        - /url: /asap/articles/89d2a13bdc75a3\n        - text: 🏆\n        - heading "イベントに登壇してきた話【資料もこちら】" [level=3] [ref=s1e186]\n        - time [ref=s1e189]: 1ヶ月前\n        - img "いいねされた数" [ref=s1e191]\n        - text: "20"\n- article [ref=s1e193]:\n    - link "tech" [ref=s1e194]:\n        - /url: /tech-or-idea\n    - link "👌 Grok API（Grok 2）で遊んでみた！【Elyza Tasks 100】 2ヶ月前 いいねされた数 11" [ref=s1e195]:\n        - /url: /asap/articles/6aa01bde32163d\n        - text: 👌\n        - heading "Grok API（Grok 2）で遊んでみた！【Elyza Tasks 100】" [level=3]\n          [ref=s1e199]\n        - time [ref=s1e202]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e204]\n        - text: "11"\n- article [ref=s1e206]:\n    - link "tech" [ref=s1e207]:\n        - /url: /tech-or-idea\n    - link "📑 Multi-head Latent Attention (MLA)の詳細【be absorbedとは？】 2ヶ月前 いいねされた数 22" [ref=s1e208]:\n        - /url: /asap/articles/be3d4b60d8ac92\n        - text: 📑\n        - heading "Multi-head Latent Attention (MLA)の詳細【be absorbedとは？】"\n          [level=3] [ref=s1e212]\n        - time [ref=s1e215]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e217]\n        - text: "22"\n- article [ref=s1e219]:\n    - link "tech" [ref=s1e220]:\n        - /url: /tech-or-idea\n    - link "⛳ LangGraphのTool Callingを利用して、RAG Agentsを構築する（前編） 2ヶ月前 いいねされた数 46" [ref=s1e221]:\n        - /url: /asap/articles/2ad7eb5e7560cf\n        - text: ⛳\n        - heading "LangGraphのTool Callingを利用して、RAG Agentsを構築する（前編）" [level=3]\n          [ref=s1e225]\n        - time [ref=s1e228]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e230]\n        - text: "46"\n- article [ref=s1e232]:\n    - link "tech" [ref=s1e233]:\n        - /url: /tech-or-idea\n    - link "🫠 【LangChain】で【Transformer】を利用する場合に【chat_template】を設定する方法【エラー対処記事】 2ヶ月前 いいねされた数 13" [ref=s1e234]:\n        - /url: /asap/articles/985f3580ea476d\n        - text: 🫠\n        - heading\n          "【LangChain】で【Transformer】を利用する場合に【chat_template】を設定する方法【エラー対処記事】"\n          [level=3] [ref=s1e238]\n        - time [ref=s1e241]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e243]\n        - text: "13"\n- article [ref=s1e245]:\n    - link "tech" [ref=s1e246]:\n        - /url: /tech-or-idea\n    - link "🌊 採点理由を出力しながらローカルLLMを評価する【Elyza Tasks 100】 2ヶ月前 いいねされた数 26" [ref=s1e247]:\n        - /url: /asap/articles/3a99c34fc1d4cb\n        - text: 🌊\n        - heading "採点理由を出力しながらローカルLLMを評価する【Elyza Tasks 100】" [level=3]\n          [ref=s1e251]\n        - time [ref=s1e254]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e256]\n        - text: "26"\n- article [ref=s1e258]:\n    - link "tech" [ref=s1e259]:\n        - /url: /tech-or-idea\n    - link "📑 DeepSeek-R1の論文読んだ？【勉強になるよ】 2ヶ月前 いいねされた数 921" [ref=s1e260]:\n        - /url: /asap/articles/34237ad87f8511\n        - text: 📑\n        - heading "DeepSeek-R1の論文読んだ？【勉強になるよ】" [level=3] [ref=s1e264]\n        - time [ref=s1e267]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e269]\n        - text: "921"\n- article [ref=s1e271]:\n    - link "tech" [ref=s1e272]:\n        - /url: /tech-or-idea\n    - link "📖 VAEって結局何者なの？ 3ヶ月前 いいねされた数 104" [ref=s1e273]:\n        - /url: /asap/articles/6caa9043276424\n        - text: 📖\n        - heading "VAEって結局何者なの？" [level=3] [ref=s1e277]\n        - time [ref=s1e280]: 3ヶ月前\n        - img "いいねされた数" [ref=s1e282]\n        - text: "104"\n- article [ref=s1e284]:\n    - link "tech" [ref=s1e285]:\n        - /url: /tech-or-idea\n    - link "🦔 初めてのSageMaker AIでSFTTrainerによるLLMのフルパラメータファインチューニングを試す 3ヶ月前 いいねされた数 9" [ref=s1e286]:\n        - /url: /asap/articles/3f31b77010624c\n        - text: 🦔\n        - heading "初めてのSageMaker AIでSFTTrainerによるLLMのフルパラメータファインチューニングを試す"\n          [level=3] [ref=s1e290]\n        - time [ref=s1e293]: 3ヶ月前\n        - img "いいねされた数" [ref=s1e295]\n        - text: "9"\n- article [ref=s1e297]:\n    - link "tech" [ref=s1e298]:\n        - /url: /tech-or-idea\n    - link "😺 【動画翻訳】OpenAIの研究者によるo1やLLMに対するディスカッションの翻訳 3ヶ月前 いいねされた数 12" [ref=s1e299]:\n        - /url: /asap/articles/2d2832bf3288aa\n        - text: 😺\n        - heading "【動画翻訳】OpenAIの研究者によるo1やLLMに対するディスカッションの翻訳" [level=3]\n          [ref=s1e303]\n        - time [ref=s1e306]: 3ヶ月前\n        - img "いいねされた数" [ref=s1e308]\n        - text: "12"\n- article [ref=s1e310]:\n    - link "tech" [ref=s1e311]:\n        - /url: /tech-or-idea\n    - link "🧚\u200d♀️ AIでオリジナル楽曲を作り、自作のかわいい声で歌ってもらう方法 3ヶ月前 いいねされた数 28" [ref=s1e312]:\n        - /url: /asap/articles/886af0fe48dda3\n        - text: 🧚\u200d♀️\n        - heading "AIでオリジナル楽曲を作り、自作のかわいい声で歌ってもらう方法" [level=3] [ref=s1e316]\n        - time [ref=s1e319]: 3ヶ月前\n        - img "いいねされた数" [ref=s1e321]\n        - text: "28"\n- article [ref=s1e323]:\n    - link "tech" [ref=s1e324]:\n        - /url: /tech-or-idea\n    - link "🎮 Google ColabでSeed-VCによる声色変換を試す 4ヶ月前 いいねされた数 27" [ref=s1e325]:\n        - /url: /asap/articles/9c54aef739a6ed\n        - text: 🎮\n        - heading "Google ColabでSeed-VCによる声色変換を試す" [level=3] [ref=s1e329]\n        - time [ref=s1e332]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e334]\n        - text: "27"\n- article [ref=s1e336]:\n    - link "tech" [ref=s1e337]:\n        - /url: /tech-or-idea\n    - link "🐡 LLMの性能における重要補題【Johnson–Lindenstrauss lemma】のお気持ち〜Attentionの数式理解を添えて〜 4ヶ月前 いいねされた数 21" [ref=s1e338]:\n        - /url: /asap/articles/6bdf89d8e964ec\n        - text: 🐡\n        - heading "LLMの性能における重要補題【Johnson–Lindenstrauss\n          lemma】のお気持ち〜Attentionの数式理解を添えて〜" [level=3] [ref=s1e342]\n        - time [ref=s1e345]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e347]\n        - text: "21"\n- article [ref=s1e349]:\n    - link "tech" [ref=s1e350]:\n        - /url: /tech-or-idea\n    - link "📏 【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】 4ヶ月前 いいねされた数 50" [ref=s1e351]:\n        - /url: /asap/articles/ea9f42edff6573\n        - text: 📏\n        - heading "【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】"\n          [level=3] [ref=s1e355]\n        - time [ref=s1e358]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e360]\n        - text: "50"\n- article [ref=s1e362]:\n    - link "tech" [ref=s1e363]:\n        - /url: /tech-or-idea\n    - link "🎙️ 高品質合成音声モデル【Style-Bert-VITS2】をGoogle Colabで学習し、【Aivis Speech】で利用する方法 4ヶ月前 いいねされた数 24" [ref=s1e364]:\n        - /url: /asap/articles/5c437ce2943ed7\n        - text: 🎙️\n        - heading "高品質合成音声モデル【Style-Bert-VITS2】をGoogle Colabで学習し、【Aivis\n          Speech】で利用する方法" [level=3] [ref=s1e368]\n        - time [ref=s1e371]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e373]\n        - text: "24"\n- article [ref=s1e375]:\n    - link "tech" [ref=s1e376]:\n        - /url: /tech-or-idea\n    - link "🎄 【LangChain】LCEL ステップバイステップ ハンズオン 〜なろう短編小説の自動生成＆評価・改善を行う「AIエージェント」を添えて〜 4ヶ月前 いいねされた数 19" [ref=s1e377]:\n        - /url: /asap/articles/aa587ad8d76105\n        - text: 🎄\n        - heading "【LangChain】LCEL ステップバイステップ ハンズオン\n          〜なろう短編小説の自動生成＆評価・改善を行う「AIエージェント」を添えて〜" [level=3] [ref=s1e381]\n        - time [ref=s1e384]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e386]\n        - text: "19"\n- article [ref=s1e388]:\n    - link "tech" [ref=s1e389]:\n        - /url: /tech-or-idea\n    - link "💻 LangGraphによる「AIエージェントWebアプリ」を作成する【Next.js】 4ヶ月前 いいねされた数 45" [ref=s1e390]:\n        - /url: /asap/articles/a350fe4ac00343\n        - text: 💻\n        - heading "LangGraphによる「AIエージェントWebアプリ」を作成する【Next.js】" [level=3]\n          [ref=s1e394]\n        - time [ref=s1e397]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e399]\n        - text: "45"\n- article [ref=s1e401]:\n    - link "tech" [ref=s1e402]:\n        - /url: /tech-or-idea\n    - link "🤔 LangGraphにて入力情報が不十分な場合に、ユーザへ追加質問をするAIエージェントを作る方法 5ヶ月前 いいねされた数 41" [ref=s1e403]:\n        - /url: /asap/articles/03548f844efe6f\n        - text: 🤔\n        - heading "LangGraphにて入力情報が不十分な場合に、ユーザへ追加質問をするAIエージェントを作る方法" [level=3]\n          [ref=s1e407]\n        - time [ref=s1e410]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e412]\n        - text: "41"\n- article [ref=s1e414]:\n    - link "tech" [ref=s1e415]:\n        - /url: /tech-or-idea\n    - link "🤖 高品質動画生成AI【Allegro】をローカルで実行する【VRAM12GB】 5ヶ月前 いいねされた数 10" [ref=s1e416]:\n        - /url: /asap/articles/dac57655eb0314\n        - text: 🤖\n        - heading "高品質動画生成AI【Allegro】をローカルで実行する【VRAM12GB】" [level=3] [ref=s1e420]\n        - time [ref=s1e423]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e425]\n        - text: "10"\n- article [ref=s1e427]:\n    - link "tech" [ref=s1e428]:\n        - /url: /tech-or-idea\n    - link "🌊 フレーム補間技術【RIFE】を利用してCogVideoXのFPSを改善しよう 5ヶ月前 いいねされた数 2" [ref=s1e429]:\n        - /url: /asap/articles/dbeef72b7bfdc3\n        - text: 🌊\n        - heading "フレーム補間技術【RIFE】を利用してCogVideoXのFPSを改善しよう" [level=3] [ref=s1e433]\n        - time [ref=s1e436]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e438]\n        - text: "2"\n- article [ref=s1e440]:\n    - link "tech" [ref=s1e441]:\n        - /url: /tech-or-idea\n    - link "🔥 LangGraphを使ってAIエージェントを実装する方法 5ヶ月前 いいねされた数 34" [ref=s1e442]:\n        - /url: /asap/articles/5da9cf01703a47\n        - text: 🔥\n        - heading "LangGraphを使ってAIエージェントを実装する方法" [level=3] [ref=s1e446]\n        - time [ref=s1e449]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e451]\n        - text: "34"\n- article [ref=s1e453]:\n    - link "tech" [ref=s1e454]:\n        - /url: /tech-or-idea\n    - link "🌈 動画生成AI【CogVideoX】をローカルで実行する 5ヶ月前 いいねされた数 9" [ref=s1e455]:\n        - /url: /asap/articles/a4077e8b9ddf1a\n        - text: 🌈\n        - heading "動画生成AI【CogVideoX】をローカルで実行する" [level=3] [ref=s1e459]\n        - time [ref=s1e462]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e464]\n        - text: "9"\n- article [ref=s1e466]:\n    - link "tech" [ref=s1e467]:\n        - /url: /tech-or-idea\n    - link "🛎️ ローカルでDIfyを使ってAIエージェントを実装する方法 5ヶ月前 いいねされた数 55" [ref=s1e468]:\n        - /url: /asap/articles/a35254020fbbae\n        - text: 🛎️\n        - heading "ローカルでDIfyを使ってAIエージェントを実装する方法" [level=3] [ref=s1e472]\n        - time [ref=s1e475]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e477]\n        - text: "55"\n- article [ref=s1e479]:\n    - link "tech" [ref=s1e480]:\n        - /url: /tech-or-idea\n    - link "💫 高品質動画生成AI【Pyramid Flow】をローカルで実行する 6ヶ月前 いいねされた数 12" [ref=s1e481]:\n        - /url: /asap/articles/e1880903625cb5\n        - text: 💫\n        - heading "高品質動画生成AI【Pyramid Flow】をローカルで実行する" [level=3] [ref=s1e485]\n        - time [ref=s1e488]: 6ヶ月前\n        - img "いいねされた数" [ref=s1e490]\n        - text: "12"\n- article [ref=s1e492]:\n    - link "tech" [ref=s1e493]:\n        - /url: /tech-or-idea\n    - link "📖 不偏分散で N-1 で割る理由 を自力で導出する 6ヶ月前 いいねされた数 5" [ref=s1e494]:\n        - /url: /asap/articles/f0cba34c2ac2ac\n        - text: 📖\n        - heading "不偏分散で N-1 で割る理由 を自力で導出する" [level=3] [ref=s1e498]\n        - time [ref=s1e501]: 6ヶ月前\n        - img "いいねされた数" [ref=s1e503]\n        - text: "5"\n- article [ref=s1e505]:\n    - link "tech" [ref=s1e506]:\n        - /url: /tech-or-idea\n    - link "✍️ 勾配降下法 ・ 誤差逆伝播法 を自力で導出する 6ヶ月前 いいねされた数 25" [ref=s1e507]:\n        - /url: /asap/articles/643f604d444813\n        - text: ✍️\n        - heading "勾配降下法 ・ 誤差逆伝播法 を自力で導出する" [level=3] [ref=s1e511]\n        - time [ref=s1e514]: 6ヶ月前\n        - img "いいねされた数" [ref=s1e516]\n        - text: "25"\n- article [ref=s1e518]:\n    - link "tech" [ref=s1e519]:\n        - /url: /tech-or-idea\n    - link "💰 Open AI Realtime APIで、会話履歴を削除して高額請求を回避したい！【Pythonサンプルコード】 2024/10/07 いいねされた数 25" [ref=s1e520]:\n        - /url: /asap/articles/af07fcedbbef61\n        - text: 💰\n        - heading "Open AI Realtime APIで、会話履歴を削除して高額請求を回避したい！【Pythonサンプルコード】"\n          [level=3] [ref=s1e524]\n        - time [ref=s1e527]: 2024/10/07\n        - img "いいねされた数" [ref=s1e529]\n        - text: "25"\n- article [ref=s1e531]:\n    - link "tech" [ref=s1e532]:\n        - /url: /tech-or-idea\n    - link "✈️ Open AI Realtime APIのPythonサンプルコードで、AI発話にカットイン（割り込み）する方法 2024/10/06 いいねされた数 24" [ref=s1e533]:\n        - /url: /asap/articles/563500af4649da\n        - text: ✈️\n        - heading "Open AI Realtime APIのPythonサンプルコードで、AI発話にカットイン（割り込み）する方法"\n          [level=3] [ref=s1e537]\n        - time [ref=s1e540]: 2024/10/06\n        - img "いいねされた数" [ref=s1e542]\n        - text: "24"\n- article [ref=s1e544]:\n    - link "tech" [ref=s1e545]:\n        - /url: /tech-or-idea\n    - link "🥰 Open AI Realtime APIのPythonサンプルコードを作成 2024/10/04 いいねされた数 79" [ref=s1e546]:\n        - /url: /asap/articles/4368fd306b592a\n        - text: 🥰\n        - heading "Open AI Realtime APIのPythonサンプルコードを作成" [level=3] [ref=s1e550]\n        - time [ref=s1e553]: 2024/10/04\n        - img "いいねされた数" [ref=s1e555]\n        - text: "79"\n- article [ref=s1e557]:\n    - link "tech" [ref=s1e558]:\n        - /url: /tech-or-idea\n    - link "📗 主成分分析を自力で導出する 2024/10/03 いいねされた数 6" [ref=s1e559]:\n        - /url: /asap/articles/ff8f34d19ca6a4\n        - text: 📗\n        - heading "主成分分析を自力で導出する" [level=3] [ref=s1e563]\n        - time [ref=s1e566]: 2024/10/03\n        - img "いいねされた数" [ref=s1e568]\n        - text: "6"\n- article [ref=s1e570]:\n    - link "tech" [ref=s1e571]:\n        - /url: /tech-or-idea\n    - link "😍 FLUX.1-dev で ControlNet-depth と LoRA【QTCanimatedFeeling-FLUX】 を試す 2024/10/02 いいねされた数 3" [ref=s1e572]:\n        - /url: /asap/articles/9192c2363dd96a\n        - text: 😍\n        - heading "FLUX.1-dev で ControlNet-depth と LoRA【QTCanimatedFeeling-FLUX】\n          を試す" [level=3] [ref=s1e576]\n        - time [ref=s1e579]: 2024/10/02\n        - img "いいねされた数" [ref=s1e581]\n        - text: "3"\n- article [ref=s1e583]:\n    - link "tech" [ref=s1e584]:\n        - /url: /tech-or-idea\n    - link "📗 拡散モデルの理論について（拡散過程の条件付き確率・逆拡散過程のスケジューラ） 2024/09/23 いいねされた数 8" [ref=s1e585]:\n        - /url: /asap/articles/8aaa896a02f168\n        - text: 📗\n        - heading "拡散モデルの理論について（拡散過程の条件付き確率・逆拡散過程のスケジューラ）" [level=3] [ref=s1e589]\n        - time [ref=s1e592]: 2024/09/23\n        - img "いいねされた数" [ref=s1e594]\n        - text: "8"\n- article [ref=s1e596]:\n    - link "tech" [ref=s1e597]:\n        - /url: /tech-or-idea\n    - link "📖 分散共分散行列の魅力 2024/09/20 いいねされた数 14" [ref=s1e598]:\n        - /url: /asap/articles/2269cd74c5ddc2\n        - text: 📖\n        - heading "分散共分散行列の魅力" [level=3] [ref=s1e602]\n        - time [ref=s1e605]: 2024/09/20\n        - img "いいねされた数" [ref=s1e607]\n        - text: "14"\n- article [ref=s1e609]:\n    - link "tech" [ref=s1e610]:\n        - /url: /tech-or-idea\n    - link "😍 超強力汎用ControlNet「AnyTest v4」を利用する方法【diffusers】 2024/09/17 いいねされた数 9" [ref=s1e611]:\n        - /url: /asap/articles/c08dffa41bec03\n        - text: 😍\n        - heading "超強力汎用ControlNet「AnyTest v4」を利用する方法【diffusers】" [level=3]\n          [ref=s1e615]\n        - time [ref=s1e618]: 2024/09/17\n        - img "いいねされた数" [ref=s1e620]\n        - text: "9"\n- article [ref=s1e622]:\n    - link "tech" [ref=s1e623]:\n        - /url: /tech-or-idea\n    - link "🌍 FLUX.1-devでControlNetを利用したら画像の質が下がったのでImg2Imgを使って改善してみた【Diffusers】 2024/09/13 いいねされた数 2" [ref=s1e624]:\n        - /url: /asap/articles/912d6de879d006\n        - text: 🌍\n        - heading\n          "FLUX.1-devでControlNetを利用したら画像の質が下がったのでImg2Imgを使って改善してみた【Diffusers】"\n          [level=3] [ref=s1e628]\n        - time [ref=s1e631]: 2024/09/13\n        - img "いいねされた数" [ref=s1e633]\n        - text: "2"\n- article [ref=s1e635]:\n    - link "tech" [ref=s1e636]:\n        - /url: /tech-or-idea\n    - link "🎮 【Next.js】ポートレートの表情を編集する自作UIを作ってみた【ComfyUI-AdvancedLivePortrait】 2024/09/11 いいねされた数 8" [ref=s1e637]:\n        - /url: /asap/articles/c49551ff14d5e9\n        - text: 🎮\n        - heading\n          "【Next.js】ポートレートの表情を編集する自作UIを作ってみた【ComfyUI-AdvancedLivePortrait】"\n          [level=3] [ref=s1e641]\n        - time [ref=s1e644]: 2024/09/11\n        - img "いいねされた数" [ref=s1e646]\n        - text: "8"\n- article [ref=s1e648]:\n    - link "tech" [ref=s1e649]:\n        - /url: /tech-or-idea\n    - link "📖 ComfyUI-AdvancedLivePortraitを使ってみる（No WebUI）（Google Colab） 2024/09/10 いいねされた数 7" [ref=s1e650]:\n        - /url: /asap/articles/c0a1b82a10f0e2\n        - text: 📖\n        - heading "ComfyUI-AdvancedLivePortraitを使ってみる（No WebUI）（Google Colab）"\n          [level=3] [ref=s1e654]\n        - time [ref=s1e657]: 2024/09/10\n        - img "いいねされた数" [ref=s1e659]\n        - text: "7"\n- article [ref=s1e661]:\n    - link "tech" [ref=s1e662]:\n        - /url: /tech-or-idea\n    - link "⛳ websocketsでリアルタイム音声処理 2024/09/06 いいねされた数 9" [ref=s1e663]:\n        - /url: /asap/articles/47f839550e5535\n        - text: ⛳\n        - heading "websocketsでリアルタイム音声処理" [level=3] [ref=s1e667]\n        - time [ref=s1e670]: 2024/09/06\n        - img "いいねされた数" [ref=s1e672]\n        - text: "9"\n- article [ref=s1e674]:\n    - link "tech" [ref=s1e675]:\n        - /url: /tech-or-idea\n    - link "😍 生成AIをGoogle Colabで簡単に 【FLUX.1-dev + ControlNet + LoRA】【Diffusers】 2024/08/31 いいねされた数 9" [ref=s1e676]:\n        - /url: /asap/articles/e4c199cecf1836\n        - text: 😍\n        - heading "生成AIをGoogle Colabで簡単に 【FLUX.1-dev + ControlNet +\n          LoRA】【Diffusers】" [level=3] [ref=s1e680]\n        - time [ref=s1e683]: 2024/08/31\n        - img "いいねされた数" [ref=s1e685]\n        - text: "9"\n- article [ref=s1e687]:\n    - link "tech" [ref=s1e688]:\n        - /url: /tech-or-idea\n    - link "👋 Style-Bert-VITS2のAPI化 2024/08/28 いいねされた数 6" [ref=s1e689]:\n        - /url: /asap/articles/12b0f31625a90f\n        - text: 👋\n        - heading "Style-Bert-VITS2のAPI化" [level=3] [ref=s1e693]\n        - time [ref=s1e696]: 2024/08/28\n        - img "いいねされた数" [ref=s1e698]\n        - text: "6"\n- article [ref=s1e700]:\n    - link "tech" [ref=s1e701]:\n        - /url: /tech-or-idea\n    - link "😊 【超簡単】1枚の画像と音声から口パク動画を作りたい！【EchoMimic】 2024/08/25 いいねされた数 6" [ref=s1e702]:\n        - /url: /asap/articles/db1ed88f291b98\n        - text: 😊\n        - heading "【超簡単】1枚の画像と音声から口パク動画を作りたい！【EchoMimic】" [level=3] [ref=s1e706]\n        - time [ref=s1e709]: 2024/08/25\n        - img "いいねされた数" [ref=s1e711]\n        - text: "6"\n- article [ref=s1e713]:\n    - link "tech" [ref=s1e714]:\n        - /url: /tech-or-idea\n    - link "✍️ 【Next.js】生成AIによる「お絵描きサポートWebアプリ」を作りたい！【全くわからない人】 2024/08/20 いいねされた数 8" [ref=s1e715]:\n        - /url: /asap/articles/8f51ffd4fc29b8\n        - text: ✍️\n        - heading "【Next.js】生成AIによる「お絵描きサポートWebアプリ」を作りたい！【全くわからない人】" [level=3]\n          [ref=s1e719]\n        - time [ref=s1e722]: 2024/08/20\n        - img "いいねされた数" [ref=s1e724]\n        - text: "8"\n- article [ref=s1e726]:\n    - link "tech" [ref=s1e727]:\n        - /url: /tech-or-idea\n    - link "🔥 GPT-SoVITS ver2を学習する【WebUI使わない】 2024/08/15 いいねされた数 8" [ref=s1e728]:\n        - /url: /asap/articles/e64c6fce28e0f0\n        - text: 🔥\n        - heading "GPT-SoVITS ver2を学習する【WebUI使わない】" [level=3] [ref=s1e732]\n        - time [ref=s1e735]: 2024/08/15\n        - img "いいねされた数" [ref=s1e737]\n        - text: "8"\n- article [ref=s1e739]:\n    - link "tech" [ref=s1e740]:\n        - /url: /tech-or-idea\n    - link "🔥 GPT-soVITS ver2をモジュールとして動かす 2024/08/08 いいねされた数 9" [ref=s1e741]:\n        - /url: /asap/articles/964a7d863d8896\n        - text: 🔥\n        - heading "GPT-soVITS ver2をモジュールとして動かす" [level=3] [ref=s1e745]\n        - time [ref=s1e748]: 2024/08/08\n        - img "いいねされた数" [ref=s1e750]\n        - text: "9"\n- article [ref=s1e752]:\n    - link "tech" [ref=s1e753]:\n        - /url: /tech-or-idea\n    - link "💯 拡散モデルの理論って面白いよね 2024/08/07 いいねされた数 87" [ref=s1e754]:\n        - /url: /asap/articles/4092ab60570b05\n        - text: 💯\n        - heading "拡散モデルの理論って面白いよね" [level=3] [ref=s1e758]\n        - time [ref=s1e761]: 2024/08/07\n        - img "いいねされた数" [ref=s1e763]\n        - text: "87"\n- article [ref=s1e765]:\n    - link "tech" [ref=s1e766]:\n        - /url: /tech-or-idea\n    - link "🙆\u200d♀️ 画像生成AIの生成途中経過を可視化したい！【Diffusers】 2024/08/06 いいねされた数 4" [ref=s1e767]:\n        - /url: /asap/articles/7940b17be86da7\n        - text: 🙆\u200d♀️\n        - heading "画像生成AIの生成途中経過を可視化したい！【Diffusers】" [level=3] [ref=s1e771]\n        - time [ref=s1e774]: 2024/08/06\n        - img "いいねされた数" [ref=s1e776]\n        - text: "4"\n- article [ref=s1e778]:\n    - link "tech" [ref=s1e779]:\n        - /url: /tech-or-idea\n    - link "🚀 生成AIをGoogle Colabで簡単に 【Part10 SDXL+ANIMAGINE +ControlNet+LoRA編（全部載せ）】 2024/08/03 いいねされた数 3" [ref=s1e780]:\n        - /url: /asap/articles/aa856819e2a722\n        - text: 🚀\n        - heading "生成AIをGoogle Colabで簡単に 【Part10 SDXL+ANIMAGINE\n          +ControlNet+LoRA編（全部載せ）】" [level=3] [ref=s1e784]\n        - time [ref=s1e787]: 2024/08/03\n        - img "いいねされた数" [ref=s1e789]\n        - text: "3"\n- article [ref=s1e791]:\n    - link "tech" [ref=s1e792]:\n        - /url: /tech-or-idea\n    - link "😸 生成AIをGoogle Colaboratoryで簡単に 【Part6.5 画像生成AI SDXL+ANIMAGINE XL 3.1編】 2024/07/31 いいねされた数 3" [ref=s1e793]:\n        - /url: /asap/articles/f9ac2e83c820ff\n        - text: 😸\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part6.5 画像生成AI SDXL+ANIMAGINE\n          XL 3.1編】" [level=3] [ref=s1e797]\n        - time [ref=s1e800]: 2024/07/31\n        - img "いいねされた数" [ref=s1e802]\n        - text: "3"\n- article [ref=s1e804]:\n    - link "tech" [ref=s1e805]:\n        - /url: /tech-or-idea\n    - link "🎃 生成AIをGoogle Colaboratoryで簡単に 【Part9 画像生成AI SD3+ControlNet編】 2024/07/29 いいねされた数 3" [ref=s1e806]:\n        - /url: /asap/articles/53560859d273e4\n        - text: 🎃\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part9 画像生成AI SD3+ControlNet編】"\n          [level=3] [ref=s1e810]\n        - time [ref=s1e813]: 2024/07/29\n        - img "いいねされた数" [ref=s1e815]\n        - text: "3"\n- article [ref=s1e817]:\n    - link "tech" [ref=s1e818]:\n        - /url: /tech-or-idea\n    - link "✨ 生成AIをGoogle Colaboratoryで簡単に 【Part8 画像生成AI Stable Diffusion 3 Medium編】 2024/07/25 いいねされた数 4" [ref=s1e819]:\n        - /url: /asap/articles/6f6ca9aa0513d8\n        - text: ✨\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part8 画像生成AI Stable Diffusion 3\n          Medium編】" [level=3] [ref=s1e823]\n        - time [ref=s1e826]: 2024/07/25\n        - img "いいねされた数" [ref=s1e828]\n        - text: "4"\n- article [ref=s1e830]:\n    - link "tech" [ref=s1e831]:\n        - /url: /tech-or-idea\n    - link "🌟 生成AIをGoogle Colaboratoryで簡単に 【Part7 画像生成AI SDXL Turbo編】 2024/07/23 いいねされた数 2" [ref=s1e832]:\n        - /url: /asap/articles/ed58a6899c1c26\n        - text: 🌟\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part7 画像生成AI SDXL Turbo編】"\n          [level=3] [ref=s1e836]\n        - time [ref=s1e839]: 2024/07/23\n        - img "いいねされた数" [ref=s1e841]\n        - text: "2"\n- article [ref=s1e843]:\n    - link "tech" [ref=s1e844]:\n        - /url: /tech-or-idea\n    - link "🌟 生成AIをGoogle Colaboratoryで簡単に 【Part6 画像生成AI Stable Diffusion XL編】 2024/07/19 いいねされた数 4" [ref=s1e845]:\n        - /url: /asap/articles/e4791be54f90ce\n        - text: 🌟\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part6 画像生成AI Stable Diffusion\n          XL編】" [level=3] [ref=s1e849]\n        - time [ref=s1e852]: 2024/07/19\n        - img "いいねされた数" [ref=s1e854]\n        - text: "4"\n- article [ref=s1e856]:\n    - link "tech" [ref=s1e857]:\n        - /url: /tech-or-idea\n    - link "🎤 生成AIをローカルで簡単に 【Part5.5 faster-whisper+マイク録音編】 2024/07/16 いいねされた数 7" [ref=s1e858]:\n        - /url: /asap/articles/2c0d421e68ef16\n        - text: 🎤\n        - heading "生成AIをローカルで簡単に 【Part5.5 faster-whisper+マイク録音編】" [level=3]\n          [ref=s1e862]\n        - time [ref=s1e865]: 2024/07/16\n        - img "いいねされた数" [ref=s1e867]\n        - text: "7"\n- article [ref=s1e869]:\n    - link "tech" [ref=s1e870]:\n        - /url: /tech-or-idea\n    - link "📖 生成AIをGoogle Colaboratoryで簡単に 【Part5 音声認識AI faster-whisper編】 2024/07/13 いいねされた数 2" [ref=s1e871]:\n        - /url: /asap/articles/ba8fcb1880165e\n        - text: 📖\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part5 音声認識AI faster-whisper編】"\n          [level=3] [ref=s1e875]\n        - time [ref=s1e878]: 2024/07/13\n        - img "いいねされた数" [ref=s1e880]\n        - text: "2"\n- article [ref=s1e882]:\n    - link "tech" [ref=s1e883]:\n        - /url: /tech-or-idea\n    - link "😸 生成AIをGoogle Colaboratoryで簡単に 【Part4 音声認識AI Whisper API編】 2024/07/11 いいねされた数 2" [ref=s1e884]:\n        - /url: /asap/articles/ecaa5631e42195\n        - text: 😸\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part4 音声認識AI Whisper API編】"\n          [level=3] [ref=s1e888]\n        - time [ref=s1e891]: 2024/07/11\n        - img "いいねされた数" [ref=s1e893]\n        - text: "2"\n- article [ref=s1e895]:\n    - link "tech" [ref=s1e896]:\n        - /url: /tech-or-idea\n    - link "😊 生成AIをGoogle Colaboratoryで簡単に 【Part3 テキスト生成 ローカルLLM編】 2024/07/08 いいねされた数 4" [ref=s1e897]:\n        - /url: /asap/articles/8c3d5164d14f6e\n        - text: 😊\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part3 テキスト生成 ローカルLLM編】"\n          [level=3] [ref=s1e901]\n        - time [ref=s1e904]: 2024/07/08\n        - img "いいねされた数" [ref=s1e906]\n        - text: "4"\n- article [ref=s1e908]:\n    - link "tech" [ref=s1e909]:\n        - /url: /tech-or-idea\n    - link "🐷 生成AIをGoogle Colaboratoryで簡単に 【Part2 テキスト生成ChatGPT API編】 2024/07/05 いいねされた数 2" [ref=s1e910]:\n        - /url: /asap/articles/f410dd5d407652\n        - text: 🐷\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part2 テキスト生成ChatGPT API編】"\n          [level=3] [ref=s1e914]\n        - time [ref=s1e917]: 2024/07/05\n        - img "いいねされた数" [ref=s1e919]\n        - text: "2"\n- article [ref=s1e921]:\n    - link "tech" [ref=s1e922]:\n        - /url: /tech-or-idea\n    - link "🗂 生成AIをGoogle Colaboratoryで簡単に 【Part1 合成音声Style-Bert-VITS2編】 2024/07/02 いいねされた数 3" [ref=s1e923]:\n        - /url: /asap/articles/7bdb20dcb5cc56\n        - text: 🗂\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part1 合成音声Style-Bert-VITS2編】"\n          [level=3] [ref=s1e927]\n        - time [ref=s1e930]: 2024/07/02\n        - img "いいねされた数" [ref=s1e932]\n        - text: "3"\n- article [ref=s1e934]:\n    - link "tech" [ref=s1e935]:\n        - /url: /tech-or-idea\n    - link "👏 Remdisの環境構築で沼った話（＋使い方の紹介） 2024/06/26 いいねされた数 14" [ref=s1e936]:\n        - /url: /asap/articles/101379d8951b08\n        - text: 👏\n        - heading "Remdisの環境構築で沼った話（＋使い方の紹介）" [level=3] [ref=s1e940]\n        - time [ref=s1e943]: 2024/06/26\n        - img "いいねされた数" [ref=s1e945]\n        - text: "14"\n- article [ref=s1e947]:\n    - link "tech" [ref=s1e948]:\n        - /url: /tech-or-idea\n    - link "🚀 Style-Bert-VITS2の使い方メモ 2024/06/17 いいねされた数 34" [ref=s1e949]:\n        - /url: /asap/articles/f8c0621cdd74cc\n        - text: 🚀\n        - heading "Style-Bert-VITS2の使い方メモ" [level=3] [ref=s1e953]\n        - time [ref=s1e956]: 2024/06/17\n        - img "いいねされた数" [ref=s1e958]\n        - text: "34"\n- article [ref=s1e960]:\n    - link "tech" [ref=s1e961]:\n        - /url: /tech-or-idea\n    - link "💭 Style-Bert-VITS2のVoice Conversion機能の復活に挑戦してみた（前編） 2024/06/09 いいねされた数 9" [ref=s1e962]:\n        - /url: /asap/articles/df2c610d1389e4\n        - text: 💭\n        - heading "Style-Bert-VITS2のVoice Conversion機能の復活に挑戦してみた（前編）" [level=3]\n          [ref=s1e966]\n        - time [ref=s1e969]: 2024/06/09\n        - img "いいねされた数" [ref=s1e971]\n        - text: "9"\n- article [ref=s1e973]:\n    - link "tech" [ref=s1e974]:\n        - /url: /tech-or-idea\n    - link "😸 OpenAI Assistants API v2を色々いじってみた（使い方）（File Search編） 2024/05/28 いいねされた数 10" [ref=s1e975]:\n        - /url: /asap/articles/5c2f8ffe2a46ce\n        - text: 😸\n        - heading "OpenAI Assistants API v2を色々いじってみた（使い方）（File Search編）"\n          [level=3] [ref=s1e979]\n        - time [ref=s1e982]: 2024/05/28\n        - img "いいねされた数" [ref=s1e984]\n        - text: "10"\n- article [ref=s1e986]:\n    - link "tech" [ref=s1e987]:\n        - /url: /tech-or-idea\n    - link "👻 OpenAI Assistants API v2を色々いじってみた（使い方）（code interpreter編） 2024/05/27 いいねされた数 5" [ref=s1e988]:\n        - /url: /asap/articles/b0ce16ea1832ac\n        - text: 👻\n        - heading "OpenAI Assistants API v2を色々いじってみた（使い方）（code interpreter編）"\n          [level=3] [ref=s1e992]\n        - time [ref=s1e995]: 2024/05/27\n        - img "いいねされた数" [ref=s1e997]\n        - text: "5"\n- article [ref=s1e999]:\n    - link "tech" [ref=s1e1000]:\n        - /url: /tech-or-idea\n    - link "😑 【自分用】「AI後輩ちゃん」の個人的つまづきポイントまとめ 2024/05/09 いいねされた数 3" [ref=s1e1001]:\n        - /url: /asap/articles/98b04c5ea82bd9\n        - text: 😑\n        - heading "【自分用】「AI後輩ちゃん」の個人的つまづきポイントまとめ" [level=3] [ref=s1e1005]\n        - time [ref=s1e1008]: 2024/05/09\n        - img "いいねされた数" [ref=s1e1010]\n        - text: "3"\n- article [ref=s1e1012]:\n    - link "tech" [ref=s1e1013]:\n        - /url: /tech-or-idea\n    - link "😍 音声対話システム「AI後輩ちゃん」を作ってみた 2024/05/04 いいねされた数 17" [ref=s1e1014]:\n        - /url: /asap/articles/5b1b7553fcaa76\n        - text: 😍\n        - heading "音声対話システム「AI後輩ちゃん」を作ってみた" [level=3] [ref=s1e1018]\n        - time [ref=s1e1021]: 2024/05/04\n        - img "いいねされた数" [ref=s1e1023]\n        - text: "17"\n- contentinfo [ref=s1e1026]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1030]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1031]\n    - paragraph [ref=s1e1039]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e1041]:\n        - heading "About" [level=4] [ref=s1e1042]\n        - list [ref=s1e1043]:\n            - listitem [ref=s1e1044]:\n                - link "Zennについて" [ref=s1e1046]:\n                    - /url: /about\n            - listitem [ref=s1e1047]:\n                - link "運営会社" [ref=s1e1048]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e1049]:\n                - link "お知らせ・リリース" [ref=s1e1050]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e1051]:\n                - link "イベント" [ref=s1e1053]:\n                    - /url: /events\n    - navigation [ref=s1e1054]:\n        - heading "Guides" [level=4] [ref=s1e1055]\n        - list [ref=s1e1056]:\n            - listitem [ref=s1e1057]:\n                - link "使い方" [ref=s1e1059]:\n                    - /url: /manual\n            - listitem [ref=s1e1060]:\n                - link "法人向けメニュー" [ref=s1e1062]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e1064]:\n                - link "Publication / Pro" [ref=s1e1066]:\n                    - /url: /publications\n            - listitem [ref=s1e1067]:\n                - link "よくある質問" [ref=s1e1069]:\n                    - /url: /faq\n    - navigation [ref=s1e1070]:\n        - heading "Links" [level=4] [ref=s1e1071]\n        - list [ref=s1e1072]:\n            - listitem [ref=s1e1073]:\n                - link "X(Twitter)" [ref=s1e1074]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e1075]:\n                - link "GitHub" [ref=s1e1076]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e1077]:\n                - link "メディアキット" [ref=s1e1079]:\n                    - /url: /mediakit\n    - navigation [ref=s1e1080]:\n        - heading "Legal" [level=4] [ref=s1e1081]\n        - list [ref=s1e1082]:\n            - listitem [ref=s1e1083]:\n                - link "利用規約" [ref=s1e1085]:\n                    - /url: /terms\n            - listitem [ref=s1e1086]:\n                - link "プライバシーポリシー" [ref=s1e1088]:\n                    - /url: /privacy\n            - listitem [ref=s1e1089]:\n                - link "特商法表記" [ref=s1e1091]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e1094]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e1095]\n- alert [ref=s1e1114]\n```\n', name='browser_navigate', tool_call_id='58fad86e-0b01-4fc6-a8cc-57deb5c48b41'), AIMessage(content='`https://zenn.dev/asap`にアクセスしました。\n\n執筆者の記事一覧から、いいねの数が多い記事を探します。\n\n- 「DeepSeek-R1の論文読んだ？【勉強になるよ】」 が921いいね\n- 「拡散モデルの理論って面白いよね」 が87いいね\n- 「Open AI Realtime APIのPythonサンプルコードを作成」 が79いいね\n- 「📏 【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】」 が50いいね\n- 「🙌 完全自律型AIエージェントOSS【LangManus】完全に理解した！」 が48いいね\n\n最もいいねが多い記事は「DeepSeek-R1の論文読んだ？【勉強になるよ】」です。\n記事の中身を取得します。', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap/articles/34237ad87f8511"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-27104099-cd7e-4de1-8c89-51060ed9c6f1-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap/articles/34237ad87f8511'}, 'id': 'd85f8505-9c2a-4960-b17f-9ef88d983d9d', 'type': 'tool_call'}], usage_metadata={'input_tokens': 17995, 'output_tokens': 216, 'total_tokens': 18211, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap/articles/34237ad87f8511\n\n- Page URL: https://zenn.dev/asap/articles/34237ad87f8511\n- Page Title: DeepSeek-R1の論文読んだ？【勉強になるよ】\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- article [ref=s1e32]:\n    - text: 📑\n    - heading "DeepSeek-R1の論文読んだ？【勉強になるよ】" [level=1] [ref=s1e38]\n    - text: 2025/01/27に公開\n    - tooltip "更新日" [ref=s1e45]:\n        - img [ref=s1e46]\n        - text: 2025/02/22 更新日\n    - link "12件" [ref=s1e51]:\n        - /url: "#discuss"\n        - img [ref=s1e52]\n        - text: 12件\n    - link "強化学習" [ref=s1e63]:\n        - /url: /topics/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92\n        - img [ref=s1e65]\n        - text: 強化学習\n    - link "蒸留" [ref=s1e67]:\n        - /url: /topics/%E8%92%B8%E7%95%99\n        - img [ref=s1e69]\n        - text: 蒸留\n    - link "LLM" [ref=s1e71]:\n        - /url: /topics/llm\n        - img [ref=s1e73]\n        - text: LLM\n    - link "生成 AI" [ref=s1e75]:\n        - /url: /topics/%E7%94%9F%E6%88%90ai\n        - img [ref=s1e77]\n        - text: 生成 AI\n    - link "DeepSeek" [ref=s1e79]:\n        - /url: /topics/deepseek\n        - img [ref=s1e81]\n        - text: DeepSeek\n    - link "tech" [ref=s1e83]:\n        - /url: /tech-or-idea\n        - img [ref=s1e85]\n        - text: tech\n    - heading "はじめに" [level=1] [ref=s1e89]\n    - paragraph [ref=s1e90]: タイトル通りDeepSeek-R1の論文を読んだので、少しでもわかりやすく説明できればと思います。\n        まだ論文を読んでいない方は、ぜひ原文も読んでみてください！\n    - paragraph [ref=s1e92]:\n        - text: 難しい数式や概念がほとんど使われていないため、論文の中では全然読みやすい方だと思います。読み物として読めるレベルです。\n        - iframe [ref=s1e94]:\n            - text: Loading...\n            - alert [ref=f1s1e8]\n    - paragraph [ref=s1e95]: とはいえ、本記事では細かい部分など間違っている部分もあるかもしれないので、ぜひ誤りがあったら、こそっと教えてください！\n    - complementary [ref=s1e96]:\n        - text: "!"\n        - paragraph [ref=s1e99]: 実は、本論文を解説する上で、著者らが公開しているもう一つの論文が大きく関連しています。 そちらの論文の内容も若干解説を入れております。\n        - paragraph [ref=s1e101]:\n            - text: 「\n            - \'link "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models" [ref=s1e102]\':\n                - /url: https://arxiv.org/html/2402.03300v3\n            - text: 」\n        - paragraph [ref=s1e103]: 本記事でも少し触れますが、上記の論文も非常に勉強になるため、ぜひ読んでみてください。\n    - complementary [ref=s1e104]:\n        - text: "!"\n        - paragraph [ref=s1e107]:\n            - text: 本記事は、\n            - link "DeepSeek-R1の論文" [ref=s1e108]:\n                - /url: https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\n            - text: と\n            - link "DeepSeekMathの論文" [ref=s1e109]:\n                - /url: https://arxiv.org/html/2402.03300v3\n            - text: を読んだ私の理解をもとに記載しています。\n        - paragraph [ref=s1e110]: 本論文で使われている技術に関しては、ある程度の知識を持っているので、大きくは外していないとは思いますが、私の主観も入っている部分もありますので、ご了承ください。\n    - paragraph [ref=s1e111]:\n        - text: また、DeepSeek-R1の論文が公開される前に、小型モデルに対して同様の実験（強化学習）をしていたグループがあるようです。\n            そちらのレポートは下記になります。\n        - iframe [ref=s1e114]:\n            - \'link "7B Model and 8K Examples: Emerging Reasoning with Reinforcement Learning is Both Effective and Effic A replication of DeepSeek-R1 training on small models with limited data hkust-nlp.notion.site favicon image hkust-nlp.notion.site hkust-nlp.notion.site thumbnail image" [ref=f2s1e6]\':\n                - /url: https://hkust-nlp.notion.site/simplerl-reason\n                - \'heading "7B Model and 8K Examples: Emerging Reasoning with\n                  Reinforcement Learning is Both Effective and Effic" [level=1]\n                  [ref=f2s1e8]\'\n                - text: A replication of DeepSeek-R1 training on small models with limited data\n                - img "hkust-nlp.notion.site favicon image" [ref=f2s1e11]\n                - text: hkust-nlp.notion.site\n                - img "hkust-nlp.notion.site thumbnail image" [ref=f2s1e13]\n            - alert [ref=f2s1e15]\n    - paragraph [ref=s1e115]: 意図せず、DeepSeek-R1-Zeroの再現実験のようなレポートになっていますが、レポートの著者はDeepSeek-R1論文の公開前から実験していると主張しています。\n        こちらも非常に興味深かったため紹介です。\n    - heading "本論文の興味深いところ" [level=1] [ref=s1e117]\n    - paragraph [ref=s1e118]: 本論文は、大きく分けて3つの構成でできています\n    - list [ref=s1e119]:\n        - listitem [ref=s1e120]:\n            - strong [ref=s1e121]: 強化学習による思考能力の強化\n            - list [ref=s1e122]:\n                - listitem [ref=s1e123]: LLM（DeepSeek-V3-Base）に対して「強化学習のみ」を適用させたところ、強力な思考能力を獲得した「DeepSeek-R1-Zero」が作成されました。\n                - listitem [ref=s1e124]: 本論文のメインストリームであり、非常に興味深い示唆が得られる部分です。\n        - listitem [ref=s1e125]:\n            - strong [ref=s1e126]: SFTによるさらなる強化\n            - list [ref=s1e127]:\n                - listitem [ref=s1e128]: DeepSeek-V3-Baseに対して、コールドスタート問題を解消するために、CoT（Chain of\n                    Thought）データセットによるSFT（教師付きファインチューニング）を実施\n                - listitem [ref=s1e129]: その後、強化学習を適用させることで、o1レベルの性能を獲得した「DeepSeek-R1」が作成されました。\n                - listitem [ref=s1e130]: 非常に強力なLLMを獲得するまでの反復学習などの試行錯誤などを学ぶことができます。\n        - listitem [ref=s1e131]:\n            - strong [ref=s1e132]: 蒸留による小型モデルの性能強化\n            - list [ref=s1e133]:\n                - listitem [ref=s1e134]: 上記で作成されたDeepSeek-R1により、他社の小型オープンソースLLMを蒸留しました。\n                - listitem [ref=s1e135]: その結果、元の性能を大幅に上回る、非常に高い性能の小型LLMが得られました。\n    - paragraph [ref=s1e136]: その中でも私が特に面白いなと思った要素は下記の要素になります\n    - list [ref=s1e137]:\n        - listitem [ref=s1e138]:\n            - strong [ref=s1e139]: 強化学習手法「GRPO」すごい！\n        - listitem [ref=s1e140]:\n            - strong [ref=s1e141]: ルールベース報酬で思考能力が向上するのがすごい！\n        - listitem [ref=s1e142]:\n            - strong [ref=s1e143]: aha moment（ハッとする瞬間）面白い！\n        - listitem [ref=s1e144]:\n            - strong [ref=s1e145]: 蒸留vs強化学習の比較実験から重要な示唆が得られた！\n    - heading "本論文の流れ" [level=1] [ref=s1e146]\n    - heading "前提のモデル構造" [level=3] [ref=s1e147]\n    - paragraph [ref=s1e148]: そもそも、大前提として、この論文は「DeepSeek-V3-Base」という強力なLLMの性能を改善する論文です。\n        これ単体でも、GPT-4oレベルの能力を持っているオープンソースLLMになります。\n    - paragraph [ref=s1e150]: 本論文のモデルアーキテクチャは「DeepSeek-V3-Base」と完全に同じものを利用します。\n        （つまりモデルアーキテクチャをかえずに、学習方法を工夫してLLMの性能を上げる挑戦になります）\n    - heading "DeepSeek-R1-Zeroを強化学習を用いて作る" [level=3] [ref=s1e152]\n    - paragraph [ref=s1e153]: その上で、まずは、「DeepSeek-V3-Base」に対して、大規模な強化学習のみを適応させることで、強化学習の可能性について探究する話が始まります。\n        結果として、「DeepSeek-R1-Zero」ができます。\n        「DeepSeek-R1-Zero」は、SFT（教師付きファインチューニング）をしなくても、強化学習のみで圧倒的な思考能力を取得できることを示す興味深いモデルです。\n    - heading "DeepSeek-R1をSFT+強化学習を用いて作る" [level=3] [ref=s1e156]\n    - paragraph [ref=s1e157]: その後、あまりにも「DeepSeek-R1-Zero」の性能が良かったことから、さらなる探究が進められました。\n    - paragraph [ref=s1e158]: この時点で、「DeepSeek-R1-Zero」には下記の課題がありました。\n    - list [ref=s1e159]:\n        - listitem [ref=s1e160]: 学習初期段階にて、強化学習の学習が不安定であり、収束に時間がかかる（コールドスタート問題）\n        - listitem [ref=s1e161]:\n            - text: 思考能力は高いが、言語が混在（日本語で話していたら急に中国語になるなど）し、人間が読みにくい\n            - list [ref=s1e162]:\n                - listitem [ref=s1e163]: 結果、高度な思考が必要なタスクには強いが、一般的タスク（ロールプレイやライティングなど）には使いにくいという評価\n    - paragraph [ref=s1e164]: そこで、「DeepSeek-V3-Base」に対して、少量のCoT（Chain of\n        Thought）データセットを用いて、SFT（教師付き微調整）を行います。このデータは、著者らが収集・構築したものになります。\n        その後、SFT後のモデルに対して、大規模強化学習を行います。\n        このとき、「DeepSeek-R1-Zero」を作成した際の報酬に加え、「言語一貫性報酬」を導入して、上述した言語の不一致の問題を緩和させます。\n    - paragraph [ref=s1e167]: その後、得られたモデル（チェックポイント）を利用して、さらなるSFT（教師付き微調整）学習データを収集し、追加SFTを行います。\n        さらに、その後、改めて２段階目の大規模強化学習を実施することで、最終的に「DeepSeek-R1」が得られます。\n    - heading "小型モデルへの蒸留を試す" [level=3] [ref=s1e169]\n    - paragraph [ref=s1e170]: 最後に、小型モデルに対する「DeepSeek-R1」による蒸留が試されています。\n        その中で、小型モデルに対して、「DeepSeek-R1」による蒸留を実施するのと、「DeepSeek-R1-Zero」と同様の強化学習手法で思考能力を強化するのと、どちらが性能向上に寄与するのかを調査してくれています。\n    - paragraph [ref=s1e172]: これが非常に興味深いです。\n    - heading "DeepSeek-R1-Zeroを作るまで" [level=1] [ref=s1e173]\n    - paragraph [ref=s1e174]: では、ここから論文の詳細な解説に入っていきます。\n    - paragraph [ref=s1e175]: まずは、「DeepSeek-R1」の前段階である「DeepSeek-R1-Zero」を作るまでの手法を解説します。\n    - paragraph [ref=s1e176]:\n        - strong [ref=s1e177]: 前段階と侮るなかれ、「DeepSeek-R1-Zero」を作るまでの手法と示唆にこそ、本論文の貴重な示唆が含まれています。\n    - heading "元々のアーキテクチャ" [level=2] [ref=s1e178]\n    - paragraph [ref=s1e179]: 今回作る「DeepSeek-R1-Zero」や「DeepSeek-R1」は、新しいモデル構造を採用して、性能を向上させたわけではありません。\n    - paragraph [ref=s1e180]: 元々のアーキテクチャは「DeepSeek-V3-Base」というアーキテクチャになります。\n        「DeepSeek-V3-Base」は、681Bのモデルパラメータを持つ大規模言語モデルで、性能はGPT-4oに匹敵する非常に高性能モデルありながら、モデル重みが完全に公開されているオープンソースモデルです。\n    - paragraph [ref=s1e182]: 「DeepSeek-V3-Base」に対して、Instructionチューニングを施したchatモデルに関しては、APIでも公開されています。\n        非常に安価に使えるにも関わらず、GPT-4oに匹敵する性能を誇ることから、人気のモデルになっております。\n    - complementary [ref=s1e184]:\n        - text: "!"\n        - paragraph [ref=s1e187]: 安価なモデルではありますが、APIの提供元は中国の企業ですので、データの取り扱いなどは十分注意してください。\n            基本的には、API利用の際には、収集されて困るような情報は入力しないようにするのが無難です。\n            （オープンソースモデルをローカルで動かす場合は、おそらく心配ありません）\n    - paragraph [ref=s1e190]: 本論文のすごいところは、モデル構造を変えずに、「DeepSeek-V3-Base」に対する学習方法を工夫しただけで、OpenAI\n        o1に匹敵する性能のLLMを開発していることです。\n        具体的には、大規模強化学習をLLMに対して適用することで、「DeepSeek-R1-Zero」を開発します。\n    - heading "すごいぞ！強化学習" [level=2] [ref=s1e192]\n    - heading "強化学習って何？" [level=3] [ref=s1e193]\n    - paragraph [ref=s1e194]: そもそも「強化学習って何？」という方もいらっしゃると思うので、簡単に説明します。\n        （以前、スイカゲームを強化学習でプレイしたこともあるので、いずれ詳しい解説記事を作る予定ですが、ここではざっくり説明します。）\n    - paragraph [ref=s1e196]: 強化学習とは、一言で言えば「一連の行動の流れ（シークエンス）によって得られる、累積報酬の期待値を最大化する」ことを目指す学習手法です。\n        ある行動をとった結果がすぐにはわからず、時間が経ってから影響が現れるような問題に適しています。\n        専門的に言うと、「マルコフ決定過程（MDP）」が成立する問題で活用されます。\n    - heading "テトリスで考える" [level=3] [ref=s1e199]\n    - paragraph [ref=s1e200]: 例えば、「テトリス」を考えてみましょう。\n    - paragraph [ref=s1e201]: テトリスは、上から「ミノ」と呼ばれる、さまざまな形のブロックを落として、横一列を綺麗に埋めてブロックを消去するゲームです。\n    - paragraph [ref=s1e202]: 横一列がすべて揃うと、その列が消えて、「ポイント」を獲得できます。\n        逆に、列を消せずに、ブロックが一定の高さまで積み上がると「ゲームオーバ」になります。\n    - paragraph [ref=s1e204]: テトリスを強化学習で解こうとすると、「どうすれば最も多くのポイントを得られるか」をAIに学習させることが重要になります。\n        単純に列を揃えて消すだけではなく、「次の手」や「その次の手」を見据えて、より良い結果につながるようにミノを配置することも必要です。\n        しかし、通常の学習方法(よくある入出力を近似させる学習方法)では、目の前の結果だけに注目してしまい、将来的な高得点につながる手を学習させることは難しいです。\n    - paragraph [ref=s1e207]:\n        - strong [ref=s1e208]: そこで役立つのが「強化学習」です。\n    - paragraph [ref=s1e209]: 強化学習では、ゲームの最初から最後までの流れを通して、「どう行動すれば最終的に得られる得点を最大化できるか」を学習できます。\n        このように、途中の行動の良し悪しがすぐにはわからなくても、最終的なゴール（得点の最大化）が設定できる問題に対して、強化学習は非常に効果的です。\n    - paragraph [ref=s1e211]:\n        - img [ref=s1e212]\n    - group [ref=s1e213]: 補足\n    - heading "強化学習の事例「AlphaGo Zero」" [level=3] [ref=s1e215]\n    - paragraph [ref=s1e216]: さらに、強化学習の威力をお伝えする事例を記載します。\n    - paragraph [ref=s1e217]: 2015年にアメリカのDeepMind社（Googleの傘下）が「AlphaGo」というモデルを開発しました。これはAIが囲碁をプレイするモデルで、当時の世界王者をハンディキャップなしで破るという、非常に衝撃的なモデルでした。\n        （当時としては、非常にセンセーショナルな出来事だったため、記憶にある方もいらっしゃるかと思います）\n    - paragraph [ref=s1e219]: この「AlphaGo」は、深層学習と強化学習により学習されています。\n        プロ棋士の棋譜データで深層学習で学習させた上で、強化学習による自己対局（AI同士の対局）を実施したモデルです。\n    - paragraph [ref=s1e221]:\n        - text: さて、ここまでも十分すごいですが、\n        - strong [ref=s1e223]: さらにすごいのはここからです。\n    - paragraph [ref=s1e224]: DeepMind社は後継モデルとして「AlphaGo Zero」というモデルを開発しました。\n        このモデルは、プロ棋士の棋譜データを「使わず」に、完全に「強化学習のみ」で学習されたモデルです。\n        つまり、囲碁のルールだけを与え、あとは最初から最後までAI同士の自己対局のみで学習されています。\n        （この場合、対局に勝利したら報酬、敗北したら罰を与える形で強化学習をし、対局に勝利した時に取っていた手（行動）の選択確率を上げるように学習しています）\n    - paragraph [ref=s1e228]:\n        - strong [ref=s1e229]: そして、その「AlphaGo Zero」は「AlphaGo」に対して、100戦全勝で圧勝しました。\n    - paragraph [ref=s1e230]: AIが囲碁の学習をする際に、人間が持つ定石の情報などは不要だったわけです。\n    - paragraph [ref=s1e231]: これが強化学習の威力です。\n        余談ですが、今回の論文で作成する「DeepSeek-R1-Zero」も（「DeepSeek-V3-Base」をベースにはしますが）強化学習のみで学習されます。\n        名前に入っている「Zero」は同じく強化学習のみで開発された「AlphaGo Zero」のオマージュかなと思います。\n    - heading "そもそもLLMで強化学習とは？" [level=2] [ref=s1e234]\n    - paragraph [ref=s1e235]: 「なんかゲームみたいなものに、強化学習を使うとうまくいきそう」というのは理解していただけたかなと思います。\n    - paragraph [ref=s1e236]:\n        - text: では、LLMに対してはどうか、というのを解説していきます。\n        - strong [ref=s1e238]: 実は、LLMと強化学習は非常に相性がいいです。\n    - paragraph [ref=s1e239]: なぜなら、LLMは一度にすべての文章を出力するAIではなく、1tokenずつ出力するモデルであり、その文章が良い文章か悪い文章かは、すべてのtokenを出力してみないとわからないからです。\n    - paragraph [ref=s1e240]:\n        - text: テトリスでは、ミノを一つずつ落とし、一列が消えたタイミングで、ポイントを取得するという形で評価されます。\n        - img [ref=s1e242]\n    - paragraph [ref=s1e243]:\n        - text: LLMでも、同様にtokenを1tokenずつ出力し、一文全部が出力されたタイミングで、その文章の良し悪しを評価することができれば、強化学習と同じ枠組みを使うことができるというわけです。\n        - img [ref=s1e245]\n    - heading "報酬モデルを作成し、累積報酬を最大化" [level=3] [ref=s1e246]\n    - paragraph [ref=s1e247]:\n        - text: テトリスでは、ゲームシステムが勝手に点数をつけてくれますが、LLMにおいて文章の良し悪しを勝手に判断してくれる便利な機能はありません。そこで我々が、その判断モデルを作ってあげる必要があります。\n            そこで、ニューラルネットワークを利用して、LLMに入力されたプロンプト\n        - math [ref=s1e252]: x\n        - text: に対して、LLMが生成した文章\n        - math [ref=s1e259]: "y"\n        - text: の点数\n        - math [ref=s1e266]: r\n        - text: を出力するようなネットワークを「報酬モデル\n        - math [ref=s1e273]: r θ ( r ∣ y , x )\n        - text: 」として作成することを考えます。\n    - paragraph [ref=s1e286]:\n        - text: まず初めに、データセットとして、「プロンプト\n        - math [ref=s1e290]: x\n        - text: 」「好ましい回答\n        - math [ref=s1e297]: y +\n        - text: 」「好ましくない回答\n        - math [ref=s1e306]: y −\n        - text: 」の3つが組となっているデータセットを用意します。 報酬モデル\n        - math [ref=s1e316]: r θ ( r ∣ y , x )\n        - text: は、「プロンプト\n        - math [ref=s1e332]: x\n        - text: 」と「出力\n        - math [ref=s1e339]: "y"\n        - text: 」の両方を入力することで、報酬（点数）を出力するモデルです。\n    - paragraph [ref=s1e343]:\n        - text: そこで、好ましい回答の報酬推定値\n        - math [ref=s1e347]: r θ ( r ∣ y + , x )\n        - text: と、好ましくない回答の報酬推定値\n        - math [ref=s1e365]: r θ ( r ∣ y − , x )\n        - text: の差を一定以上にするような損失関数を用いて学習を行います。 すると、報酬モデル\n        - math [ref=s1e384]: r θ ( r ∣ y , x )\n        - text: は適切に、好ましい回答\n        - math [ref=s1e400]: y +\n        - text: には高い点数を、好ましくない回答\n        - math [ref=s1e409]: y −\n        - text: には低い点数をつけるようになります。\n    - paragraph [ref=s1e415]:\n        - text: 報酬モデル\n        - math [ref=s1e419]: r θ ( r ∣ y , x )\n        - text: か完成したら、強化学習をするための要素は整います。\n    - paragraph [ref=s1e432]:\n        - text: 状態\n        - math [ref=s1e436]: x\n        - text: は、LLMに入力される「プロンプト」で、LLMの出力が\n        - math [ref=s1e443]: "y"\n        - text: とします。 報酬\n        - math [ref=s1e451]: r\n        - text: は、報酬モデル\n        - math [ref=s1e458]: r θ ( r ∣ y , x )\n        - text: の出力値です。 LLMが全文出力したら、プロンプト\n        - math [ref=s1e475]: x\n        - text: と、LLMの出力\n        - math [ref=s1e482]: "y"\n        - text: を報酬モデルに与えることで、取得します。 方策は、LLMが出力する各tokenごとの確率分布自体です。 このLLM自体を「方策モデル\n        - math [ref=s1e491]: π ϕ ( y t ∣ x , y < t )\n        - text: 」と呼びます。\n    - complementary [ref=s1e510]:\n        - text: "!"\n        - paragraph [ref=s1e513]:\n            - text: LLM（\n            - math [ref=s1e517]: π ϕ ( y t ∣ x , y < t )\n            - text: ）はユーザが入力したプロンプト\n            - math [ref=s1e539]: x\n            - text: と、それまでにLLMが出力した内容\n            - math [ref=s1e546]: y < t\n            - text: をもとに、\n            - math [ref=s1e557]: t\n            - text: ステップ目のtoken\n            - math [ref=s1e564]: y t\n            - text: を出力します。\n        - paragraph [ref=s1e570]: これが、LLMは、「文章の次に出てくる単語を一つずつ予測するモデル」と言われる所以です。\n    - paragraph [ref=s1e571]: これにより、累積報酬を最大化するように方策モデル（LLM）を学習することで、LLMは好ましい回答を出力するようにネットワークを更新できます。\n    - paragraph [ref=s1e572]: これが、LLMを強化学習で学習するということです。 このような手法をRLHF（Reinforcement\n        learning from human feedback）と総称します。\n    - group [ref=s1e574]: 細かい話\n    - heading "つよつよ手法「PPO（Proximal Policy Optimization）」" [level=3] [ref=s1e576]\n    - paragraph [ref=s1e577]: ここまでで、「LLMと強化学習は相性が良い」こと、「強化学習では累積報酬の期待値（状態価値）を最大化することが重要」であることを話しました。\n    - paragraph [ref=s1e578]: さて、実際にLLMに強化学習を組み込む場合、しばしばPPO（Proximal Policy\n        Optimization）という手法が利用されます。\n        （なんなら、LLM以外で強化学習を実用する上でも、最初に選択肢になるベースライン的に強力な手法でもあります）\n    - paragraph [ref=s1e580]: 今回はPPOが主題ではないので、簡単に説明します。\n    - heading "PPOの簡易的な説明" [level=4] [ref=s1e581]\n    - paragraph [ref=s1e582]:\n        - text: PPOでは、行動を確率的に出力する方策モデル（LLM）と、現在の状態価値を推定する状態価値モデルを用意します。 状態価値\n        - math [ref=s1e587]: V t\n        - text: とは、「累積報酬の期待値」として定義できます。 したがって、現時点から、方策に従って行動をした場合の、将来的な報酬総和の期待値が「状態価値\n        - math [ref=s1e597]: V t\n        - text: 」です。\n    - paragraph [ref=s1e603]:\n        - text: 強化学習の枠組みで累積報酬\n        - math [ref=s1e607]: G t\n        - text: が計算されるため、その期待値である状態価値\n        - math [ref=s1e616]: V t\n        - text: も学習が可能です。 一方で、方策モデルを学習する際に、Advantage\n        - math [ref=s1e626]: A t\n        - text: というものを利用します。\n    - paragraph [ref=s1e632]:\n        - text: Advantage\n        - math [ref=s1e636]: A t\n        - text: というのは下記の式で表されます。\n    - math [ref=s1e648]: A t = G t − V t ( v t ∣ y < t , x )\n    - paragraph [ref=s1e675]:\n        - text: 状態価値\n        - math [ref=s1e679]: V t ( v t ∣ y < t , x )\n        - text: は、累積報酬\n        - math [ref=s1e701]: G t\n        - text: の期待値（大雑把にいうと平均）になるように学習されているという前提があるため、Advantage\n        - math [ref=s1e710]: A t\n        - text: が正になるということは、今回LLMが出力したtoken\n        - math [ref=s1e719]: y t\n        - text: は、将来的に高い報酬を得ることができる「良い出力であった」と判断できます。\n    - paragraph [ref=s1e725]:\n        - text: したがって、方策モデル\n        - math [ref=s1e729]: π ϕ ( y t ∣ x , y < t )\n        - text: はAdvantage\n        - math [ref=s1e751]: A t\n        - text: を最大化するように学習します。\n    - paragraph [ref=s1e757]:\n        - text: Advantage\n        - math [ref=s1e761]: A t\n        - text: を考えずに、単純に累積報酬\n        - math [ref=s1e770]: G t\n        - text: を最大化するように考えることもできます。（むしろそれが初期の強化学習です）\n    - complementary [ref=s1e776]:\n        - text: "!"\n        - paragraph [ref=s1e779]: （2025年2月15日 追記） この辺りの表現は、わかりやすくするために若干誤解を招く表現になっています。\n            下に「細かい話」として補足説明を書きました。\n    - paragraph [ref=s1e782]: しかしその場合、そもそもどんな行動を取っても報酬が高くなる状態と、どんな行動を取っても報酬が低くなってしまう状態において、勾配のスケールがばらつくため、学習の安定性が低くなります。\n    - paragraph [ref=s1e783]:\n        - text: PPOでは、Advantage\n        - math [ref=s1e787]: A t\n        - text: を考えることにより、出力したtokenが平均よりも良いか悪いかだけで学習をすることができ、状態ごとの報酬の差をモデルが考慮する必要がなくなります。\n    - complementary [ref=s1e793]:\n        - text: "!"\n        - paragraph [ref=s1e796]: 本来、PPOの貢献は、あるパラメータ更新時において、更新前と更新後の確率分布が大きく変わらないような制約を、少ない計算量で加えることです。\n            ただ、この話は、DeepSeek-R1-Zeroにおける強化学習を理解するためには、必ずしも重要ではありません。\n            したがって、今回の記事では、必要な部分だけに絞り説明をしました。\n    - group [ref=s1e799]: 細かい話（追記）\n    - heading "著者発案「GRPO（Group Relative Policy Optimization）」" [level=3]\n      [ref=s1e801]\n    - paragraph [ref=s1e802]: さて、PPOは強化学習の手法において、高い性能を発揮しますが、LLMに適用することを考えると、状態価値モデルを別途学習しないといけないという点において、計算量的に大変な手法です。\n    - paragraph [ref=s1e803]: "一方で、PPOの状態価値モデルがやっていたことは、Advantage A_t(a_t|y_t,\n        y_{<t},x) を計算し、今回出力したtokenが、平均よりも良いのか悪いのかを判断することです。"\n    - paragraph [ref=s1e806]: "そこで、LLMにおいては、わざわざ状態価値モデルを用意して平均を計算しなくても、実際に複数文章を生成して、そ\\\n        れぞれの報酬を平均することで、Advantage A_t(a_t|y_t, y_{<t},x) を計算しようと考えました。\n        こうすることで、状態価値モデルなしで、Advantage A_t(a_t|y_t, y_{<t},x)\n        を計算できるため、計算量を大幅に削減できることができます。"\n    - paragraph [ref=s1e812]:\n        - text: この手法は「GRPO（Group Relative Policy Optimization）」と呼びます。\n            本論文の著者が、過去に公開した論文で提案した手法です。\n        - iframe [ref=s1e815]: <could not take iframe snapshot>\n    - paragraph [ref=s1e816]:\n        - text: 下記が、論文中で提示されている「PPO」と「GRPO」の違いを説明する図です。\n        - img [ref=s1e818]\n        - text: "このように状態価値モデル（Value model）なしに、Advantage A_t(a_t|y_t, y_{<t},x)\n            が計算できていることがわかります。"\n    - complementary [ref=s1e821]:\n        - text: "!"\n        - paragraph [ref=s1e824]: 本記事でのアルファベットの付け方と、上記の図のアルファベットの付け方が異なることに注意してください。\n        - paragraph [ref=s1e825]: 上記の図（GRPO）では、 プロンプトを q 、LLMの各出力文章を o_i （ i\n            はそれぞれの文章単位でのインデックス）、各文章ごとの報酬モデルの出力を r_i 、報酬 r_i\n            の平均から計算された各文章ごとのAdvantageを A_i としています。\n        - paragraph [ref=s1e839]: 本記事での添え字は、token単位での添え字を採用しています。 例えば A_t なら t token目でのAdvantageです。\n        - paragraph [ref=s1e845]: 上記の図では、文章単位での添え字です。 例えば、 A_i なら、 i 文章目でのAdvantageとなります。\n            従って A_i はベクトルとなっており、中にtokenごとのAdvantageの値も格納されているはずです。\n    - heading "報酬モデルを作らずルールベースで実施" [level=3] [ref=s1e854]\n    - paragraph [ref=s1e855]:\n        - text: 改めて、LLMに強化学習を適用するために必要なものをまとめます。 （下記の図のGRPOの手法を利用します）\n        - img [ref=s1e858]\n    - paragraph [ref=s1e859]: "まず、大前提として方策モデル \\\\pi_\\\\phi(y_t|x,y_{<t}) （LLM）は必要です。"\n    - paragraph [ref=s1e862]: "上の図中における「Reference Model」とは、「強化学習をする前」の方策モデル\n        \\\\pi_{ref}(y_t|x,y_{<t}) （つまり、DeepSeek-V3-Base）です。 また、「Policy\n        Model」というのは、「強化学習中」の現時点での方策モデル \\\\pi_\\\\phi(y_t|x,y_{<t}) を指しています。"\n    - paragraph [ref=s1e868]: そして、「プロンプト」「良い回答」「悪い回答」の3つを組とするデータセットで学習された、報酬モデル\n        r_\\theta(r|y,x) （Reward Model）が必要です。 報酬モデル r_\\theta(r|y,x) は「プロンプト x\n        」と「LLMの出力回答 y 」を入力することで、「報酬 r 」を出力します。\n    - paragraph [ref=s1e880]: この二つがあれば、著者が提案したGRPOという手法で、強化学習が可能になります。 （GRPOでは、状態価値モデルは不要です）\n    - paragraph [ref=s1e882]:\n        - text: しかしながら著者らは、さらに驚くべきことに、「DeepSeek-R1-Zero」を学習する際に、\n        - strong [ref=s1e884]: 報酬モデルを使わずに、ルールベースで報酬を計算しています。\n        - text: それにより、圧倒的な計算効率を達成しています。\n    - paragraph [ref=s1e886]: 著者らが設定しているルールは下記の二つです。\n    - list [ref=s1e887]:\n        - listitem [ref=s1e888]:\n            - strong [ref=s1e889]: 精度報酬\n            - list [ref=s1e890]:\n                - listitem [ref=s1e891]: 応答が正しいかどうかを評価する\n                - listitem [ref=s1e892]: 例えば、数学の問題であれば、最終的な回答が答えと合っているかどうかで報酬を渡す。\n        - listitem [ref=s1e893]:\n            - strong [ref=s1e894]: フォーマット報酬\n            - list [ref=s1e895]:\n                - listitem [ref=s1e896]:\n                    - text: 思考プロセスを\n                    - code [ref=s1e897]: <think>\n                    - text: と\n                    - code [ref=s1e898]: </think>\n                    - text: タグの間に置くことをモデルに強制し、合っていない場合はペナルティを課す。\n    - paragraph [ref=s1e899]:\n        - text: このたった2つの報酬のみをルールベースで設定しており、\n        - strong [ref=s1e901]: ニューラルネットワークベースの報酬モデルは利用していません\n    - paragraph [ref=s1e902]: ニューラルネットワークの報酬モデルを利用せずに、ルールベース報酬だけで強化学習をすることにより、報酬モデル分の計算量を削減しています。\n    - heading "この報酬設計で良い理由" [level=4] [ref=s1e903]\n    - paragraph [ref=s1e904]: フォーマット報酬は、モデルに出力形式を強制するものであるので、思考能力を向上させる直接的な報酬ではありません。\n        であれば、「DeepSeek-R1-Zero」の圧倒的な思考能力を支えているのは、「精度報酬」になります。\n    - paragraph [ref=s1e906]: 本論文での、強化学習の段階で利用しているデータセットは、おそらく数学データセットです。\n    - paragraph [ref=s1e907]:\n        - text: 数学の問題は、基本的に回答が一意に定まるため、ルールベースであっても出力が正しいかどうかは判断できます。\n        - strong [ref=s1e909]: そして、数学の問題に正答するためには、思考能力を強化する必要があります。\n    - complementary [ref=s1e910]:\n        - text: "!"\n        - paragraph [ref=s1e913]: ここが本論文の非常に面白い部分です。\n            DeepSeek-R1-Zeroを学習する際に、より長い長考をさせるようなデータセットによる学習や、思考部分に対して報酬を与えるような仕組みは「全く無く」、ただ、数学の問題の回答が合っているか間違っているかのゴール報酬のみが与えられています。\n        - paragraph [ref=s1e915]: それだけにも関わらず、自動的に数学の問題を正答するために、思考能力を強化するように学習が進んでおり、強化学習の威力を感じます。\n    - paragraph [ref=s1e916]: 強化学習の枠組みにおいて、最終的に報酬を得る（問題に正解する）ために、今、何のtokenを出力しないといけないのか、が最適化されます。\n        結果として、問題の正当に近づくようなtokenを出力する能力、すなわち問題をステップバイステップで解く能力が磨かれることになります。\n    - paragraph [ref=s1e918]: このように、数学という問題の答えを合わせるように報酬設計をすることで、勝手に思考能力が強化される、というのは非常に面白いなと思います。\n    - heading "（補足）データセットの作り方（の考察）" [level=4] [ref=s1e919]\n    - paragraph [ref=s1e920]:\n        - text: 本論文での、強化学習に利用するデータセットは、おそらく\n        - link "上述した論文" [ref=s1e921]:\n            - /url: https://arxiv.org/html/2402.03300v3\n        - text: の前半に記載されている数学データセットです。\n    - complementary [ref=s1e922]:\n        - text: "!"\n        - paragraph [ref=s1e925]: 強化学習に利用したデータセットに関しては、本論文で言及されていなかったと思うので憶測です。間違っていたら教えてください。\n    - paragraph [ref=s1e926]: 本論文の主要部分ではないですが、興味深いため紹介します。\n    - paragraph [ref=s1e927]:\n        - img [ref=s1e928]\n        - text: 引用：\n        - \'link "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models" [ref=s1e929]\':\n            - /url: https://arxiv.org/html/2402.03300v3\n    - paragraph [ref=s1e930]:\n        - strong [ref=s1e931]: 1. Train a FastText Model\n        - text: まず初めに、「OpenWebMath」という高品質の数学のウェブテキストのデータセットを初期のシードコーパスとして利用します。\n            このコーパスから50万のデータを正例、 Common\n            Crawlデータセット（大規模なWebページコーパス）から取得した50万のデータを負例として、fastTextモデルを学習します。\n            こうすることにより、fastTextモデルを利用して、Webページから、それが数学コンテンツか、非数学コンテンツかを分類することができます。\n    - paragraph [ref=s1e935]:\n        - strong [ref=s1e936]: 2. Recall Math-Related Webpages From Common Crawl\n    - paragraph [ref=s1e937]: 学習した、fastTextモデルを利用して、Common\n        Crawlデータセットから数学のWebページを取得します。 このとき、低品質な数学コンテンツを除外するために、 fastText\n        モデルによって予測されたスコアをもとにランク付けした、上位のスコアのページのみを取得します。\n    - paragraph [ref=s1e939]:\n        - strong [ref=s1e940]: 3. Discover Math-Related Domains\n    - paragraph [ref=s1e941]: この方法で取得されたWebページは、数学の問題ではありますが、偏りがあります。\n        なぜなら、初期の学習に利用された「OpenWebMath」データセットには、少数のデータしかなく、さらに上位のスコアのWebコーパス（類似度の高いWebコーパス）しか収集していないため、「OpenWebMath」データセットと似たような数学Webページしか、収集できていないからです。\n    - paragraph [ref=s1e943]: そこで、上記の方法で収集したWebページの親ドメインに着目します。\n        そして、親ドメイン内のWebページのうち、10%以上のWebページが、今回の方法で収集されている場合、親ドメイン自体が数学に関連するドメインであると、考えることができます。\n    - complementary [ref=s1e945]:\n        - text: "!"\n        - paragraph [ref=s1e948]: 例えば、「mathhoge.com」の配下のページの10%以上がfastText\n            モデルによって収集されていた場合は、「mathhoge.com」自体を数学コンテンツであると分類します。\n    - paragraph [ref=s1e949]: あとは、この親ドメイン内の、まだ収集されていないWebページから、高品質な数学Webページを収集することで、幅広い数学のコンテンツを収集できそうです。\n    - paragraph [ref=s1e950]:\n        - strong [ref=s1e951]: 4. Annotate Math-Related URL Path From Labelers\n    - paragraph [ref=s1e952]: 分類されたドメイン内の数学コンテンツに対して、「手動」でURLに注釈をつけ、注釈をつけたもののうち、またシードコーパスに収集されていないWebページを、シードコーパスに追加します。\n    - paragraph [ref=s1e953]: この方法を繰り返すことで、高品質かつ多様な数学データセットを収集することができます。\n        著者らはこのデータ収集を4回繰り返したのちに、3,550万の数学Webページ、合計1200億トークンのコーパスを完成させました。\n    - paragraph [ref=s1e955]: 4回の繰り返しの段階で、データのうち98%が3回目までの繰り返しですでに収集されていたことがわかったため、4回で繰り返し作業を終了したとのことです。\n    - heading "（補足）DPO（Direct Preference Optimization）との違い" [level=3]\n      [ref=s1e956]\n    - paragraph [ref=s1e957]: DPO（Direct Preference Optimization）という手法があります。\n        こちらは、強化学習を利用して、LLMに好ましい回答を学習させるRLHF（Reinforcement learning from human\n        feedback）と同様の効果を、強化学習を使わずに学習させることができる手法です。\n    - paragraph [ref=s1e959]: "強化学習をしないため、報酬モデルも状態価値モデルも必要なく、単純に「プロンプト x 」と「好ましい回答 y^+\n        」「好ましくない回答 y^- 」の3つが組となっているデータセット D を用意すれば、あとは \\\\pi_\\\\phi(y^+|x) 、\n        \\\\pi_\\\\phi(y^-|x) 、 \\\\pi_{ref}(y^+|x) 、 \\\\pi_{ref}(y^+|x)\n        を計算して、損失関数に落とし込むことで、学習が可能になります。"\n    - paragraph [ref=s1e976]: このように、「報酬モデルも状態価値モデルも必要ない」の部分が、本論文で提案している「GRPO（Group\n        Relative Policy Optimization）+ルールベース報酬」の手法の利点と近いです。\n    - paragraph [ref=s1e977]: ただし、DPOでは、上述した通り、「プロンプト x 」と「好ましい回答 y^+ 」「好ましくない回答 y^-\n        」の3つが組となっているデータセットが必要です。 逆に言えば、このデータセットの範囲内でしか学習ができません。\n    - paragraph [ref=s1e985]: 一方で、著者らが提案する「GRPO（Group Relative Policy\n        Optimization）+ルールベース報酬」の手法では、数学などの答えが一意に決まり、ルールベースで正解不正解を判断できるデータセット（つまり、数学データセット）であれば、「プロンプト\n        x 」と「回答 y 」の組だけで、学習が可能です。\n    - paragraph [ref=s1e990]:\n        - strong [ref=s1e991]: したがって、DPOよりもはるかに多くのデータを用いて学習ができます。\n        - text: これが、今回の提案手法の利点になりそうです。\n    - heading "著者らの執念" [level=3] [ref=s1e993]\n    - paragraph [ref=s1e994]: 著者らは、LLMを強化学習に適用する上で、計算量的に障害になる報酬モデルと状態価値モデルを利用しない斬新な手法を提案しました。\n        これにより、圧倒的な計算効率にて、強化学習を適用させました。\n    - paragraph [ref=s1e996]: また、似た効果を持つ手法としてDPOという手法があります。 ただし、DPOでは、利用できるデータセットが制限されるため、大規模な学習は困難です。\n    - paragraph [ref=s1e998]: しかしながら著者らの手法は、ただ数学のように一定の推論の結果、答えが一意に定まるような問題を解かせ続ければ、ルールベース報酬による強化学習で、思考機能を強化させることができてしまっています。\n    - paragraph [ref=s1e999]:\n        - strong [ref=s1e1000]: 絶対に、学習データの量を減らさずに、計算量を徹底的に削減してやろうという著者らの執念を感じるとともに、こういう工夫に積み重ねが、高性能と圧倒的に格安なAPI料金の両立を達成しているのだろうなと感じました。\n    - heading "ZeroのSystemプロンプト" [level=2] [ref=s1e1001]\n    - paragraph [ref=s1e1002]: 続いては、少し話が変わり、「DeepSeek-R1-Zero」に与えているsystemプロンプトについて紹介します。\n        このように、systemプロンプトが公開されているというのは、非常に勉強になりますね。とても嬉しいです\n    - paragraph [ref=s1e1004]: systemプロンプトは下記です。\n    - paragraph [ref=s1e1005]: "A conversation between User and Assistant. The user\n        asks a question, and the Assistant solves it. The assistant first thinks\n        about the reasoning process in the mind and then provides the user with\n        the answer. The reasoning process and answer are enclosed within <think>\n        </think> and <answer> </answer> tags, respectively, i.e., <think>\n        reasoning process here </think> <answer> answer here </answer>. User:\n        prompt. Assistant:"\n    - paragraph [ref=s1e1006]: 訳）\n        ユーザーとアシスタントの会話。ユーザーが質問し、アシスタントがそれを解決する。アシスタントは、まず頭の中で推論プロセスを考え、それからユーザに答えを提供します。推論過程と答えはそれぞれ<think>\n        </think>タグと<answer> </answer>タグで囲まれます。つまり、<think> 推論過程はここ</think>\n        <answer>答えはここ</answer>です。ユーザー：プロンプト アシスタント：\n    - paragraph [ref=s1e1008]: 上記部分の「prompt」の部分は、ユーザが入力したプロンプトの内容に書き換えられます。\n        このように、systemプロンプトにおいて<think> </think>タグを利用して、その中で思考をさせる形になっています。\n    - heading "Zeroの「Aha Moment」" [level=2] [ref=s1e1010]\n    - paragraph [ref=s1e1011]: 「Aha Moment」とは、辞書で調べると「〔問題の答えが突然ひらめいたときなどの〕なるほど！と思う瞬間」とのことです。\n    - paragraph [ref=s1e1012]: 実は、DeepSeek-R1-Zeroの出力において、それが発生したそうです。 数学の問題を解く途中で、下記のような出力が見られました。\n    - paragraph [ref=s1e1014]: "Wait, wait. Wait. That’s an aha moment I can flag\n        here. Let’s reevaluate this step-by-step to identify if the correct sum\n        can be · · · We started with the equation:"\n    - paragraph [ref=s1e1017]: 訳） 待て、待て。待ってくれ。これはハッとさせられる瞬間だ。 正しい和が - - - -\n        になるかどうかを確認するために、ステップ・バイ・ステップで再評価してみよう。\n    - paragraph [ref=s1e1020]: これは恐ろしいことです。\n        モデルは途中まで推論を実施しながら、途中で自身の出力結果を見直し、さらにもう一度ステップバイステップで検討をするという、高度な問題解決能力を獲得しています。\n        （自分の考えを見直して、さらに良い戦略を発見する。これができる人間がどれくらいいるのだろうか）\n    - paragraph [ref=s1e1023]: 何度も記載していますが、DeepSeek-R1-Zeroを学習する際に、問題の解き方を明示的にモデルに教えることや、思考部分を強化させるような報酬は与えていません。\n        ただ、数学の問題に正解することで得られる報酬を与えるだけで、モデルが自律的に高度な問題解決戦略を獲得しています。\n        （もちろん、推論の途中で、検算を行うなどのデータセットで学習もさせていません。もしかしたら、DeepSeek-V3-Baseを学習させるときのSFTデータセットには含まれているかもですが。）\n    - paragraph [ref=s1e1026]: これこそが、本論文で何度も述べている、「強化学習の可能性」です。\n        強化学習は（そのほかの学習手法とは異なり）（人間にとって）予想外で、かつ洗練された結果を導き、人工知能が新たなレベルの知性を手にいれるための核となる技術になりそうです。\n    - heading "DeepSeek-R1-Zeroのまとめ" [level=2] [ref=s1e1028]\n    - paragraph [ref=s1e1029]: DeepSeek-R1-ZeroはDeepSeek-V3-Baseに対して、「GRPO+ルールベース報酬」の強化学習「のみ」を適用し、圧倒的な思考能力を獲得したモデルになります。\n    - paragraph [ref=s1e1030]: DeepSeek-R1-Zeroを学習する際に、問題の解き方を明示的にモデルに教えることや、思考部分を強化させるような報酬は与えていません。\n        ただ、数学の問題に正解することで得られる報酬を与えるだけで、モデルが自律的に高度な問題解決戦略を獲得するという、とても興味深く、恐ろしい結果が示されています。\n    - heading "DeepSeek-R1への道" [level=1] [ref=s1e1032]\n    - paragraph [ref=s1e1033]: さて、ここまで、「DeepSeek-R1-Zero」について、たくさん語ってきましたが、とても魅力的でしたよね？\n        あくまで、「DeepSeek-R1-Zero」は「DeepSeek-V3-Base」に対して、強化学習のみを適用させて、強化学習の可能性を探ったモデルでした。\n    - paragraph [ref=s1e1035]:\n        - text: その結果、こんなにも魅力的なモデル（DeepSeek-R1-Zero）ができてしまったら、、、\n        - strong [ref=s1e1037]: SFTも入れて完全体のモデルを作りたくなりますよね？？？\n    - paragraph [ref=s1e1038]: というモチベーションで「DeepSeek-R1」は作られています。\n    - paragraph [ref=s1e1039]: もし、DeepSeek-R1-Zeroの性能がそんなに高くなかったら、今騒がれているDeepSeek-R1は作ろうという発想にすらならなかったと思います。\n    - heading "DeepSeek-R1-Zeroの課題" [level=2] [ref=s1e1040]\n    - paragraph [ref=s1e1041]: さて、非常に高性能で、示唆に富んだDeepSeek-R1-Zeroでしたが、課題はいくつかあります。 それは下記です。\n    - list [ref=s1e1043]:\n        - listitem [ref=s1e1044]: 学習初期は強化学習の学習が不安定で収束に時間がかかる（コールドスタート問題）\n        - listitem [ref=s1e1045]:\n            - text: 思考能力は高いが、言語が混在（日本語で話していたら急に中国語になるなど）し、人間が理解しにくい\n            - list [ref=s1e1046]:\n                - listitem [ref=s1e1047]: 結果、高度な思考タスクには強いが、（ロールプレイなどの）一般的なタスクには使えないという評価\n    - paragraph [ref=s1e1048]: そこで、上記の課題を解決する旅に出ます。\n    - heading "課題対処への道" [level=2] [ref=s1e1049]\n    - paragraph [ref=s1e1050]: DeepSeek-R1-Zeroでは、ベースモデルであるDeepSeek-V3-Baseに対して、そのまま上述した強化学習を用いて学習をしていました。\n    - paragraph [ref=s1e1051]:\n        - text: しかし、もともとDeepSeek-V3-Baseは長考モデルとして学習されたモデルではないため、いくらsystemプロンプトで長考をさせる制限（\n        - code [ref=s1e1052]: <think>\n        - text: タグ）を入れたとしても、強化学習の初期フェーズは不安定な学習になってしまいます。\n            （例えば、「そもそも適切な形式で思考をしない」、「思考形式は合っているが、意味のある思考をしていない」、その結果として「数学の問題になかなか正答できないまま、報酬が得られず、学習が不安定になる」が考えられます）\n    - paragraph [ref=s1e1054]: そこで、学習初期のコールドスタート問題を防ぐために、DeepSeek-V3-Baseに対して、SFT（Supervised\n        Fine-Tuning）を実施します。 そのために著者らは、少量のCoT（Chain-of-Thought）データセットを構築しています。\n    - heading "CoTデータセットの構築+SFT" [level=3] [ref=s1e1056]\n    - paragraph [ref=s1e1057]: データセットの構築方法は下記が一例として提示されております。\n    - list [ref=s1e1058]:\n        - listitem [ref=s1e1059]: いくつかの長いCoT解答を提示し、モデルに長いCoT形式の回答を生成させてデータセットに追加する\n        - listitem [ref=s1e1060]: モデルに対して、自己反省や検証を行いながら、詳細な回答生成を促すようなプロンプトを送信し、その出力をデータセットに追加する\n        - listitem [ref=s1e1061]: DeepSeek-R1-Zeroの出力を、読みやすい形式に整理してからデータセットに追加する\n        - listitem [ref=s1e1062]: モデルの出力結果を人間が精査して、より適切なCoTデータの形に整理してから、データセットに追加する\n    - paragraph [ref=s1e1063]: 上記の方法で、数千件のCoTデータを構築し、そのデータを用いて、DeepSeek-V3-BaseをSFTしています。\n    - heading "CoTデータセットを用いることの利点" [level=3] [ref=s1e1064]\n    - paragraph [ref=s1e1065]: 著者らは、下記の要素において、CoTデータセットを用いることに利点があると考えています。\n    - paragraph [ref=s1e1066]:\n        - strong [ref=s1e1067]: 読みやすさ\n        - text: DeepSeek-R1-Zeroの一番の課題は、その出力結果が人間が理解するのが難しいことです。\n            回答には、複数の言語が混在していたり、マークダウン形式などの読みやすい形式が適切に使われていないことで、重要な情報が何かわかりにくい状態でした。\n    - paragraph [ref=s1e1070]: そこで、今回のデータセットを作成する際には、各回答の末尾に「要約文」を入れるようにすることで、人間が読みやすい形に出力させるようにしています。\n        具体的には下記のような出力形式で、全てのCoTデータセットは作成されています。\n    - code [ref=s1e1075]: "|special_token|<reasoning_process>|special_token|<summary>"\n    - button "クリップボードにコピー" [ref=s1e1076]:\n        - img [ref=s1e1077]\n    - button "右端で折り返す" [ref=s1e1078]:\n        - img [ref=s1e1079]\n    - paragraph [ref=s1e1080]:\n        - code [ref=s1e1081]: <reasoning_process>\n        - text: の部分に、モデルの思考に該当する部分が、\n        - code [ref=s1e1082]: <summary>\n        - text: の部分に最終的な結論の要約が記載されます。\n    - paragraph [ref=s1e1083]: これにより、「DeepSeek-R1-Zero」にあった、重要な情報が何かわかりにくいという人間が理解するのが難しい問題を緩和します。\n        （言語の混在に関しての対処は後述します）\n    - paragraph [ref=s1e1085]:\n        - strong [ref=s1e1086]: ポテンシャル\n        - text: 今回、強化学習のみを実施するのではなく、人間の事前知識に基づいて作成したCoTデータセットでSFTを実施しています。\n            また、SFTと強化学習を複数回反復して、繰り返し学習をしています。\n            その結果、DeepSeek-R1はDeepSeek-R1-Zeroよりも良い結果を得ることができました。\n    - paragraph [ref=s1e1090]: 著者らは、このような反復的な学習こそが、長考モデル（reasoning models）を学習する上で良い方法であると主張しています。\n    - heading "推論思考の強化学習" [level=3] [ref=s1e1091]\n    - paragraph [ref=s1e1092]: CoTデータセットによりSFTしたDeepSeek-V3-Baseに対して、上述した（DeepSeek-R1-Zeroを学習した）強化学習手法を適用します。\n        これにより、モデルの思考能力が大幅に強化されます。\n    - paragraph [ref=s1e1094]: 一方で、「DeepSeek-R1-Zero」には、言語不一致の問題がありました。\n        そこで、DeepSeek-R1-Zeroのルールベース報酬に、「言語一貫性報酬」を追加で導入しています。\n    - paragraph [ref=s1e1096]: この報酬導入で、実は、モデルの思考能力が若干落ちてしまうことが実験的にわかっています。\n        しかしながら、この報酬があることで、ようやく人間が理解しやすい文章を生成してくれるようになります。\n    - heading "さらなる追加のSFT" [level=3] [ref=s1e1098]\n    - paragraph [ref=s1e1099]: 上記の強化学習が収束したら、さらにデータを追加してSFTをやり直します。\n        ここでは、初期段階の、CoTに重きを置いた、コールドスタートを解消するための学習データだけにとどまらず、ライティング、ロールプレイといった汎用タスクの性能を向上させるため、他ドメインからのデータも合わせて収集します。\n    - heading "Reasoning Data" [level=4] [ref=s1e1101]\n    - paragraph [ref=s1e1102]: まずは、前段階と同様に思考能力を向上させるために、CoT学習データ（Reasoning Data）を取得します。\n    - paragraph [ref=s1e1103]: 取得方法は以下です。\n    - list [ref=s1e1104]:\n        - listitem [ref=s1e1105]:\n            - text: 強化学習済みモデルの出力から、適切なものを収集して学習データに追加\n            - list [ref=s1e1106]:\n                - listitem [ref=s1e1107]: 前段の強化学習では、ルールベースの報酬モデルを利用していたので、それが使える問題（数学など）のみを学習データとしていたが、次の強化学習では報酬モデルを作成するため、もっと広い範囲でCoTができるデータも学習データに追加\n                - listitem [ref=s1e1108]: モデルの出力はまだ読みにくいところがあるため、さまざまフィルタリング\n    - paragraph [ref=s1e1109]: この結果、合計で600kのReasoning Data（学習データ）を収集しています。\n    - heading "Non-Reasoning Data" [level=4] [ref=s1e1110]\n    - paragraph [ref=s1e1111]: ライティングやQ＆A、翻訳などのNon-Reasoning\n        Dataについては、DeepSeek-V3のSFTデータセットの一部を再利用して、学習データを収集しています。 上記のReasoning\n        Data以外のタスク（Non-Reasoning\n        Data）に関しては、DeepSeek-V3によって簡易的なCoTを実施して、回答を生成し、学習データとします。\n        ただし、本当に簡単なタスク（あいさつとか）に関しては、CoTを学習データに追加しません。\n    - paragraph [ref=s1e1114]: こうすることで、合計200kのトレーニングサンプルを最終的に収集したそうです。\n    - paragraph [ref=s1e1115]: 上記の合計800kサンプルの学習データを利用して、DeepSeek-V3-Base（初回強化学習済み）を2epoch分SFTを実施しています。\n    - heading "最後の強化学習" [level=3] [ref=s1e1116]\n    - paragraph [ref=s1e1117]: 最後に、モデルの思考能力をさらに磨くと同時に、モデルの無害性などを向上させることを目的に、強化学習を実施します。\n    - paragraph [ref=s1e1118]: 思考タスクの強化学習に関しては、これまで説明したのと同様に、数学、コード、論理的推論の領域でルールベース報酬を利用して強化学習をします。\n        一方で、思考タスク以外の一般的なデータに対しては、ルールベースで回答の良し悪しを判断することはできないため、報酬モデルを別途DeepSeek-V3パイプラインをベースに作成しています。\n    - heading "DeepSeek-R1のまとめ" [level=2] [ref=s1e1120]\n    - paragraph [ref=s1e1121]: 以上より、DeepSeek-R1では、DeepSeek-V3-Baseに対して、SFTと強化学習を2回実施することにより構築されています。\n    - paragraph [ref=s1e1122]: SFTにより、強化学習のコールドスタート問題の解消と、モデル出力のユーザフレンドリー性の向上、一般的なタスクに対する性能向上に貢献しています。\n    - heading "結果" [level=1] [ref=s1e1123]\n    - paragraph [ref=s1e1124]: 結果に関しては、他の方がさまざま検証していたり、話題になっているので詳しくは書きません。\n        ただ、DeepSeek-R1は、現在のオープンソースモデルの中では、最も高い性能と言ってよく、また、Open\n        AIのo1モデルにも匹敵する性能だと言われています。\n    - heading "蒸留" [level=1] [ref=s1e1126]\n    - paragraph [ref=s1e1127]: 最後に、蒸留モデルについての解説します。（論文でも言及されています）\n        DeepSeek-R1は671Bものモデルパラメータを持つため、一般的な家庭用PCで動作させるのは難しいです。\n    - paragraph [ref=s1e1129]: そこで、より小さなモデルパラメータのモデルに、DeepSeek-R1のような思考能力を与えるために、蒸留を実施しています。\n        （公式が、個人の利用者のことも考えて、家庭用PCで動作する思考モデルを開発してくれるのは非常にありがたいですね）\n    - paragraph [ref=s1e1131]: 具体的には、Qwen2.5-Math-1.5B、Qwen2.5-Math-7B、Qwen2.5-14B、Qwen2.5-32B、Llama-3.1-8B、Llama-3.3-70B-Instructに対して、DeepSeek-R1から取得した800kのトレーニングサンプルを利用して、直接FineTuningする形で、蒸留を実施しています。\n    - paragraph [ref=s1e1132]: 蒸留モデルに対しては、SFTのみを実施し、後段の強化学習は実施しておらず、蒸留モデルに対する追加の強化学習の探究は、著者らが実施する予定はないそうです。\n        （著者らは、強化学習を追加で実施することにより性能が上がると思っているが、その探究は、別のコミュニティに任せると記述しています）\n    - heading "蒸留 vs 強化学習" [level=2] [ref=s1e1134]\n    - paragraph [ref=s1e1135]: 蒸留後の強化学習に関しては、著者らは実験していない、と記述していました。\n        では、小型モデルに対して、蒸留を実施する前に、強化学習を実施したらどうなるのでしょうか？\n        DeepSeek-V3-Baseのように、大幅に思考能力が強化されて、性能向上が期待できるのでしょうか？\n    - paragraph [ref=s1e1138]: それに関しては、著者らが実験してくれています。\n    - paragraph [ref=s1e1139]: ここでは、「Qwen-32B-Base」モデルを利用して、10kステップ以上の大規模強化学習を実施し、「DeepSeek-R1-Zero-Qwen-32B」を構築しました。\n        一方で、「Qwen-32B-Base」モデルに対して、蒸留を実施したモデルを、「DeepSeek-R1Distill-Qwen-32B」モデルとして、この二つのモデルを比較します。\n    - paragraph [ref=s1e1141]: その結果を比較すると、小型モデルに対する強化学習を適用した「DeepSeek-R1-Zero-Qwen-32B」よりも、小型モデルに蒸留を適用した「DeepSeek-R1Distill-Qwen-32B」モデルの方がはるかに高い性能でした。\n    - paragraph [ref=s1e1142]:\n        - strong [ref=s1e1143]: 以下は私の主観ですが、\n        - text: 以上の結果から、\n        - strong [ref=s1e1146]: 強化学習による思考能力の向上は、安易に小型モデルに試していい戦略でない\n        - text: ということが示唆されていると思います。\n    - paragraph [ref=s1e1148]: 強化学習による思考能力の大幅な強化は、おそらくですが「DeepSeek-V3」という元々大規模かつ性能の高いモデルであったからこそ、達成されたのだと思います。\n    - paragraph [ref=s1e1149]: DeepSeek-R1-Zeroの課題でも説明した通り、今回のルールベース報酬では、正しい数学の解答を生成できないと永遠に報酬が獲得できません。\n        小型モデルでは、このコールドスタートの問題が、より致命的なレベルで発生するのだと思います。\n    - paragraph [ref=s1e1151]: したがって、思考モデルを作りたいからといって、小型モデルに対して、安易にルールベース強化学習を試すのはおすすめしません。\n        しかしながら、コールドスタート問題を解決できる戦略があるのであれば、有効な手段になると思われます。\n        （例えば、DeepSeek-R1で蒸留済みの小型モデルを利用する、最初から数学に強い小型モデルで強化学習を試すなどです。重要なのは強化学習のコールドスタートを解消できるかどうかだと思われます。）\n    - paragraph [ref=s1e1154]:\n        - text: （追記） また、同様の実験を小型モデルに対して試したグループもいるようです。 そちらの実験レポートが下記になります。\n        - iframe [ref=s1e1158]: <could not take iframe snapshot>\n        - text: （DeepSeek-R1-Zeroと同様に、「Aha Moment」が見られることもわかっており、非常に興味深いです）\n    - paragraph [ref=s1e1159]: ただ、こちらのレポートにおいても、ベースモデルとしては数学に特化したモデルを利用しています。\n        このことからも、ある程度の数学能力は必要なのだなと思います。\n    - heading "今後の展望" [level=1] [ref=s1e1161]\n    - paragraph [ref=s1e1162]: DeepSeek-R1は、さまざまなタスクにおいて、OpenAI\n        o1に匹敵する性能を達成した、驚異的なオープンソースモデルですが、苦手としている部分もあります。 それが今後の展望に記載されています。\n        （今後の展望を見れば、DeepSeek-R1が苦手な部分を知ることができます）\n    - paragraph [ref=s1e1165]: それは以下の項目です。\n    - list [ref=s1e1166]:\n        - listitem [ref=s1e1167]:\n            - strong [ref=s1e1168]: 一般的な能力\n            - list [ref=s1e1169]:\n                - listitem [ref=s1e1170]: 関数呼び出しやマルチターン対話、複雑なロールプレイング。、json出力において、DeepSeek-V3に及ばない\n        - listitem [ref=s1e1171]:\n            - strong [ref=s1e1172]: 言語の混在\n            - list [ref=s1e1173]:\n                - listitem [ref=s1e1174]: 現在、DeepSeek-R1は英語と中国語に最適化されているため、そのほかの言語を利用した場合、言語の混在が発生する可能性がある\n        - listitem [ref=s1e1175]:\n            - strong [ref=s1e1176]: プロンプトエンジニアリング\n            - list [ref=s1e1177]:\n                - listitem [ref=s1e1178]: DeepSeek-R1はプロンプトに非常に敏感で、特にFewーshotプロンプトを利用すると、パフォーマンスが低下する。\n                - listitem [ref=s1e1179]: 基本的には、問題を直接シンプルに記述し、例を提示しない（Zero-shotプロンプト）を利用することを推奨している\n        - listitem [ref=s1e1180]:\n            - strong [ref=s1e1181]: ソフトウェアエンジニアリングタスク\n            - list [ref=s1e1182]:\n                - listitem [ref=s1e1183]: コード生成やコード保管などのソフトウェアエンジニアリングタスクは、評価に時間がかかるため、強化学習の効率が悪くなることから、現時点では強化学習において、このようなタスクは学習していない。\n                    大々的には学習しておらず、一部のテスト可能な範囲でしか学習できていない。\n                - listitem [ref=s1e1185]: 結果として、このタスクの性能は、DeepSeek-V3からほとんど改善していない。\n    - complementary [ref=s1e1186]:\n        - text: "!"\n        - paragraph [ref=s1e1189]: （追記） masaishiさんのコメントに基づき修正させていただきました。 私の読み間違いで誤解を与えてしまい申し訳ございません。\n        - blockquote [ref=s1e1192]:\n            - paragraph [ref=s1e1193]: 2.2.2. Reward Modeling ・・・ ・Accuracy\n                rewards:・・・Similarly, for LeetCode problems, a compiler can be\n                used to generate feedback based on predefined test cases.\n        - paragraph [ref=s1e1196]: と記載のあるように、全くコード生成などを学習していないわけではなく、一部の、簡単にテストができる問題のみ学習しているようです。\n            ただ、そういう簡易的なタスクだけしか学習できていないため、性能向上は小さめということだと思われます。\n    - paragraph [ref=s1e1198]: 上記の内容が、現時点での「DeepSeek-R1」の課題です。\n        ですが、今後のアップデートで改善していくとも記載されているため、現時点で不満があったとしても、上記に記載されている項目であれば、今後改善する可能性が高いです。\n    - heading "まとめ" [level=1] [ref=s1e1200]\n    - paragraph [ref=s1e1201]: ここまで読んでくださった方、本当にお疲れ様でした。\n        （これでも説明はだいぶ短くした方なんです・・・本当は、もっと強化学習の部分とかを数式をもとに説明したかったです）\n    - paragraph [ref=s1e1203]: DeepSeek-R1の論文いかがでしたか？\n        私にとっては、非常に勉強になるような示唆や知見に溢れており、こんな論文を公開してくれて、かつオープンソースのモデルとしても公開してくれる「DeepSeek-AI」には感謝しかありません。\n    - paragraph [ref=s1e1205]: 本記事が、皆様の理解の助けになり、一人でも多くの方が原論文に興味を持って、読んでくれましたら幸いです。\n        ここまで読んでくださってありがとうございました！\n    - heading "参考文献" [level=1] [ref=s1e1207]\n    - paragraph [ref=s1e1208]: 本論文は、論文の中では読みやすい方だとは思いますが、強化学習に関しては、一定程度の基礎知識は要求されます。\n        そこで、本論文を読む上で必要な知識を収集できるおすすめの書籍に関して記載します。 （少なくとも私はこの書籍で学習しました）\n    - paragraph [ref=s1e1211]: （書籍のリンクはamazonアフィリエイトリンクになります）\n    - heading "強化学習" [level=4] [ref=s1e1212]\n    - paragraph [ref=s1e1213]: 強化学習に関しておすすめの本は、下記の本たちです。\n    - paragraph [ref=s1e1214]:\n        - link "ゼロから作るDeep Learning ❹ ―強化学習編" [ref=s1e1215]:\n            - /url: https://amzn.to/40qFuNg\n        - text: まずは、定番の本です。\n            強化学習に関して、本当に最初の部分（マルコフ決定過程とは何か）から、非常に詳細かつわかりやすく解説されており、深層学習を利用して、どのように強化学習をすることできるのかまで、しっかり書かれているためおすすめです。\n            また、コードと理論のバランスもちょうどよく、理系大学生とかであれば問題なく読み進められると思います。\n            特に、強化学習の基礎の部分が、詳細かつ分かりやすく書かれているため、一冊目におすすめです。\n    - paragraph [ref=s1e1220]:\n        - link "機械学習スタートアップシリーズ Ｐｙｔｈｏｎで学ぶ強化学習 ［改訂第２版］ 入門から実践まで (ＫＳ情報科学専門書)" [ref=s1e1221]:\n            - /url: https://amzn.to/4joKbjg\n    - paragraph [ref=s1e1223]: 普段からpythonを使っており、数式よりも実装に興味がある方にとっては、こちらの書籍もおすすめです。\n        こちらの書籍は、強化学習の基礎よりは、少し応用というか、実用を意識した書籍になっており、実務でよく使われるモデルなどに関しても、詳しく説明されています。\n        強化学習の基礎はわかっている人や、もっと実装をベースに学びたいというかたにおすすめです。\n    - paragraph [ref=s1e1226]:\n        - link "強化学習 (機械学習プロフェッショナルシリーズ)" [ref=s1e1227]:\n            - /url: https://amzn.to/40NX5QM\n        - link "強化学習から信頼できる意思決定へ (AI/データサイエンスライブラリ“基礎から応用へ” 5)" [ref=s1e1229]:\n            - /url: https://amzn.to/4jtFq8d\n    - paragraph [ref=s1e1230]: 名著です。\n        どちらも、理論説明の部分が非常に面白く、正直難易度は高いですが、強化学習の理論をちゃんと理解したいという方におすすめの書籍です。\n        下の本に関しては、2024年の本ということもあり、近年の強化学習の実世界での応用と、理論的な定式化の架け橋になっているような本になっており、非常に面白いです。\n    - heading "AlphaGO Zero" [level=4] [ref=s1e1233]\n    - paragraph [ref=s1e1234]:\n        - link "AlphaZero 深層学習・強化学習・探索 人工知能プログラミング実践入門" [ref=s1e1235]:\n            - /url: https://amzn.to/3Cdnuy1\n    - paragraph [ref=s1e1236]: だいぶ昔に購入させていただいた本ですが、まさか2025年に活躍することがあろうとは。\n        という感じですが、こちらの書籍も参考にさせていただきました。 Alpha\n        Zeroを作る上で核となる技術を余すところなく解説されており、私は非常に勉強になりました。 Alpha\n        Zeroに興味がある方は、ぜひ一度ご一読ください。\n    - heading "LLM、RLHF, DPO" [level=4] [ref=s1e1240]\n    - paragraph [ref=s1e1241]:\n        - link "大規模言語モデル入門Ⅱ〜生成型LLMの実装と評価" [ref=s1e1242]:\n            - /url: https://amzn.to/3Cs3Uhn\n        - text: よく紹介させていただいておりますが、こちらの書籍は、LLMのファインチューニングから、RLHF、RAG、分散学習にかけて、本当に幅広く解説されており、いつも参考にさせていただいております。\n            今回のDPOやRLHFの解説部分でも、とても活躍してくれました。\n            LLMを取り扱っている方は、とりあえず買っておいても損はないと思います。\n            さまざまな章、ページで読めば読むほど新しい発見が得られる、スルメのような本だなといつも思っております。\n    - button "いいね" [ref=s1e1249]:\n        - img [ref=s1e1250]\n    - text: "921"\n    - button "ブックマーク" [ref=s1e1266]:\n        - img [ref=s1e1267]\n    - text: "344"\n    - button "その他の操作" [ref=s1e1272]:\n        - img [ref=s1e1273]\n    - tooltip "X(Twitter)にポスト" [ref=s1e1276]:\n        - img [ref=s1e1277]\n        - text: X(Twitter)にポスト\n    - tooltip "Facebookに投稿" [ref=s1e1283]:\n        - img [ref=s1e1284]\n        - text: Facebookに投稿\n    - tooltip "はてなブックマークに登録" [ref=s1e1290]:\n        - img [ref=s1e1291]\n        - text: はてなブックマークに登録\n    - complementary [ref=s1e1294]:\n        - link "asap" [ref=s1e1296]:\n            - /url: /asap\n            - img "asap" [ref=s1e1297]\n        - link "asap" [ref=s1e1299]:\n            - /url: /asap\n        - paragraph [ref=s1e1301]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n        - button "フォロー" [ref=s1e1305]\n        - tooltip "@personabb" [ref=s1e1306]:\n            - img "GitHub" [ref=s1e1307]\n            - text: "@personabb"\n        - tooltip "@asap2650" [ref=s1e1309]:\n            - img "X(Twitter)" [ref=s1e1310]\n            - text: "@asap2650"\n        - tooltip "yard.tips" [ref=s1e1316]:\n            - img "リンク" [ref=s1e1317]\n            - text: yard.tips\n    - complementary [ref=s1e1320]:\n        - text: バッジを贈って著者を応援しよう\n        - paragraph [ref=s1e1324]: バッジを受け取った著者にはZennから現金やAmazonギフトカードが還元されます。\n        - button "バッジを贈る" [ref=s1e1326]\n        - img [ref=s1e1327]\n    - text: この記事に贈られたバッジ\n    - img "Thank you" [ref=s1e1338]\n    - img [ref=s1e1340]\n    - img "参考になった" [ref=s1e1352]\n    - img [ref=s1e1354]\n    - img "もっと読みたい" [ref=s1e1363]\n    - img [ref=s1e1365]\n    - img "参考になった" [ref=s1e1375]\n    - img [ref=s1e1377]\n    - img "もっと読みたい" [ref=s1e1386]\n    - img [ref=s1e1388]\n    - heading "Discussion" [level=3] [ref=s1e1400]\n    - article [ref=s1e1403]:\n        - link "はまなすなぎさ" [ref=s1e1405]:\n            - /url: /nhamanasu\n            - img "はまなすなぎさ" [ref=s1e1406]\n        - link "はまなすなぎさ" [ref=s1e1407]:\n            - /url: /nhamanasu\n        - link "2025年1月28日 8時33分" [ref=s1e1408]:\n            - /url: "#comment-f230c9d295b3f5"\n            - tooltip "2025年1月28日 8時33分" [ref=s1e1409]: 2ヶ月前 2025年1月28日 8時33分\n        - button "メニューを開く" [ref=s1e1411]:\n            - img [ref=s1e1412]\n        - paragraph [ref=s1e1417]: 非常にためになる骨太な記事、ありがとうございます！とても勉強になります。\n        - paragraph [ref=s1e1418]: ちょっとした誤字かも？の箇所なのですが、冒頭の「強化学習手法「DRPO」すごい！」の部分は「GRPO」ではないかなと思いました。ご報告まで。\n        - button "いいね" [ref=s1e1421]:\n            - img [ref=s1e1422]\n        - text: "1"\n        - button "コメント 1" [ref=s1e1437]:\n            - img "コメント" [ref=s1e1438]\n            - text: "1"\n        - article [ref=s1e1442]:\n            - link "asap" [ref=s1e1444]:\n                - /url: /asap\n                - img "asap" [ref=s1e1445]\n            - link "asap" [ref=s1e1446]:\n                - /url: /asap\n            - link "2025年1月28日 10時23分" [ref=s1e1447]:\n                - /url: "#comment-41cdebd92d8d44"\n                - tooltip "2025年1月28日 10時23分" [ref=s1e1448]: 2ヶ月前 2025年1月28日 10時23分\n            - button "メニューを開く" [ref=s1e1450]:\n                - img [ref=s1e1451]\n            - paragraph [ref=s1e1456]: 読んでくださりありがとうございます！ おっしゃる通りです！修正させていただきました！\n            - button "いいね" [ref=s1e1460]:\n                - img [ref=s1e1461]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1477]\n    - article [ref=s1e1479]:\n        - link "bang" [ref=s1e1481]:\n            - /url: /zenmnm\n            - img "bang" [ref=s1e1482]\n        - link "bang" [ref=s1e1483]:\n            - /url: /zenmnm\n        - link "2025年1月28日 13時46分" [ref=s1e1484]:\n            - /url: "#comment-d0aac9375fbbf7"\n            - tooltip "2025年1月28日 13時46分" [ref=s1e1485]: 2ヶ月前 2025年1月28日 13時46分\n        - button "メニューを開く" [ref=s1e1487]:\n            - img [ref=s1e1488]\n        - paragraph [ref=s1e1493]: AIについてはまだ詳しくありませんが、興味はあります。同じように古いモデルを使って新しいモデルを作るのですが、蒸留方法の代わりにモデルを組み合わせることは可能でしょうか?sakana\n            aiのようにです\n        - button "いいね" [ref=s1e1496]:\n            - img [ref=s1e1497]\n        - text: "1"\n        - button "コメント 2" [ref=s1e1512]:\n            - img "コメント" [ref=s1e1513]\n            - text: "2"\n        - article [ref=s1e1517]:\n            - link "asap" [ref=s1e1519]:\n                - /url: /asap\n                - img "asap" [ref=s1e1520]\n            - link "asap" [ref=s1e1521]:\n                - /url: /asap\n            - link "2025年1月28日 15時33分" [ref=s1e1522]:\n                - /url: "#comment-27e79c6ee51f2a"\n                - tooltip "2025年1月28日 15時33分" [ref=s1e1523]: 2ヶ月前 2025年1月28日 15時33分\n            - button "メニューを開く" [ref=s1e1525]:\n                - img [ref=s1e1526]\n            - paragraph [ref=s1e1531]: モデルマージに関してはあまり詳しくないですが、基本的には同じアーキテクチャのモデルを利用することになると思います。したがってDeepSeek-R1によって蒸留されたLlamaモデルとかであれば、他のLlamaモデル（同じモデルアーキテクチャ）とモデルマージすることで、推論機能を付け加えたり、日本語機能を強化したりすることが期待できると思います。\n            - button "いいね" [ref=s1e1534]:\n                - img [ref=s1e1535]\n        - article [ref=s1e1549]:\n            - link "bang" [ref=s1e1551]:\n                - /url: /zenmnm\n                - img "bang" [ref=s1e1552]\n            - link "bang" [ref=s1e1553]:\n                - /url: /zenmnm\n            - link "2025年1月28日 16時42分" [ref=s1e1554]:\n                - /url: "#comment-0221309240798f"\n                - tooltip "2025年1月28日 16時42分" [ref=s1e1555]: 2ヶ月前 2025年1月28日 16時42分\n            - button "メニューを開く" [ref=s1e1557]:\n                - img [ref=s1e1558]\n            - paragraph [ref=s1e1563]: お返事ありがとうございます。私はまだ初心者なので、これは私の軽い考えです。今では研究のプロセスごとにいくつかの方法があり、それらを組み合わせたり微調整したりすれば、たくさんのモデルができるようになっているでしょう。\n            - button "いいね" [ref=s1e1566]:\n                - img [ref=s1e1567]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1583]\n    - article [ref=s1e1585]:\n        - link "HWSeungwoo" [ref=s1e1587]:\n            - /url: /syoyuu96\n            - img "HWSeungwoo" [ref=s1e1588]\n        - link "HWSeungwoo" [ref=s1e1589]:\n            - /url: /syoyuu96\n        - link "2025年1月28日 19時28分" [ref=s1e1590]:\n            - /url: "#comment-a07230ddbdaebe"\n            - tooltip "2025年1月28日 19時28分" [ref=s1e1591]: 2ヶ月前 2025年1月28日 19時28分\n        - button "メニューを開く" [ref=s1e1593]:\n            - img [ref=s1e1594]\n        - paragraph [ref=s1e1599]: Deep\n            Seekに関する記事やニュースはたくさん目にしますが、ここまで深掘りされた内容には驚きました。とても勉強になりました。ありがとうございます！\n        - button "いいね" [ref=s1e1602]:\n            - img [ref=s1e1603]\n        - text: "1"\n        - button "コメント 1" [ref=s1e1618]:\n            - img "コメント" [ref=s1e1619]\n            - text: "1"\n        - article [ref=s1e1623]:\n            - link "asap" [ref=s1e1625]:\n                - /url: /asap\n                - img "asap" [ref=s1e1626]\n            - link "asap" [ref=s1e1627]:\n                - /url: /asap\n            - link "2025年1月28日 19時32分" [ref=s1e1628]:\n                - /url: "#comment-1e1e3abc86df00"\n                - tooltip "2025年1月28日 19時32分" [ref=s1e1629]: 2ヶ月前 2025年1月28日 19時32分\n            - button "メニューを開く" [ref=s1e1631]:\n                - img [ref=s1e1632]\n            - paragraph [ref=s1e1637]: 読んでくださってありがとうございます！ コメント嬉しいです！ありがとうございます！\n            - button "いいね" [ref=s1e1641]:\n                - img [ref=s1e1642]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1658]\n    - article [ref=s1e1660]:\n        - link "masaishi" [ref=s1e1662]:\n            - /url: /masaishi\n            - img "masaishi" [ref=s1e1663]\n        - link "masaishi" [ref=s1e1664]:\n            - /url: /masaishi\n        - link "2025年1月29日 16時14分" [ref=s1e1665]:\n            - /url: "#comment-a0d75bd96cadfc"\n            - tooltip "2025年1月29日 16時14分" [ref=s1e1666]: 2ヶ月前 2025年1月29日 16時14分\n        - button "メニューを開く" [ref=s1e1668]:\n            - img [ref=s1e1669]\n        - paragraph [ref=s1e1674]: 素晴らしい記事をありがとうございます！\n            GRPOなどの強化学習手法に関する解説も大変勉強になりました。この記事をきっかけに、DeepSeek-R1の論文にも興味を持ち、読んでみようと思えました。\n        - paragraph [ref=s1e1676]: 一点だけ気になったことがあり、もし私の理解が誤っていたら申し訳ありません。\n            "コード生成やコード保管などのソフトウェアエンジニアリングタスク"の強化学習に用いられていないとありますが、論文の\n            2.2.2（Reward Modeling）の\n        - blockquote [ref=s1e1678]:\n            - paragraph [ref=s1e1679]: For LeetCode problems, a compiler can be used to\n                generate feedback based on predefined test cases.\n        - paragraph [ref=s1e1680]: という記述を見ると、少なくともアルゴリズム問題のコード生成には強化学習が用いられているのかなと思います。\n        - paragraph [ref=s1e1681]: ソフトウェアエンジニアリングタスクは強化学習していませんが、コード生成自体は行っているような気がするので、注釈などで少し補足していただけると、読者としては誤解が減って助かるかもしれません。しかし、かなり細かい点ですので、最終的には著者さまの判断にお任せいたします。\n        - button "いいね" [ref=s1e1684]:\n            - img [ref=s1e1685]\n        - text: "2"\n        - button "コメント 2" [ref=s1e1700]:\n            - img "コメント" [ref=s1e1701]\n            - text: "2"\n        - article [ref=s1e1705]:\n            - link "asap" [ref=s1e1707]:\n                - /url: /asap\n                - img "asap" [ref=s1e1708]\n            - link "asap" [ref=s1e1709]:\n                - /url: /asap\n            - link "2025年1月29日 16時37分" [ref=s1e1710]:\n                - /url: "#comment-afb254c15106b7"\n                - tooltip "2025年1月29日 16時37分" [ref=s1e1711]: 2ヶ月前 2025年1月29日 16時37分\n            - button "メニューを開く" [ref=s1e1713]:\n                - img [ref=s1e1714]\n            - paragraph [ref=s1e1719]: コメントありがとうございます！ おっしゃる通りでした！読み間違いです。ご指摘助かります。\n            - blockquote [ref=s1e1721]:\n                - paragraph [ref=s1e1722]: Due to the long evaluation times, which impact the\n                    efficiency of the RL process, large-scale RL has not been\n                    applied extensively in software engineering tasks.\n            - paragraph [ref=s1e1723]: この部分の文章をもとに記載していましたが、大々的には適用されていないだけで、一部は学習に入っているっぽいですね。\n            - paragraph [ref=s1e1724]: 助かります！修正もこれからさせていただきます\n            - button "いいね" [ref=s1e1727]:\n                - img [ref=s1e1728]\n            - text: "2"\n        - article [ref=s1e1743]:\n            - link "masaishi" [ref=s1e1745]:\n                - /url: /masaishi\n                - img "masaishi" [ref=s1e1746]\n            - link "masaishi" [ref=s1e1747]:\n                - /url: /masaishi\n            - link "2025年1月29日 19時5分" [ref=s1e1748]:\n                - /url: "#comment-debff7de1aad74"\n                - tooltip "2025年1月29日 19時5分" [ref=s1e1749]: 2ヶ月前 2025年1月29日 19時5分\n            - button "メニューを開く" [ref=s1e1751]:\n                - img [ref=s1e1752]\n            - paragraph [ref=s1e1757]: ご丁寧にありがとうございます！\n            - button "いいね" [ref=s1e1760]:\n                - img [ref=s1e1761]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1777]\n    - article [ref=s1e1779]:\n        - link "RyotoTanaka" [ref=s1e1781]:\n            - /url: /ryototanaka\n            - img "RyotoTanaka" [ref=s1e1782]\n        - link "RyotoTanaka" [ref=s1e1783]:\n            - /url: /ryototanaka\n        - link "2025年1月30日 8時0分" [ref=s1e1784]:\n            - /url: "#comment-4ae639fe8d1892"\n            - tooltip "2025年1月30日 8時0分" [ref=s1e1785]: 2ヶ月前 2025年1月30日 8時0分\n        - button "メニューを開く" [ref=s1e1787]:\n            - img [ref=s1e1788]\n        - paragraph [ref=s1e1793]: 読み応えがありました。素晴らしい記事をありがとうございます！ 些細な事なのですが、DPOは Direct\n            Preference Optimization ではないでしょうか。\n        - button "いいね" [ref=s1e1797]:\n            - img [ref=s1e1798]\n        - text: "1"\n        - button "コメント 1" [ref=s1e1813]:\n            - img "コメント" [ref=s1e1814]\n            - text: "1"\n        - article [ref=s1e1818]:\n            - link "asap" [ref=s1e1820]:\n                - /url: /asap\n                - img "asap" [ref=s1e1821]\n            - link "asap" [ref=s1e1822]:\n                - /url: /asap\n            - link "2025年1月30日 13時15分" [ref=s1e1823]:\n                - /url: "#comment-ec7e829404f7bc"\n                - tooltip "2025年1月30日 13時15分" [ref=s1e1824]: 2ヶ月前 2025年1月30日 13時15分\n            - button "メニューを開く" [ref=s1e1826]:\n                - img [ref=s1e1827]\n            - paragraph [ref=s1e1832]: ほんとですね！間違えて入力してました ありがとうございます！\n            - button "いいね" [ref=s1e1836]:\n                - img [ref=s1e1837]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1853]\n    - text: ログインするとコメントできます\n    - button "Login" [ref=s1e1857]\n    - complementary [ref=s1e1858]:\n        - link "asap" [ref=s1e1863]:\n            - /url: /asap\n            - img "asap" [ref=s1e1864]\n        - link "asap" [ref=s1e1866]:\n            - /url: /asap\n        - button "フォロー" [ref=s1e1868]\n        - link "@personabb" [ref=s1e1869]:\n            - /url: https://github.com/personabb\n            - tooltip "@personabb" [ref=s1e1870]:\n                - img "GitHub" [ref=s1e1871]\n                - text: "@personabb"\n        - link "@asap2650" [ref=s1e1873]:\n            - /url: https://twitter.com/asap2650\n            - tooltip "@asap2650" [ref=s1e1874]:\n                - img "X(Twitter)" [ref=s1e1875]\n                - text: "@asap2650"\n        - link "yard.tips" [ref=s1e1881]:\n            - /url: https://yard.tips/asap2650\n            - tooltip "yard.tips" [ref=s1e1882]:\n                - img "リンク" [ref=s1e1883]\n                - text: yard.tips\n        - paragraph [ref=s1e1886]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n        - button "バッジを贈る" [ref=s1e1889]\n        - link "バッジを贈るとは" [ref=s1e1891]:\n            - /url: /faq#badges\n            - text: バッジを贈るとは\n            - img [ref=s1e1892]\n        - text: 目次\n        - list [ref=s1e1901]:\n            - listitem [ref=s1e1902]:\n                - link "はじめに" [ref=s1e1903]:\n                    - /url: "#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB"\n            - listitem [ref=s1e1904]:\n                - link "本論文の興味深いところ" [ref=s1e1905]:\n                    - /url: "#%E6%9C%AC%E8%AB%96%E6%96%87%E3%81%AE%E8%88%88%E5%91%B3%E6%B7%B1%E3%81%8\\\n                        4%E3%81%A8%E3%81%93%E3%82%8D"\n            - listitem [ref=s1e1906]:\n                - link "本論文の流れ" [ref=s1e1907]:\n                    - /url: "#%E6%9C%AC%E8%AB%96%E6%96%87%E3%81%AE%E6%B5%81%E3%82%8C"\n                - list [ref=s1e1908]:\n                    - listitem [ref=s1e1909]:\n                        - link "前提のモデル構造" [ref=s1e1910]:\n                            - /url: "#%E5%89%8D%E6%8F%90%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E6%A7%8B%E9%80%A\\\n                                0"\n                    - listitem [ref=s1e1911]:\n                        - link "DeepSeek-R1-Zeroを強化学習を用いて作る" [ref=s1e1912]:\n                            - /url: "#deepseek-r1-zero%E3%82%92%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E3%82%92%\\\n                                E7%94%A8%E3%81%84%E3%81%A6%E4%BD%9C%E3%82%8B"\n                    - listitem [ref=s1e1913]:\n                        - link "DeepSeek-R1をSFT+強化学習を用いて作る" [ref=s1e1914]:\n                            - /url: "#deepseek-r1%E3%82%92sft%2B%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E3%82%92\\\n                                %E7%94%A8%E3%81%84%E3%81%A6%E4%BD%9C%E3%82%8B"\n                    - listitem [ref=s1e1915]:\n                        - link "小型モデルへの蒸留を試す" [ref=s1e1916]:\n                            - /url: "#%E5%B0%8F%E5%9E%8B%E3%83%A2%E3%83%87%E3%83%AB%E3%81%B8%E3%81%AE%E8%92%B\\\n                                8%E7%95%99%E3%82%92%E8%A9%A6%E3%81%99"\n            - listitem [ref=s1e1917]:\n                - link "DeepSeek-R1-Zeroを作るまで" [ref=s1e1918]:\n                    - /url: "#deepseek-r1-zero%E3%82%92%E4%BD%9C%E3%82%8B%E3%81%BE%E3%81%A7"\n                - list [ref=s1e1919]:\n                    - listitem [ref=s1e1920]:\n                        - link "元々のアーキテクチャ" [ref=s1e1921]:\n                            - /url: "#%E5%85%83%E3%80%85%E3%81%AE%E3%82%A2%E3%83%BC%E3%82%AD%E3%83%86%E3%82%A\\\n                                F%E3%83%81%E3%83%A3"\n                    - listitem [ref=s1e1922]:\n                        - link "すごいぞ！強化学習" [ref=s1e1923]:\n                            - /url: "#%E3%81%99%E3%81%94%E3%81%84%E3%81%9E%EF%BC%81%E5%BC%B7%E5%8C%96%E5%AD%A\\\n                                6%E7%BF%92"\n                    - listitem [ref=s1e1924]:\n                        - link "そもそもLLMで強化学習とは？" [ref=s1e1925]:\n                            - /url: "#%E3%81%9D%E3%82%82%E3%81%9D%E3%82%82llm%E3%81%A7%E5%BC%B7%E5%8C%96%E5%A\\\n                                D%A6%E7%BF%92%E3%81%A8%E3%81%AF%EF%BC%9F"\n                    - listitem [ref=s1e1926]:\n                        - link "ZeroのSystemプロンプト" [ref=s1e1927]:\n                            - /url: "#zero%E3%81%AEsystem%E3%83%97%E3%83%AD%E3%83%B3%E3%83%97%E3%83%88"\n                    - listitem [ref=s1e1928]:\n                        - link "Zeroの「Aha Moment」" [ref=s1e1929]:\n                            - /url: "#zero%E3%81%AE%E3%80%8Caha-moment%E3%80%8D"\n                    - listitem [ref=s1e1930]:\n                        - link "DeepSeek-R1-Zeroのまとめ" [ref=s1e1931]:\n                            - /url: "#deepseek-r1-zero%E3%81%AE%E3%81%BE%E3%81%A8%E3%82%81"\n            - listitem [ref=s1e1932]:\n                - link "DeepSeek-R1への道" [ref=s1e1933]:\n                    - /url: "#deepseek-r1%E3%81%B8%E3%81%AE%E9%81%93"\n                - list [ref=s1e1934]:\n                    - listitem [ref=s1e1935]:\n                        - link "DeepSeek-R1-Zeroの課題" [ref=s1e1936]:\n                            - /url: "#deepseek-r1-zero%E3%81%AE%E8%AA%B2%E9%A1%8C"\n                    - listitem [ref=s1e1937]:\n                        - link "課題対処への道" [ref=s1e1938]:\n                            - /url: "#%E8%AA%B2%E9%A1%8C%E5%AF%BE%E5%87%A6%E3%81%B8%E3%81%AE%E9%81%93"\n                    - listitem [ref=s1e1939]:\n                        - link "DeepSeek-R1のまとめ" [ref=s1e1940]:\n                            - /url: "#deepseek-r1%E3%81%AE%E3%81%BE%E3%81%A8%E3%82%81"\n            - listitem [ref=s1e1941]:\n                - link "結果" [ref=s1e1942]:\n                    - /url: "#%E7%B5%90%E6%9E%9C"\n            - listitem [ref=s1e1943]:\n                - link "蒸留" [ref=s1e1944]:\n                    - /url: "#%E8%92%B8%E7%95%99"\n                - list [ref=s1e1945]:\n                    - listitem [ref=s1e1946]:\n                        - link "蒸留 vs 強化学習" [ref=s1e1947]:\n                            - /url: "#%E8%92%B8%E7%95%99-vs-%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92"\n            - listitem [ref=s1e1948]:\n                - link "今後の展望" [ref=s1e1949]:\n                    - /url: "#%E4%BB%8A%E5%BE%8C%E3%81%AE%E5%B1%95%E6%9C%9B"\n            - listitem [ref=s1e1950]:\n                - link "まとめ" [ref=s1e1951]:\n                    - /url: "#%E3%81%BE%E3%81%A8%E3%82%81"\n            - listitem [ref=s1e1952]:\n                - link "参考文献" [ref=s1e1953]:\n                    - /url: "#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"\n        - paragraph [ref=s1e1955]: Zennからのお知らせ\n        - article [ref=s1e1958]:\n            - link "第２回 AI Agent Hackathon with Google Cloud 第２回 AI Agent Hackathon with Google Cloud 4/14登録受付開始！" [ref=s1e1959]:\n                - /url: /hackathons/google-cloud-japan-ai-hackathon-vol2\n                - img "第２回 AI Agent Hackathon with Google Cloud" [ref=s1e1961]\n                - paragraph [ref=s1e1962]: 第２回 AI Agent Hackathon with Google Cloud 4/14登録受付開始！\n        - article [ref=s1e1963]:\n            - link "Zenncafe日比谷#2 生成AIで変わる仕事 Zenncafe日比谷#2 生成AIで変わる仕事" [ref=s1e1964]:\n                - /url: https://zenn.connpass.com/event/351131/\n                - img "Zenncafe日比谷#2 生成AIで変わる仕事" [ref=s1e1966]\n                - paragraph [ref=s1e1967]: Zenncafe日比谷#2 生成AIで変わる仕事\n- contentinfo [ref=s1e1969]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1973]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1974]\n    - paragraph [ref=s1e1982]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e1984]:\n        - heading "About" [level=4] [ref=s1e1985]\n        - list [ref=s1e1986]:\n            - listitem [ref=s1e1987]:\n                - link "Zennについて" [ref=s1e1989]:\n                    - /url: /about\n            - listitem [ref=s1e1990]:\n                - link "運営会社" [ref=s1e1991]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e1992]:\n                - link "お知らせ・リリース" [ref=s1e1993]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e1994]:\n                - link "イベント" [ref=s1e1996]:\n                    - /url: /events\n    - navigation [ref=s1e1997]:\n        - heading "Guides" [level=4] [ref=s1e1998]\n        - list [ref=s1e1999]:\n            - listitem [ref=s1e2000]:\n                - link "使い方" [ref=s1e2002]:\n                    - /url: /manual\n            - listitem [ref=s1e2003]:\n                - link "法人向けメニュー" [ref=s1e2005]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e2007]:\n                - link "Publication / Pro" [ref=s1e2009]:\n                    - /url: /publications\n            - listitem [ref=s1e2010]:\n                - link "よくある質問" [ref=s1e2012]:\n                    - /url: /faq\n    - navigation [ref=s1e2013]:\n        - heading "Links" [level=4] [ref=s1e2014]\n        - list [ref=s1e2015]:\n            - listitem [ref=s1e2016]:\n                - link "X(Twitter)" [ref=s1e2017]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e2018]:\n                - link "GitHub" [ref=s1e2019]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e2020]:\n                - link "メディアキット" [ref=s1e2022]:\n                    - /url: /mediakit\n    - navigation [ref=s1e2023]:\n        - heading "Legal" [level=4] [ref=s1e2024]\n        - list [ref=s1e2025]:\n            - listitem [ref=s1e2026]:\n                - link "利用規約" [ref=s1e2028]:\n                    - /url: /terms\n            - listitem [ref=s1e2029]:\n                - link "プライバシーポリシー" [ref=s1e2031]:\n                    - /url: /privacy\n            - listitem [ref=s1e2032]:\n                - link "特商法表記" [ref=s1e2034]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e2037]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e2038]\n- alert [ref=s1e2057]\n```\n', name='browser_navigate', tool_call_id='d85f8505-9c2a-4960-b17f-9ef88d983d9d'), AIMessage(content='まず、指定されたZennの著者のページにアクセスし、最もいいねが多い記事を特定しました。その結果、「DeepSeek-R1の論文読んだ？【勉強になるよ】」という記事が最も評価されていることがわかりました。\n\n次に、その記事の内容を詳しく調べました。このブログ記事は、DeepSeek-R1という大規模言語モデルに関する論文を解説したものです。記事は、論文の内容をわかりやすく説明することに重点を置いており、特に強化学習の概念や、DeepSeek-R1を開発する上で用いられたGRPO（Group Relative Policy Optimization）という手法について詳しく解説しています。\n\n記事が評価されている理由として、以下の点が考えられます。\n\n1.  **技術的な深さと分かりやすさの両立:** 大規模言語モデルや強化学習といった複雑なトピックを扱いながらも、専門知識がない読者にも理解できるように、かみ砕いた説明がなされています。\n2.  **論文の内容の網羅性:** DeepSeek-R1の論文の重要なポイントを漏れなくカバーしており、論文を読むための良い入門となっています。\n3.  **著者自身の考察:** 論文の内容を解説するだけでなく、著者自身の考察や解釈が加えられており、読者に新たな視点を提供しています。\n4.  **読者への配慮:** 記事の冒頭で、誤りがあるかもしれないことを断り、コメントでの指摘を歓迎するなど、読者とのコミュニケーションを重視する姿勢が見られます。\n5.  **参考文献の紹介:** 記事の最後に、強化学習や関連技術を学ぶための参考文献が紹介されており、読者の学習をサポートする姿勢が評価されていると考えられます。\n\nこれらの要素が組み合わさることで、この記事はDeepSeek-R1に関心のあるエンジニアにとって非常に価値のある情報源となり、高い評価を得ていると考えられます。', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-d5ac6129-cca3-4759-ac6a-eafe68db505f-0', usage_metadata={'input_tokens': 47987, 'output_tokens': 387, 'total_tokens': 48374, 'input_token_details': {'cache_read': 0}}), HumanMessage(content=[{'type': 'text', 'text': 'LLMに強化学習ってどういうこと？GRPOって何？'}], additional_kwargs={}, response_metadata={}), AIMessage(content='LLM（大規模言語モデル）に対する強化学習と、GRPO（Group Relative Policy Optimization）について説明します。\n\n### LLMに対する強化学習\n\nLLMに対する強化学習は、LLMがより良いテキストを生成するように学習させるための手法です。通常のLLMの学習では、大量のテキストデータを用いて、与えられたテキストの続きを予測するように学習します。これに対して、強化学習では、LLMが生成したテキストに対して「報酬」を与え、その報酬を最大化するように学習します。\n\n**なぜ強化学習が必要なのか？**\n\nLLMは、大量のテキストデータから学習することで、自然な文章を生成する能力を獲得します。しかし、必ずしも人間が望むような文章を生成するとは限りません。例えば、以下のような課題があります。\n\n*   **目的の達成:** 特定のタスク（質問応答、要約など）において、必ずしも最適な回答を生成するとは限らない。\n*   **安全性:** 有害な情報や偏見のある情報を生成する可能性がある。\n*   **創造性:** 新しいアイデアや表現を生み出す能力が限られている。\n\n強化学習は、これらの課題を克服するために用いられます。\n\n**強化学習の基本的な流れ**\n\n1.  **LLM（エージェント）:** テキストを生成する主体。\n2.  **環境:** LLMがテキストを生成する対象となるタスクや状況。\n3.  **報酬:** LLMが生成したテキストの良し悪しを評価する指標。\n4.  **学習:** LLMが報酬を最大化するように、テキスト生成の方針（方策）を調整する。\n\n具体的には、以下のような手順で学習が進みます。\n\n1.  LLMがプロンプト（指示）を受け取り、テキストを生成します。\n2.  生成されたテキストに対して、報酬関数が報酬を与えます。\n    *   例えば、質問応答タスクであれば、正解との一致度合いが高いほど高い報酬を与えます。\n    *   安全性に関する報酬であれば、有害な情報を生成した場合に低い報酬を与えます。\n3.  LLMは、その報酬に基づいて、自身のテキスト生成の方針を調整します。\n    *   具体的には、報酬が高くなるような単語やフレーズを選択する確率を高めます。\n4.  このプロセスを繰り返すことで、LLMはより高い報酬を得られるようなテキストを生成するように学習していきます。\n\n### GRPO（Group Relative Policy Optimization）とは？\n\nGRPOは、LLMに対する強化学習の一種で、特に大規模なLLMを効率的に学習させるために開発された手法です。\n\n**PPO（Proximal Policy Optimization）との比較**\n\nLLMの強化学習でよく用いられる手法として、PPO（Proximal Policy Optimization）があります。PPOでは、報酬に加えて、LLMの出力が大きく変化しないように制約を加えながら学習を進めます。しかし、PPOでは、状態価値モデルという追加のモデルを学習する必要があり、計算コストが高くなるという課題がありました。\n\nGRPOは、この状態価値モデルを不要にし、より効率的に学習を進めることを目指します。\n\n**GRPOの仕組み**\n\nGRPOでは、LLMに複数のテキストを生成させ、それらのテキストに対する報酬を比較することで、相対的な評価を行います。具体的には、以下の手順で学習が進みます。\n\n1.  LLMが同じプロンプトに対して、複数のテキスト（例えば4つ）を生成します。\n2.  それぞれのテキストに対して報酬関数が報酬を与えます。\n3.  それぞれのテキストの報酬を比較し、最も高い報酬を得たテキストを「良いテキスト」、それ以外のテキストを「悪いテキスト」とします。\n4.  LLMは、「良いテキスト」を生成する確率を高め、「悪いテキスト」を生成する確率を下げるように、自身のテキスト生成の方針を調整します。\n\nこのように、GRPOでは、テキスト間の相対的な評価を用いることで、状態価値モデルを学習する必要がなくなり、計算コストを削減することができます。\n\n**GRPOの利点**\n\n*   **計算効率:** 状態価値モデルが不要なため、PPOよりも計算コストが低い。\n*   **大規模LLMへの適用:** 大規模なLLMでも効率的に学習できる。\n\n**まとめ**\n\nLLMに対する強化学習は、LLMがより良いテキストを生成するように学習させるための手法であり、GRPOはその効率的な学習を可能にする手法の一つです。GRPOは、テキスト間の相対的な評価を用いることで、状態価値モデルを不要にし、計算コストを削減することができます。', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-4d11a554-e7d5-4420-9ac9-b8efaccf8b11-0', usage_metadata={'input_tokens': 48388, 'output_tokens': 996, 'total_tokens': 49384, 'input_token_details': {'cache_read': 0}})]}
=================================
LLM（大規模言語モデル）に対する強化学習と、GRPO（Group Relative Policy Optimization）について説明します。

### LLMに対する強化学習

LLMに対する強化学習は、LLMがより良いテキストを生成するように学習させるための手法です。通常のLLMの学習では、大量のテキストデータを用いて、与えられたテキストの続きを予測するように学習します。これに対して、強化学習では、LLMが生成したテキストに対して「報酬」を与え、その報酬を最大化するように学習します。

**なぜ強化学習が必要なのか？**

LLMは、大量のテキストデータから学習することで、自然な文章を生成する能力を獲得します。しかし、必ずしも人間が望むような文章を生成するとは限りません。例えば、以下のような課題があります。

*   **目的の達成:** 特定のタスク（質問応答、要約など）において、必ずしも最適な回答を生成するとは限らない。
*   **安全性:** 有害な情報や偏見のある情報を生成する可能性がある。
*   **創造性:** 新しいアイデアや表現を生み出す能力が限られている。

強化学習は、これらの課題を克服するために用いられます。

**強化学習の基本的な流れ**

1.  **LLM（エージェント）:** テキストを生成する主体。
2.  **環境:** LLMがテキストを生成する対象となるタスクや状況。
3.  **報酬:** LLMが生成したテキストの良し悪しを評価する指標。
4.  **学習:** LLMが報酬を最大化するように、テキスト生成の方針（方策）を調整する。

具体的には、以下のような手順で学習が進みます。

1.  LLMがプロンプト（指示）を受け取り、テキストを生成します。
2.  生成されたテキストに対して、報酬関数が報酬を与えます。
    *   例えば、質問応答タスクであれば、正解との一致度合いが高いほど高い報酬を与えます。
    *   安全性に関する報酬であれば、有害な情報を生成した場合に低い報酬を与えます。
3.  LLMは、その報酬に基づいて、自身のテキスト生成の方針を調整します。
    *   具体的には、報酬が高くなるような単語やフレーズを選択する確率を高めます。
4.  このプロセスを繰り返すことで、LLMはより高い報酬を得られるようなテキストを生成するように学習していきます。

### GRPO（Group Relative Policy Optimization）とは？

GRPOは、LLMに対する強化学習の一種で、特に大規模なLLMを効率的に学習させるために開発された手法です。

**PPO（Proximal Policy Optimization）との比較**

LLMの強化学習でよく用いられる手法として、PPO（Proximal Policy Optimization）があります。PPOでは、報酬に加えて、LLMの出力が大きく変化しないように制約を加えながら学習を進めます。しかし、PPOでは、状態価値モデルという追加のモデルを学習する必要があり、計算コストが高くなるという課題がありました。

GRPOは、この状態価値モデルを不要にし、より効率的に学習を進めることを目指します。

**GRPOの仕組み**

GRPOでは、LLMに複数のテキストを生成させ、それらのテキストに対する報酬を比較することで、相対的な評価を行います。具体的には、以下の手順で学習が進みます。

1.  LLMが同じプロンプトに対して、複数のテキスト（例えば4つ）を生成します。
2.  それぞれのテキストに対して報酬関数が報酬を与えます。
3.  それぞれのテキストの報酬を比較し、最も高い報酬を得たテキストを「良いテキスト」、それ以外のテキストを「悪いテキスト」とします。
4.  LLMは、「良いテキスト」を生成する確率を高め、「悪いテキスト」を生成する確率を下げるように、自身のテキスト生成の方針を調整します。

このように、GRPOでは、テキスト間の相対的な評価を用いることで、状態価値モデルを学習する必要がなくなり、計算コストを削減することができます。

**GRPOの利点**

*   **計算効率:** 状態価値モデルが不要なため、PPOよりも計算コストが低い。
*   **大規模LLMへの適用:** 大規模なLLMでも効率的に学習できる。

**まとめ**

LLMに対する強化学習は、LLMがより良いテキストを生成するように学習させるための手法であり、GRPOはその効率的な学習を可能にする手法の一つです。GRPOは、テキスト間の相対的な評価を用いることで、状態価値モデルを不要にし、計算コストを削減することができます。










入力してください: 2番目にいいねが多い記事は何？
response:  {'messages': [HumanMessage(content=[{'type': 'text', 'text': '「https://zenn.dev/asap」この執筆者のもっともいいねがついている記事を調査して、その記事の中身を取得し、その記事がなぜ評価されているのかを分析してください。'}], additional_kwargs={}, response_metadata={}), AIMessage(content='zenn.devのasap氏の最もいいねが多い記事を調べ、その内容を取得し、評価されている理由を分析する、というご要望ですね。\n\nまず、zenn.dev/asapにアクセスして、最もいいねが多い記事のタイトルとURLを特定する必要があります。その後、そのURLにアクセスして記事の内容を取得します。最後に、記事の内容から評価されている理由を推測します。\n\nでは、始めましょう。\n\n1.  zenn.dev/asapにアクセスし、最もいいねが多い記事の情報を取得します。', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-989fe5ca-2637-4757-878f-db26e13526b8-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap'}, 'id': '58fad86e-0b01-4fc6-a8cc-57deb5c48b41', 'type': 'tool_call'}], usage_metadata={'input_tokens': 778, 'output_tokens': 128, 'total_tokens': 906, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap\n\n- Page URL: https://zenn.dev/asap\n- Page Title: asapさんの記事一覧 | Zenn\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- banner [ref=s1e32]:\n    - img "asap" [ref=s1e36]\n    - heading "asap" [level=1] [ref=s1e39]\n    - button "フォロー" [ref=s1e41]\n    - paragraph [ref=s1e43]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n    - button "2,214 Likes" [ref=s1e46]\n    - button "310 Followers" [ref=s1e48]\n    - link "@personabb" [ref=s1e51]:\n        - /url: https://github.com/personabb\n        - tooltip "@personabb" [ref=s1e52]:\n            - img "GitHub" [ref=s1e53]\n            - text: "@personabb"\n    - link "@asap2650" [ref=s1e55]:\n        - /url: https://twitter.com/asap2650\n        - tooltip "@asap2650" [ref=s1e56]:\n            - img "X(Twitter)" [ref=s1e57]\n            - text: "@asap2650"\n    - link "yard.tips" [ref=s1e63]:\n        - /url: https://yard.tips/asap2650\n        - tooltip "yard.tips" [ref=s1e64]:\n            - img [ref=s1e65]\n            - text: yard.tips\n    - link "RSS" [ref=s1e67]:\n        - /url: /asap/feed\n        - tooltip "RSS" [ref=s1e68]:\n            - img [ref=s1e69]\n            - text: RSS\n- link "Articles 72" [ref=s1e80]:\n    - /url: /asap\n- link "Scraps 5" [ref=s1e82]:\n    - /url: /asap?tab=scraps\n- link "Comments" [ref=s1e84]:\n    - /url: /asap?tab=comments\n- article [ref=s1e89]:\n    - link "tech" [ref=s1e90]:\n        - /url: /tech-or-idea\n    - link "❄️ LangChainでGemini-2.0 APIの画像編集を実行する方法 6日前 いいねされた数 18" [ref=s1e91]:\n        - /url: /asap/articles/1d5e6ec06c380f\n        - text: ❄️\n        - heading "LangChainでGemini-2.0 APIの画像編集を実行する方法" [level=3] [ref=s1e95]\n        - time [ref=s1e98]: 6日前\n        - img "いいねされた数" [ref=s1e100]\n        - text: "18"\n- article [ref=s1e102]:\n    - link "tech" [ref=s1e103]:\n        - /url: /tech-or-idea\n    - link "🙌 完全自律型AIエージェントOSS【LangManus】完全に理解した！ 13日前 いいねされた数 48" [ref=s1e104]:\n        - /url: /asap/articles/b27e832c2a49b7\n        - text: 🙌\n        - heading "完全自律型AIエージェントOSS【LangManus】完全に理解した！" [level=3] [ref=s1e108]\n        - time [ref=s1e111]: 13日前\n        - img "いいねされた数" [ref=s1e113]\n        - text: "48"\n- article [ref=s1e115]:\n    - link "tech" [ref=s1e116]:\n        - /url: /tech-or-idea\n    - link "💫 LangChainでOpenAI gpt-4o / o1にPDFを入力する方法 20日前 いいねされた数 9" [ref=s1e117]:\n        - /url: /asap/articles/74885fab863c9e\n        - text: 💫\n        - heading "LangChainでOpenAI gpt-4o / o1にPDFを入力する方法" [level=3]\n          [ref=s1e121]\n        - time [ref=s1e124]: 20日前\n        - img "いいねされた数" [ref=s1e126]\n        - text: "9"\n- article [ref=s1e128]:\n    - link "tech" [ref=s1e129]:\n        - /url: /tech-or-idea\n    - link "👣 Ollama + LangChainで画像を入力する方法【Gemma3 27B】 27日前 いいねされた数 14" [ref=s1e130]:\n        - /url: /asap/articles/18b110d551b811\n        - text: 👣\n        - heading "Ollama + LangChainで画像を入力する方法【Gemma3 27B】" [level=3]\n          [ref=s1e134]\n        - time [ref=s1e137]: 27日前\n        - img "いいねされた数" [ref=s1e139]\n        - text: "14"\n- article [ref=s1e141]:\n    - link "tech" [ref=s1e142]:\n        - /url: /tech-or-idea\n    - link "🦄 Google CloudのVector StoreでRAG Agentsを構築する【Big Querty編】 28日前 いいねされた数 12" [ref=s1e143]:\n        - /url: /asap/articles/3d258861c6c528\n        - text: 🦄\n        - heading "Google CloudのVector StoreでRAG Agentsを構築する【Big Querty編】"\n          [level=3] [ref=s1e147]\n        - time [ref=s1e150]: 28日前\n        - img "いいねされた数" [ref=s1e152]\n        - text: "12"\n- article [ref=s1e154]:\n    - link "tech" [ref=s1e155]:\n        - /url: /tech-or-idea\n    - link "🎶 最新音楽生成AI 【DiffRhythm】をGoogle Colabで動かせるようにしました！（簡易版） 1ヶ月前 いいねされた数 39" [ref=s1e156]:\n        - /url: /asap/articles/4edc4a71de8961\n        - text: 🎶\n        - heading "最新音楽生成AI 【DiffRhythm】をGoogle Colabで動かせるようにしました！（簡易版）"\n          [level=3] [ref=s1e160]\n        - time [ref=s1e163]: 1ヶ月前\n        - img "いいねされた数" [ref=s1e165]\n        - text: "39"\n- article [ref=s1e167]:\n    - link "tech" [ref=s1e168]:\n        - /url: /tech-or-idea\n    - link "🐤 LangGraphのTool Callingを利用して、RAG Agentsを構築する（後編） 1ヶ月前 いいねされた数 12" [ref=s1e169]:\n        - /url: /asap/articles/c58cfab9c1362a\n        - text: 🐤\n        - heading "LangGraphのTool Callingを利用して、RAG Agentsを構築する（後編）" [level=3]\n          [ref=s1e173]\n        - time [ref=s1e176]: 1ヶ月前\n        - img "いいねされた数" [ref=s1e178]\n        - text: "12"\n- article [ref=s1e180]:\n    - link "idea" [ref=s1e181]:\n        - /url: /tech-or-idea\n    - link "🏆 イベントに登壇してきた話【資料もこちら】 1ヶ月前 いいねされた数 20" [ref=s1e182]:\n        - /url: /asap/articles/89d2a13bdc75a3\n        - text: 🏆\n        - heading "イベントに登壇してきた話【資料もこちら】" [level=3] [ref=s1e186]\n        - time [ref=s1e189]: 1ヶ月前\n        - img "いいねされた数" [ref=s1e191]\n        - text: "20"\n- article [ref=s1e193]:\n    - link "tech" [ref=s1e194]:\n        - /url: /tech-or-idea\n    - link "👌 Grok API（Grok 2）で遊んでみた！【Elyza Tasks 100】 2ヶ月前 いいねされた数 11" [ref=s1e195]:\n        - /url: /asap/articles/6aa01bde32163d\n        - text: 👌\n        - heading "Grok API（Grok 2）で遊んでみた！【Elyza Tasks 100】" [level=3]\n          [ref=s1e199]\n        - time [ref=s1e202]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e204]\n        - text: "11"\n- article [ref=s1e206]:\n    - link "tech" [ref=s1e207]:\n        - /url: /tech-or-idea\n    - link "📑 Multi-head Latent Attention (MLA)の詳細【be absorbedとは？】 2ヶ月前 いいねされた数 22" [ref=s1e208]:\n        - /url: /asap/articles/be3d4b60d8ac92\n        - text: 📑\n        - heading "Multi-head Latent Attention (MLA)の詳細【be absorbedとは？】"\n          [level=3] [ref=s1e212]\n        - time [ref=s1e215]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e217]\n        - text: "22"\n- article [ref=s1e219]:\n    - link "tech" [ref=s1e220]:\n        - /url: /tech-or-idea\n    - link "⛳ LangGraphのTool Callingを利用して、RAG Agentsを構築する（前編） 2ヶ月前 いいねされた数 46" [ref=s1e221]:\n        - /url: /asap/articles/2ad7eb5e7560cf\n        - text: ⛳\n        - heading "LangGraphのTool Callingを利用して、RAG Agentsを構築する（前編）" [level=3]\n          [ref=s1e225]\n        - time [ref=s1e228]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e230]\n        - text: "46"\n- article [ref=s1e232]:\n    - link "tech" [ref=s1e233]:\n        - /url: /tech-or-idea\n    - link "🫠 【LangChain】で【Transformer】を利用する場合に【chat_template】を設定する方法【エラー対処記事】 2ヶ月前 いいねされた数 13" [ref=s1e234]:\n        - /url: /asap/articles/985f3580ea476d\n        - text: 🫠\n        - heading\n          "【LangChain】で【Transformer】を利用する場合に【chat_template】を設定する方法【エラー対処記事】"\n          [level=3] [ref=s1e238]\n        - time [ref=s1e241]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e243]\n        - text: "13"\n- article [ref=s1e245]:\n    - link "tech" [ref=s1e246]:\n        - /url: /tech-or-idea\n    - link "🌊 採点理由を出力しながらローカルLLMを評価する【Elyza Tasks 100】 2ヶ月前 いいねされた数 26" [ref=s1e247]:\n        - /url: /asap/articles/3a99c34fc1d4cb\n        - text: 🌊\n        - heading "採点理由を出力しながらローカルLLMを評価する【Elyza Tasks 100】" [level=3]\n          [ref=s1e251]\n        - time [ref=s1e254]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e256]\n        - text: "26"\n- article [ref=s1e258]:\n    - link "tech" [ref=s1e259]:\n        - /url: /tech-or-idea\n    - link "📑 DeepSeek-R1の論文読んだ？【勉強になるよ】 2ヶ月前 いいねされた数 921" [ref=s1e260]:\n        - /url: /asap/articles/34237ad87f8511\n        - text: 📑\n        - heading "DeepSeek-R1の論文読んだ？【勉強になるよ】" [level=3] [ref=s1e264]\n        - time [ref=s1e267]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e269]\n        - text: "921"\n- article [ref=s1e271]:\n    - link "tech" [ref=s1e272]:\n        - /url: /tech-or-idea\n    - link "📖 VAEって結局何者なの？ 3ヶ月前 いいねされた数 104" [ref=s1e273]:\n        - /url: /asap/articles/6caa9043276424\n        - text: 📖\n        - heading "VAEって結局何者なの？" [level=3] [ref=s1e277]\n        - time [ref=s1e280]: 3ヶ月前\n        - img "いいねされた数" [ref=s1e282]\n        - text: "104"\n- article [ref=s1e284]:\n    - link "tech" [ref=s1e285]:\n        - /url: /tech-or-idea\n    - link "🦔 初めてのSageMaker AIでSFTTrainerによるLLMのフルパラメータファインチューニングを試す 3ヶ月前 いいねされた数 9" [ref=s1e286]:\n        - /url: /asap/articles/3f31b77010624c\n        - text: 🦔\n        - heading "初めてのSageMaker AIでSFTTrainerによるLLMのフルパラメータファインチューニングを試す"\n          [level=3] [ref=s1e290]\n        - time [ref=s1e293]: 3ヶ月前\n        - img "いいねされた数" [ref=s1e295]\n        - text: "9"\n- article [ref=s1e297]:\n    - link "tech" [ref=s1e298]:\n        - /url: /tech-or-idea\n    - link "😺 【動画翻訳】OpenAIの研究者によるo1やLLMに対するディスカッションの翻訳 3ヶ月前 いいねされた数 12" [ref=s1e299]:\n        - /url: /asap/articles/2d2832bf3288aa\n        - text: 😺\n        - heading "【動画翻訳】OpenAIの研究者によるo1やLLMに対するディスカッションの翻訳" [level=3]\n          [ref=s1e303]\n        - time [ref=s1e306]: 3ヶ月前\n        - img "いいねされた数" [ref=s1e308]\n        - text: "12"\n- article [ref=s1e310]:\n    - link "tech" [ref=s1e311]:\n        - /url: /tech-or-idea\n    - link "🧚\u200d♀️ AIでオリジナル楽曲を作り、自作のかわいい声で歌ってもらう方法 3ヶ月前 いいねされた数 28" [ref=s1e312]:\n        - /url: /asap/articles/886af0fe48dda3\n        - text: 🧚\u200d♀️\n        - heading "AIでオリジナル楽曲を作り、自作のかわいい声で歌ってもらう方法" [level=3] [ref=s1e316]\n        - time [ref=s1e319]: 3ヶ月前\n        - img "いいねされた数" [ref=s1e321]\n        - text: "28"\n- article [ref=s1e323]:\n    - link "tech" [ref=s1e324]:\n        - /url: /tech-or-idea\n    - link "🎮 Google ColabでSeed-VCによる声色変換を試す 4ヶ月前 いいねされた数 27" [ref=s1e325]:\n        - /url: /asap/articles/9c54aef739a6ed\n        - text: 🎮\n        - heading "Google ColabでSeed-VCによる声色変換を試す" [level=3] [ref=s1e329]\n        - time [ref=s1e332]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e334]\n        - text: "27"\n- article [ref=s1e336]:\n    - link "tech" [ref=s1e337]:\n        - /url: /tech-or-idea\n    - link "🐡 LLMの性能における重要補題【Johnson–Lindenstrauss lemma】のお気持ち〜Attentionの数式理解を添えて〜 4ヶ月前 いいねされた数 21" [ref=s1e338]:\n        - /url: /asap/articles/6bdf89d8e964ec\n        - text: 🐡\n        - heading "LLMの性能における重要補題【Johnson–Lindenstrauss\n          lemma】のお気持ち〜Attentionの数式理解を添えて〜" [level=3] [ref=s1e342]\n        - time [ref=s1e345]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e347]\n        - text: "21"\n- article [ref=s1e349]:\n    - link "tech" [ref=s1e350]:\n        - /url: /tech-or-idea\n    - link "📏 【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】 4ヶ月前 いいねされた数 50" [ref=s1e351]:\n        - /url: /asap/articles/ea9f42edff6573\n        - text: 📏\n        - heading "【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】"\n          [level=3] [ref=s1e355]\n        - time [ref=s1e358]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e360]\n        - text: "50"\n- article [ref=s1e362]:\n    - link "tech" [ref=s1e363]:\n        - /url: /tech-or-idea\n    - link "🎙️ 高品質合成音声モデル【Style-Bert-VITS2】をGoogle Colabで学習し、【Aivis Speech】で利用する方法 4ヶ月前 いいねされた数 24" [ref=s1e364]:\n        - /url: /asap/articles/5c437ce2943ed7\n        - text: 🎙️\n        - heading "高品質合成音声モデル【Style-Bert-VITS2】をGoogle Colabで学習し、【Aivis\n          Speech】で利用する方法" [level=3] [ref=s1e368]\n        - time [ref=s1e371]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e373]\n        - text: "24"\n- article [ref=s1e375]:\n    - link "tech" [ref=s1e376]:\n        - /url: /tech-or-idea\n    - link "🎄 【LangChain】LCEL ステップバイステップ ハンズオン 〜なろう短編小説の自動生成＆評価・改善を行う「AIエージェント」を添えて〜 4ヶ月前 いいねされた数 19" [ref=s1e377]:\n        - /url: /asap/articles/aa587ad8d76105\n        - text: 🎄\n        - heading "【LangChain】LCEL ステップバイステップ ハンズオン\n          〜なろう短編小説の自動生成＆評価・改善を行う「AIエージェント」を添えて〜" [level=3] [ref=s1e381]\n        - time [ref=s1e384]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e386]\n        - text: "19"\n- article [ref=s1e388]:\n    - link "tech" [ref=s1e389]:\n        - /url: /tech-or-idea\n    - link "💻 LangGraphによる「AIエージェントWebアプリ」を作成する【Next.js】 4ヶ月前 いいねされた数 45" [ref=s1e390]:\n        - /url: /asap/articles/a350fe4ac00343\n        - text: 💻\n        - heading "LangGraphによる「AIエージェントWebアプリ」を作成する【Next.js】" [level=3]\n          [ref=s1e394]\n        - time [ref=s1e397]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e399]\n        - text: "45"\n- article [ref=s1e401]:\n    - link "tech" [ref=s1e402]:\n        - /url: /tech-or-idea\n    - link "🤔 LangGraphにて入力情報が不十分な場合に、ユーザへ追加質問をするAIエージェントを作る方法 5ヶ月前 いいねされた数 41" [ref=s1e403]:\n        - /url: /asap/articles/03548f844efe6f\n        - text: 🤔\n        - heading "LangGraphにて入力情報が不十分な場合に、ユーザへ追加質問をするAIエージェントを作る方法" [level=3]\n          [ref=s1e407]\n        - time [ref=s1e410]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e412]\n        - text: "41"\n- article [ref=s1e414]:\n    - link "tech" [ref=s1e415]:\n        - /url: /tech-or-idea\n    - link "🤖 高品質動画生成AI【Allegro】をローカルで実行する【VRAM12GB】 5ヶ月前 いいねされた数 10" [ref=s1e416]:\n        - /url: /asap/articles/dac57655eb0314\n        - text: 🤖\n        - heading "高品質動画生成AI【Allegro】をローカルで実行する【VRAM12GB】" [level=3] [ref=s1e420]\n        - time [ref=s1e423]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e425]\n        - text: "10"\n- article [ref=s1e427]:\n    - link "tech" [ref=s1e428]:\n        - /url: /tech-or-idea\n    - link "🌊 フレーム補間技術【RIFE】を利用してCogVideoXのFPSを改善しよう 5ヶ月前 いいねされた数 2" [ref=s1e429]:\n        - /url: /asap/articles/dbeef72b7bfdc3\n        - text: 🌊\n        - heading "フレーム補間技術【RIFE】を利用してCogVideoXのFPSを改善しよう" [level=3] [ref=s1e433]\n        - time [ref=s1e436]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e438]\n        - text: "2"\n- article [ref=s1e440]:\n    - link "tech" [ref=s1e441]:\n        - /url: /tech-or-idea\n    - link "🔥 LangGraphを使ってAIエージェントを実装する方法 5ヶ月前 いいねされた数 34" [ref=s1e442]:\n        - /url: /asap/articles/5da9cf01703a47\n        - text: 🔥\n        - heading "LangGraphを使ってAIエージェントを実装する方法" [level=3] [ref=s1e446]\n        - time [ref=s1e449]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e451]\n        - text: "34"\n- article [ref=s1e453]:\n    - link "tech" [ref=s1e454]:\n        - /url: /tech-or-idea\n    - link "🌈 動画生成AI【CogVideoX】をローカルで実行する 5ヶ月前 いいねされた数 9" [ref=s1e455]:\n        - /url: /asap/articles/a4077e8b9ddf1a\n        - text: 🌈\n        - heading "動画生成AI【CogVideoX】をローカルで実行する" [level=3] [ref=s1e459]\n        - time [ref=s1e462]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e464]\n        - text: "9"\n- article [ref=s1e466]:\n    - link "tech" [ref=s1e467]:\n        - /url: /tech-or-idea\n    - link "🛎️ ローカルでDIfyを使ってAIエージェントを実装する方法 5ヶ月前 いいねされた数 55" [ref=s1e468]:\n        - /url: /asap/articles/a35254020fbbae\n        - text: 🛎️\n        - heading "ローカルでDIfyを使ってAIエージェントを実装する方法" [level=3] [ref=s1e472]\n        - time [ref=s1e475]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e477]\n        - text: "55"\n- article [ref=s1e479]:\n    - link "tech" [ref=s1e480]:\n        - /url: /tech-or-idea\n    - link "💫 高品質動画生成AI【Pyramid Flow】をローカルで実行する 6ヶ月前 いいねされた数 12" [ref=s1e481]:\n        - /url: /asap/articles/e1880903625cb5\n        - text: 💫\n        - heading "高品質動画生成AI【Pyramid Flow】をローカルで実行する" [level=3] [ref=s1e485]\n        - time [ref=s1e488]: 6ヶ月前\n        - img "いいねされた数" [ref=s1e490]\n        - text: "12"\n- article [ref=s1e492]:\n    - link "tech" [ref=s1e493]:\n        - /url: /tech-or-idea\n    - link "📖 不偏分散で N-1 で割る理由 を自力で導出する 6ヶ月前 いいねされた数 5" [ref=s1e494]:\n        - /url: /asap/articles/f0cba34c2ac2ac\n        - text: 📖\n        - heading "不偏分散で N-1 で割る理由 を自力で導出する" [level=3] [ref=s1e498]\n        - time [ref=s1e501]: 6ヶ月前\n        - img "いいねされた数" [ref=s1e503]\n        - text: "5"\n- article [ref=s1e505]:\n    - link "tech" [ref=s1e506]:\n        - /url: /tech-or-idea\n    - link "✍️ 勾配降下法 ・ 誤差逆伝播法 を自力で導出する 6ヶ月前 いいねされた数 25" [ref=s1e507]:\n        - /url: /asap/articles/643f604d444813\n        - text: ✍️\n        - heading "勾配降下法 ・ 誤差逆伝播法 を自力で導出する" [level=3] [ref=s1e511]\n        - time [ref=s1e514]: 6ヶ月前\n        - img "いいねされた数" [ref=s1e516]\n        - text: "25"\n- article [ref=s1e518]:\n    - link "tech" [ref=s1e519]:\n        - /url: /tech-or-idea\n    - link "💰 Open AI Realtime APIで、会話履歴を削除して高額請求を回避したい！【Pythonサンプルコード】 2024/10/07 いいねされた数 25" [ref=s1e520]:\n        - /url: /asap/articles/af07fcedbbef61\n        - text: 💰\n        - heading "Open AI Realtime APIで、会話履歴を削除して高額請求を回避したい！【Pythonサンプルコード】"\n          [level=3] [ref=s1e524]\n        - time [ref=s1e527]: 2024/10/07\n        - img "いいねされた数" [ref=s1e529]\n        - text: "25"\n- article [ref=s1e531]:\n    - link "tech" [ref=s1e532]:\n        - /url: /tech-or-idea\n    - link "✈️ Open AI Realtime APIのPythonサンプルコードで、AI発話にカットイン（割り込み）する方法 2024/10/06 いいねされた数 24" [ref=s1e533]:\n        - /url: /asap/articles/563500af4649da\n        - text: ✈️\n        - heading "Open AI Realtime APIのPythonサンプルコードで、AI発話にカットイン（割り込み）する方法"\n          [level=3] [ref=s1e537]\n        - time [ref=s1e540]: 2024/10/06\n        - img "いいねされた数" [ref=s1e542]\n        - text: "24"\n- article [ref=s1e544]:\n    - link "tech" [ref=s1e545]:\n        - /url: /tech-or-idea\n    - link "🥰 Open AI Realtime APIのPythonサンプルコードを作成 2024/10/04 いいねされた数 79" [ref=s1e546]:\n        - /url: /asap/articles/4368fd306b592a\n        - text: 🥰\n        - heading "Open AI Realtime APIのPythonサンプルコードを作成" [level=3] [ref=s1e550]\n        - time [ref=s1e553]: 2024/10/04\n        - img "いいねされた数" [ref=s1e555]\n        - text: "79"\n- article [ref=s1e557]:\n    - link "tech" [ref=s1e558]:\n        - /url: /tech-or-idea\n    - link "📗 主成分分析を自力で導出する 2024/10/03 いいねされた数 6" [ref=s1e559]:\n        - /url: /asap/articles/ff8f34d19ca6a4\n        - text: 📗\n        - heading "主成分分析を自力で導出する" [level=3] [ref=s1e563]\n        - time [ref=s1e566]: 2024/10/03\n        - img "いいねされた数" [ref=s1e568]\n        - text: "6"\n- article [ref=s1e570]:\n    - link "tech" [ref=s1e571]:\n        - /url: /tech-or-idea\n    - link "😍 FLUX.1-dev で ControlNet-depth と LoRA【QTCanimatedFeeling-FLUX】 を試す 2024/10/02 いいねされた数 3" [ref=s1e572]:\n        - /url: /asap/articles/9192c2363dd96a\n        - text: 😍\n        - heading "FLUX.1-dev で ControlNet-depth と LoRA【QTCanimatedFeeling-FLUX】\n          を試す" [level=3] [ref=s1e576]\n        - time [ref=s1e579]: 2024/10/02\n        - img "いいねされた数" [ref=s1e581]\n        - text: "3"\n- article [ref=s1e583]:\n    - link "tech" [ref=s1e584]:\n        - /url: /tech-or-idea\n    - link "📗 拡散モデルの理論について（拡散過程の条件付き確率・逆拡散過程のスケジューラ） 2024/09/23 いいねされた数 8" [ref=s1e585]:\n        - /url: /asap/articles/8aaa896a02f168\n        - text: 📗\n        - heading "拡散モデルの理論について（拡散過程の条件付き確率・逆拡散過程のスケジューラ）" [level=3] [ref=s1e589]\n        - time [ref=s1e592]: 2024/09/23\n        - img "いいねされた数" [ref=s1e594]\n        - text: "8"\n- article [ref=s1e596]:\n    - link "tech" [ref=s1e597]:\n        - /url: /tech-or-idea\n    - link "📖 分散共分散行列の魅力 2024/09/20 いいねされた数 14" [ref=s1e598]:\n        - /url: /asap/articles/2269cd74c5ddc2\n        - text: 📖\n        - heading "分散共分散行列の魅力" [level=3] [ref=s1e602]\n        - time [ref=s1e605]: 2024/09/20\n        - img "いいねされた数" [ref=s1e607]\n        - text: "14"\n- article [ref=s1e609]:\n    - link "tech" [ref=s1e610]:\n        - /url: /tech-or-idea\n    - link "😍 超強力汎用ControlNet「AnyTest v4」を利用する方法【diffusers】 2024/09/17 いいねされた数 9" [ref=s1e611]:\n        - /url: /asap/articles/c08dffa41bec03\n        - text: 😍\n        - heading "超強力汎用ControlNet「AnyTest v4」を利用する方法【diffusers】" [level=3]\n          [ref=s1e615]\n        - time [ref=s1e618]: 2024/09/17\n        - img "いいねされた数" [ref=s1e620]\n        - text: "9"\n- article [ref=s1e622]:\n    - link "tech" [ref=s1e623]:\n        - /url: /tech-or-idea\n    - link "🌍 FLUX.1-devでControlNetを利用したら画像の質が下がったのでImg2Imgを使って改善してみた【Diffusers】 2024/09/13 いいねされた数 2" [ref=s1e624]:\n        - /url: /asap/articles/912d6de879d006\n        - text: 🌍\n        - heading\n          "FLUX.1-devでControlNetを利用したら画像の質が下がったのでImg2Imgを使って改善してみた【Diffusers】"\n          [level=3] [ref=s1e628]\n        - time [ref=s1e631]: 2024/09/13\n        - img "いいねされた数" [ref=s1e633]\n        - text: "2"\n- article [ref=s1e635]:\n    - link "tech" [ref=s1e636]:\n        - /url: /tech-or-idea\n    - link "🎮 【Next.js】ポートレートの表情を編集する自作UIを作ってみた【ComfyUI-AdvancedLivePortrait】 2024/09/11 いいねされた数 8" [ref=s1e637]:\n        - /url: /asap/articles/c49551ff14d5e9\n        - text: 🎮\n        - heading\n          "【Next.js】ポートレートの表情を編集する自作UIを作ってみた【ComfyUI-AdvancedLivePortrait】"\n          [level=3] [ref=s1e641]\n        - time [ref=s1e644]: 2024/09/11\n        - img "いいねされた数" [ref=s1e646]\n        - text: "8"\n- article [ref=s1e648]:\n    - link "tech" [ref=s1e649]:\n        - /url: /tech-or-idea\n    - link "📖 ComfyUI-AdvancedLivePortraitを使ってみる（No WebUI）（Google Colab） 2024/09/10 いいねされた数 7" [ref=s1e650]:\n        - /url: /asap/articles/c0a1b82a10f0e2\n        - text: 📖\n        - heading "ComfyUI-AdvancedLivePortraitを使ってみる（No WebUI）（Google Colab）"\n          [level=3] [ref=s1e654]\n        - time [ref=s1e657]: 2024/09/10\n        - img "いいねされた数" [ref=s1e659]\n        - text: "7"\n- article [ref=s1e661]:\n    - link "tech" [ref=s1e662]:\n        - /url: /tech-or-idea\n    - link "⛳ websocketsでリアルタイム音声処理 2024/09/06 いいねされた数 9" [ref=s1e663]:\n        - /url: /asap/articles/47f839550e5535\n        - text: ⛳\n        - heading "websocketsでリアルタイム音声処理" [level=3] [ref=s1e667]\n        - time [ref=s1e670]: 2024/09/06\n        - img "いいねされた数" [ref=s1e672]\n        - text: "9"\n- article [ref=s1e674]:\n    - link "tech" [ref=s1e675]:\n        - /url: /tech-or-idea\n    - link "😍 生成AIをGoogle Colabで簡単に 【FLUX.1-dev + ControlNet + LoRA】【Diffusers】 2024/08/31 いいねされた数 9" [ref=s1e676]:\n        - /url: /asap/articles/e4c199cecf1836\n        - text: 😍\n        - heading "生成AIをGoogle Colabで簡単に 【FLUX.1-dev + ControlNet +\n          LoRA】【Diffusers】" [level=3] [ref=s1e680]\n        - time [ref=s1e683]: 2024/08/31\n        - img "いいねされた数" [ref=s1e685]\n        - text: "9"\n- article [ref=s1e687]:\n    - link "tech" [ref=s1e688]:\n        - /url: /tech-or-idea\n    - link "👋 Style-Bert-VITS2のAPI化 2024/08/28 いいねされた数 6" [ref=s1e689]:\n        - /url: /asap/articles/12b0f31625a90f\n        - text: 👋\n        - heading "Style-Bert-VITS2のAPI化" [level=3] [ref=s1e693]\n        - time [ref=s1e696]: 2024/08/28\n        - img "いいねされた数" [ref=s1e698]\n        - text: "6"\n- article [ref=s1e700]:\n    - link "tech" [ref=s1e701]:\n        - /url: /tech-or-idea\n    - link "😊 【超簡単】1枚の画像と音声から口パク動画を作りたい！【EchoMimic】 2024/08/25 いいねされた数 6" [ref=s1e702]:\n        - /url: /asap/articles/db1ed88f291b98\n        - text: 😊\n        - heading "【超簡単】1枚の画像と音声から口パク動画を作りたい！【EchoMimic】" [level=3] [ref=s1e706]\n        - time [ref=s1e709]: 2024/08/25\n        - img "いいねされた数" [ref=s1e711]\n        - text: "6"\n- article [ref=s1e713]:\n    - link "tech" [ref=s1e714]:\n        - /url: /tech-or-idea\n    - link "✍️ 【Next.js】生成AIによる「お絵描きサポートWebアプリ」を作りたい！【全くわからない人】 2024/08/20 いいねされた数 8" [ref=s1e715]:\n        - /url: /asap/articles/8f51ffd4fc29b8\n        - text: ✍️\n        - heading "【Next.js】生成AIによる「お絵描きサポートWebアプリ」を作りたい！【全くわからない人】" [level=3]\n          [ref=s1e719]\n        - time [ref=s1e722]: 2024/08/20\n        - img "いいねされた数" [ref=s1e724]\n        - text: "8"\n- article [ref=s1e726]:\n    - link "tech" [ref=s1e727]:\n        - /url: /tech-or-idea\n    - link "🔥 GPT-SoVITS ver2を学習する【WebUI使わない】 2024/08/15 いいねされた数 8" [ref=s1e728]:\n        - /url: /asap/articles/e64c6fce28e0f0\n        - text: 🔥\n        - heading "GPT-SoVITS ver2を学習する【WebUI使わない】" [level=3] [ref=s1e732]\n        - time [ref=s1e735]: 2024/08/15\n        - img "いいねされた数" [ref=s1e737]\n        - text: "8"\n- article [ref=s1e739]:\n    - link "tech" [ref=s1e740]:\n        - /url: /tech-or-idea\n    - link "🔥 GPT-soVITS ver2をモジュールとして動かす 2024/08/08 いいねされた数 9" [ref=s1e741]:\n        - /url: /asap/articles/964a7d863d8896\n        - text: 🔥\n        - heading "GPT-soVITS ver2をモジュールとして動かす" [level=3] [ref=s1e745]\n        - time [ref=s1e748]: 2024/08/08\n        - img "いいねされた数" [ref=s1e750]\n        - text: "9"\n- article [ref=s1e752]:\n    - link "tech" [ref=s1e753]:\n        - /url: /tech-or-idea\n    - link "💯 拡散モデルの理論って面白いよね 2024/08/07 いいねされた数 87" [ref=s1e754]:\n        - /url: /asap/articles/4092ab60570b05\n        - text: 💯\n        - heading "拡散モデルの理論って面白いよね" [level=3] [ref=s1e758]\n        - time [ref=s1e761]: 2024/08/07\n        - img "いいねされた数" [ref=s1e763]\n        - text: "87"\n- article [ref=s1e765]:\n    - link "tech" [ref=s1e766]:\n        - /url: /tech-or-idea\n    - link "🙆\u200d♀️ 画像生成AIの生成途中経過を可視化したい！【Diffusers】 2024/08/06 いいねされた数 4" [ref=s1e767]:\n        - /url: /asap/articles/7940b17be86da7\n        - text: 🙆\u200d♀️\n        - heading "画像生成AIの生成途中経過を可視化したい！【Diffusers】" [level=3] [ref=s1e771]\n        - time [ref=s1e774]: 2024/08/06\n        - img "いいねされた数" [ref=s1e776]\n        - text: "4"\n- article [ref=s1e778]:\n    - link "tech" [ref=s1e779]:\n        - /url: /tech-or-idea\n    - link "🚀 生成AIをGoogle Colabで簡単に 【Part10 SDXL+ANIMAGINE +ControlNet+LoRA編（全部載せ）】 2024/08/03 いいねされた数 3" [ref=s1e780]:\n        - /url: /asap/articles/aa856819e2a722\n        - text: 🚀\n        - heading "生成AIをGoogle Colabで簡単に 【Part10 SDXL+ANIMAGINE\n          +ControlNet+LoRA編（全部載せ）】" [level=3] [ref=s1e784]\n        - time [ref=s1e787]: 2024/08/03\n        - img "いいねされた数" [ref=s1e789]\n        - text: "3"\n- article [ref=s1e791]:\n    - link "tech" [ref=s1e792]:\n        - /url: /tech-or-idea\n    - link "😸 生成AIをGoogle Colaboratoryで簡単に 【Part6.5 画像生成AI SDXL+ANIMAGINE XL 3.1編】 2024/07/31 いいねされた数 3" [ref=s1e793]:\n        - /url: /asap/articles/f9ac2e83c820ff\n        - text: 😸\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part6.5 画像生成AI SDXL+ANIMAGINE\n          XL 3.1編】" [level=3] [ref=s1e797]\n        - time [ref=s1e800]: 2024/07/31\n        - img "いいねされた数" [ref=s1e802]\n        - text: "3"\n- article [ref=s1e804]:\n    - link "tech" [ref=s1e805]:\n        - /url: /tech-or-idea\n    - link "🎃 生成AIをGoogle Colaboratoryで簡単に 【Part9 画像生成AI SD3+ControlNet編】 2024/07/29 いいねされた数 3" [ref=s1e806]:\n        - /url: /asap/articles/53560859d273e4\n        - text: 🎃\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part9 画像生成AI SD3+ControlNet編】"\n          [level=3] [ref=s1e810]\n        - time [ref=s1e813]: 2024/07/29\n        - img "いいねされた数" [ref=s1e815]\n        - text: "3"\n- article [ref=s1e817]:\n    - link "tech" [ref=s1e818]:\n        - /url: /tech-or-idea\n    - link "✨ 生成AIをGoogle Colaboratoryで簡単に 【Part8 画像生成AI Stable Diffusion 3 Medium編】 2024/07/25 いいねされた数 4" [ref=s1e819]:\n        - /url: /asap/articles/6f6ca9aa0513d8\n        - text: ✨\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part8 画像生成AI Stable Diffusion 3\n          Medium編】" [level=3] [ref=s1e823]\n        - time [ref=s1e826]: 2024/07/25\n        - img "いいねされた数" [ref=s1e828]\n        - text: "4"\n- article [ref=s1e830]:\n    - link "tech" [ref=s1e831]:\n        - /url: /tech-or-idea\n    - link "🌟 生成AIをGoogle Colaboratoryで簡単に 【Part7 画像生成AI SDXL Turbo編】 2024/07/23 いいねされた数 2" [ref=s1e832]:\n        - /url: /asap/articles/ed58a6899c1c26\n        - text: 🌟\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part7 画像生成AI SDXL Turbo編】"\n          [level=3] [ref=s1e836]\n        - time [ref=s1e839]: 2024/07/23\n        - img "いいねされた数" [ref=s1e841]\n        - text: "2"\n- article [ref=s1e843]:\n    - link "tech" [ref=s1e844]:\n        - /url: /tech-or-idea\n    - link "🌟 生成AIをGoogle Colaboratoryで簡単に 【Part6 画像生成AI Stable Diffusion XL編】 2024/07/19 いいねされた数 4" [ref=s1e845]:\n        - /url: /asap/articles/e4791be54f90ce\n        - text: 🌟\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part6 画像生成AI Stable Diffusion\n          XL編】" [level=3] [ref=s1e849]\n        - time [ref=s1e852]: 2024/07/19\n        - img "いいねされた数" [ref=s1e854]\n        - text: "4"\n- article [ref=s1e856]:\n    - link "tech" [ref=s1e857]:\n        - /url: /tech-or-idea\n    - link "🎤 生成AIをローカルで簡単に 【Part5.5 faster-whisper+マイク録音編】 2024/07/16 いいねされた数 7" [ref=s1e858]:\n        - /url: /asap/articles/2c0d421e68ef16\n        - text: 🎤\n        - heading "生成AIをローカルで簡単に 【Part5.5 faster-whisper+マイク録音編】" [level=3]\n          [ref=s1e862]\n        - time [ref=s1e865]: 2024/07/16\n        - img "いいねされた数" [ref=s1e867]\n        - text: "7"\n- article [ref=s1e869]:\n    - link "tech" [ref=s1e870]:\n        - /url: /tech-or-idea\n    - link "📖 生成AIをGoogle Colaboratoryで簡単に 【Part5 音声認識AI faster-whisper編】 2024/07/13 いいねされた数 2" [ref=s1e871]:\n        - /url: /asap/articles/ba8fcb1880165e\n        - text: 📖\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part5 音声認識AI faster-whisper編】"\n          [level=3] [ref=s1e875]\n        - time [ref=s1e878]: 2024/07/13\n        - img "いいねされた数" [ref=s1e880]\n        - text: "2"\n- article [ref=s1e882]:\n    - link "tech" [ref=s1e883]:\n        - /url: /tech-or-idea\n    - link "😸 生成AIをGoogle Colaboratoryで簡単に 【Part4 音声認識AI Whisper API編】 2024/07/11 いいねされた数 2" [ref=s1e884]:\n        - /url: /asap/articles/ecaa5631e42195\n        - text: 😸\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part4 音声認識AI Whisper API編】"\n          [level=3] [ref=s1e888]\n        - time [ref=s1e891]: 2024/07/11\n        - img "いいねされた数" [ref=s1e893]\n        - text: "2"\n- article [ref=s1e895]:\n    - link "tech" [ref=s1e896]:\n        - /url: /tech-or-idea\n    - link "😊 生成AIをGoogle Colaboratoryで簡単に 【Part3 テキスト生成 ローカルLLM編】 2024/07/08 いいねされた数 4" [ref=s1e897]:\n        - /url: /asap/articles/8c3d5164d14f6e\n        - text: 😊\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part3 テキスト生成 ローカルLLM編】"\n          [level=3] [ref=s1e901]\n        - time [ref=s1e904]: 2024/07/08\n        - img "いいねされた数" [ref=s1e906]\n        - text: "4"\n- article [ref=s1e908]:\n    - link "tech" [ref=s1e909]:\n        - /url: /tech-or-idea\n    - link "🐷 生成AIをGoogle Colaboratoryで簡単に 【Part2 テキスト生成ChatGPT API編】 2024/07/05 いいねされた数 2" [ref=s1e910]:\n        - /url: /asap/articles/f410dd5d407652\n        - text: 🐷\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part2 テキスト生成ChatGPT API編】"\n          [level=3] [ref=s1e914]\n        - time [ref=s1e917]: 2024/07/05\n        - img "いいねされた数" [ref=s1e919]\n        - text: "2"\n- article [ref=s1e921]:\n    - link "tech" [ref=s1e922]:\n        - /url: /tech-or-idea\n    - link "🗂 生成AIをGoogle Colaboratoryで簡単に 【Part1 合成音声Style-Bert-VITS2編】 2024/07/02 いいねされた数 3" [ref=s1e923]:\n        - /url: /asap/articles/7bdb20dcb5cc56\n        - text: 🗂\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part1 合成音声Style-Bert-VITS2編】"\n          [level=3] [ref=s1e927]\n        - time [ref=s1e930]: 2024/07/02\n        - img "いいねされた数" [ref=s1e932]\n        - text: "3"\n- article [ref=s1e934]:\n    - link "tech" [ref=s1e935]:\n        - /url: /tech-or-idea\n    - link "👏 Remdisの環境構築で沼った話（＋使い方の紹介） 2024/06/26 いいねされた数 14" [ref=s1e936]:\n        - /url: /asap/articles/101379d8951b08\n        - text: 👏\n        - heading "Remdisの環境構築で沼った話（＋使い方の紹介）" [level=3] [ref=s1e940]\n        - time [ref=s1e943]: 2024/06/26\n        - img "いいねされた数" [ref=s1e945]\n        - text: "14"\n- article [ref=s1e947]:\n    - link "tech" [ref=s1e948]:\n        - /url: /tech-or-idea\n    - link "🚀 Style-Bert-VITS2の使い方メモ 2024/06/17 いいねされた数 34" [ref=s1e949]:\n        - /url: /asap/articles/f8c0621cdd74cc\n        - text: 🚀\n        - heading "Style-Bert-VITS2の使い方メモ" [level=3] [ref=s1e953]\n        - time [ref=s1e956]: 2024/06/17\n        - img "いいねされた数" [ref=s1e958]\n        - text: "34"\n- article [ref=s1e960]:\n    - link "tech" [ref=s1e961]:\n        - /url: /tech-or-idea\n    - link "💭 Style-Bert-VITS2のVoice Conversion機能の復活に挑戦してみた（前編） 2024/06/09 いいねされた数 9" [ref=s1e962]:\n        - /url: /asap/articles/df2c610d1389e4\n        - text: 💭\n        - heading "Style-Bert-VITS2のVoice Conversion機能の復活に挑戦してみた（前編）" [level=3]\n          [ref=s1e966]\n        - time [ref=s1e969]: 2024/06/09\n        - img "いいねされた数" [ref=s1e971]\n        - text: "9"\n- article [ref=s1e973]:\n    - link "tech" [ref=s1e974]:\n        - /url: /tech-or-idea\n    - link "😸 OpenAI Assistants API v2を色々いじってみた（使い方）（File Search編） 2024/05/28 いいねされた数 10" [ref=s1e975]:\n        - /url: /asap/articles/5c2f8ffe2a46ce\n        - text: 😸\n        - heading "OpenAI Assistants API v2を色々いじってみた（使い方）（File Search編）"\n          [level=3] [ref=s1e979]\n        - time [ref=s1e982]: 2024/05/28\n        - img "いいねされた数" [ref=s1e984]\n        - text: "10"\n- article [ref=s1e986]:\n    - link "tech" [ref=s1e987]:\n        - /url: /tech-or-idea\n    - link "👻 OpenAI Assistants API v2を色々いじってみた（使い方）（code interpreter編） 2024/05/27 いいねされた数 5" [ref=s1e988]:\n        - /url: /asap/articles/b0ce16ea1832ac\n        - text: 👻\n        - heading "OpenAI Assistants API v2を色々いじってみた（使い方）（code interpreter編）"\n          [level=3] [ref=s1e992]\n        - time [ref=s1e995]: 2024/05/27\n        - img "いいねされた数" [ref=s1e997]\n        - text: "5"\n- article [ref=s1e999]:\n    - link "tech" [ref=s1e1000]:\n        - /url: /tech-or-idea\n    - link "😑 【自分用】「AI後輩ちゃん」の個人的つまづきポイントまとめ 2024/05/09 いいねされた数 3" [ref=s1e1001]:\n        - /url: /asap/articles/98b04c5ea82bd9\n        - text: 😑\n        - heading "【自分用】「AI後輩ちゃん」の個人的つまづきポイントまとめ" [level=3] [ref=s1e1005]\n        - time [ref=s1e1008]: 2024/05/09\n        - img "いいねされた数" [ref=s1e1010]\n        - text: "3"\n- article [ref=s1e1012]:\n    - link "tech" [ref=s1e1013]:\n        - /url: /tech-or-idea\n    - link "😍 音声対話システム「AI後輩ちゃん」を作ってみた 2024/05/04 いいねされた数 17" [ref=s1e1014]:\n        - /url: /asap/articles/5b1b7553fcaa76\n        - text: 😍\n        - heading "音声対話システム「AI後輩ちゃん」を作ってみた" [level=3] [ref=s1e1018]\n        - time [ref=s1e1021]: 2024/05/04\n        - img "いいねされた数" [ref=s1e1023]\n        - text: "17"\n- contentinfo [ref=s1e1026]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1030]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1031]\n    - paragraph [ref=s1e1039]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e1041]:\n        - heading "About" [level=4] [ref=s1e1042]\n        - list [ref=s1e1043]:\n            - listitem [ref=s1e1044]:\n                - link "Zennについて" [ref=s1e1046]:\n                    - /url: /about\n            - listitem [ref=s1e1047]:\n                - link "運営会社" [ref=s1e1048]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e1049]:\n                - link "お知らせ・リリース" [ref=s1e1050]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e1051]:\n                - link "イベント" [ref=s1e1053]:\n                    - /url: /events\n    - navigation [ref=s1e1054]:\n        - heading "Guides" [level=4] [ref=s1e1055]\n        - list [ref=s1e1056]:\n            - listitem [ref=s1e1057]:\n                - link "使い方" [ref=s1e1059]:\n                    - /url: /manual\n            - listitem [ref=s1e1060]:\n                - link "法人向けメニュー" [ref=s1e1062]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e1064]:\n                - link "Publication / Pro" [ref=s1e1066]:\n                    - /url: /publications\n            - listitem [ref=s1e1067]:\n                - link "よくある質問" [ref=s1e1069]:\n                    - /url: /faq\n    - navigation [ref=s1e1070]:\n        - heading "Links" [level=4] [ref=s1e1071]\n        - list [ref=s1e1072]:\n            - listitem [ref=s1e1073]:\n                - link "X(Twitter)" [ref=s1e1074]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e1075]:\n                - link "GitHub" [ref=s1e1076]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e1077]:\n                - link "メディアキット" [ref=s1e1079]:\n                    - /url: /mediakit\n    - navigation [ref=s1e1080]:\n        - heading "Legal" [level=4] [ref=s1e1081]\n        - list [ref=s1e1082]:\n            - listitem [ref=s1e1083]:\n                - link "利用規約" [ref=s1e1085]:\n                    - /url: /terms\n            - listitem [ref=s1e1086]:\n                - link "プライバシーポリシー" [ref=s1e1088]:\n                    - /url: /privacy\n            - listitem [ref=s1e1089]:\n                - link "特商法表記" [ref=s1e1091]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e1094]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e1095]\n- alert [ref=s1e1114]\n```\n', name='browser_navigate', tool_call_id='58fad86e-0b01-4fc6-a8cc-57deb5c48b41'), AIMessage(content='`https://zenn.dev/asap`にアクセスしました。\n\n執筆者の記事一覧から、いいねの数が多い記事を探します。\n\n- 「DeepSeek-R1の論文読んだ？【勉強になるよ】」 が921いいね\n- 「拡散モデルの理論って面白いよね」 が87いいね\n- 「Open AI Realtime APIのPythonサンプルコードを作成」 が79いいね\n- 「📏 【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】」 が50いいね\n- 「🙌 完全自律型AIエージェントOSS【LangManus】完全に理解した！」 が48いいね\n\n最もいいねが多い記事は「DeepSeek-R1の論文読んだ？【勉強になるよ】」です。\n記事の中身を取得します。', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap/articles/34237ad87f8511"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-27104099-cd7e-4de1-8c89-51060ed9c6f1-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap/articles/34237ad87f8511'}, 'id': 'd85f8505-9c2a-4960-b17f-9ef88d983d9d', 'type': 'tool_call'}], usage_metadata={'input_tokens': 17995, 'output_tokens': 216, 'total_tokens': 18211, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap/articles/34237ad87f8511\n\n- Page URL: https://zenn.dev/asap/articles/34237ad87f8511\n- Page Title: DeepSeek-R1の論文読んだ？【勉強になるよ】\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- article [ref=s1e32]:\n    - text: 📑\n    - heading "DeepSeek-R1の論文読んだ？【勉強になるよ】" [level=1] [ref=s1e38]\n    - text: 2025/01/27に公開\n    - tooltip "更新日" [ref=s1e45]:\n        - img [ref=s1e46]\n        - text: 2025/02/22 更新日\n    - link "12件" [ref=s1e51]:\n        - /url: "#discuss"\n        - img [ref=s1e52]\n        - text: 12件\n    - link "強化学習" [ref=s1e63]:\n        - /url: /topics/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92\n        - img [ref=s1e65]\n        - text: 強化学習\n    - link "蒸留" [ref=s1e67]:\n        - /url: /topics/%E8%92%B8%E7%95%99\n        - img [ref=s1e69]\n        - text: 蒸留\n    - link "LLM" [ref=s1e71]:\n        - /url: /topics/llm\n        - img [ref=s1e73]\n        - text: LLM\n    - link "生成 AI" [ref=s1e75]:\n        - /url: /topics/%E7%94%9F%E6%88%90ai\n        - img [ref=s1e77]\n        - text: 生成 AI\n    - link "DeepSeek" [ref=s1e79]:\n        - /url: /topics/deepseek\n        - img [ref=s1e81]\n        - text: DeepSeek\n    - link "tech" [ref=s1e83]:\n        - /url: /tech-or-idea\n        - img [ref=s1e85]\n        - text: tech\n    - heading "はじめに" [level=1] [ref=s1e89]\n    - paragraph [ref=s1e90]: タイトル通りDeepSeek-R1の論文を読んだので、少しでもわかりやすく説明できればと思います。\n        まだ論文を読んでいない方は、ぜひ原文も読んでみてください！\n    - paragraph [ref=s1e92]:\n        - text: 難しい数式や概念がほとんど使われていないため、論文の中では全然読みやすい方だと思います。読み物として読めるレベルです。\n        - iframe [ref=s1e94]:\n            - text: Loading...\n            - alert [ref=f1s1e8]\n    - paragraph [ref=s1e95]: とはいえ、本記事では細かい部分など間違っている部分もあるかもしれないので、ぜひ誤りがあったら、こそっと教えてください！\n    - complementary [ref=s1e96]:\n        - text: "!"\n        - paragraph [ref=s1e99]: 実は、本論文を解説する上で、著者らが公開しているもう一つの論文が大きく関連しています。 そちらの論文の内容も若干解説を入れております。\n        - paragraph [ref=s1e101]:\n            - text: 「\n            - \'link "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models" [ref=s1e102]\':\n                - /url: https://arxiv.org/html/2402.03300v3\n            - text: 」\n        - paragraph [ref=s1e103]: 本記事でも少し触れますが、上記の論文も非常に勉強になるため、ぜひ読んでみてください。\n    - complementary [ref=s1e104]:\n        - text: "!"\n        - paragraph [ref=s1e107]:\n            - text: 本記事は、\n            - link "DeepSeek-R1の論文" [ref=s1e108]:\n                - /url: https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\n            - text: と\n            - link "DeepSeekMathの論文" [ref=s1e109]:\n                - /url: https://arxiv.org/html/2402.03300v3\n            - text: を読んだ私の理解をもとに記載しています。\n        - paragraph [ref=s1e110]: 本論文で使われている技術に関しては、ある程度の知識を持っているので、大きくは外していないとは思いますが、私の主観も入っている部分もありますので、ご了承ください。\n    - paragraph [ref=s1e111]:\n        - text: また、DeepSeek-R1の論文が公開される前に、小型モデルに対して同様の実験（強化学習）をしていたグループがあるようです。\n            そちらのレポートは下記になります。\n        - iframe [ref=s1e114]:\n            - \'link "7B Model and 8K Examples: Emerging Reasoning with Reinforcement Learning is Both Effective and Effic A replication of DeepSeek-R1 training on small models with limited data hkust-nlp.notion.site favicon image hkust-nlp.notion.site hkust-nlp.notion.site thumbnail image" [ref=f2s1e6]\':\n                - /url: https://hkust-nlp.notion.site/simplerl-reason\n                - \'heading "7B Model and 8K Examples: Emerging Reasoning with\n                  Reinforcement Learning is Both Effective and Effic" [level=1]\n                  [ref=f2s1e8]\'\n                - text: A replication of DeepSeek-R1 training on small models with limited data\n                - img "hkust-nlp.notion.site favicon image" [ref=f2s1e11]\n                - text: hkust-nlp.notion.site\n                - img "hkust-nlp.notion.site thumbnail image" [ref=f2s1e13]\n            - alert [ref=f2s1e15]\n    - paragraph [ref=s1e115]: 意図せず、DeepSeek-R1-Zeroの再現実験のようなレポートになっていますが、レポートの著者はDeepSeek-R1論文の公開前から実験していると主張しています。\n        こちらも非常に興味深かったため紹介です。\n    - heading "本論文の興味深いところ" [level=1] [ref=s1e117]\n    - paragraph [ref=s1e118]: 本論文は、大きく分けて3つの構成でできています\n    - list [ref=s1e119]:\n        - listitem [ref=s1e120]:\n            - strong [ref=s1e121]: 強化学習による思考能力の強化\n            - list [ref=s1e122]:\n                - listitem [ref=s1e123]: LLM（DeepSeek-V3-Base）に対して「強化学習のみ」を適用させたところ、強力な思考能力を獲得した「DeepSeek-R1-Zero」が作成されました。\n                - listitem [ref=s1e124]: 本論文のメインストリームであり、非常に興味深い示唆が得られる部分です。\n        - listitem [ref=s1e125]:\n            - strong [ref=s1e126]: SFTによるさらなる強化\n            - list [ref=s1e127]:\n                - listitem [ref=s1e128]: DeepSeek-V3-Baseに対して、コールドスタート問題を解消するために、CoT（Chain of\n                    Thought）データセットによるSFT（教師付きファインチューニング）を実施\n                - listitem [ref=s1e129]: その後、強化学習を適用させることで、o1レベルの性能を獲得した「DeepSeek-R1」が作成されました。\n                - listitem [ref=s1e130]: 非常に強力なLLMを獲得するまでの反復学習などの試行錯誤などを学ぶことができます。\n        - listitem [ref=s1e131]:\n            - strong [ref=s1e132]: 蒸留による小型モデルの性能強化\n            - list [ref=s1e133]:\n                - listitem [ref=s1e134]: 上記で作成されたDeepSeek-R1により、他社の小型オープンソースLLMを蒸留しました。\n                - listitem [ref=s1e135]: その結果、元の性能を大幅に上回る、非常に高い性能の小型LLMが得られました。\n    - paragraph [ref=s1e136]: その中でも私が特に面白いなと思った要素は下記の要素になります\n    - list [ref=s1e137]:\n        - listitem [ref=s1e138]:\n            - strong [ref=s1e139]: 強化学習手法「GRPO」すごい！\n        - listitem [ref=s1e140]:\n            - strong [ref=s1e141]: ルールベース報酬で思考能力が向上するのがすごい！\n        - listitem [ref=s1e142]:\n            - strong [ref=s1e143]: aha moment（ハッとする瞬間）面白い！\n        - listitem [ref=s1e144]:\n            - strong [ref=s1e145]: 蒸留vs強化学習の比較実験から重要な示唆が得られた！\n    - heading "本論文の流れ" [level=1] [ref=s1e146]\n    - heading "前提のモデル構造" [level=3] [ref=s1e147]\n    - paragraph [ref=s1e148]: そもそも、大前提として、この論文は「DeepSeek-V3-Base」という強力なLLMの性能を改善する論文です。\n        これ単体でも、GPT-4oレベルの能力を持っているオープンソースLLMになります。\n    - paragraph [ref=s1e150]: 本論文のモデルアーキテクチャは「DeepSeek-V3-Base」と完全に同じものを利用します。\n        （つまりモデルアーキテクチャをかえずに、学習方法を工夫してLLMの性能を上げる挑戦になります）\n    - heading "DeepSeek-R1-Zeroを強化学習を用いて作る" [level=3] [ref=s1e152]\n    - paragraph [ref=s1e153]: その上で、まずは、「DeepSeek-V3-Base」に対して、大規模な強化学習のみを適応させることで、強化学習の可能性について探究する話が始まります。\n        結果として、「DeepSeek-R1-Zero」ができます。\n        「DeepSeek-R1-Zero」は、SFT（教師付きファインチューニング）をしなくても、強化学習のみで圧倒的な思考能力を取得できることを示す興味深いモデルです。\n    - heading "DeepSeek-R1をSFT+強化学習を用いて作る" [level=3] [ref=s1e156]\n    - paragraph [ref=s1e157]: その後、あまりにも「DeepSeek-R1-Zero」の性能が良かったことから、さらなる探究が進められました。\n    - paragraph [ref=s1e158]: この時点で、「DeepSeek-R1-Zero」には下記の課題がありました。\n    - list [ref=s1e159]:\n        - listitem [ref=s1e160]: 学習初期段階にて、強化学習の学習が不安定であり、収束に時間がかかる（コールドスタート問題）\n        - listitem [ref=s1e161]:\n            - text: 思考能力は高いが、言語が混在（日本語で話していたら急に中国語になるなど）し、人間が読みにくい\n            - list [ref=s1e162]:\n                - listitem [ref=s1e163]: 結果、高度な思考が必要なタスクには強いが、一般的タスク（ロールプレイやライティングなど）には使いにくいという評価\n    - paragraph [ref=s1e164]: そこで、「DeepSeek-V3-Base」に対して、少量のCoT（Chain of\n        Thought）データセットを用いて、SFT（教師付き微調整）を行います。このデータは、著者らが収集・構築したものになります。\n        その後、SFT後のモデルに対して、大規模強化学習を行います。\n        このとき、「DeepSeek-R1-Zero」を作成した際の報酬に加え、「言語一貫性報酬」を導入して、上述した言語の不一致の問題を緩和させます。\n    - paragraph [ref=s1e167]: その後、得られたモデル（チェックポイント）を利用して、さらなるSFT（教師付き微調整）学習データを収集し、追加SFTを行います。\n        さらに、その後、改めて２段階目の大規模強化学習を実施することで、最終的に「DeepSeek-R1」が得られます。\n    - heading "小型モデルへの蒸留を試す" [level=3] [ref=s1e169]\n    - paragraph [ref=s1e170]: 最後に、小型モデルに対する「DeepSeek-R1」による蒸留が試されています。\n        その中で、小型モデルに対して、「DeepSeek-R1」による蒸留を実施するのと、「DeepSeek-R1-Zero」と同様の強化学習手法で思考能力を強化するのと、どちらが性能向上に寄与するのかを調査してくれています。\n    - paragraph [ref=s1e172]: これが非常に興味深いです。\n    - heading "DeepSeek-R1-Zeroを作るまで" [level=1] [ref=s1e173]\n    - paragraph [ref=s1e174]: では、ここから論文の詳細な解説に入っていきます。\n    - paragraph [ref=s1e175]: まずは、「DeepSeek-R1」の前段階である「DeepSeek-R1-Zero」を作るまでの手法を解説します。\n    - paragraph [ref=s1e176]:\n        - strong [ref=s1e177]: 前段階と侮るなかれ、「DeepSeek-R1-Zero」を作るまでの手法と示唆にこそ、本論文の貴重な示唆が含まれています。\n    - heading "元々のアーキテクチャ" [level=2] [ref=s1e178]\n    - paragraph [ref=s1e179]: 今回作る「DeepSeek-R1-Zero」や「DeepSeek-R1」は、新しいモデル構造を採用して、性能を向上させたわけではありません。\n    - paragraph [ref=s1e180]: 元々のアーキテクチャは「DeepSeek-V3-Base」というアーキテクチャになります。\n        「DeepSeek-V3-Base」は、681Bのモデルパラメータを持つ大規模言語モデルで、性能はGPT-4oに匹敵する非常に高性能モデルありながら、モデル重みが完全に公開されているオープンソースモデルです。\n    - paragraph [ref=s1e182]: 「DeepSeek-V3-Base」に対して、Instructionチューニングを施したchatモデルに関しては、APIでも公開されています。\n        非常に安価に使えるにも関わらず、GPT-4oに匹敵する性能を誇ることから、人気のモデルになっております。\n    - complementary [ref=s1e184]:\n        - text: "!"\n        - paragraph [ref=s1e187]: 安価なモデルではありますが、APIの提供元は中国の企業ですので、データの取り扱いなどは十分注意してください。\n            基本的には、API利用の際には、収集されて困るような情報は入力しないようにするのが無難です。\n            （オープンソースモデルをローカルで動かす場合は、おそらく心配ありません）\n    - paragraph [ref=s1e190]: 本論文のすごいところは、モデル構造を変えずに、「DeepSeek-V3-Base」に対する学習方法を工夫しただけで、OpenAI\n        o1に匹敵する性能のLLMを開発していることです。\n        具体的には、大規模強化学習をLLMに対して適用することで、「DeepSeek-R1-Zero」を開発します。\n    - heading "すごいぞ！強化学習" [level=2] [ref=s1e192]\n    - heading "強化学習って何？" [level=3] [ref=s1e193]\n    - paragraph [ref=s1e194]: そもそも「強化学習って何？」という方もいらっしゃると思うので、簡単に説明します。\n        （以前、スイカゲームを強化学習でプレイしたこともあるので、いずれ詳しい解説記事を作る予定ですが、ここではざっくり説明します。）\n    - paragraph [ref=s1e196]: 強化学習とは、一言で言えば「一連の行動の流れ（シークエンス）によって得られる、累積報酬の期待値を最大化する」ことを目指す学習手法です。\n        ある行動をとった結果がすぐにはわからず、時間が経ってから影響が現れるような問題に適しています。\n        専門的に言うと、「マルコフ決定過程（MDP）」が成立する問題で活用されます。\n    - heading "テトリスで考える" [level=3] [ref=s1e199]\n    - paragraph [ref=s1e200]: 例えば、「テトリス」を考えてみましょう。\n    - paragraph [ref=s1e201]: テトリスは、上から「ミノ」と呼ばれる、さまざまな形のブロックを落として、横一列を綺麗に埋めてブロックを消去するゲームです。\n    - paragraph [ref=s1e202]: 横一列がすべて揃うと、その列が消えて、「ポイント」を獲得できます。\n        逆に、列を消せずに、ブロックが一定の高さまで積み上がると「ゲームオーバ」になります。\n    - paragraph [ref=s1e204]: テトリスを強化学習で解こうとすると、「どうすれば最も多くのポイントを得られるか」をAIに学習させることが重要になります。\n        単純に列を揃えて消すだけではなく、「次の手」や「その次の手」を見据えて、より良い結果につながるようにミノを配置することも必要です。\n        しかし、通常の学習方法(よくある入出力を近似させる学習方法)では、目の前の結果だけに注目してしまい、将来的な高得点につながる手を学習させることは難しいです。\n    - paragraph [ref=s1e207]:\n        - strong [ref=s1e208]: そこで役立つのが「強化学習」です。\n    - paragraph [ref=s1e209]: 強化学習では、ゲームの最初から最後までの流れを通して、「どう行動すれば最終的に得られる得点を最大化できるか」を学習できます。\n        このように、途中の行動の良し悪しがすぐにはわからなくても、最終的なゴール（得点の最大化）が設定できる問題に対して、強化学習は非常に効果的です。\n    - paragraph [ref=s1e211]:\n        - img [ref=s1e212]\n    - group [ref=s1e213]: 補足\n    - heading "強化学習の事例「AlphaGo Zero」" [level=3] [ref=s1e215]\n    - paragraph [ref=s1e216]: さらに、強化学習の威力をお伝えする事例を記載します。\n    - paragraph [ref=s1e217]: 2015年にアメリカのDeepMind社（Googleの傘下）が「AlphaGo」というモデルを開発しました。これはAIが囲碁をプレイするモデルで、当時の世界王者をハンディキャップなしで破るという、非常に衝撃的なモデルでした。\n        （当時としては、非常にセンセーショナルな出来事だったため、記憶にある方もいらっしゃるかと思います）\n    - paragraph [ref=s1e219]: この「AlphaGo」は、深層学習と強化学習により学習されています。\n        プロ棋士の棋譜データで深層学習で学習させた上で、強化学習による自己対局（AI同士の対局）を実施したモデルです。\n    - paragraph [ref=s1e221]:\n        - text: さて、ここまでも十分すごいですが、\n        - strong [ref=s1e223]: さらにすごいのはここからです。\n    - paragraph [ref=s1e224]: DeepMind社は後継モデルとして「AlphaGo Zero」というモデルを開発しました。\n        このモデルは、プロ棋士の棋譜データを「使わず」に、完全に「強化学習のみ」で学習されたモデルです。\n        つまり、囲碁のルールだけを与え、あとは最初から最後までAI同士の自己対局のみで学習されています。\n        （この場合、対局に勝利したら報酬、敗北したら罰を与える形で強化学習をし、対局に勝利した時に取っていた手（行動）の選択確率を上げるように学習しています）\n    - paragraph [ref=s1e228]:\n        - strong [ref=s1e229]: そして、その「AlphaGo Zero」は「AlphaGo」に対して、100戦全勝で圧勝しました。\n    - paragraph [ref=s1e230]: AIが囲碁の学習をする際に、人間が持つ定石の情報などは不要だったわけです。\n    - paragraph [ref=s1e231]: これが強化学習の威力です。\n        余談ですが、今回の論文で作成する「DeepSeek-R1-Zero」も（「DeepSeek-V3-Base」をベースにはしますが）強化学習のみで学習されます。\n        名前に入っている「Zero」は同じく強化学習のみで開発された「AlphaGo Zero」のオマージュかなと思います。\n    - heading "そもそもLLMで強化学習とは？" [level=2] [ref=s1e234]\n    - paragraph [ref=s1e235]: 「なんかゲームみたいなものに、強化学習を使うとうまくいきそう」というのは理解していただけたかなと思います。\n    - paragraph [ref=s1e236]:\n        - text: では、LLMに対してはどうか、というのを解説していきます。\n        - strong [ref=s1e238]: 実は、LLMと強化学習は非常に相性がいいです。\n    - paragraph [ref=s1e239]: なぜなら、LLMは一度にすべての文章を出力するAIではなく、1tokenずつ出力するモデルであり、その文章が良い文章か悪い文章かは、すべてのtokenを出力してみないとわからないからです。\n    - paragraph [ref=s1e240]:\n        - text: テトリスでは、ミノを一つずつ落とし、一列が消えたタイミングで、ポイントを取得するという形で評価されます。\n        - img [ref=s1e242]\n    - paragraph [ref=s1e243]:\n        - text: LLMでも、同様にtokenを1tokenずつ出力し、一文全部が出力されたタイミングで、その文章の良し悪しを評価することができれば、強化学習と同じ枠組みを使うことができるというわけです。\n        - img [ref=s1e245]\n    - heading "報酬モデルを作成し、累積報酬を最大化" [level=3] [ref=s1e246]\n    - paragraph [ref=s1e247]:\n        - text: テトリスでは、ゲームシステムが勝手に点数をつけてくれますが、LLMにおいて文章の良し悪しを勝手に判断してくれる便利な機能はありません。そこで我々が、その判断モデルを作ってあげる必要があります。\n            そこで、ニューラルネットワークを利用して、LLMに入力されたプロンプト\n        - math [ref=s1e252]: x\n        - text: に対して、LLMが生成した文章\n        - math [ref=s1e259]: "y"\n        - text: の点数\n        - math [ref=s1e266]: r\n        - text: を出力するようなネットワークを「報酬モデル\n        - math [ref=s1e273]: r θ ( r ∣ y , x )\n        - text: 」として作成することを考えます。\n    - paragraph [ref=s1e286]:\n        - text: まず初めに、データセットとして、「プロンプト\n        - math [ref=s1e290]: x\n        - text: 」「好ましい回答\n        - math [ref=s1e297]: y +\n        - text: 」「好ましくない回答\n        - math [ref=s1e306]: y −\n        - text: 」の3つが組となっているデータセットを用意します。 報酬モデル\n        - math [ref=s1e316]: r θ ( r ∣ y , x )\n        - text: は、「プロンプト\n        - math [ref=s1e332]: x\n        - text: 」と「出力\n        - math [ref=s1e339]: "y"\n        - text: 」の両方を入力することで、報酬（点数）を出力するモデルです。\n    - paragraph [ref=s1e343]:\n        - text: そこで、好ましい回答の報酬推定値\n        - math [ref=s1e347]: r θ ( r ∣ y + , x )\n        - text: と、好ましくない回答の報酬推定値\n        - math [ref=s1e365]: r θ ( r ∣ y − , x )\n        - text: の差を一定以上にするような損失関数を用いて学習を行います。 すると、報酬モデル\n        - math [ref=s1e384]: r θ ( r ∣ y , x )\n        - text: は適切に、好ましい回答\n        - math [ref=s1e400]: y +\n        - text: には高い点数を、好ましくない回答\n        - math [ref=s1e409]: y −\n        - text: には低い点数をつけるようになります。\n    - paragraph [ref=s1e415]:\n        - text: 報酬モデル\n        - math [ref=s1e419]: r θ ( r ∣ y , x )\n        - text: か完成したら、強化学習をするための要素は整います。\n    - paragraph [ref=s1e432]:\n        - text: 状態\n        - math [ref=s1e436]: x\n        - text: は、LLMに入力される「プロンプト」で、LLMの出力が\n        - math [ref=s1e443]: "y"\n        - text: とします。 報酬\n        - math [ref=s1e451]: r\n        - text: は、報酬モデル\n        - math [ref=s1e458]: r θ ( r ∣ y , x )\n        - text: の出力値です。 LLMが全文出力したら、プロンプト\n        - math [ref=s1e475]: x\n        - text: と、LLMの出力\n        - math [ref=s1e482]: "y"\n        - text: を報酬モデルに与えることで、取得します。 方策は、LLMが出力する各tokenごとの確率分布自体です。 このLLM自体を「方策モデル\n        - math [ref=s1e491]: π ϕ ( y t ∣ x , y < t )\n        - text: 」と呼びます。\n    - complementary [ref=s1e510]:\n        - text: "!"\n        - paragraph [ref=s1e513]:\n            - text: LLM（\n            - math [ref=s1e517]: π ϕ ( y t ∣ x , y < t )\n            - text: ）はユーザが入力したプロンプト\n            - math [ref=s1e539]: x\n            - text: と、それまでにLLMが出力した内容\n            - math [ref=s1e546]: y < t\n            - text: をもとに、\n            - math [ref=s1e557]: t\n            - text: ステップ目のtoken\n            - math [ref=s1e564]: y t\n            - text: を出力します。\n        - paragraph [ref=s1e570]: これが、LLMは、「文章の次に出てくる単語を一つずつ予測するモデル」と言われる所以です。\n    - paragraph [ref=s1e571]: これにより、累積報酬を最大化するように方策モデル（LLM）を学習することで、LLMは好ましい回答を出力するようにネットワークを更新できます。\n    - paragraph [ref=s1e572]: これが、LLMを強化学習で学習するということです。 このような手法をRLHF（Reinforcement\n        learning from human feedback）と総称します。\n    - group [ref=s1e574]: 細かい話\n    - heading "つよつよ手法「PPO（Proximal Policy Optimization）」" [level=3] [ref=s1e576]\n    - paragraph [ref=s1e577]: ここまでで、「LLMと強化学習は相性が良い」こと、「強化学習では累積報酬の期待値（状態価値）を最大化することが重要」であることを話しました。\n    - paragraph [ref=s1e578]: さて、実際にLLMに強化学習を組み込む場合、しばしばPPO（Proximal Policy\n        Optimization）という手法が利用されます。\n        （なんなら、LLM以外で強化学習を実用する上でも、最初に選択肢になるベースライン的に強力な手法でもあります）\n    - paragraph [ref=s1e580]: 今回はPPOが主題ではないので、簡単に説明します。\n    - heading "PPOの簡易的な説明" [level=4] [ref=s1e581]\n    - paragraph [ref=s1e582]:\n        - text: PPOでは、行動を確率的に出力する方策モデル（LLM）と、現在の状態価値を推定する状態価値モデルを用意します。 状態価値\n        - math [ref=s1e587]: V t\n        - text: とは、「累積報酬の期待値」として定義できます。 したがって、現時点から、方策に従って行動をした場合の、将来的な報酬総和の期待値が「状態価値\n        - math [ref=s1e597]: V t\n        - text: 」です。\n    - paragraph [ref=s1e603]:\n        - text: 強化学習の枠組みで累積報酬\n        - math [ref=s1e607]: G t\n        - text: が計算されるため、その期待値である状態価値\n        - math [ref=s1e616]: V t\n        - text: も学習が可能です。 一方で、方策モデルを学習する際に、Advantage\n        - math [ref=s1e626]: A t\n        - text: というものを利用します。\n    - paragraph [ref=s1e632]:\n        - text: Advantage\n        - math [ref=s1e636]: A t\n        - text: というのは下記の式で表されます。\n    - math [ref=s1e648]: A t = G t − V t ( v t ∣ y < t , x )\n    - paragraph [ref=s1e675]:\n        - text: 状態価値\n        - math [ref=s1e679]: V t ( v t ∣ y < t , x )\n        - text: は、累積報酬\n        - math [ref=s1e701]: G t\n        - text: の期待値（大雑把にいうと平均）になるように学習されているという前提があるため、Advantage\n        - math [ref=s1e710]: A t\n        - text: が正になるということは、今回LLMが出力したtoken\n        - math [ref=s1e719]: y t\n        - text: は、将来的に高い報酬を得ることができる「良い出力であった」と判断できます。\n    - paragraph [ref=s1e725]:\n        - text: したがって、方策モデル\n        - math [ref=s1e729]: π ϕ ( y t ∣ x , y < t )\n        - text: はAdvantage\n        - math [ref=s1e751]: A t\n        - text: を最大化するように学習します。\n    - paragraph [ref=s1e757]:\n        - text: Advantage\n        - math [ref=s1e761]: A t\n        - text: を考えずに、単純に累積報酬\n        - math [ref=s1e770]: G t\n        - text: を最大化するように考えることもできます。（むしろそれが初期の強化学習です）\n    - complementary [ref=s1e776]:\n        - text: "!"\n        - paragraph [ref=s1e779]: （2025年2月15日 追記） この辺りの表現は、わかりやすくするために若干誤解を招く表現になっています。\n            下に「細かい話」として補足説明を書きました。\n    - paragraph [ref=s1e782]: しかしその場合、そもそもどんな行動を取っても報酬が高くなる状態と、どんな行動を取っても報酬が低くなってしまう状態において、勾配のスケールがばらつくため、学習の安定性が低くなります。\n    - paragraph [ref=s1e783]:\n        - text: PPOでは、Advantage\n        - math [ref=s1e787]: A t\n        - text: を考えることにより、出力したtokenが平均よりも良いか悪いかだけで学習をすることができ、状態ごとの報酬の差をモデルが考慮する必要がなくなります。\n    - complementary [ref=s1e793]:\n        - text: "!"\n        - paragraph [ref=s1e796]: 本来、PPOの貢献は、あるパラメータ更新時において、更新前と更新後の確率分布が大きく変わらないような制約を、少ない計算量で加えることです。\n            ただ、この話は、DeepSeek-R1-Zeroにおける強化学習を理解するためには、必ずしも重要ではありません。\n            したがって、今回の記事では、必要な部分だけに絞り説明をしました。\n    - group [ref=s1e799]: 細かい話（追記）\n    - heading "著者発案「GRPO（Group Relative Policy Optimization）」" [level=3]\n      [ref=s1e801]\n    - paragraph [ref=s1e802]: さて、PPOは強化学習の手法において、高い性能を発揮しますが、LLMに適用することを考えると、状態価値モデルを別途学習しないといけないという点において、計算量的に大変な手法です。\n    - paragraph [ref=s1e803]: "一方で、PPOの状態価値モデルがやっていたことは、Advantage A_t(a_t|y_t,\n        y_{<t},x) を計算し、今回出力したtokenが、平均よりも良いのか悪いのかを判断することです。"\n    - paragraph [ref=s1e806]: "そこで、LLMにおいては、わざわざ状態価値モデルを用意して平均を計算しなくても、実際に複数文章を生成して、そ\\\n        れぞれの報酬を平均することで、Advantage A_t(a_t|y_t, y_{<t},x) を計算しようと考えました。\n        こうすることで、状態価値モデルなしで、Advantage A_t(a_t|y_t, y_{<t},x)\n        を計算できるため、計算量を大幅に削減できることができます。"\n    - paragraph [ref=s1e812]:\n        - text: この手法は「GRPO（Group Relative Policy Optimization）」と呼びます。\n            本論文の著者が、過去に公開した論文で提案した手法です。\n        - iframe [ref=s1e815]: <could not take iframe snapshot>\n    - paragraph [ref=s1e816]:\n        - text: 下記が、論文中で提示されている「PPO」と「GRPO」の違いを説明する図です。\n        - img [ref=s1e818]\n        - text: "このように状態価値モデル（Value model）なしに、Advantage A_t(a_t|y_t, y_{<t},x)\n            が計算できていることがわかります。"\n    - complementary [ref=s1e821]:\n        - text: "!"\n        - paragraph [ref=s1e824]: 本記事でのアルファベットの付け方と、上記の図のアルファベットの付け方が異なることに注意してください。\n        - paragraph [ref=s1e825]: 上記の図（GRPO）では、 プロンプトを q 、LLMの各出力文章を o_i （ i\n            はそれぞれの文章単位でのインデックス）、各文章ごとの報酬モデルの出力を r_i 、報酬 r_i\n            の平均から計算された各文章ごとのAdvantageを A_i としています。\n        - paragraph [ref=s1e839]: 本記事での添え字は、token単位での添え字を採用しています。 例えば A_t なら t token目でのAdvantageです。\n        - paragraph [ref=s1e845]: 上記の図では、文章単位での添え字です。 例えば、 A_i なら、 i 文章目でのAdvantageとなります。\n            従って A_i はベクトルとなっており、中にtokenごとのAdvantageの値も格納されているはずです。\n    - heading "報酬モデルを作らずルールベースで実施" [level=3] [ref=s1e854]\n    - paragraph [ref=s1e855]:\n        - text: 改めて、LLMに強化学習を適用するために必要なものをまとめます。 （下記の図のGRPOの手法を利用します）\n        - img [ref=s1e858]\n    - paragraph [ref=s1e859]: "まず、大前提として方策モデル \\\\pi_\\\\phi(y_t|x,y_{<t}) （LLM）は必要です。"\n    - paragraph [ref=s1e862]: "上の図中における「Reference Model」とは、「強化学習をする前」の方策モデル\n        \\\\pi_{ref}(y_t|x,y_{<t}) （つまり、DeepSeek-V3-Base）です。 また、「Policy\n        Model」というのは、「強化学習中」の現時点での方策モデル \\\\pi_\\\\phi(y_t|x,y_{<t}) を指しています。"\n    - paragraph [ref=s1e868]: そして、「プロンプト」「良い回答」「悪い回答」の3つを組とするデータセットで学習された、報酬モデル\n        r_\\theta(r|y,x) （Reward Model）が必要です。 報酬モデル r_\\theta(r|y,x) は「プロンプト x\n        」と「LLMの出力回答 y 」を入力することで、「報酬 r 」を出力します。\n    - paragraph [ref=s1e880]: この二つがあれば、著者が提案したGRPOという手法で、強化学習が可能になります。 （GRPOでは、状態価値モデルは不要です）\n    - paragraph [ref=s1e882]:\n        - text: しかしながら著者らは、さらに驚くべきことに、「DeepSeek-R1-Zero」を学習する際に、\n        - strong [ref=s1e884]: 報酬モデルを使わずに、ルールベースで報酬を計算しています。\n        - text: それにより、圧倒的な計算効率を達成しています。\n    - paragraph [ref=s1e886]: 著者らが設定しているルールは下記の二つです。\n    - list [ref=s1e887]:\n        - listitem [ref=s1e888]:\n            - strong [ref=s1e889]: 精度報酬\n            - list [ref=s1e890]:\n                - listitem [ref=s1e891]: 応答が正しいかどうかを評価する\n                - listitem [ref=s1e892]: 例えば、数学の問題であれば、最終的な回答が答えと合っているかどうかで報酬を渡す。\n        - listitem [ref=s1e893]:\n            - strong [ref=s1e894]: フォーマット報酬\n            - list [ref=s1e895]:\n                - listitem [ref=s1e896]:\n                    - text: 思考プロセスを\n                    - code [ref=s1e897]: <think>\n                    - text: と\n                    - code [ref=s1e898]: </think>\n                    - text: タグの間に置くことをモデルに強制し、合っていない場合はペナルティを課す。\n    - paragraph [ref=s1e899]:\n        - text: このたった2つの報酬のみをルールベースで設定しており、\n        - strong [ref=s1e901]: ニューラルネットワークベースの報酬モデルは利用していません\n    - paragraph [ref=s1e902]: ニューラルネットワークの報酬モデルを利用せずに、ルールベース報酬だけで強化学習をすることにより、報酬モデル分の計算量を削減しています。\n    - heading "この報酬設計で良い理由" [level=4] [ref=s1e903]\n    - paragraph [ref=s1e904]: フォーマット報酬は、モデルに出力形式を強制するものであるので、思考能力を向上させる直接的な報酬ではありません。\n        であれば、「DeepSeek-R1-Zero」の圧倒的な思考能力を支えているのは、「精度報酬」になります。\n    - paragraph [ref=s1e906]: 本論文での、強化学習の段階で利用しているデータセットは、おそらく数学データセットです。\n    - paragraph [ref=s1e907]:\n        - text: 数学の問題は、基本的に回答が一意に定まるため、ルールベースであっても出力が正しいかどうかは判断できます。\n        - strong [ref=s1e909]: そして、数学の問題に正答するためには、思考能力を強化する必要があります。\n    - complementary [ref=s1e910]:\n        - text: "!"\n        - paragraph [ref=s1e913]: ここが本論文の非常に面白い部分です。\n            DeepSeek-R1-Zeroを学習する際に、より長い長考をさせるようなデータセットによる学習や、思考部分に対して報酬を与えるような仕組みは「全く無く」、ただ、数学の問題の回答が合っているか間違っているかのゴール報酬のみが与えられています。\n        - paragraph [ref=s1e915]: それだけにも関わらず、自動的に数学の問題を正答するために、思考能力を強化するように学習が進んでおり、強化学習の威力を感じます。\n    - paragraph [ref=s1e916]: 強化学習の枠組みにおいて、最終的に報酬を得る（問題に正解する）ために、今、何のtokenを出力しないといけないのか、が最適化されます。\n        結果として、問題の正当に近づくようなtokenを出力する能力、すなわち問題をステップバイステップで解く能力が磨かれることになります。\n    - paragraph [ref=s1e918]: このように、数学という問題の答えを合わせるように報酬設計をすることで、勝手に思考能力が強化される、というのは非常に面白いなと思います。\n    - heading "（補足）データセットの作り方（の考察）" [level=4] [ref=s1e919]\n    - paragraph [ref=s1e920]:\n        - text: 本論文での、強化学習に利用するデータセットは、おそらく\n        - link "上述した論文" [ref=s1e921]:\n            - /url: https://arxiv.org/html/2402.03300v3\n        - text: の前半に記載されている数学データセットです。\n    - complementary [ref=s1e922]:\n        - text: "!"\n        - paragraph [ref=s1e925]: 強化学習に利用したデータセットに関しては、本論文で言及されていなかったと思うので憶測です。間違っていたら教えてください。\n    - paragraph [ref=s1e926]: 本論文の主要部分ではないですが、興味深いため紹介します。\n    - paragraph [ref=s1e927]:\n        - img [ref=s1e928]\n        - text: 引用：\n        - \'link "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models" [ref=s1e929]\':\n            - /url: https://arxiv.org/html/2402.03300v3\n    - paragraph [ref=s1e930]:\n        - strong [ref=s1e931]: 1. Train a FastText Model\n        - text: まず初めに、「OpenWebMath」という高品質の数学のウェブテキストのデータセットを初期のシードコーパスとして利用します。\n            このコーパスから50万のデータを正例、 Common\n            Crawlデータセット（大規模なWebページコーパス）から取得した50万のデータを負例として、fastTextモデルを学習します。\n            こうすることにより、fastTextモデルを利用して、Webページから、それが数学コンテンツか、非数学コンテンツかを分類することができます。\n    - paragraph [ref=s1e935]:\n        - strong [ref=s1e936]: 2. Recall Math-Related Webpages From Common Crawl\n    - paragraph [ref=s1e937]: 学習した、fastTextモデルを利用して、Common\n        Crawlデータセットから数学のWebページを取得します。 このとき、低品質な数学コンテンツを除外するために、 fastText\n        モデルによって予測されたスコアをもとにランク付けした、上位のスコアのページのみを取得します。\n    - paragraph [ref=s1e939]:\n        - strong [ref=s1e940]: 3. Discover Math-Related Domains\n    - paragraph [ref=s1e941]: この方法で取得されたWebページは、数学の問題ではありますが、偏りがあります。\n        なぜなら、初期の学習に利用された「OpenWebMath」データセットには、少数のデータしかなく、さらに上位のスコアのWebコーパス（類似度の高いWebコーパス）しか収集していないため、「OpenWebMath」データセットと似たような数学Webページしか、収集できていないからです。\n    - paragraph [ref=s1e943]: そこで、上記の方法で収集したWebページの親ドメインに着目します。\n        そして、親ドメイン内のWebページのうち、10%以上のWebページが、今回の方法で収集されている場合、親ドメイン自体が数学に関連するドメインであると、考えることができます。\n    - complementary [ref=s1e945]:\n        - text: "!"\n        - paragraph [ref=s1e948]: 例えば、「mathhoge.com」の配下のページの10%以上がfastText\n            モデルによって収集されていた場合は、「mathhoge.com」自体を数学コンテンツであると分類します。\n    - paragraph [ref=s1e949]: あとは、この親ドメイン内の、まだ収集されていないWebページから、高品質な数学Webページを収集することで、幅広い数学のコンテンツを収集できそうです。\n    - paragraph [ref=s1e950]:\n        - strong [ref=s1e951]: 4. Annotate Math-Related URL Path From Labelers\n    - paragraph [ref=s1e952]: 分類されたドメイン内の数学コンテンツに対して、「手動」でURLに注釈をつけ、注釈をつけたもののうち、またシードコーパスに収集されていないWebページを、シードコーパスに追加します。\n    - paragraph [ref=s1e953]: この方法を繰り返すことで、高品質かつ多様な数学データセットを収集することができます。\n        著者らはこのデータ収集を4回繰り返したのちに、3,550万の数学Webページ、合計1200億トークンのコーパスを完成させました。\n    - paragraph [ref=s1e955]: 4回の繰り返しの段階で、データのうち98%が3回目までの繰り返しですでに収集されていたことがわかったため、4回で繰り返し作業を終了したとのことです。\n    - heading "（補足）DPO（Direct Preference Optimization）との違い" [level=3]\n      [ref=s1e956]\n    - paragraph [ref=s1e957]: DPO（Direct Preference Optimization）という手法があります。\n        こちらは、強化学習を利用して、LLMに好ましい回答を学習させるRLHF（Reinforcement learning from human\n        feedback）と同様の効果を、強化学習を使わずに学習させることができる手法です。\n    - paragraph [ref=s1e959]: "強化学習をしないため、報酬モデルも状態価値モデルも必要なく、単純に「プロンプト x 」と「好ましい回答 y^+\n        」「好ましくない回答 y^- 」の3つが組となっているデータセット D を用意すれば、あとは \\\\pi_\\\\phi(y^+|x) 、\n        \\\\pi_\\\\phi(y^-|x) 、 \\\\pi_{ref}(y^+|x) 、 \\\\pi_{ref}(y^+|x)\n        を計算して、損失関数に落とし込むことで、学習が可能になります。"\n    - paragraph [ref=s1e976]: このように、「報酬モデルも状態価値モデルも必要ない」の部分が、本論文で提案している「GRPO（Group\n        Relative Policy Optimization）+ルールベース報酬」の手法の利点と近いです。\n    - paragraph [ref=s1e977]: ただし、DPOでは、上述した通り、「プロンプト x 」と「好ましい回答 y^+ 」「好ましくない回答 y^-\n        」の3つが組となっているデータセットが必要です。 逆に言えば、このデータセットの範囲内でしか学習ができません。\n    - paragraph [ref=s1e985]: 一方で、著者らが提案する「GRPO（Group Relative Policy\n        Optimization）+ルールベース報酬」の手法では、数学などの答えが一意に決まり、ルールベースで正解不正解を判断できるデータセット（つまり、数学データセット）であれば、「プロンプト\n        x 」と「回答 y 」の組だけで、学習が可能です。\n    - paragraph [ref=s1e990]:\n        - strong [ref=s1e991]: したがって、DPOよりもはるかに多くのデータを用いて学習ができます。\n        - text: これが、今回の提案手法の利点になりそうです。\n    - heading "著者らの執念" [level=3] [ref=s1e993]\n    - paragraph [ref=s1e994]: 著者らは、LLMを強化学習に適用する上で、計算量的に障害になる報酬モデルと状態価値モデルを利用しない斬新な手法を提案しました。\n        これにより、圧倒的な計算効率にて、強化学習を適用させました。\n    - paragraph [ref=s1e996]: また、似た効果を持つ手法としてDPOという手法があります。 ただし、DPOでは、利用できるデータセットが制限されるため、大規模な学習は困難です。\n    - paragraph [ref=s1e998]: しかしながら著者らの手法は、ただ数学のように一定の推論の結果、答えが一意に定まるような問題を解かせ続ければ、ルールベース報酬による強化学習で、思考機能を強化させることができてしまっています。\n    - paragraph [ref=s1e999]:\n        - strong [ref=s1e1000]: 絶対に、学習データの量を減らさずに、計算量を徹底的に削減してやろうという著者らの執念を感じるとともに、こういう工夫に積み重ねが、高性能と圧倒的に格安なAPI料金の両立を達成しているのだろうなと感じました。\n    - heading "ZeroのSystemプロンプト" [level=2] [ref=s1e1001]\n    - paragraph [ref=s1e1002]: 続いては、少し話が変わり、「DeepSeek-R1-Zero」に与えているsystemプロンプトについて紹介します。\n        このように、systemプロンプトが公開されているというのは、非常に勉強になりますね。とても嬉しいです\n    - paragraph [ref=s1e1004]: systemプロンプトは下記です。\n    - paragraph [ref=s1e1005]: "A conversation between User and Assistant. The user\n        asks a question, and the Assistant solves it. The assistant first thinks\n        about the reasoning process in the mind and then provides the user with\n        the answer. The reasoning process and answer are enclosed within <think>\n        </think> and <answer> </answer> tags, respectively, i.e., <think>\n        reasoning process here </think> <answer> answer here </answer>. User:\n        prompt. Assistant:"\n    - paragraph [ref=s1e1006]: 訳）\n        ユーザーとアシスタントの会話。ユーザーが質問し、アシスタントがそれを解決する。アシスタントは、まず頭の中で推論プロセスを考え、それからユーザに答えを提供します。推論過程と答えはそれぞれ<think>\n        </think>タグと<answer> </answer>タグで囲まれます。つまり、<think> 推論過程はここ</think>\n        <answer>答えはここ</answer>です。ユーザー：プロンプト アシスタント：\n    - paragraph [ref=s1e1008]: 上記部分の「prompt」の部分は、ユーザが入力したプロンプトの内容に書き換えられます。\n        このように、systemプロンプトにおいて<think> </think>タグを利用して、その中で思考をさせる形になっています。\n    - heading "Zeroの「Aha Moment」" [level=2] [ref=s1e1010]\n    - paragraph [ref=s1e1011]: 「Aha Moment」とは、辞書で調べると「〔問題の答えが突然ひらめいたときなどの〕なるほど！と思う瞬間」とのことです。\n    - paragraph [ref=s1e1012]: 実は、DeepSeek-R1-Zeroの出力において、それが発生したそうです。 数学の問題を解く途中で、下記のような出力が見られました。\n    - paragraph [ref=s1e1014]: "Wait, wait. Wait. That’s an aha moment I can flag\n        here. Let’s reevaluate this step-by-step to identify if the correct sum\n        can be · · · We started with the equation:"\n    - paragraph [ref=s1e1017]: 訳） 待て、待て。待ってくれ。これはハッとさせられる瞬間だ。 正しい和が - - - -\n        になるかどうかを確認するために、ステップ・バイ・ステップで再評価してみよう。\n    - paragraph [ref=s1e1020]: これは恐ろしいことです。\n        モデルは途中まで推論を実施しながら、途中で自身の出力結果を見直し、さらにもう一度ステップバイステップで検討をするという、高度な問題解決能力を獲得しています。\n        （自分の考えを見直して、さらに良い戦略を発見する。これができる人間がどれくらいいるのだろうか）\n    - paragraph [ref=s1e1023]: 何度も記載していますが、DeepSeek-R1-Zeroを学習する際に、問題の解き方を明示的にモデルに教えることや、思考部分を強化させるような報酬は与えていません。\n        ただ、数学の問題に正解することで得られる報酬を与えるだけで、モデルが自律的に高度な問題解決戦略を獲得しています。\n        （もちろん、推論の途中で、検算を行うなどのデータセットで学習もさせていません。もしかしたら、DeepSeek-V3-Baseを学習させるときのSFTデータセットには含まれているかもですが。）\n    - paragraph [ref=s1e1026]: これこそが、本論文で何度も述べている、「強化学習の可能性」です。\n        強化学習は（そのほかの学習手法とは異なり）（人間にとって）予想外で、かつ洗練された結果を導き、人工知能が新たなレベルの知性を手にいれるための核となる技術になりそうです。\n    - heading "DeepSeek-R1-Zeroのまとめ" [level=2] [ref=s1e1028]\n    - paragraph [ref=s1e1029]: DeepSeek-R1-ZeroはDeepSeek-V3-Baseに対して、「GRPO+ルールベース報酬」の強化学習「のみ」を適用し、圧倒的な思考能力を獲得したモデルになります。\n    - paragraph [ref=s1e1030]: DeepSeek-R1-Zeroを学習する際に、問題の解き方を明示的にモデルに教えることや、思考部分を強化させるような報酬は与えていません。\n        ただ、数学の問題に正解することで得られる報酬を与えるだけで、モデルが自律的に高度な問題解決戦略を獲得するという、とても興味深く、恐ろしい結果が示されています。\n    - heading "DeepSeek-R1への道" [level=1] [ref=s1e1032]\n    - paragraph [ref=s1e1033]: さて、ここまで、「DeepSeek-R1-Zero」について、たくさん語ってきましたが、とても魅力的でしたよね？\n        あくまで、「DeepSeek-R1-Zero」は「DeepSeek-V3-Base」に対して、強化学習のみを適用させて、強化学習の可能性を探ったモデルでした。\n    - paragraph [ref=s1e1035]:\n        - text: その結果、こんなにも魅力的なモデル（DeepSeek-R1-Zero）ができてしまったら、、、\n        - strong [ref=s1e1037]: SFTも入れて完全体のモデルを作りたくなりますよね？？？\n    - paragraph [ref=s1e1038]: というモチベーションで「DeepSeek-R1」は作られています。\n    - paragraph [ref=s1e1039]: もし、DeepSeek-R1-Zeroの性能がそんなに高くなかったら、今騒がれているDeepSeek-R1は作ろうという発想にすらならなかったと思います。\n    - heading "DeepSeek-R1-Zeroの課題" [level=2] [ref=s1e1040]\n    - paragraph [ref=s1e1041]: さて、非常に高性能で、示唆に富んだDeepSeek-R1-Zeroでしたが、課題はいくつかあります。 それは下記です。\n    - list [ref=s1e1043]:\n        - listitem [ref=s1e1044]: 学習初期は強化学習の学習が不安定で収束に時間がかかる（コールドスタート問題）\n        - listitem [ref=s1e1045]:\n            - text: 思考能力は高いが、言語が混在（日本語で話していたら急に中国語になるなど）し、人間が理解しにくい\n            - list [ref=s1e1046]:\n                - listitem [ref=s1e1047]: 結果、高度な思考タスクには強いが、（ロールプレイなどの）一般的なタスクには使えないという評価\n    - paragraph [ref=s1e1048]: そこで、上記の課題を解決する旅に出ます。\n    - heading "課題対処への道" [level=2] [ref=s1e1049]\n    - paragraph [ref=s1e1050]: DeepSeek-R1-Zeroでは、ベースモデルであるDeepSeek-V3-Baseに対して、そのまま上述した強化学習を用いて学習をしていました。\n    - paragraph [ref=s1e1051]:\n        - text: しかし、もともとDeepSeek-V3-Baseは長考モデルとして学習されたモデルではないため、いくらsystemプロンプトで長考をさせる制限（\n        - code [ref=s1e1052]: <think>\n        - text: タグ）を入れたとしても、強化学習の初期フェーズは不安定な学習になってしまいます。\n            （例えば、「そもそも適切な形式で思考をしない」、「思考形式は合っているが、意味のある思考をしていない」、その結果として「数学の問題になかなか正答できないまま、報酬が得られず、学習が不安定になる」が考えられます）\n    - paragraph [ref=s1e1054]: そこで、学習初期のコールドスタート問題を防ぐために、DeepSeek-V3-Baseに対して、SFT（Supervised\n        Fine-Tuning）を実施します。 そのために著者らは、少量のCoT（Chain-of-Thought）データセットを構築しています。\n    - heading "CoTデータセットの構築+SFT" [level=3] [ref=s1e1056]\n    - paragraph [ref=s1e1057]: データセットの構築方法は下記が一例として提示されております。\n    - list [ref=s1e1058]:\n        - listitem [ref=s1e1059]: いくつかの長いCoT解答を提示し、モデルに長いCoT形式の回答を生成させてデータセットに追加する\n        - listitem [ref=s1e1060]: モデルに対して、自己反省や検証を行いながら、詳細な回答生成を促すようなプロンプトを送信し、その出力をデータセットに追加する\n        - listitem [ref=s1e1061]: DeepSeek-R1-Zeroの出力を、読みやすい形式に整理してからデータセットに追加する\n        - listitem [ref=s1e1062]: モデルの出力結果を人間が精査して、より適切なCoTデータの形に整理してから、データセットに追加する\n    - paragraph [ref=s1e1063]: 上記の方法で、数千件のCoTデータを構築し、そのデータを用いて、DeepSeek-V3-BaseをSFTしています。\n    - heading "CoTデータセットを用いることの利点" [level=3] [ref=s1e1064]\n    - paragraph [ref=s1e1065]: 著者らは、下記の要素において、CoTデータセットを用いることに利点があると考えています。\n    - paragraph [ref=s1e1066]:\n        - strong [ref=s1e1067]: 読みやすさ\n        - text: DeepSeek-R1-Zeroの一番の課題は、その出力結果が人間が理解するのが難しいことです。\n            回答には、複数の言語が混在していたり、マークダウン形式などの読みやすい形式が適切に使われていないことで、重要な情報が何かわかりにくい状態でした。\n    - paragraph [ref=s1e1070]: そこで、今回のデータセットを作成する際には、各回答の末尾に「要約文」を入れるようにすることで、人間が読みやすい形に出力させるようにしています。\n        具体的には下記のような出力形式で、全てのCoTデータセットは作成されています。\n    - code [ref=s1e1075]: "|special_token|<reasoning_process>|special_token|<summary>"\n    - button "クリップボードにコピー" [ref=s1e1076]:\n        - img [ref=s1e1077]\n    - button "右端で折り返す" [ref=s1e1078]:\n        - img [ref=s1e1079]\n    - paragraph [ref=s1e1080]:\n        - code [ref=s1e1081]: <reasoning_process>\n        - text: の部分に、モデルの思考に該当する部分が、\n        - code [ref=s1e1082]: <summary>\n        - text: の部分に最終的な結論の要約が記載されます。\n    - paragraph [ref=s1e1083]: これにより、「DeepSeek-R1-Zero」にあった、重要な情報が何かわかりにくいという人間が理解するのが難しい問題を緩和します。\n        （言語の混在に関しての対処は後述します）\n    - paragraph [ref=s1e1085]:\n        - strong [ref=s1e1086]: ポテンシャル\n        - text: 今回、強化学習のみを実施するのではなく、人間の事前知識に基づいて作成したCoTデータセットでSFTを実施しています。\n            また、SFTと強化学習を複数回反復して、繰り返し学習をしています。\n            その結果、DeepSeek-R1はDeepSeek-R1-Zeroよりも良い結果を得ることができました。\n    - paragraph [ref=s1e1090]: 著者らは、このような反復的な学習こそが、長考モデル（reasoning models）を学習する上で良い方法であると主張しています。\n    - heading "推論思考の強化学習" [level=3] [ref=s1e1091]\n    - paragraph [ref=s1e1092]: CoTデータセットによりSFTしたDeepSeek-V3-Baseに対して、上述した（DeepSeek-R1-Zeroを学習した）強化学習手法を適用します。\n        これにより、モデルの思考能力が大幅に強化されます。\n    - paragraph [ref=s1e1094]: 一方で、「DeepSeek-R1-Zero」には、言語不一致の問題がありました。\n        そこで、DeepSeek-R1-Zeroのルールベース報酬に、「言語一貫性報酬」を追加で導入しています。\n    - paragraph [ref=s1e1096]: この報酬導入で、実は、モデルの思考能力が若干落ちてしまうことが実験的にわかっています。\n        しかしながら、この報酬があることで、ようやく人間が理解しやすい文章を生成してくれるようになります。\n    - heading "さらなる追加のSFT" [level=3] [ref=s1e1098]\n    - paragraph [ref=s1e1099]: 上記の強化学習が収束したら、さらにデータを追加してSFTをやり直します。\n        ここでは、初期段階の、CoTに重きを置いた、コールドスタートを解消するための学習データだけにとどまらず、ライティング、ロールプレイといった汎用タスクの性能を向上させるため、他ドメインからのデータも合わせて収集します。\n    - heading "Reasoning Data" [level=4] [ref=s1e1101]\n    - paragraph [ref=s1e1102]: まずは、前段階と同様に思考能力を向上させるために、CoT学習データ（Reasoning Data）を取得します。\n    - paragraph [ref=s1e1103]: 取得方法は以下です。\n    - list [ref=s1e1104]:\n        - listitem [ref=s1e1105]:\n            - text: 強化学習済みモデルの出力から、適切なものを収集して学習データに追加\n            - list [ref=s1e1106]:\n                - listitem [ref=s1e1107]: 前段の強化学習では、ルールベースの報酬モデルを利用していたので、それが使える問題（数学など）のみを学習データとしていたが、次の強化学習では報酬モデルを作成するため、もっと広い範囲でCoTができるデータも学習データに追加\n                - listitem [ref=s1e1108]: モデルの出力はまだ読みにくいところがあるため、さまざまフィルタリング\n    - paragraph [ref=s1e1109]: この結果、合計で600kのReasoning Data（学習データ）を収集しています。\n    - heading "Non-Reasoning Data" [level=4] [ref=s1e1110]\n    - paragraph [ref=s1e1111]: ライティングやQ＆A、翻訳などのNon-Reasoning\n        Dataについては、DeepSeek-V3のSFTデータセットの一部を再利用して、学習データを収集しています。 上記のReasoning\n        Data以外のタスク（Non-Reasoning\n        Data）に関しては、DeepSeek-V3によって簡易的なCoTを実施して、回答を生成し、学習データとします。\n        ただし、本当に簡単なタスク（あいさつとか）に関しては、CoTを学習データに追加しません。\n    - paragraph [ref=s1e1114]: こうすることで、合計200kのトレーニングサンプルを最終的に収集したそうです。\n    - paragraph [ref=s1e1115]: 上記の合計800kサンプルの学習データを利用して、DeepSeek-V3-Base（初回強化学習済み）を2epoch分SFTを実施しています。\n    - heading "最後の強化学習" [level=3] [ref=s1e1116]\n    - paragraph [ref=s1e1117]: 最後に、モデルの思考能力をさらに磨くと同時に、モデルの無害性などを向上させることを目的に、強化学習を実施します。\n    - paragraph [ref=s1e1118]: 思考タスクの強化学習に関しては、これまで説明したのと同様に、数学、コード、論理的推論の領域でルールベース報酬を利用して強化学習をします。\n        一方で、思考タスク以外の一般的なデータに対しては、ルールベースで回答の良し悪しを判断することはできないため、報酬モデルを別途DeepSeek-V3パイプラインをベースに作成しています。\n    - heading "DeepSeek-R1のまとめ" [level=2] [ref=s1e1120]\n    - paragraph [ref=s1e1121]: 以上より、DeepSeek-R1では、DeepSeek-V3-Baseに対して、SFTと強化学習を2回実施することにより構築されています。\n    - paragraph [ref=s1e1122]: SFTにより、強化学習のコールドスタート問題の解消と、モデル出力のユーザフレンドリー性の向上、一般的なタスクに対する性能向上に貢献しています。\n    - heading "結果" [level=1] [ref=s1e1123]\n    - paragraph [ref=s1e1124]: 結果に関しては、他の方がさまざま検証していたり、話題になっているので詳しくは書きません。\n        ただ、DeepSeek-R1は、現在のオープンソースモデルの中では、最も高い性能と言ってよく、また、Open\n        AIのo1モデルにも匹敵する性能だと言われています。\n    - heading "蒸留" [level=1] [ref=s1e1126]\n    - paragraph [ref=s1e1127]: 最後に、蒸留モデルについての解説します。（論文でも言及されています）\n        DeepSeek-R1は671Bものモデルパラメータを持つため、一般的な家庭用PCで動作させるのは難しいです。\n    - paragraph [ref=s1e1129]: そこで、より小さなモデルパラメータのモデルに、DeepSeek-R1のような思考能力を与えるために、蒸留を実施しています。\n        （公式が、個人の利用者のことも考えて、家庭用PCで動作する思考モデルを開発してくれるのは非常にありがたいですね）\n    - paragraph [ref=s1e1131]: 具体的には、Qwen2.5-Math-1.5B、Qwen2.5-Math-7B、Qwen2.5-14B、Qwen2.5-32B、Llama-3.1-8B、Llama-3.3-70B-Instructに対して、DeepSeek-R1から取得した800kのトレーニングサンプルを利用して、直接FineTuningする形で、蒸留を実施しています。\n    - paragraph [ref=s1e1132]: 蒸留モデルに対しては、SFTのみを実施し、後段の強化学習は実施しておらず、蒸留モデルに対する追加の強化学習の探究は、著者らが実施する予定はないそうです。\n        （著者らは、強化学習を追加で実施することにより性能が上がると思っているが、その探究は、別のコミュニティに任せると記述しています）\n    - heading "蒸留 vs 強化学習" [level=2] [ref=s1e1134]\n    - paragraph [ref=s1e1135]: 蒸留後の強化学習に関しては、著者らは実験していない、と記述していました。\n        では、小型モデルに対して、蒸留を実施する前に、強化学習を実施したらどうなるのでしょうか？\n        DeepSeek-V3-Baseのように、大幅に思考能力が強化されて、性能向上が期待できるのでしょうか？\n    - paragraph [ref=s1e1138]: それに関しては、著者らが実験してくれています。\n    - paragraph [ref=s1e1139]: ここでは、「Qwen-32B-Base」モデルを利用して、10kステップ以上の大規模強化学習を実施し、「DeepSeek-R1-Zero-Qwen-32B」を構築しました。\n        一方で、「Qwen-32B-Base」モデルに対して、蒸留を実施したモデルを、「DeepSeek-R1Distill-Qwen-32B」モデルとして、この二つのモデルを比較します。\n    - paragraph [ref=s1e1141]: その結果を比較すると、小型モデルに対する強化学習を適用した「DeepSeek-R1-Zero-Qwen-32B」よりも、小型モデルに蒸留を適用した「DeepSeek-R1Distill-Qwen-32B」モデルの方がはるかに高い性能でした。\n    - paragraph [ref=s1e1142]:\n        - strong [ref=s1e1143]: 以下は私の主観ですが、\n        - text: 以上の結果から、\n        - strong [ref=s1e1146]: 強化学習による思考能力の向上は、安易に小型モデルに試していい戦略でない\n        - text: ということが示唆されていると思います。\n    - paragraph [ref=s1e1148]: 強化学習による思考能力の大幅な強化は、おそらくですが「DeepSeek-V3」という元々大規模かつ性能の高いモデルであったからこそ、達成されたのだと思います。\n    - paragraph [ref=s1e1149]: DeepSeek-R1-Zeroの課題でも説明した通り、今回のルールベース報酬では、正しい数学の解答を生成できないと永遠に報酬が獲得できません。\n        小型モデルでは、このコールドスタートの問題が、より致命的なレベルで発生するのだと思います。\n    - paragraph [ref=s1e1151]: したがって、思考モデルを作りたいからといって、小型モデルに対して、安易にルールベース強化学習を試すのはおすすめしません。\n        しかしながら、コールドスタート問題を解決できる戦略があるのであれば、有効な手段になると思われます。\n        （例えば、DeepSeek-R1で蒸留済みの小型モデルを利用する、最初から数学に強い小型モデルで強化学習を試すなどです。重要なのは強化学習のコールドスタートを解消できるかどうかだと思われます。）\n    - paragraph [ref=s1e1154]:\n        - text: （追記） また、同様の実験を小型モデルに対して試したグループもいるようです。 そちらの実験レポートが下記になります。\n        - iframe [ref=s1e1158]: <could not take iframe snapshot>\n        - text: （DeepSeek-R1-Zeroと同様に、「Aha Moment」が見られることもわかっており、非常に興味深いです）\n    - paragraph [ref=s1e1159]: ただ、こちらのレポートにおいても、ベースモデルとしては数学に特化したモデルを利用しています。\n        このことからも、ある程度の数学能力は必要なのだなと思います。\n    - heading "今後の展望" [level=1] [ref=s1e1161]\n    - paragraph [ref=s1e1162]: DeepSeek-R1は、さまざまなタスクにおいて、OpenAI\n        o1に匹敵する性能を達成した、驚異的なオープンソースモデルですが、苦手としている部分もあります。 それが今後の展望に記載されています。\n        （今後の展望を見れば、DeepSeek-R1が苦手な部分を知ることができます）\n    - paragraph [ref=s1e1165]: それは以下の項目です。\n    - list [ref=s1e1166]:\n        - listitem [ref=s1e1167]:\n            - strong [ref=s1e1168]: 一般的な能力\n            - list [ref=s1e1169]:\n                - listitem [ref=s1e1170]: 関数呼び出しやマルチターン対話、複雑なロールプレイング。、json出力において、DeepSeek-V3に及ばない\n        - listitem [ref=s1e1171]:\n            - strong [ref=s1e1172]: 言語の混在\n            - list [ref=s1e1173]:\n                - listitem [ref=s1e1174]: 現在、DeepSeek-R1は英語と中国語に最適化されているため、そのほかの言語を利用した場合、言語の混在が発生する可能性がある\n        - listitem [ref=s1e1175]:\n            - strong [ref=s1e1176]: プロンプトエンジニアリング\n            - list [ref=s1e1177]:\n                - listitem [ref=s1e1178]: DeepSeek-R1はプロンプトに非常に敏感で、特にFewーshotプロンプトを利用すると、パフォーマンスが低下する。\n                - listitem [ref=s1e1179]: 基本的には、問題を直接シンプルに記述し、例を提示しない（Zero-shotプロンプト）を利用することを推奨している\n        - listitem [ref=s1e1180]:\n            - strong [ref=s1e1181]: ソフトウェアエンジニアリングタスク\n            - list [ref=s1e1182]:\n                - listitem [ref=s1e1183]: コード生成やコード保管などのソフトウェアエンジニアリングタスクは、評価に時間がかかるため、強化学習の効率が悪くなることから、現時点では強化学習において、このようなタスクは学習していない。\n                    大々的には学習しておらず、一部のテスト可能な範囲でしか学習できていない。\n                - listitem [ref=s1e1185]: 結果として、このタスクの性能は、DeepSeek-V3からほとんど改善していない。\n    - complementary [ref=s1e1186]:\n        - text: "!"\n        - paragraph [ref=s1e1189]: （追記） masaishiさんのコメントに基づき修正させていただきました。 私の読み間違いで誤解を与えてしまい申し訳ございません。\n        - blockquote [ref=s1e1192]:\n            - paragraph [ref=s1e1193]: 2.2.2. Reward Modeling ・・・ ・Accuracy\n                rewards:・・・Similarly, for LeetCode problems, a compiler can be\n                used to generate feedback based on predefined test cases.\n        - paragraph [ref=s1e1196]: と記載のあるように、全くコード生成などを学習していないわけではなく、一部の、簡単にテストができる問題のみ学習しているようです。\n            ただ、そういう簡易的なタスクだけしか学習できていないため、性能向上は小さめということだと思われます。\n    - paragraph [ref=s1e1198]: 上記の内容が、現時点での「DeepSeek-R1」の課題です。\n        ですが、今後のアップデートで改善していくとも記載されているため、現時点で不満があったとしても、上記に記載されている項目であれば、今後改善する可能性が高いです。\n    - heading "まとめ" [level=1] [ref=s1e1200]\n    - paragraph [ref=s1e1201]: ここまで読んでくださった方、本当にお疲れ様でした。\n        （これでも説明はだいぶ短くした方なんです・・・本当は、もっと強化学習の部分とかを数式をもとに説明したかったです）\n    - paragraph [ref=s1e1203]: DeepSeek-R1の論文いかがでしたか？\n        私にとっては、非常に勉強になるような示唆や知見に溢れており、こんな論文を公開してくれて、かつオープンソースのモデルとしても公開してくれる「DeepSeek-AI」には感謝しかありません。\n    - paragraph [ref=s1e1205]: 本記事が、皆様の理解の助けになり、一人でも多くの方が原論文に興味を持って、読んでくれましたら幸いです。\n        ここまで読んでくださってありがとうございました！\n    - heading "参考文献" [level=1] [ref=s1e1207]\n    - paragraph [ref=s1e1208]: 本論文は、論文の中では読みやすい方だとは思いますが、強化学習に関しては、一定程度の基礎知識は要求されます。\n        そこで、本論文を読む上で必要な知識を収集できるおすすめの書籍に関して記載します。 （少なくとも私はこの書籍で学習しました）\n    - paragraph [ref=s1e1211]: （書籍のリンクはamazonアフィリエイトリンクになります）\n    - heading "強化学習" [level=4] [ref=s1e1212]\n    - paragraph [ref=s1e1213]: 強化学習に関しておすすめの本は、下記の本たちです。\n    - paragraph [ref=s1e1214]:\n        - link "ゼロから作るDeep Learning ❹ ―強化学習編" [ref=s1e1215]:\n            - /url: https://amzn.to/40qFuNg\n        - text: まずは、定番の本です。\n            強化学習に関して、本当に最初の部分（マルコフ決定過程とは何か）から、非常に詳細かつわかりやすく解説されており、深層学習を利用して、どのように強化学習をすることできるのかまで、しっかり書かれているためおすすめです。\n            また、コードと理論のバランスもちょうどよく、理系大学生とかであれば問題なく読み進められると思います。\n            特に、強化学習の基礎の部分が、詳細かつ分かりやすく書かれているため、一冊目におすすめです。\n    - paragraph [ref=s1e1220]:\n        - link "機械学習スタートアップシリーズ Ｐｙｔｈｏｎで学ぶ強化学習 ［改訂第２版］ 入門から実践まで (ＫＳ情報科学専門書)" [ref=s1e1221]:\n            - /url: https://amzn.to/4joKbjg\n    - paragraph [ref=s1e1223]: 普段からpythonを使っており、数式よりも実装に興味がある方にとっては、こちらの書籍もおすすめです。\n        こちらの書籍は、強化学習の基礎よりは、少し応用というか、実用を意識した書籍になっており、実務でよく使われるモデルなどに関しても、詳しく説明されています。\n        強化学習の基礎はわかっている人や、もっと実装をベースに学びたいというかたにおすすめです。\n    - paragraph [ref=s1e1226]:\n        - link "強化学習 (機械学習プロフェッショナルシリーズ)" [ref=s1e1227]:\n            - /url: https://amzn.to/40NX5QM\n        - link "強化学習から信頼できる意思決定へ (AI/データサイエンスライブラリ“基礎から応用へ” 5)" [ref=s1e1229]:\n            - /url: https://amzn.to/4jtFq8d\n    - paragraph [ref=s1e1230]: 名著です。\n        どちらも、理論説明の部分が非常に面白く、正直難易度は高いですが、強化学習の理論をちゃんと理解したいという方におすすめの書籍です。\n        下の本に関しては、2024年の本ということもあり、近年の強化学習の実世界での応用と、理論的な定式化の架け橋になっているような本になっており、非常に面白いです。\n    - heading "AlphaGO Zero" [level=4] [ref=s1e1233]\n    - paragraph [ref=s1e1234]:\n        - link "AlphaZero 深層学習・強化学習・探索 人工知能プログラミング実践入門" [ref=s1e1235]:\n            - /url: https://amzn.to/3Cdnuy1\n    - paragraph [ref=s1e1236]: だいぶ昔に購入させていただいた本ですが、まさか2025年に活躍することがあろうとは。\n        という感じですが、こちらの書籍も参考にさせていただきました。 Alpha\n        Zeroを作る上で核となる技術を余すところなく解説されており、私は非常に勉強になりました。 Alpha\n        Zeroに興味がある方は、ぜひ一度ご一読ください。\n    - heading "LLM、RLHF, DPO" [level=4] [ref=s1e1240]\n    - paragraph [ref=s1e1241]:\n        - link "大規模言語モデル入門Ⅱ〜生成型LLMの実装と評価" [ref=s1e1242]:\n            - /url: https://amzn.to/3Cs3Uhn\n        - text: よく紹介させていただいておりますが、こちらの書籍は、LLMのファインチューニングから、RLHF、RAG、分散学習にかけて、本当に幅広く解説されており、いつも参考にさせていただいております。\n            今回のDPOやRLHFの解説部分でも、とても活躍してくれました。\n            LLMを取り扱っている方は、とりあえず買っておいても損はないと思います。\n            さまざまな章、ページで読めば読むほど新しい発見が得られる、スルメのような本だなといつも思っております。\n    - button "いいね" [ref=s1e1249]:\n        - img [ref=s1e1250]\n    - text: "921"\n    - button "ブックマーク" [ref=s1e1266]:\n        - img [ref=s1e1267]\n    - text: "344"\n    - button "その他の操作" [ref=s1e1272]:\n        - img [ref=s1e1273]\n    - tooltip "X(Twitter)にポスト" [ref=s1e1276]:\n        - img [ref=s1e1277]\n        - text: X(Twitter)にポスト\n    - tooltip "Facebookに投稿" [ref=s1e1283]:\n        - img [ref=s1e1284]\n        - text: Facebookに投稿\n    - tooltip "はてなブックマークに登録" [ref=s1e1290]:\n        - img [ref=s1e1291]\n        - text: はてなブックマークに登録\n    - complementary [ref=s1e1294]:\n        - link "asap" [ref=s1e1296]:\n            - /url: /asap\n            - img "asap" [ref=s1e1297]\n        - link "asap" [ref=s1e1299]:\n            - /url: /asap\n        - paragraph [ref=s1e1301]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n        - button "フォロー" [ref=s1e1305]\n        - tooltip "@personabb" [ref=s1e1306]:\n            - img "GitHub" [ref=s1e1307]\n            - text: "@personabb"\n        - tooltip "@asap2650" [ref=s1e1309]:\n            - img "X(Twitter)" [ref=s1e1310]\n            - text: "@asap2650"\n        - tooltip "yard.tips" [ref=s1e1316]:\n            - img "リンク" [ref=s1e1317]\n            - text: yard.tips\n    - complementary [ref=s1e1320]:\n        - text: バッジを贈って著者を応援しよう\n        - paragraph [ref=s1e1324]: バッジを受け取った著者にはZennから現金やAmazonギフトカードが還元されます。\n        - button "バッジを贈る" [ref=s1e1326]\n        - img [ref=s1e1327]\n    - text: この記事に贈られたバッジ\n    - img "Thank you" [ref=s1e1338]\n    - img [ref=s1e1340]\n    - img "参考になった" [ref=s1e1352]\n    - img [ref=s1e1354]\n    - img "もっと読みたい" [ref=s1e1363]\n    - img [ref=s1e1365]\n    - img "参考になった" [ref=s1e1375]\n    - img [ref=s1e1377]\n    - img "もっと読みたい" [ref=s1e1386]\n    - img [ref=s1e1388]\n    - heading "Discussion" [level=3] [ref=s1e1400]\n    - article [ref=s1e1403]:\n        - link "はまなすなぎさ" [ref=s1e1405]:\n            - /url: /nhamanasu\n            - img "はまなすなぎさ" [ref=s1e1406]\n        - link "はまなすなぎさ" [ref=s1e1407]:\n            - /url: /nhamanasu\n        - link "2025年1月28日 8時33分" [ref=s1e1408]:\n            - /url: "#comment-f230c9d295b3f5"\n            - tooltip "2025年1月28日 8時33分" [ref=s1e1409]: 2ヶ月前 2025年1月28日 8時33分\n        - button "メニューを開く" [ref=s1e1411]:\n            - img [ref=s1e1412]\n        - paragraph [ref=s1e1417]: 非常にためになる骨太な記事、ありがとうございます！とても勉強になります。\n        - paragraph [ref=s1e1418]: ちょっとした誤字かも？の箇所なのですが、冒頭の「強化学習手法「DRPO」すごい！」の部分は「GRPO」ではないかなと思いました。ご報告まで。\n        - button "いいね" [ref=s1e1421]:\n            - img [ref=s1e1422]\n        - text: "1"\n        - button "コメント 1" [ref=s1e1437]:\n            - img "コメント" [ref=s1e1438]\n            - text: "1"\n        - article [ref=s1e1442]:\n            - link "asap" [ref=s1e1444]:\n                - /url: /asap\n                - img "asap" [ref=s1e1445]\n            - link "asap" [ref=s1e1446]:\n                - /url: /asap\n            - link "2025年1月28日 10時23分" [ref=s1e1447]:\n                - /url: "#comment-41cdebd92d8d44"\n                - tooltip "2025年1月28日 10時23分" [ref=s1e1448]: 2ヶ月前 2025年1月28日 10時23分\n            - button "メニューを開く" [ref=s1e1450]:\n                - img [ref=s1e1451]\n            - paragraph [ref=s1e1456]: 読んでくださりありがとうございます！ おっしゃる通りです！修正させていただきました！\n            - button "いいね" [ref=s1e1460]:\n                - img [ref=s1e1461]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1477]\n    - article [ref=s1e1479]:\n        - link "bang" [ref=s1e1481]:\n            - /url: /zenmnm\n            - img "bang" [ref=s1e1482]\n        - link "bang" [ref=s1e1483]:\n            - /url: /zenmnm\n        - link "2025年1月28日 13時46分" [ref=s1e1484]:\n            - /url: "#comment-d0aac9375fbbf7"\n            - tooltip "2025年1月28日 13時46分" [ref=s1e1485]: 2ヶ月前 2025年1月28日 13時46分\n        - button "メニューを開く" [ref=s1e1487]:\n            - img [ref=s1e1488]\n        - paragraph [ref=s1e1493]: AIについてはまだ詳しくありませんが、興味はあります。同じように古いモデルを使って新しいモデルを作るのですが、蒸留方法の代わりにモデルを組み合わせることは可能でしょうか?sakana\n            aiのようにです\n        - button "いいね" [ref=s1e1496]:\n            - img [ref=s1e1497]\n        - text: "1"\n        - button "コメント 2" [ref=s1e1512]:\n            - img "コメント" [ref=s1e1513]\n            - text: "2"\n        - article [ref=s1e1517]:\n            - link "asap" [ref=s1e1519]:\n                - /url: /asap\n                - img "asap" [ref=s1e1520]\n            - link "asap" [ref=s1e1521]:\n                - /url: /asap\n            - link "2025年1月28日 15時33分" [ref=s1e1522]:\n                - /url: "#comment-27e79c6ee51f2a"\n                - tooltip "2025年1月28日 15時33分" [ref=s1e1523]: 2ヶ月前 2025年1月28日 15時33分\n            - button "メニューを開く" [ref=s1e1525]:\n                - img [ref=s1e1526]\n            - paragraph [ref=s1e1531]: モデルマージに関してはあまり詳しくないですが、基本的には同じアーキテクチャのモデルを利用することになると思います。したがってDeepSeek-R1によって蒸留されたLlamaモデルとかであれば、他のLlamaモデル（同じモデルアーキテクチャ）とモデルマージすることで、推論機能を付け加えたり、日本語機能を強化したりすることが期待できると思います。\n            - button "いいね" [ref=s1e1534]:\n                - img [ref=s1e1535]\n        - article [ref=s1e1549]:\n            - link "bang" [ref=s1e1551]:\n                - /url: /zenmnm\n                - img "bang" [ref=s1e1552]\n            - link "bang" [ref=s1e1553]:\n                - /url: /zenmnm\n            - link "2025年1月28日 16時42分" [ref=s1e1554]:\n                - /url: "#comment-0221309240798f"\n                - tooltip "2025年1月28日 16時42分" [ref=s1e1555]: 2ヶ月前 2025年1月28日 16時42分\n            - button "メニューを開く" [ref=s1e1557]:\n                - img [ref=s1e1558]\n            - paragraph [ref=s1e1563]: お返事ありがとうございます。私はまだ初心者なので、これは私の軽い考えです。今では研究のプロセスごとにいくつかの方法があり、それらを組み合わせたり微調整したりすれば、たくさんのモデルができるようになっているでしょう。\n            - button "いいね" [ref=s1e1566]:\n                - img [ref=s1e1567]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1583]\n    - article [ref=s1e1585]:\n        - link "HWSeungwoo" [ref=s1e1587]:\n            - /url: /syoyuu96\n            - img "HWSeungwoo" [ref=s1e1588]\n        - link "HWSeungwoo" [ref=s1e1589]:\n            - /url: /syoyuu96\n        - link "2025年1月28日 19時28分" [ref=s1e1590]:\n            - /url: "#comment-a07230ddbdaebe"\n            - tooltip "2025年1月28日 19時28分" [ref=s1e1591]: 2ヶ月前 2025年1月28日 19時28分\n        - button "メニューを開く" [ref=s1e1593]:\n            - img [ref=s1e1594]\n        - paragraph [ref=s1e1599]: Deep\n            Seekに関する記事やニュースはたくさん目にしますが、ここまで深掘りされた内容には驚きました。とても勉強になりました。ありがとうございます！\n        - button "いいね" [ref=s1e1602]:\n            - img [ref=s1e1603]\n        - text: "1"\n        - button "コメント 1" [ref=s1e1618]:\n            - img "コメント" [ref=s1e1619]\n            - text: "1"\n        - article [ref=s1e1623]:\n            - link "asap" [ref=s1e1625]:\n                - /url: /asap\n                - img "asap" [ref=s1e1626]\n            - link "asap" [ref=s1e1627]:\n                - /url: /asap\n            - link "2025年1月28日 19時32分" [ref=s1e1628]:\n                - /url: "#comment-1e1e3abc86df00"\n                - tooltip "2025年1月28日 19時32分" [ref=s1e1629]: 2ヶ月前 2025年1月28日 19時32分\n            - button "メニューを開く" [ref=s1e1631]:\n                - img [ref=s1e1632]\n            - paragraph [ref=s1e1637]: 読んでくださってありがとうございます！ コメント嬉しいです！ありがとうございます！\n            - button "いいね" [ref=s1e1641]:\n                - img [ref=s1e1642]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1658]\n    - article [ref=s1e1660]:\n        - link "masaishi" [ref=s1e1662]:\n            - /url: /masaishi\n            - img "masaishi" [ref=s1e1663]\n        - link "masaishi" [ref=s1e1664]:\n            - /url: /masaishi\n        - link "2025年1月29日 16時14分" [ref=s1e1665]:\n            - /url: "#comment-a0d75bd96cadfc"\n            - tooltip "2025年1月29日 16時14分" [ref=s1e1666]: 2ヶ月前 2025年1月29日 16時14分\n        - button "メニューを開く" [ref=s1e1668]:\n            - img [ref=s1e1669]\n        - paragraph [ref=s1e1674]: 素晴らしい記事をありがとうございます！\n            GRPOなどの強化学習手法に関する解説も大変勉強になりました。この記事をきっかけに、DeepSeek-R1の論文にも興味を持ち、読んでみようと思えました。\n        - paragraph [ref=s1e1676]: 一点だけ気になったことがあり、もし私の理解が誤っていたら申し訳ありません。\n            "コード生成やコード保管などのソフトウェアエンジニアリングタスク"の強化学習に用いられていないとありますが、論文の\n            2.2.2（Reward Modeling）の\n        - blockquote [ref=s1e1678]:\n            - paragraph [ref=s1e1679]: For LeetCode problems, a compiler can be used to\n                generate feedback based on predefined test cases.\n        - paragraph [ref=s1e1680]: という記述を見ると、少なくともアルゴリズム問題のコード生成には強化学習が用いられているのかなと思います。\n        - paragraph [ref=s1e1681]: ソフトウェアエンジニアリングタスクは強化学習していませんが、コード生成自体は行っているような気がするので、注釈などで少し補足していただけると、読者としては誤解が減って助かるかもしれません。しかし、かなり細かい点ですので、最終的には著者さまの判断にお任せいたします。\n        - button "いいね" [ref=s1e1684]:\n            - img [ref=s1e1685]\n        - text: "2"\n        - button "コメント 2" [ref=s1e1700]:\n            - img "コメント" [ref=s1e1701]\n            - text: "2"\n        - article [ref=s1e1705]:\n            - link "asap" [ref=s1e1707]:\n                - /url: /asap\n                - img "asap" [ref=s1e1708]\n            - link "asap" [ref=s1e1709]:\n                - /url: /asap\n            - link "2025年1月29日 16時37分" [ref=s1e1710]:\n                - /url: "#comment-afb254c15106b7"\n                - tooltip "2025年1月29日 16時37分" [ref=s1e1711]: 2ヶ月前 2025年1月29日 16時37分\n            - button "メニューを開く" [ref=s1e1713]:\n                - img [ref=s1e1714]\n            - paragraph [ref=s1e1719]: コメントありがとうございます！ おっしゃる通りでした！読み間違いです。ご指摘助かります。\n            - blockquote [ref=s1e1721]:\n                - paragraph [ref=s1e1722]: Due to the long evaluation times, which impact the\n                    efficiency of the RL process, large-scale RL has not been\n                    applied extensively in software engineering tasks.\n            - paragraph [ref=s1e1723]: この部分の文章をもとに記載していましたが、大々的には適用されていないだけで、一部は学習に入っているっぽいですね。\n            - paragraph [ref=s1e1724]: 助かります！修正もこれからさせていただきます\n            - button "いいね" [ref=s1e1727]:\n                - img [ref=s1e1728]\n            - text: "2"\n        - article [ref=s1e1743]:\n            - link "masaishi" [ref=s1e1745]:\n                - /url: /masaishi\n                - img "masaishi" [ref=s1e1746]\n            - link "masaishi" [ref=s1e1747]:\n                - /url: /masaishi\n            - link "2025年1月29日 19時5分" [ref=s1e1748]:\n                - /url: "#comment-debff7de1aad74"\n                - tooltip "2025年1月29日 19時5分" [ref=s1e1749]: 2ヶ月前 2025年1月29日 19時5分\n            - button "メニューを開く" [ref=s1e1751]:\n                - img [ref=s1e1752]\n            - paragraph [ref=s1e1757]: ご丁寧にありがとうございます！\n            - button "いいね" [ref=s1e1760]:\n                - img [ref=s1e1761]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1777]\n    - article [ref=s1e1779]:\n        - link "RyotoTanaka" [ref=s1e1781]:\n            - /url: /ryototanaka\n            - img "RyotoTanaka" [ref=s1e1782]\n        - link "RyotoTanaka" [ref=s1e1783]:\n            - /url: /ryototanaka\n        - link "2025年1月30日 8時0分" [ref=s1e1784]:\n            - /url: "#comment-4ae639fe8d1892"\n            - tooltip "2025年1月30日 8時0分" [ref=s1e1785]: 2ヶ月前 2025年1月30日 8時0分\n        - button "メニューを開く" [ref=s1e1787]:\n            - img [ref=s1e1788]\n        - paragraph [ref=s1e1793]: 読み応えがありました。素晴らしい記事をありがとうございます！ 些細な事なのですが、DPOは Direct\n            Preference Optimization ではないでしょうか。\n        - button "いいね" [ref=s1e1797]:\n            - img [ref=s1e1798]\n        - text: "1"\n        - button "コメント 1" [ref=s1e1813]:\n            - img "コメント" [ref=s1e1814]\n            - text: "1"\n        - article [ref=s1e1818]:\n            - link "asap" [ref=s1e1820]:\n                - /url: /asap\n                - img "asap" [ref=s1e1821]\n            - link "asap" [ref=s1e1822]:\n                - /url: /asap\n            - link "2025年1月30日 13時15分" [ref=s1e1823]:\n                - /url: "#comment-ec7e829404f7bc"\n                - tooltip "2025年1月30日 13時15分" [ref=s1e1824]: 2ヶ月前 2025年1月30日 13時15分\n            - button "メニューを開く" [ref=s1e1826]:\n                - img [ref=s1e1827]\n            - paragraph [ref=s1e1832]: ほんとですね！間違えて入力してました ありがとうございます！\n            - button "いいね" [ref=s1e1836]:\n                - img [ref=s1e1837]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1853]\n    - text: ログインするとコメントできます\n    - button "Login" [ref=s1e1857]\n    - complementary [ref=s1e1858]:\n        - link "asap" [ref=s1e1863]:\n            - /url: /asap\n            - img "asap" [ref=s1e1864]\n        - link "asap" [ref=s1e1866]:\n            - /url: /asap\n        - button "フォロー" [ref=s1e1868]\n        - link "@personabb" [ref=s1e1869]:\n            - /url: https://github.com/personabb\n            - tooltip "@personabb" [ref=s1e1870]:\n                - img "GitHub" [ref=s1e1871]\n                - text: "@personabb"\n        - link "@asap2650" [ref=s1e1873]:\n            - /url: https://twitter.com/asap2650\n            - tooltip "@asap2650" [ref=s1e1874]:\n                - img "X(Twitter)" [ref=s1e1875]\n                - text: "@asap2650"\n        - link "yard.tips" [ref=s1e1881]:\n            - /url: https://yard.tips/asap2650\n            - tooltip "yard.tips" [ref=s1e1882]:\n                - img "リンク" [ref=s1e1883]\n                - text: yard.tips\n        - paragraph [ref=s1e1886]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n        - button "バッジを贈る" [ref=s1e1889]\n        - link "バッジを贈るとは" [ref=s1e1891]:\n            - /url: /faq#badges\n            - text: バッジを贈るとは\n            - img [ref=s1e1892]\n        - text: 目次\n        - list [ref=s1e1901]:\n            - listitem [ref=s1e1902]:\n                - link "はじめに" [ref=s1e1903]:\n                    - /url: "#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB"\n            - listitem [ref=s1e1904]:\n                - link "本論文の興味深いところ" [ref=s1e1905]:\n                    - /url: "#%E6%9C%AC%E8%AB%96%E6%96%87%E3%81%AE%E8%88%88%E5%91%B3%E6%B7%B1%E3%81%8\\\n                        4%E3%81%A8%E3%81%93%E3%82%8D"\n            - listitem [ref=s1e1906]:\n                - link "本論文の流れ" [ref=s1e1907]:\n                    - /url: "#%E6%9C%AC%E8%AB%96%E6%96%87%E3%81%AE%E6%B5%81%E3%82%8C"\n                - list [ref=s1e1908]:\n                    - listitem [ref=s1e1909]:\n                        - link "前提のモデル構造" [ref=s1e1910]:\n                            - /url: "#%E5%89%8D%E6%8F%90%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E6%A7%8B%E9%80%A\\\n                                0"\n                    - listitem [ref=s1e1911]:\n                        - link "DeepSeek-R1-Zeroを強化学習を用いて作る" [ref=s1e1912]:\n                            - /url: "#deepseek-r1-zero%E3%82%92%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E3%82%92%\\\n                                E7%94%A8%E3%81%84%E3%81%A6%E4%BD%9C%E3%82%8B"\n                    - listitem [ref=s1e1913]:\n                        - link "DeepSeek-R1をSFT+強化学習を用いて作る" [ref=s1e1914]:\n                            - /url: "#deepseek-r1%E3%82%92sft%2B%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E3%82%92\\\n                                %E7%94%A8%E3%81%84%E3%81%A6%E4%BD%9C%E3%82%8B"\n                    - listitem [ref=s1e1915]:\n                        - link "小型モデルへの蒸留を試す" [ref=s1e1916]:\n                            - /url: "#%E5%B0%8F%E5%9E%8B%E3%83%A2%E3%83%87%E3%83%AB%E3%81%B8%E3%81%AE%E8%92%B\\\n                                8%E7%95%99%E3%82%92%E8%A9%A6%E3%81%99"\n            - listitem [ref=s1e1917]:\n                - link "DeepSeek-R1-Zeroを作るまで" [ref=s1e1918]:\n                    - /url: "#deepseek-r1-zero%E3%82%92%E4%BD%9C%E3%82%8B%E3%81%BE%E3%81%A7"\n                - list [ref=s1e1919]:\n                    - listitem [ref=s1e1920]:\n                        - link "元々のアーキテクチャ" [ref=s1e1921]:\n                            - /url: "#%E5%85%83%E3%80%85%E3%81%AE%E3%82%A2%E3%83%BC%E3%82%AD%E3%83%86%E3%82%A\\\n                                F%E3%83%81%E3%83%A3"\n                    - listitem [ref=s1e1922]:\n                        - link "すごいぞ！強化学習" [ref=s1e1923]:\n                            - /url: "#%E3%81%99%E3%81%94%E3%81%84%E3%81%9E%EF%BC%81%E5%BC%B7%E5%8C%96%E5%AD%A\\\n                                6%E7%BF%92"\n                    - listitem [ref=s1e1924]:\n                        - link "そもそもLLMで強化学習とは？" [ref=s1e1925]:\n                            - /url: "#%E3%81%9D%E3%82%82%E3%81%9D%E3%82%82llm%E3%81%A7%E5%BC%B7%E5%8C%96%E5%A\\\n                                D%A6%E7%BF%92%E3%81%A8%E3%81%AF%EF%BC%9F"\n                    - listitem [ref=s1e1926]:\n                        - link "ZeroのSystemプロンプト" [ref=s1e1927]:\n                            - /url: "#zero%E3%81%AEsystem%E3%83%97%E3%83%AD%E3%83%B3%E3%83%97%E3%83%88"\n                    - listitem [ref=s1e1928]:\n                        - link "Zeroの「Aha Moment」" [ref=s1e1929]:\n                            - /url: "#zero%E3%81%AE%E3%80%8Caha-moment%E3%80%8D"\n                    - listitem [ref=s1e1930]:\n                        - link "DeepSeek-R1-Zeroのまとめ" [ref=s1e1931]:\n                            - /url: "#deepseek-r1-zero%E3%81%AE%E3%81%BE%E3%81%A8%E3%82%81"\n            - listitem [ref=s1e1932]:\n                - link "DeepSeek-R1への道" [ref=s1e1933]:\n                    - /url: "#deepseek-r1%E3%81%B8%E3%81%AE%E9%81%93"\n                - list [ref=s1e1934]:\n                    - listitem [ref=s1e1935]:\n                        - link "DeepSeek-R1-Zeroの課題" [ref=s1e1936]:\n                            - /url: "#deepseek-r1-zero%E3%81%AE%E8%AA%B2%E9%A1%8C"\n                    - listitem [ref=s1e1937]:\n                        - link "課題対処への道" [ref=s1e1938]:\n                            - /url: "#%E8%AA%B2%E9%A1%8C%E5%AF%BE%E5%87%A6%E3%81%B8%E3%81%AE%E9%81%93"\n                    - listitem [ref=s1e1939]:\n                        - link "DeepSeek-R1のまとめ" [ref=s1e1940]:\n                            - /url: "#deepseek-r1%E3%81%AE%E3%81%BE%E3%81%A8%E3%82%81"\n            - listitem [ref=s1e1941]:\n                - link "結果" [ref=s1e1942]:\n                    - /url: "#%E7%B5%90%E6%9E%9C"\n            - listitem [ref=s1e1943]:\n                - link "蒸留" [ref=s1e1944]:\n                    - /url: "#%E8%92%B8%E7%95%99"\n                - list [ref=s1e1945]:\n                    - listitem [ref=s1e1946]:\n                        - link "蒸留 vs 強化学習" [ref=s1e1947]:\n                            - /url: "#%E8%92%B8%E7%95%99-vs-%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92"\n            - listitem [ref=s1e1948]:\n                - link "今後の展望" [ref=s1e1949]:\n                    - /url: "#%E4%BB%8A%E5%BE%8C%E3%81%AE%E5%B1%95%E6%9C%9B"\n            - listitem [ref=s1e1950]:\n                - link "まとめ" [ref=s1e1951]:\n                    - /url: "#%E3%81%BE%E3%81%A8%E3%82%81"\n            - listitem [ref=s1e1952]:\n                - link "参考文献" [ref=s1e1953]:\n                    - /url: "#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"\n        - paragraph [ref=s1e1955]: Zennからのお知らせ\n        - article [ref=s1e1958]:\n            - link "第２回 AI Agent Hackathon with Google Cloud 第２回 AI Agent Hackathon with Google Cloud 4/14登録受付開始！" [ref=s1e1959]:\n                - /url: /hackathons/google-cloud-japan-ai-hackathon-vol2\n                - img "第２回 AI Agent Hackathon with Google Cloud" [ref=s1e1961]\n                - paragraph [ref=s1e1962]: 第２回 AI Agent Hackathon with Google Cloud 4/14登録受付開始！\n        - article [ref=s1e1963]:\n            - link "Zenncafe日比谷#2 生成AIで変わる仕事 Zenncafe日比谷#2 生成AIで変わる仕事" [ref=s1e1964]:\n                - /url: https://zenn.connpass.com/event/351131/\n                - img "Zenncafe日比谷#2 生成AIで変わる仕事" [ref=s1e1966]\n                - paragraph [ref=s1e1967]: Zenncafe日比谷#2 生成AIで変わる仕事\n- contentinfo [ref=s1e1969]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1973]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1974]\n    - paragraph [ref=s1e1982]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e1984]:\n        - heading "About" [level=4] [ref=s1e1985]\n        - list [ref=s1e1986]:\n            - listitem [ref=s1e1987]:\n                - link "Zennについて" [ref=s1e1989]:\n                    - /url: /about\n            - listitem [ref=s1e1990]:\n                - link "運営会社" [ref=s1e1991]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e1992]:\n                - link "お知らせ・リリース" [ref=s1e1993]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e1994]:\n                - link "イベント" [ref=s1e1996]:\n                    - /url: /events\n    - navigation [ref=s1e1997]:\n        - heading "Guides" [level=4] [ref=s1e1998]\n        - list [ref=s1e1999]:\n            - listitem [ref=s1e2000]:\n                - link "使い方" [ref=s1e2002]:\n                    - /url: /manual\n            - listitem [ref=s1e2003]:\n                - link "法人向けメニュー" [ref=s1e2005]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e2007]:\n                - link "Publication / Pro" [ref=s1e2009]:\n                    - /url: /publications\n            - listitem [ref=s1e2010]:\n                - link "よくある質問" [ref=s1e2012]:\n                    - /url: /faq\n    - navigation [ref=s1e2013]:\n        - heading "Links" [level=4] [ref=s1e2014]\n        - list [ref=s1e2015]:\n            - listitem [ref=s1e2016]:\n                - link "X(Twitter)" [ref=s1e2017]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e2018]:\n                - link "GitHub" [ref=s1e2019]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e2020]:\n                - link "メディアキット" [ref=s1e2022]:\n                    - /url: /mediakit\n    - navigation [ref=s1e2023]:\n        - heading "Legal" [level=4] [ref=s1e2024]\n        - list [ref=s1e2025]:\n            - listitem [ref=s1e2026]:\n                - link "利用規約" [ref=s1e2028]:\n                    - /url: /terms\n            - listitem [ref=s1e2029]:\n                - link "プライバシーポリシー" [ref=s1e2031]:\n                    - /url: /privacy\n            - listitem [ref=s1e2032]:\n                - link "特商法表記" [ref=s1e2034]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e2037]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e2038]\n- alert [ref=s1e2057]\n```\n', name='browser_navigate', tool_call_id='d85f8505-9c2a-4960-b17f-9ef88d983d9d'), AIMessage(content='まず、指定されたZennの著者のページにアクセスし、最もいいねが多い記事を特定しました。その結果、「DeepSeek-R1の論文読んだ？【勉強になるよ】」という記事が最も評価されていることがわかりました。\n\n次に、その記事の内容を詳しく調べました。このブログ記事は、DeepSeek-R1という大規模言語モデルに関する論文を解説したものです。記事は、論文の内容をわかりやすく説明することに重点を置いており、特に強化学習の概念や、DeepSeek-R1を開発する上で用いられたGRPO（Group Relative Policy Optimization）という手法について詳しく解説しています。\n\n記事が評価されている理由として、以下の点が考えられます。\n\n1.  **技術的な深さと分かりやすさの両立:** 大規模言語モデルや強化学習といった複雑なトピックを扱いながらも、専門知識がない読者にも理解できるように、かみ砕いた説明がなされています。\n2.  **論文の内容の網羅性:** DeepSeek-R1の論文の重要なポイントを漏れなくカバーしており、論文を読むための良い入門となっています。\n3.  **著者自身の考察:** 論文の内容を解説するだけでなく、著者自身の考察や解釈が加えられており、読者に新たな視点を提供しています。\n4.  **読者への配慮:** 記事の冒頭で、誤りがあるかもしれないことを断り、コメントでの指摘を歓迎するなど、読者とのコミュニケーションを重視する姿勢が見られます。\n5.  **参考文献の紹介:** 記事の最後に、強化学習や関連技術を学ぶための参考文献が紹介されており、読者の学習をサポートする姿勢が評価されていると考えられます。\n\nこれらの要素が組み合わさることで、この記事はDeepSeek-R1に関心のあるエンジニアにとって非常に価値のある情報源となり、高い評価を得ていると考えられます。', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-d5ac6129-cca3-4759-ac6a-eafe68db505f-0', usage_metadata={'input_tokens': 47987, 'output_tokens': 387, 'total_tokens': 48374, 'input_token_details': {'cache_read': 0}}), HumanMessage(content=[{'type': 'text', 'text': 'LLMに強化学習ってどういうこと？GRPOって何？'}], additional_kwargs={}, response_metadata={}), AIMessage(content='LLM（大規模言語モデル）に対する強化学習と、GRPO（Group Relative Policy Optimization）について説明します。\n\n### LLMに対する強化学習\n\nLLMに対する強化学習は、LLMがより良いテキストを生成するように学習させるための手法です。通常のLLMの学習では、大量のテキストデータを用いて、与えられたテキストの続きを予測するように学習します。これに対して、強化学習では、LLMが生成したテキストに対して「報酬」を与え、その報酬を最大化するように学習します。\n\n**なぜ強化学習が必要なのか？**\n\nLLMは、大量のテキストデータから学習することで、自然な文章を生成する能力を獲得します。しかし、必ずしも人間が望むような文章を生成するとは限りません。例えば、以下のような課題があります。\n\n*   **目的の達成:** 特定のタスク（質問応答、要約など）において、必ずしも最適な回答を生成するとは限らない。\n*   **安全性:** 有害な情報や偏見のある情報を生成する可能性がある。\n*   **創造性:** 新しいアイデアや表現を生み出す能力が限られている。\n\n強化学習は、これらの課題を克服するために用いられます。\n\n**強化学習の基本的な流れ**\n\n1.  **LLM（エージェント）:** テキストを生成する主体。\n2.  **環境:** LLMがテキストを生成する対象となるタスクや状況。\n3.  **報酬:** LLMが生成したテキストの良し悪しを評価する指標。\n4.  **学習:** LLMが報酬を最大化するように、テキスト生成の方針（方策）を調整する。\n\n具体的には、以下のような手順で学習が進みます。\n\n1.  LLMがプロンプト（指示）を受け取り、テキストを生成します。\n2.  生成されたテキストに対して、報酬関数が報酬を与えます。\n    *   例えば、質問応答タスクであれば、正解との一致度合いが高いほど高い報酬を与えます。\n    *   安全性に関する報酬であれば、有害な情報を生成した場合に低い報酬を与えます。\n3.  LLMは、その報酬に基づいて、自身のテキスト生成の方針を調整します。\n    *   具体的には、報酬が高くなるような単語やフレーズを選択する確率を高めます。\n4.  このプロセスを繰り返すことで、LLMはより高い報酬を得られるようなテキストを生成するように学習していきます。\n\n### GRPO（Group Relative Policy Optimization）とは？\n\nGRPOは、LLMに対する強化学習の一種で、特に大規模なLLMを効率的に学習させるために開発された手法です。\n\n**PPO（Proximal Policy Optimization）との比較**\n\nLLMの強化学習でよく用いられる手法として、PPO（Proximal Policy Optimization）があります。PPOでは、報酬に加えて、LLMの出力が大きく変化しないように制約を加えながら学習を進めます。しかし、PPOでは、状態価値モデルという追加のモデルを学習する必要があり、計算コストが高くなるという課題がありました。\n\nGRPOは、この状態価値モデルを不要にし、より効率的に学習を進めることを目指します。\n\n**GRPOの仕組み**\n\nGRPOでは、LLMに複数のテキストを生成させ、それらのテキストに対する報酬を比較することで、相対的な評価を行います。具体的には、以下の手順で学習が進みます。\n\n1.  LLMが同じプロンプトに対して、複数のテキスト（例えば4つ）を生成します。\n2.  それぞれのテキストに対して報酬関数が報酬を与えます。\n3.  それぞれのテキストの報酬を比較し、最も高い報酬を得たテキストを「良いテキスト」、それ以外のテキストを「悪いテキスト」とします。\n4.  LLMは、「良いテキスト」を生成する確率を高め、「悪いテキスト」を生成する確率を下げるように、自身のテキスト生成の方針を調整します。\n\nこのように、GRPOでは、テキスト間の相対的な評価を用いることで、状態価値モデルを学習する必要がなくなり、計算コストを削減することができます。\n\n**GRPOの利点**\n\n*   **計算効率:** 状態価値モデルが不要なため、PPOよりも計算コストが低い。\n*   **大規模LLMへの適用:** 大規模なLLMでも効率的に学習できる。\n\n**まとめ**\n\nLLMに対する強化学習は、LLMがより良いテキストを生成するように学習させるための手法であり、GRPOはその効率的な学習を可能にする手法の一つです。GRPOは、テキスト間の相対的な評価を用いることで、状態価値モデルを不要にし、計算コストを削減することができます。', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-4d11a554-e7d5-4420-9ac9-b8efaccf8b11-0', usage_metadata={'input_tokens': 48388, 'output_tokens': 996, 'total_tokens': 49384, 'input_token_details': {'cache_read': 0}}), HumanMessage(content=[{'type': 'text', 'text': '2番目にいいねが多い記事は何？'}], additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-aae69546-e8ee-4891-9040-daee70739fa4-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap'}, 'id': '61c67f9f-6525-4601-af0d-36cd5f4631a2', 'type': 'tool_call'}], usage_metadata={'input_tokens': 49392, 'output_tokens': 13, 'total_tokens': 49405, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap\n\n- Page URL: https://zenn.dev/asap\n- Page Title: asapさんの記事一覧 | Zenn\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- banner [ref=s1e32]:\n    - img "asap" [ref=s1e36]\n    - heading "asap" [level=1] [ref=s1e39]\n    - button "フォロー" [ref=s1e41]\n    - paragraph [ref=s1e43]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n    - button "2,214 Likes" [ref=s1e46]\n    - button "310 Followers" [ref=s1e48]\n    - link "@personabb" [ref=s1e51]:\n        - /url: https://github.com/personabb\n        - tooltip "@personabb" [ref=s1e52]:\n            - img "GitHub" [ref=s1e53]\n            - text: "@personabb"\n    - link "@asap2650" [ref=s1e55]:\n        - /url: https://twitter.com/asap2650\n        - tooltip "@asap2650" [ref=s1e56]:\n            - img "X(Twitter)" [ref=s1e57]\n            - text: "@asap2650"\n    - link "yard.tips" [ref=s1e63]:\n        - /url: https://yard.tips/asap2650\n        - tooltip "yard.tips" [ref=s1e64]:\n            - img [ref=s1e65]\n            - text: yard.tips\n    - link "RSS" [ref=s1e67]:\n        - /url: /asap/feed\n        - tooltip "RSS" [ref=s1e68]:\n            - img [ref=s1e69]\n            - text: RSS\n- link "Articles 72" [ref=s1e80]:\n    - /url: /asap\n- link "Scraps 5" [ref=s1e82]:\n    - /url: /asap?tab=scraps\n- link "Comments" [ref=s1e84]:\n    - /url: /asap?tab=comments\n- contentinfo [ref=s1e88]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e92]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e93]\n    - paragraph [ref=s1e101]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e103]:\n        - heading "About" [level=4] [ref=s1e104]\n        - list [ref=s1e105]:\n            - listitem [ref=s1e106]:\n                - link "Zennについて" [ref=s1e108]:\n                    - /url: /about\n            - listitem [ref=s1e109]:\n                - link "運営会社" [ref=s1e110]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e111]:\n                - link "お知らせ・リリース" [ref=s1e112]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e113]:\n                - link "イベント" [ref=s1e115]:\n                    - /url: /events\n    - navigation [ref=s1e116]:\n        - heading "Guides" [level=4] [ref=s1e117]\n        - list [ref=s1e118]:\n            - listitem [ref=s1e119]:\n                - link "使い方" [ref=s1e121]:\n                    - /url: /manual\n            - listitem [ref=s1e122]:\n                - link "法人向けメニュー" [ref=s1e124]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e126]:\n                - link "Publication / Pro" [ref=s1e128]:\n                    - /url: /publications\n            - listitem [ref=s1e129]:\n                - link "よくある質問" [ref=s1e131]:\n                    - /url: /faq\n    - navigation [ref=s1e132]:\n        - heading "Links" [level=4] [ref=s1e133]\n        - list [ref=s1e134]:\n            - listitem [ref=s1e135]:\n                - link "X(Twitter)" [ref=s1e136]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e137]:\n                - link "GitHub" [ref=s1e138]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e139]:\n                - link "メディアキット" [ref=s1e141]:\n                    - /url: /mediakit\n    - navigation [ref=s1e142]:\n        - heading "Legal" [level=4] [ref=s1e143]\n        - list [ref=s1e144]:\n            - listitem [ref=s1e145]:\n                - link "利用規約" [ref=s1e147]:\n                    - /url: /terms\n            - listitem [ref=s1e148]:\n                - link "プライバシーポリシー" [ref=s1e150]:\n                    - /url: /privacy\n            - listitem [ref=s1e151]:\n                - link "特商法表記" [ref=s1e153]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e156]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e157]\n- alert [ref=s1e176]\n```\n', name='browser_navigate', tool_call_id='61c67f9f-6525-4601-af0d-36cd5f4631a2'), AIMessage(content='asapさんのZennの記事の中で2番目にいいねが多い記事は、「VAEって結局何者なの？」で、いいね数は104です。', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-7c7bf5a2-7650-41d4-9cc5-c1fb3e0de2f1-0', usage_metadata={'input_tokens': 51212, 'output_tokens': 32, 'total_tokens': 51244, 'input_token_details': {'cache_read': 0}})]}
=================================
asapさんのZennの記事の中で2番目にいいねが多い記事は、「VAEって結局何者なの？」で、いいね数は104です。










入力してください: どんな記事ですか？解説してください。
response:  {'messages': [HumanMessage(content=[{'type': 'text', 'text': '「https://zenn.dev/asap」この執筆者のもっともいいねがついている記事を調査して、その記事の中身を取得し、その記事がなぜ評価されているのかを分析してください。'}], additional_kwargs={}, response_metadata={}), AIMessage(content='zenn.devのasap氏の最もいいねが多い記事を調べ、その内容を取得し、評価されている理由を分析する、というご要望ですね。\n\nまず、zenn.dev/asapにアクセスして、最もいいねが多い記事のタイトルとURLを特定する必要があります。その後、そのURLにアクセスして記事の内容を取得します。最後に、記事の内容から評価されている理由を推測します。\n\nでは、始めましょう。\n\n1.  zenn.dev/asapにアクセスし、最もいいねが多い記事の情報を取得します。', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-989fe5ca-2637-4757-878f-db26e13526b8-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap'}, 'id': '58fad86e-0b01-4fc6-a8cc-57deb5c48b41', 'type': 'tool_call'}], usage_metadata={'input_tokens': 778, 'output_tokens': 128, 'total_tokens': 906, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap\n\n- Page URL: https://zenn.dev/asap\n- Page Title: asapさんの記事一覧 | Zenn\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- banner [ref=s1e32]:\n    - img "asap" [ref=s1e36]\n    - heading "asap" [level=1] [ref=s1e39]\n    - button "フォロー" [ref=s1e41]\n    - paragraph [ref=s1e43]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n    - button "2,214 Likes" [ref=s1e46]\n    - button "310 Followers" [ref=s1e48]\n    - link "@personabb" [ref=s1e51]:\n        - /url: https://github.com/personabb\n        - tooltip "@personabb" [ref=s1e52]:\n            - img "GitHub" [ref=s1e53]\n            - text: "@personabb"\n    - link "@asap2650" [ref=s1e55]:\n        - /url: https://twitter.com/asap2650\n        - tooltip "@asap2650" [ref=s1e56]:\n            - img "X(Twitter)" [ref=s1e57]\n            - text: "@asap2650"\n    - link "yard.tips" [ref=s1e63]:\n        - /url: https://yard.tips/asap2650\n        - tooltip "yard.tips" [ref=s1e64]:\n            - img [ref=s1e65]\n            - text: yard.tips\n    - link "RSS" [ref=s1e67]:\n        - /url: /asap/feed\n        - tooltip "RSS" [ref=s1e68]:\n            - img [ref=s1e69]\n            - text: RSS\n- link "Articles 72" [ref=s1e80]:\n    - /url: /asap\n- link "Scraps 5" [ref=s1e82]:\n    - /url: /asap?tab=scraps\n- link "Comments" [ref=s1e84]:\n    - /url: /asap?tab=comments\n- article [ref=s1e89]:\n    - link "tech" [ref=s1e90]:\n        - /url: /tech-or-idea\n    - link "❄️ LangChainでGemini-2.0 APIの画像編集を実行する方法 6日前 いいねされた数 18" [ref=s1e91]:\n        - /url: /asap/articles/1d5e6ec06c380f\n        - text: ❄️\n        - heading "LangChainでGemini-2.0 APIの画像編集を実行する方法" [level=3] [ref=s1e95]\n        - time [ref=s1e98]: 6日前\n        - img "いいねされた数" [ref=s1e100]\n        - text: "18"\n- article [ref=s1e102]:\n    - link "tech" [ref=s1e103]:\n        - /url: /tech-or-idea\n    - link "🙌 完全自律型AIエージェントOSS【LangManus】完全に理解した！ 13日前 いいねされた数 48" [ref=s1e104]:\n        - /url: /asap/articles/b27e832c2a49b7\n        - text: 🙌\n        - heading "完全自律型AIエージェントOSS【LangManus】完全に理解した！" [level=3] [ref=s1e108]\n        - time [ref=s1e111]: 13日前\n        - img "いいねされた数" [ref=s1e113]\n        - text: "48"\n- article [ref=s1e115]:\n    - link "tech" [ref=s1e116]:\n        - /url: /tech-or-idea\n    - link "💫 LangChainでOpenAI gpt-4o / o1にPDFを入力する方法 20日前 いいねされた数 9" [ref=s1e117]:\n        - /url: /asap/articles/74885fab863c9e\n        - text: 💫\n        - heading "LangChainでOpenAI gpt-4o / o1にPDFを入力する方法" [level=3]\n          [ref=s1e121]\n        - time [ref=s1e124]: 20日前\n        - img "いいねされた数" [ref=s1e126]\n        - text: "9"\n- article [ref=s1e128]:\n    - link "tech" [ref=s1e129]:\n        - /url: /tech-or-idea\n    - link "👣 Ollama + LangChainで画像を入力する方法【Gemma3 27B】 27日前 いいねされた数 14" [ref=s1e130]:\n        - /url: /asap/articles/18b110d551b811\n        - text: 👣\n        - heading "Ollama + LangChainで画像を入力する方法【Gemma3 27B】" [level=3]\n          [ref=s1e134]\n        - time [ref=s1e137]: 27日前\n        - img "いいねされた数" [ref=s1e139]\n        - text: "14"\n- article [ref=s1e141]:\n    - link "tech" [ref=s1e142]:\n        - /url: /tech-or-idea\n    - link "🦄 Google CloudのVector StoreでRAG Agentsを構築する【Big Querty編】 28日前 いいねされた数 12" [ref=s1e143]:\n        - /url: /asap/articles/3d258861c6c528\n        - text: 🦄\n        - heading "Google CloudのVector StoreでRAG Agentsを構築する【Big Querty編】"\n          [level=3] [ref=s1e147]\n        - time [ref=s1e150]: 28日前\n        - img "いいねされた数" [ref=s1e152]\n        - text: "12"\n- article [ref=s1e154]:\n    - link "tech" [ref=s1e155]:\n        - /url: /tech-or-idea\n    - link "🎶 最新音楽生成AI 【DiffRhythm】をGoogle Colabで動かせるようにしました！（簡易版） 1ヶ月前 いいねされた数 39" [ref=s1e156]:\n        - /url: /asap/articles/4edc4a71de8961\n        - text: 🎶\n        - heading "最新音楽生成AI 【DiffRhythm】をGoogle Colabで動かせるようにしました！（簡易版）"\n          [level=3] [ref=s1e160]\n        - time [ref=s1e163]: 1ヶ月前\n        - img "いいねされた数" [ref=s1e165]\n        - text: "39"\n- article [ref=s1e167]:\n    - link "tech" [ref=s1e168]:\n        - /url: /tech-or-idea\n    - link "🐤 LangGraphのTool Callingを利用して、RAG Agentsを構築する（後編） 1ヶ月前 いいねされた数 12" [ref=s1e169]:\n        - /url: /asap/articles/c58cfab9c1362a\n        - text: 🐤\n        - heading "LangGraphのTool Callingを利用して、RAG Agentsを構築する（後編）" [level=3]\n          [ref=s1e173]\n        - time [ref=s1e176]: 1ヶ月前\n        - img "いいねされた数" [ref=s1e178]\n        - text: "12"\n- article [ref=s1e180]:\n    - link "idea" [ref=s1e181]:\n        - /url: /tech-or-idea\n    - link "🏆 イベントに登壇してきた話【資料もこちら】 1ヶ月前 いいねされた数 20" [ref=s1e182]:\n        - /url: /asap/articles/89d2a13bdc75a3\n        - text: 🏆\n        - heading "イベントに登壇してきた話【資料もこちら】" [level=3] [ref=s1e186]\n        - time [ref=s1e189]: 1ヶ月前\n        - img "いいねされた数" [ref=s1e191]\n        - text: "20"\n- article [ref=s1e193]:\n    - link "tech" [ref=s1e194]:\n        - /url: /tech-or-idea\n    - link "👌 Grok API（Grok 2）で遊んでみた！【Elyza Tasks 100】 2ヶ月前 いいねされた数 11" [ref=s1e195]:\n        - /url: /asap/articles/6aa01bde32163d\n        - text: 👌\n        - heading "Grok API（Grok 2）で遊んでみた！【Elyza Tasks 100】" [level=3]\n          [ref=s1e199]\n        - time [ref=s1e202]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e204]\n        - text: "11"\n- article [ref=s1e206]:\n    - link "tech" [ref=s1e207]:\n        - /url: /tech-or-idea\n    - link "📑 Multi-head Latent Attention (MLA)の詳細【be absorbedとは？】 2ヶ月前 いいねされた数 22" [ref=s1e208]:\n        - /url: /asap/articles/be3d4b60d8ac92\n        - text: 📑\n        - heading "Multi-head Latent Attention (MLA)の詳細【be absorbedとは？】"\n          [level=3] [ref=s1e212]\n        - time [ref=s1e215]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e217]\n        - text: "22"\n- article [ref=s1e219]:\n    - link "tech" [ref=s1e220]:\n        - /url: /tech-or-idea\n    - link "⛳ LangGraphのTool Callingを利用して、RAG Agentsを構築する（前編） 2ヶ月前 いいねされた数 46" [ref=s1e221]:\n        - /url: /asap/articles/2ad7eb5e7560cf\n        - text: ⛳\n        - heading "LangGraphのTool Callingを利用して、RAG Agentsを構築する（前編）" [level=3]\n          [ref=s1e225]\n        - time [ref=s1e228]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e230]\n        - text: "46"\n- article [ref=s1e232]:\n    - link "tech" [ref=s1e233]:\n        - /url: /tech-or-idea\n    - link "🫠 【LangChain】で【Transformer】を利用する場合に【chat_template】を設定する方法【エラー対処記事】 2ヶ月前 いいねされた数 13" [ref=s1e234]:\n        - /url: /asap/articles/985f3580ea476d\n        - text: 🫠\n        - heading\n          "【LangChain】で【Transformer】を利用する場合に【chat_template】を設定する方法【エラー対処記事】"\n          [level=3] [ref=s1e238]\n        - time [ref=s1e241]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e243]\n        - text: "13"\n- article [ref=s1e245]:\n    - link "tech" [ref=s1e246]:\n        - /url: /tech-or-idea\n    - link "🌊 採点理由を出力しながらローカルLLMを評価する【Elyza Tasks 100】 2ヶ月前 いいねされた数 26" [ref=s1e247]:\n        - /url: /asap/articles/3a99c34fc1d4cb\n        - text: 🌊\n        - heading "採点理由を出力しながらローカルLLMを評価する【Elyza Tasks 100】" [level=3]\n          [ref=s1e251]\n        - time [ref=s1e254]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e256]\n        - text: "26"\n- article [ref=s1e258]:\n    - link "tech" [ref=s1e259]:\n        - /url: /tech-or-idea\n    - link "📑 DeepSeek-R1の論文読んだ？【勉強になるよ】 2ヶ月前 いいねされた数 921" [ref=s1e260]:\n        - /url: /asap/articles/34237ad87f8511\n        - text: 📑\n        - heading "DeepSeek-R1の論文読んだ？【勉強になるよ】" [level=3] [ref=s1e264]\n        - time [ref=s1e267]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e269]\n        - text: "921"\n- article [ref=s1e271]:\n    - link "tech" [ref=s1e272]:\n        - /url: /tech-or-idea\n    - link "📖 VAEって結局何者なの？ 3ヶ月前 いいねされた数 104" [ref=s1e273]:\n        - /url: /asap/articles/6caa9043276424\n        - text: 📖\n        - heading "VAEって結局何者なの？" [level=3] [ref=s1e277]\n        - time [ref=s1e280]: 3ヶ月前\n        - img "いいねされた数" [ref=s1e282]\n        - text: "104"\n- article [ref=s1e284]:\n    - link "tech" [ref=s1e285]:\n        - /url: /tech-or-idea\n    - link "🦔 初めてのSageMaker AIでSFTTrainerによるLLMのフルパラメータファインチューニングを試す 3ヶ月前 いいねされた数 9" [ref=s1e286]:\n        - /url: /asap/articles/3f31b77010624c\n        - text: 🦔\n        - heading "初めてのSageMaker AIでSFTTrainerによるLLMのフルパラメータファインチューニングを試す"\n          [level=3] [ref=s1e290]\n        - time [ref=s1e293]: 3ヶ月前\n        - img "いいねされた数" [ref=s1e295]\n        - text: "9"\n- article [ref=s1e297]:\n    - link "tech" [ref=s1e298]:\n        - /url: /tech-or-idea\n    - link "😺 【動画翻訳】OpenAIの研究者によるo1やLLMに対するディスカッションの翻訳 3ヶ月前 いいねされた数 12" [ref=s1e299]:\n        - /url: /asap/articles/2d2832bf3288aa\n        - text: 😺\n        - heading "【動画翻訳】OpenAIの研究者によるo1やLLMに対するディスカッションの翻訳" [level=3]\n          [ref=s1e303]\n        - time [ref=s1e306]: 3ヶ月前\n        - img "いいねされた数" [ref=s1e308]\n        - text: "12"\n- article [ref=s1e310]:\n    - link "tech" [ref=s1e311]:\n        - /url: /tech-or-idea\n    - link "🧚\u200d♀️ AIでオリジナル楽曲を作り、自作のかわいい声で歌ってもらう方法 3ヶ月前 いいねされた数 28" [ref=s1e312]:\n        - /url: /asap/articles/886af0fe48dda3\n        - text: 🧚\u200d♀️\n        - heading "AIでオリジナル楽曲を作り、自作のかわいい声で歌ってもらう方法" [level=3] [ref=s1e316]\n        - time [ref=s1e319]: 3ヶ月前\n        - img "いいねされた数" [ref=s1e321]\n        - text: "28"\n- article [ref=s1e323]:\n    - link "tech" [ref=s1e324]:\n        - /url: /tech-or-idea\n    - link "🎮 Google ColabでSeed-VCによる声色変換を試す 4ヶ月前 いいねされた数 27" [ref=s1e325]:\n        - /url: /asap/articles/9c54aef739a6ed\n        - text: 🎮\n        - heading "Google ColabでSeed-VCによる声色変換を試す" [level=3] [ref=s1e329]\n        - time [ref=s1e332]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e334]\n        - text: "27"\n- article [ref=s1e336]:\n    - link "tech" [ref=s1e337]:\n        - /url: /tech-or-idea\n    - link "🐡 LLMの性能における重要補題【Johnson–Lindenstrauss lemma】のお気持ち〜Attentionの数式理解を添えて〜 4ヶ月前 いいねされた数 21" [ref=s1e338]:\n        - /url: /asap/articles/6bdf89d8e964ec\n        - text: 🐡\n        - heading "LLMの性能における重要補題【Johnson–Lindenstrauss\n          lemma】のお気持ち〜Attentionの数式理解を添えて〜" [level=3] [ref=s1e342]\n        - time [ref=s1e345]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e347]\n        - text: "21"\n- article [ref=s1e349]:\n    - link "tech" [ref=s1e350]:\n        - /url: /tech-or-idea\n    - link "📏 【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】 4ヶ月前 いいねされた数 50" [ref=s1e351]:\n        - /url: /asap/articles/ea9f42edff6573\n        - text: 📏\n        - heading "【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】"\n          [level=3] [ref=s1e355]\n        - time [ref=s1e358]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e360]\n        - text: "50"\n- article [ref=s1e362]:\n    - link "tech" [ref=s1e363]:\n        - /url: /tech-or-idea\n    - link "🎙️ 高品質合成音声モデル【Style-Bert-VITS2】をGoogle Colabで学習し、【Aivis Speech】で利用する方法 4ヶ月前 いいねされた数 24" [ref=s1e364]:\n        - /url: /asap/articles/5c437ce2943ed7\n        - text: 🎙️\n        - heading "高品質合成音声モデル【Style-Bert-VITS2】をGoogle Colabで学習し、【Aivis\n          Speech】で利用する方法" [level=3] [ref=s1e368]\n        - time [ref=s1e371]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e373]\n        - text: "24"\n- article [ref=s1e375]:\n    - link "tech" [ref=s1e376]:\n        - /url: /tech-or-idea\n    - link "🎄 【LangChain】LCEL ステップバイステップ ハンズオン 〜なろう短編小説の自動生成＆評価・改善を行う「AIエージェント」を添えて〜 4ヶ月前 いいねされた数 19" [ref=s1e377]:\n        - /url: /asap/articles/aa587ad8d76105\n        - text: 🎄\n        - heading "【LangChain】LCEL ステップバイステップ ハンズオン\n          〜なろう短編小説の自動生成＆評価・改善を行う「AIエージェント」を添えて〜" [level=3] [ref=s1e381]\n        - time [ref=s1e384]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e386]\n        - text: "19"\n- article [ref=s1e388]:\n    - link "tech" [ref=s1e389]:\n        - /url: /tech-or-idea\n    - link "💻 LangGraphによる「AIエージェントWebアプリ」を作成する【Next.js】 4ヶ月前 いいねされた数 45" [ref=s1e390]:\n        - /url: /asap/articles/a350fe4ac00343\n        - text: 💻\n        - heading "LangGraphによる「AIエージェントWebアプリ」を作成する【Next.js】" [level=3]\n          [ref=s1e394]\n        - time [ref=s1e397]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e399]\n        - text: "45"\n- article [ref=s1e401]:\n    - link "tech" [ref=s1e402]:\n        - /url: /tech-or-idea\n    - link "🤔 LangGraphにて入力情報が不十分な場合に、ユーザへ追加質問をするAIエージェントを作る方法 5ヶ月前 いいねされた数 41" [ref=s1e403]:\n        - /url: /asap/articles/03548f844efe6f\n        - text: 🤔\n        - heading "LangGraphにて入力情報が不十分な場合に、ユーザへ追加質問をするAIエージェントを作る方法" [level=3]\n          [ref=s1e407]\n        - time [ref=s1e410]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e412]\n        - text: "41"\n- article [ref=s1e414]:\n    - link "tech" [ref=s1e415]:\n        - /url: /tech-or-idea\n    - link "🤖 高品質動画生成AI【Allegro】をローカルで実行する【VRAM12GB】 5ヶ月前 いいねされた数 10" [ref=s1e416]:\n        - /url: /asap/articles/dac57655eb0314\n        - text: 🤖\n        - heading "高品質動画生成AI【Allegro】をローカルで実行する【VRAM12GB】" [level=3] [ref=s1e420]\n        - time [ref=s1e423]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e425]\n        - text: "10"\n- article [ref=s1e427]:\n    - link "tech" [ref=s1e428]:\n        - /url: /tech-or-idea\n    - link "🌊 フレーム補間技術【RIFE】を利用してCogVideoXのFPSを改善しよう 5ヶ月前 いいねされた数 2" [ref=s1e429]:\n        - /url: /asap/articles/dbeef72b7bfdc3\n        - text: 🌊\n        - heading "フレーム補間技術【RIFE】を利用してCogVideoXのFPSを改善しよう" [level=3] [ref=s1e433]\n        - time [ref=s1e436]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e438]\n        - text: "2"\n- article [ref=s1e440]:\n    - link "tech" [ref=s1e441]:\n        - /url: /tech-or-idea\n    - link "🔥 LangGraphを使ってAIエージェントを実装する方法 5ヶ月前 いいねされた数 34" [ref=s1e442]:\n        - /url: /asap/articles/5da9cf01703a47\n        - text: 🔥\n        - heading "LangGraphを使ってAIエージェントを実装する方法" [level=3] [ref=s1e446]\n        - time [ref=s1e449]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e451]\n        - text: "34"\n- article [ref=s1e453]:\n    - link "tech" [ref=s1e454]:\n        - /url: /tech-or-idea\n    - link "🌈 動画生成AI【CogVideoX】をローカルで実行する 5ヶ月前 いいねされた数 9" [ref=s1e455]:\n        - /url: /asap/articles/a4077e8b9ddf1a\n        - text: 🌈\n        - heading "動画生成AI【CogVideoX】をローカルで実行する" [level=3] [ref=s1e459]\n        - time [ref=s1e462]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e464]\n        - text: "9"\n- article [ref=s1e466]:\n    - link "tech" [ref=s1e467]:\n        - /url: /tech-or-idea\n    - link "🛎️ ローカルでDIfyを使ってAIエージェントを実装する方法 5ヶ月前 いいねされた数 55" [ref=s1e468]:\n        - /url: /asap/articles/a35254020fbbae\n        - text: 🛎️\n        - heading "ローカルでDIfyを使ってAIエージェントを実装する方法" [level=3] [ref=s1e472]\n        - time [ref=s1e475]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e477]\n        - text: "55"\n- article [ref=s1e479]:\n    - link "tech" [ref=s1e480]:\n        - /url: /tech-or-idea\n    - link "💫 高品質動画生成AI【Pyramid Flow】をローカルで実行する 6ヶ月前 いいねされた数 12" [ref=s1e481]:\n        - /url: /asap/articles/e1880903625cb5\n        - text: 💫\n        - heading "高品質動画生成AI【Pyramid Flow】をローカルで実行する" [level=3] [ref=s1e485]\n        - time [ref=s1e488]: 6ヶ月前\n        - img "いいねされた数" [ref=s1e490]\n        - text: "12"\n- article [ref=s1e492]:\n    - link "tech" [ref=s1e493]:\n        - /url: /tech-or-idea\n    - link "📖 不偏分散で N-1 で割る理由 を自力で導出する 6ヶ月前 いいねされた数 5" [ref=s1e494]:\n        - /url: /asap/articles/f0cba34c2ac2ac\n        - text: 📖\n        - heading "不偏分散で N-1 で割る理由 を自力で導出する" [level=3] [ref=s1e498]\n        - time [ref=s1e501]: 6ヶ月前\n        - img "いいねされた数" [ref=s1e503]\n        - text: "5"\n- article [ref=s1e505]:\n    - link "tech" [ref=s1e506]:\n        - /url: /tech-or-idea\n    - link "✍️ 勾配降下法 ・ 誤差逆伝播法 を自力で導出する 6ヶ月前 いいねされた数 25" [ref=s1e507]:\n        - /url: /asap/articles/643f604d444813\n        - text: ✍️\n        - heading "勾配降下法 ・ 誤差逆伝播法 を自力で導出する" [level=3] [ref=s1e511]\n        - time [ref=s1e514]: 6ヶ月前\n        - img "いいねされた数" [ref=s1e516]\n        - text: "25"\n- article [ref=s1e518]:\n    - link "tech" [ref=s1e519]:\n        - /url: /tech-or-idea\n    - link "💰 Open AI Realtime APIで、会話履歴を削除して高額請求を回避したい！【Pythonサンプルコード】 2024/10/07 いいねされた数 25" [ref=s1e520]:\n        - /url: /asap/articles/af07fcedbbef61\n        - text: 💰\n        - heading "Open AI Realtime APIで、会話履歴を削除して高額請求を回避したい！【Pythonサンプルコード】"\n          [level=3] [ref=s1e524]\n        - time [ref=s1e527]: 2024/10/07\n        - img "いいねされた数" [ref=s1e529]\n        - text: "25"\n- article [ref=s1e531]:\n    - link "tech" [ref=s1e532]:\n        - /url: /tech-or-idea\n    - link "✈️ Open AI Realtime APIのPythonサンプルコードで、AI発話にカットイン（割り込み）する方法 2024/10/06 いいねされた数 24" [ref=s1e533]:\n        - /url: /asap/articles/563500af4649da\n        - text: ✈️\n        - heading "Open AI Realtime APIのPythonサンプルコードで、AI発話にカットイン（割り込み）する方法"\n          [level=3] [ref=s1e537]\n        - time [ref=s1e540]: 2024/10/06\n        - img "いいねされた数" [ref=s1e542]\n        - text: "24"\n- article [ref=s1e544]:\n    - link "tech" [ref=s1e545]:\n        - /url: /tech-or-idea\n    - link "🥰 Open AI Realtime APIのPythonサンプルコードを作成 2024/10/04 いいねされた数 79" [ref=s1e546]:\n        - /url: /asap/articles/4368fd306b592a\n        - text: 🥰\n        - heading "Open AI Realtime APIのPythonサンプルコードを作成" [level=3] [ref=s1e550]\n        - time [ref=s1e553]: 2024/10/04\n        - img "いいねされた数" [ref=s1e555]\n        - text: "79"\n- article [ref=s1e557]:\n    - link "tech" [ref=s1e558]:\n        - /url: /tech-or-idea\n    - link "📗 主成分分析を自力で導出する 2024/10/03 いいねされた数 6" [ref=s1e559]:\n        - /url: /asap/articles/ff8f34d19ca6a4\n        - text: 📗\n        - heading "主成分分析を自力で導出する" [level=3] [ref=s1e563]\n        - time [ref=s1e566]: 2024/10/03\n        - img "いいねされた数" [ref=s1e568]\n        - text: "6"\n- article [ref=s1e570]:\n    - link "tech" [ref=s1e571]:\n        - /url: /tech-or-idea\n    - link "😍 FLUX.1-dev で ControlNet-depth と LoRA【QTCanimatedFeeling-FLUX】 を試す 2024/10/02 いいねされた数 3" [ref=s1e572]:\n        - /url: /asap/articles/9192c2363dd96a\n        - text: 😍\n        - heading "FLUX.1-dev で ControlNet-depth と LoRA【QTCanimatedFeeling-FLUX】\n          を試す" [level=3] [ref=s1e576]\n        - time [ref=s1e579]: 2024/10/02\n        - img "いいねされた数" [ref=s1e581]\n        - text: "3"\n- article [ref=s1e583]:\n    - link "tech" [ref=s1e584]:\n        - /url: /tech-or-idea\n    - link "📗 拡散モデルの理論について（拡散過程の条件付き確率・逆拡散過程のスケジューラ） 2024/09/23 いいねされた数 8" [ref=s1e585]:\n        - /url: /asap/articles/8aaa896a02f168\n        - text: 📗\n        - heading "拡散モデルの理論について（拡散過程の条件付き確率・逆拡散過程のスケジューラ）" [level=3] [ref=s1e589]\n        - time [ref=s1e592]: 2024/09/23\n        - img "いいねされた数" [ref=s1e594]\n        - text: "8"\n- article [ref=s1e596]:\n    - link "tech" [ref=s1e597]:\n        - /url: /tech-or-idea\n    - link "📖 分散共分散行列の魅力 2024/09/20 いいねされた数 14" [ref=s1e598]:\n        - /url: /asap/articles/2269cd74c5ddc2\n        - text: 📖\n        - heading "分散共分散行列の魅力" [level=3] [ref=s1e602]\n        - time [ref=s1e605]: 2024/09/20\n        - img "いいねされた数" [ref=s1e607]\n        - text: "14"\n- article [ref=s1e609]:\n    - link "tech" [ref=s1e610]:\n        - /url: /tech-or-idea\n    - link "😍 超強力汎用ControlNet「AnyTest v4」を利用する方法【diffusers】 2024/09/17 いいねされた数 9" [ref=s1e611]:\n        - /url: /asap/articles/c08dffa41bec03\n        - text: 😍\n        - heading "超強力汎用ControlNet「AnyTest v4」を利用する方法【diffusers】" [level=3]\n          [ref=s1e615]\n        - time [ref=s1e618]: 2024/09/17\n        - img "いいねされた数" [ref=s1e620]\n        - text: "9"\n- article [ref=s1e622]:\n    - link "tech" [ref=s1e623]:\n        - /url: /tech-or-idea\n    - link "🌍 FLUX.1-devでControlNetを利用したら画像の質が下がったのでImg2Imgを使って改善してみた【Diffusers】 2024/09/13 いいねされた数 2" [ref=s1e624]:\n        - /url: /asap/articles/912d6de879d006\n        - text: 🌍\n        - heading\n          "FLUX.1-devでControlNetを利用したら画像の質が下がったのでImg2Imgを使って改善してみた【Diffusers】"\n          [level=3] [ref=s1e628]\n        - time [ref=s1e631]: 2024/09/13\n        - img "いいねされた数" [ref=s1e633]\n        - text: "2"\n- article [ref=s1e635]:\n    - link "tech" [ref=s1e636]:\n        - /url: /tech-or-idea\n    - link "🎮 【Next.js】ポートレートの表情を編集する自作UIを作ってみた【ComfyUI-AdvancedLivePortrait】 2024/09/11 いいねされた数 8" [ref=s1e637]:\n        - /url: /asap/articles/c49551ff14d5e9\n        - text: 🎮\n        - heading\n          "【Next.js】ポートレートの表情を編集する自作UIを作ってみた【ComfyUI-AdvancedLivePortrait】"\n          [level=3] [ref=s1e641]\n        - time [ref=s1e644]: 2024/09/11\n        - img "いいねされた数" [ref=s1e646]\n        - text: "8"\n- article [ref=s1e648]:\n    - link "tech" [ref=s1e649]:\n        - /url: /tech-or-idea\n    - link "📖 ComfyUI-AdvancedLivePortraitを使ってみる（No WebUI）（Google Colab） 2024/09/10 いいねされた数 7" [ref=s1e650]:\n        - /url: /asap/articles/c0a1b82a10f0e2\n        - text: 📖\n        - heading "ComfyUI-AdvancedLivePortraitを使ってみる（No WebUI）（Google Colab）"\n          [level=3] [ref=s1e654]\n        - time [ref=s1e657]: 2024/09/10\n        - img "いいねされた数" [ref=s1e659]\n        - text: "7"\n- article [ref=s1e661]:\n    - link "tech" [ref=s1e662]:\n        - /url: /tech-or-idea\n    - link "⛳ websocketsでリアルタイム音声処理 2024/09/06 いいねされた数 9" [ref=s1e663]:\n        - /url: /asap/articles/47f839550e5535\n        - text: ⛳\n        - heading "websocketsでリアルタイム音声処理" [level=3] [ref=s1e667]\n        - time [ref=s1e670]: 2024/09/06\n        - img "いいねされた数" [ref=s1e672]\n        - text: "9"\n- article [ref=s1e674]:\n    - link "tech" [ref=s1e675]:\n        - /url: /tech-or-idea\n    - link "😍 生成AIをGoogle Colabで簡単に 【FLUX.1-dev + ControlNet + LoRA】【Diffusers】 2024/08/31 いいねされた数 9" [ref=s1e676]:\n        - /url: /asap/articles/e4c199cecf1836\n        - text: 😍\n        - heading "生成AIをGoogle Colabで簡単に 【FLUX.1-dev + ControlNet +\n          LoRA】【Diffusers】" [level=3] [ref=s1e680]\n        - time [ref=s1e683]: 2024/08/31\n        - img "いいねされた数" [ref=s1e685]\n        - text: "9"\n- article [ref=s1e687]:\n    - link "tech" [ref=s1e688]:\n        - /url: /tech-or-idea\n    - link "👋 Style-Bert-VITS2のAPI化 2024/08/28 いいねされた数 6" [ref=s1e689]:\n        - /url: /asap/articles/12b0f31625a90f\n        - text: 👋\n        - heading "Style-Bert-VITS2のAPI化" [level=3] [ref=s1e693]\n        - time [ref=s1e696]: 2024/08/28\n        - img "いいねされた数" [ref=s1e698]\n        - text: "6"\n- article [ref=s1e700]:\n    - link "tech" [ref=s1e701]:\n        - /url: /tech-or-idea\n    - link "😊 【超簡単】1枚の画像と音声から口パク動画を作りたい！【EchoMimic】 2024/08/25 いいねされた数 6" [ref=s1e702]:\n        - /url: /asap/articles/db1ed88f291b98\n        - text: 😊\n        - heading "【超簡単】1枚の画像と音声から口パク動画を作りたい！【EchoMimic】" [level=3] [ref=s1e706]\n        - time [ref=s1e709]: 2024/08/25\n        - img "いいねされた数" [ref=s1e711]\n        - text: "6"\n- article [ref=s1e713]:\n    - link "tech" [ref=s1e714]:\n        - /url: /tech-or-idea\n    - link "✍️ 【Next.js】生成AIによる「お絵描きサポートWebアプリ」を作りたい！【全くわからない人】 2024/08/20 いいねされた数 8" [ref=s1e715]:\n        - /url: /asap/articles/8f51ffd4fc29b8\n        - text: ✍️\n        - heading "【Next.js】生成AIによる「お絵描きサポートWebアプリ」を作りたい！【全くわからない人】" [level=3]\n          [ref=s1e719]\n        - time [ref=s1e722]: 2024/08/20\n        - img "いいねされた数" [ref=s1e724]\n        - text: "8"\n- article [ref=s1e726]:\n    - link "tech" [ref=s1e727]:\n        - /url: /tech-or-idea\n    - link "🔥 GPT-SoVITS ver2を学習する【WebUI使わない】 2024/08/15 いいねされた数 8" [ref=s1e728]:\n        - /url: /asap/articles/e64c6fce28e0f0\n        - text: 🔥\n        - heading "GPT-SoVITS ver2を学習する【WebUI使わない】" [level=3] [ref=s1e732]\n        - time [ref=s1e735]: 2024/08/15\n        - img "いいねされた数" [ref=s1e737]\n        - text: "8"\n- article [ref=s1e739]:\n    - link "tech" [ref=s1e740]:\n        - /url: /tech-or-idea\n    - link "🔥 GPT-soVITS ver2をモジュールとして動かす 2024/08/08 いいねされた数 9" [ref=s1e741]:\n        - /url: /asap/articles/964a7d863d8896\n        - text: 🔥\n        - heading "GPT-soVITS ver2をモジュールとして動かす" [level=3] [ref=s1e745]\n        - time [ref=s1e748]: 2024/08/08\n        - img "いいねされた数" [ref=s1e750]\n        - text: "9"\n- article [ref=s1e752]:\n    - link "tech" [ref=s1e753]:\n        - /url: /tech-or-idea\n    - link "💯 拡散モデルの理論って面白いよね 2024/08/07 いいねされた数 87" [ref=s1e754]:\n        - /url: /asap/articles/4092ab60570b05\n        - text: 💯\n        - heading "拡散モデルの理論って面白いよね" [level=3] [ref=s1e758]\n        - time [ref=s1e761]: 2024/08/07\n        - img "いいねされた数" [ref=s1e763]\n        - text: "87"\n- article [ref=s1e765]:\n    - link "tech" [ref=s1e766]:\n        - /url: /tech-or-idea\n    - link "🙆\u200d♀️ 画像生成AIの生成途中経過を可視化したい！【Diffusers】 2024/08/06 いいねされた数 4" [ref=s1e767]:\n        - /url: /asap/articles/7940b17be86da7\n        - text: 🙆\u200d♀️\n        - heading "画像生成AIの生成途中経過を可視化したい！【Diffusers】" [level=3] [ref=s1e771]\n        - time [ref=s1e774]: 2024/08/06\n        - img "いいねされた数" [ref=s1e776]\n        - text: "4"\n- article [ref=s1e778]:\n    - link "tech" [ref=s1e779]:\n        - /url: /tech-or-idea\n    - link "🚀 生成AIをGoogle Colabで簡単に 【Part10 SDXL+ANIMAGINE +ControlNet+LoRA編（全部載せ）】 2024/08/03 いいねされた数 3" [ref=s1e780]:\n        - /url: /asap/articles/aa856819e2a722\n        - text: 🚀\n        - heading "生成AIをGoogle Colabで簡単に 【Part10 SDXL+ANIMAGINE\n          +ControlNet+LoRA編（全部載せ）】" [level=3] [ref=s1e784]\n        - time [ref=s1e787]: 2024/08/03\n        - img "いいねされた数" [ref=s1e789]\n        - text: "3"\n- article [ref=s1e791]:\n    - link "tech" [ref=s1e792]:\n        - /url: /tech-or-idea\n    - link "😸 生成AIをGoogle Colaboratoryで簡単に 【Part6.5 画像生成AI SDXL+ANIMAGINE XL 3.1編】 2024/07/31 いいねされた数 3" [ref=s1e793]:\n        - /url: /asap/articles/f9ac2e83c820ff\n        - text: 😸\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part6.5 画像生成AI SDXL+ANIMAGINE\n          XL 3.1編】" [level=3] [ref=s1e797]\n        - time [ref=s1e800]: 2024/07/31\n        - img "いいねされた数" [ref=s1e802]\n        - text: "3"\n- article [ref=s1e804]:\n    - link "tech" [ref=s1e805]:\n        - /url: /tech-or-idea\n    - link "🎃 生成AIをGoogle Colaboratoryで簡単に 【Part9 画像生成AI SD3+ControlNet編】 2024/07/29 いいねされた数 3" [ref=s1e806]:\n        - /url: /asap/articles/53560859d273e4\n        - text: 🎃\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part9 画像生成AI SD3+ControlNet編】"\n          [level=3] [ref=s1e810]\n        - time [ref=s1e813]: 2024/07/29\n        - img "いいねされた数" [ref=s1e815]\n        - text: "3"\n- article [ref=s1e817]:\n    - link "tech" [ref=s1e818]:\n        - /url: /tech-or-idea\n    - link "✨ 生成AIをGoogle Colaboratoryで簡単に 【Part8 画像生成AI Stable Diffusion 3 Medium編】 2024/07/25 いいねされた数 4" [ref=s1e819]:\n        - /url: /asap/articles/6f6ca9aa0513d8\n        - text: ✨\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part8 画像生成AI Stable Diffusion 3\n          Medium編】" [level=3] [ref=s1e823]\n        - time [ref=s1e826]: 2024/07/25\n        - img "いいねされた数" [ref=s1e828]\n        - text: "4"\n- article [ref=s1e830]:\n    - link "tech" [ref=s1e831]:\n        - /url: /tech-or-idea\n    - link "🌟 生成AIをGoogle Colaboratoryで簡単に 【Part7 画像生成AI SDXL Turbo編】 2024/07/23 いいねされた数 2" [ref=s1e832]:\n        - /url: /asap/articles/ed58a6899c1c26\n        - text: 🌟\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part7 画像生成AI SDXL Turbo編】"\n          [level=3] [ref=s1e836]\n        - time [ref=s1e839]: 2024/07/23\n        - img "いいねされた数" [ref=s1e841]\n        - text: "2"\n- article [ref=s1e843]:\n    - link "tech" [ref=s1e844]:\n        - /url: /tech-or-idea\n    - link "🌟 生成AIをGoogle Colaboratoryで簡単に 【Part6 画像生成AI Stable Diffusion XL編】 2024/07/19 いいねされた数 4" [ref=s1e845]:\n        - /url: /asap/articles/e4791be54f90ce\n        - text: 🌟\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part6 画像生成AI Stable Diffusion\n          XL編】" [level=3] [ref=s1e849]\n        - time [ref=s1e852]: 2024/07/19\n        - img "いいねされた数" [ref=s1e854]\n        - text: "4"\n- article [ref=s1e856]:\n    - link "tech" [ref=s1e857]:\n        - /url: /tech-or-idea\n    - link "🎤 生成AIをローカルで簡単に 【Part5.5 faster-whisper+マイク録音編】 2024/07/16 いいねされた数 7" [ref=s1e858]:\n        - /url: /asap/articles/2c0d421e68ef16\n        - text: 🎤\n        - heading "生成AIをローカルで簡単に 【Part5.5 faster-whisper+マイク録音編】" [level=3]\n          [ref=s1e862]\n        - time [ref=s1e865]: 2024/07/16\n        - img "いいねされた数" [ref=s1e867]\n        - text: "7"\n- article [ref=s1e869]:\n    - link "tech" [ref=s1e870]:\n        - /url: /tech-or-idea\n    - link "📖 生成AIをGoogle Colaboratoryで簡単に 【Part5 音声認識AI faster-whisper編】 2024/07/13 いいねされた数 2" [ref=s1e871]:\n        - /url: /asap/articles/ba8fcb1880165e\n        - text: 📖\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part5 音声認識AI faster-whisper編】"\n          [level=3] [ref=s1e875]\n        - time [ref=s1e878]: 2024/07/13\n        - img "いいねされた数" [ref=s1e880]\n        - text: "2"\n- article [ref=s1e882]:\n    - link "tech" [ref=s1e883]:\n        - /url: /tech-or-idea\n    - link "😸 生成AIをGoogle Colaboratoryで簡単に 【Part4 音声認識AI Whisper API編】 2024/07/11 いいねされた数 2" [ref=s1e884]:\n        - /url: /asap/articles/ecaa5631e42195\n        - text: 😸\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part4 音声認識AI Whisper API編】"\n          [level=3] [ref=s1e888]\n        - time [ref=s1e891]: 2024/07/11\n        - img "いいねされた数" [ref=s1e893]\n        - text: "2"\n- article [ref=s1e895]:\n    - link "tech" [ref=s1e896]:\n        - /url: /tech-or-idea\n    - link "😊 生成AIをGoogle Colaboratoryで簡単に 【Part3 テキスト生成 ローカルLLM編】 2024/07/08 いいねされた数 4" [ref=s1e897]:\n        - /url: /asap/articles/8c3d5164d14f6e\n        - text: 😊\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part3 テキスト生成 ローカルLLM編】"\n          [level=3] [ref=s1e901]\n        - time [ref=s1e904]: 2024/07/08\n        - img "いいねされた数" [ref=s1e906]\n        - text: "4"\n- article [ref=s1e908]:\n    - link "tech" [ref=s1e909]:\n        - /url: /tech-or-idea\n    - link "🐷 生成AIをGoogle Colaboratoryで簡単に 【Part2 テキスト生成ChatGPT API編】 2024/07/05 いいねされた数 2" [ref=s1e910]:\n        - /url: /asap/articles/f410dd5d407652\n        - text: 🐷\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part2 テキスト生成ChatGPT API編】"\n          [level=3] [ref=s1e914]\n        - time [ref=s1e917]: 2024/07/05\n        - img "いいねされた数" [ref=s1e919]\n        - text: "2"\n- article [ref=s1e921]:\n    - link "tech" [ref=s1e922]:\n        - /url: /tech-or-idea\n    - link "🗂 生成AIをGoogle Colaboratoryで簡単に 【Part1 合成音声Style-Bert-VITS2編】 2024/07/02 いいねされた数 3" [ref=s1e923]:\n        - /url: /asap/articles/7bdb20dcb5cc56\n        - text: 🗂\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part1 合成音声Style-Bert-VITS2編】"\n          [level=3] [ref=s1e927]\n        - time [ref=s1e930]: 2024/07/02\n        - img "いいねされた数" [ref=s1e932]\n        - text: "3"\n- article [ref=s1e934]:\n    - link "tech" [ref=s1e935]:\n        - /url: /tech-or-idea\n    - link "👏 Remdisの環境構築で沼った話（＋使い方の紹介） 2024/06/26 いいねされた数 14" [ref=s1e936]:\n        - /url: /asap/articles/101379d8951b08\n        - text: 👏\n        - heading "Remdisの環境構築で沼った話（＋使い方の紹介）" [level=3] [ref=s1e940]\n        - time [ref=s1e943]: 2024/06/26\n        - img "いいねされた数" [ref=s1e945]\n        - text: "14"\n- article [ref=s1e947]:\n    - link "tech" [ref=s1e948]:\n        - /url: /tech-or-idea\n    - link "🚀 Style-Bert-VITS2の使い方メモ 2024/06/17 いいねされた数 34" [ref=s1e949]:\n        - /url: /asap/articles/f8c0621cdd74cc\n        - text: 🚀\n        - heading "Style-Bert-VITS2の使い方メモ" [level=3] [ref=s1e953]\n        - time [ref=s1e956]: 2024/06/17\n        - img "いいねされた数" [ref=s1e958]\n        - text: "34"\n- article [ref=s1e960]:\n    - link "tech" [ref=s1e961]:\n        - /url: /tech-or-idea\n    - link "💭 Style-Bert-VITS2のVoice Conversion機能の復活に挑戦してみた（前編） 2024/06/09 いいねされた数 9" [ref=s1e962]:\n        - /url: /asap/articles/df2c610d1389e4\n        - text: 💭\n        - heading "Style-Bert-VITS2のVoice Conversion機能の復活に挑戦してみた（前編）" [level=3]\n          [ref=s1e966]\n        - time [ref=s1e969]: 2024/06/09\n        - img "いいねされた数" [ref=s1e971]\n        - text: "9"\n- article [ref=s1e973]:\n    - link "tech" [ref=s1e974]:\n        - /url: /tech-or-idea\n    - link "😸 OpenAI Assistants API v2を色々いじってみた（使い方）（File Search編） 2024/05/28 いいねされた数 10" [ref=s1e975]:\n        - /url: /asap/articles/5c2f8ffe2a46ce\n        - text: 😸\n        - heading "OpenAI Assistants API v2を色々いじってみた（使い方）（File Search編）"\n          [level=3] [ref=s1e979]\n        - time [ref=s1e982]: 2024/05/28\n        - img "いいねされた数" [ref=s1e984]\n        - text: "10"\n- article [ref=s1e986]:\n    - link "tech" [ref=s1e987]:\n        - /url: /tech-or-idea\n    - link "👻 OpenAI Assistants API v2を色々いじってみた（使い方）（code interpreter編） 2024/05/27 いいねされた数 5" [ref=s1e988]:\n        - /url: /asap/articles/b0ce16ea1832ac\n        - text: 👻\n        - heading "OpenAI Assistants API v2を色々いじってみた（使い方）（code interpreter編）"\n          [level=3] [ref=s1e992]\n        - time [ref=s1e995]: 2024/05/27\n        - img "いいねされた数" [ref=s1e997]\n        - text: "5"\n- article [ref=s1e999]:\n    - link "tech" [ref=s1e1000]:\n        - /url: /tech-or-idea\n    - link "😑 【自分用】「AI後輩ちゃん」の個人的つまづきポイントまとめ 2024/05/09 いいねされた数 3" [ref=s1e1001]:\n        - /url: /asap/articles/98b04c5ea82bd9\n        - text: 😑\n        - heading "【自分用】「AI後輩ちゃん」の個人的つまづきポイントまとめ" [level=3] [ref=s1e1005]\n        - time [ref=s1e1008]: 2024/05/09\n        - img "いいねされた数" [ref=s1e1010]\n        - text: "3"\n- article [ref=s1e1012]:\n    - link "tech" [ref=s1e1013]:\n        - /url: /tech-or-idea\n    - link "😍 音声対話システム「AI後輩ちゃん」を作ってみた 2024/05/04 いいねされた数 17" [ref=s1e1014]:\n        - /url: /asap/articles/5b1b7553fcaa76\n        - text: 😍\n        - heading "音声対話システム「AI後輩ちゃん」を作ってみた" [level=3] [ref=s1e1018]\n        - time [ref=s1e1021]: 2024/05/04\n        - img "いいねされた数" [ref=s1e1023]\n        - text: "17"\n- contentinfo [ref=s1e1026]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1030]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1031]\n    - paragraph [ref=s1e1039]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e1041]:\n        - heading "About" [level=4] [ref=s1e1042]\n        - list [ref=s1e1043]:\n            - listitem [ref=s1e1044]:\n                - link "Zennについて" [ref=s1e1046]:\n                    - /url: /about\n            - listitem [ref=s1e1047]:\n                - link "運営会社" [ref=s1e1048]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e1049]:\n                - link "お知らせ・リリース" [ref=s1e1050]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e1051]:\n                - link "イベント" [ref=s1e1053]:\n                    - /url: /events\n    - navigation [ref=s1e1054]:\n        - heading "Guides" [level=4] [ref=s1e1055]\n        - list [ref=s1e1056]:\n            - listitem [ref=s1e1057]:\n                - link "使い方" [ref=s1e1059]:\n                    - /url: /manual\n            - listitem [ref=s1e1060]:\n                - link "法人向けメニュー" [ref=s1e1062]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e1064]:\n                - link "Publication / Pro" [ref=s1e1066]:\n                    - /url: /publications\n            - listitem [ref=s1e1067]:\n                - link "よくある質問" [ref=s1e1069]:\n                    - /url: /faq\n    - navigation [ref=s1e1070]:\n        - heading "Links" [level=4] [ref=s1e1071]\n        - list [ref=s1e1072]:\n            - listitem [ref=s1e1073]:\n                - link "X(Twitter)" [ref=s1e1074]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e1075]:\n                - link "GitHub" [ref=s1e1076]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e1077]:\n                - link "メディアキット" [ref=s1e1079]:\n                    - /url: /mediakit\n    - navigation [ref=s1e1080]:\n        - heading "Legal" [level=4] [ref=s1e1081]\n        - list [ref=s1e1082]:\n            - listitem [ref=s1e1083]:\n                - link "利用規約" [ref=s1e1085]:\n                    - /url: /terms\n            - listitem [ref=s1e1086]:\n                - link "プライバシーポリシー" [ref=s1e1088]:\n                    - /url: /privacy\n            - listitem [ref=s1e1089]:\n                - link "特商法表記" [ref=s1e1091]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e1094]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e1095]\n- alert [ref=s1e1114]\n```\n', name='browser_navigate', tool_call_id='58fad86e-0b01-4fc6-a8cc-57deb5c48b41'), AIMessage(content='`https://zenn.dev/asap`にアクセスしました。\n\n執筆者の記事一覧から、いいねの数が多い記事を探します。\n\n- 「DeepSeek-R1の論文読んだ？【勉強になるよ】」 が921いいね\n- 「拡散モデルの理論って面白いよね」 が87いいね\n- 「Open AI Realtime APIのPythonサンプルコードを作成」 が79いいね\n- 「📏 【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】」 が50いいね\n- 「🙌 完全自律型AIエージェントOSS【LangManus】完全に理解した！」 が48いいね\n\n最もいいねが多い記事は「DeepSeek-R1の論文読んだ？【勉強になるよ】」です。\n記事の中身を取得します。', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap/articles/34237ad87f8511"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-27104099-cd7e-4de1-8c89-51060ed9c6f1-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap/articles/34237ad87f8511'}, 'id': 'd85f8505-9c2a-4960-b17f-9ef88d983d9d', 'type': 'tool_call'}], usage_metadata={'input_tokens': 17995, 'output_tokens': 216, 'total_tokens': 18211, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap/articles/34237ad87f8511\n\n- Page URL: https://zenn.dev/asap/articles/34237ad87f8511\n- Page Title: DeepSeek-R1の論文読んだ？【勉強になるよ】\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- article [ref=s1e32]:\n    - text: 📑\n    - heading "DeepSeek-R1の論文読んだ？【勉強になるよ】" [level=1] [ref=s1e38]\n    - text: 2025/01/27に公開\n    - tooltip "更新日" [ref=s1e45]:\n        - img [ref=s1e46]\n        - text: 2025/02/22 更新日\n    - link "12件" [ref=s1e51]:\n        - /url: "#discuss"\n        - img [ref=s1e52]\n        - text: 12件\n    - link "強化学習" [ref=s1e63]:\n        - /url: /topics/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92\n        - img [ref=s1e65]\n        - text: 強化学習\n    - link "蒸留" [ref=s1e67]:\n        - /url: /topics/%E8%92%B8%E7%95%99\n        - img [ref=s1e69]\n        - text: 蒸留\n    - link "LLM" [ref=s1e71]:\n        - /url: /topics/llm\n        - img [ref=s1e73]\n        - text: LLM\n    - link "生成 AI" [ref=s1e75]:\n        - /url: /topics/%E7%94%9F%E6%88%90ai\n        - img [ref=s1e77]\n        - text: 生成 AI\n    - link "DeepSeek" [ref=s1e79]:\n        - /url: /topics/deepseek\n        - img [ref=s1e81]\n        - text: DeepSeek\n    - link "tech" [ref=s1e83]:\n        - /url: /tech-or-idea\n        - img [ref=s1e85]\n        - text: tech\n    - heading "はじめに" [level=1] [ref=s1e89]\n    - paragraph [ref=s1e90]: タイトル通りDeepSeek-R1の論文を読んだので、少しでもわかりやすく説明できればと思います。\n        まだ論文を読んでいない方は、ぜひ原文も読んでみてください！\n    - paragraph [ref=s1e92]:\n        - text: 難しい数式や概念がほとんど使われていないため、論文の中では全然読みやすい方だと思います。読み物として読めるレベルです。\n        - iframe [ref=s1e94]:\n            - text: Loading...\n            - alert [ref=f1s1e8]\n    - paragraph [ref=s1e95]: とはいえ、本記事では細かい部分など間違っている部分もあるかもしれないので、ぜひ誤りがあったら、こそっと教えてください！\n    - complementary [ref=s1e96]:\n        - text: "!"\n        - paragraph [ref=s1e99]: 実は、本論文を解説する上で、著者らが公開しているもう一つの論文が大きく関連しています。 そちらの論文の内容も若干解説を入れております。\n        - paragraph [ref=s1e101]:\n            - text: 「\n            - \'link "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models" [ref=s1e102]\':\n                - /url: https://arxiv.org/html/2402.03300v3\n            - text: 」\n        - paragraph [ref=s1e103]: 本記事でも少し触れますが、上記の論文も非常に勉強になるため、ぜひ読んでみてください。\n    - complementary [ref=s1e104]:\n        - text: "!"\n        - paragraph [ref=s1e107]:\n            - text: 本記事は、\n            - link "DeepSeek-R1の論文" [ref=s1e108]:\n                - /url: https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\n            - text: と\n            - link "DeepSeekMathの論文" [ref=s1e109]:\n                - /url: https://arxiv.org/html/2402.03300v3\n            - text: を読んだ私の理解をもとに記載しています。\n        - paragraph [ref=s1e110]: 本論文で使われている技術に関しては、ある程度の知識を持っているので、大きくは外していないとは思いますが、私の主観も入っている部分もありますので、ご了承ください。\n    - paragraph [ref=s1e111]:\n        - text: また、DeepSeek-R1の論文が公開される前に、小型モデルに対して同様の実験（強化学習）をしていたグループがあるようです。\n            そちらのレポートは下記になります。\n        - iframe [ref=s1e114]:\n            - \'link "7B Model and 8K Examples: Emerging Reasoning with Reinforcement Learning is Both Effective and Effic A replication of DeepSeek-R1 training on small models with limited data hkust-nlp.notion.site favicon image hkust-nlp.notion.site hkust-nlp.notion.site thumbnail image" [ref=f2s1e6]\':\n                - /url: https://hkust-nlp.notion.site/simplerl-reason\n                - \'heading "7B Model and 8K Examples: Emerging Reasoning with\n                  Reinforcement Learning is Both Effective and Effic" [level=1]\n                  [ref=f2s1e8]\'\n                - text: A replication of DeepSeek-R1 training on small models with limited data\n                - img "hkust-nlp.notion.site favicon image" [ref=f2s1e11]\n                - text: hkust-nlp.notion.site\n                - img "hkust-nlp.notion.site thumbnail image" [ref=f2s1e13]\n            - alert [ref=f2s1e15]\n    - paragraph [ref=s1e115]: 意図せず、DeepSeek-R1-Zeroの再現実験のようなレポートになっていますが、レポートの著者はDeepSeek-R1論文の公開前から実験していると主張しています。\n        こちらも非常に興味深かったため紹介です。\n    - heading "本論文の興味深いところ" [level=1] [ref=s1e117]\n    - paragraph [ref=s1e118]: 本論文は、大きく分けて3つの構成でできています\n    - list [ref=s1e119]:\n        - listitem [ref=s1e120]:\n            - strong [ref=s1e121]: 強化学習による思考能力の強化\n            - list [ref=s1e122]:\n                - listitem [ref=s1e123]: LLM（DeepSeek-V3-Base）に対して「強化学習のみ」を適用させたところ、強力な思考能力を獲得した「DeepSeek-R1-Zero」が作成されました。\n                - listitem [ref=s1e124]: 本論文のメインストリームであり、非常に興味深い示唆が得られる部分です。\n        - listitem [ref=s1e125]:\n            - strong [ref=s1e126]: SFTによるさらなる強化\n            - list [ref=s1e127]:\n                - listitem [ref=s1e128]: DeepSeek-V3-Baseに対して、コールドスタート問題を解消するために、CoT（Chain of\n                    Thought）データセットによるSFT（教師付きファインチューニング）を実施\n                - listitem [ref=s1e129]: その後、強化学習を適用させることで、o1レベルの性能を獲得した「DeepSeek-R1」が作成されました。\n                - listitem [ref=s1e130]: 非常に強力なLLMを獲得するまでの反復学習などの試行錯誤などを学ぶことができます。\n        - listitem [ref=s1e131]:\n            - strong [ref=s1e132]: 蒸留による小型モデルの性能強化\n            - list [ref=s1e133]:\n                - listitem [ref=s1e134]: 上記で作成されたDeepSeek-R1により、他社の小型オープンソースLLMを蒸留しました。\n                - listitem [ref=s1e135]: その結果、元の性能を大幅に上回る、非常に高い性能の小型LLMが得られました。\n    - paragraph [ref=s1e136]: その中でも私が特に面白いなと思った要素は下記の要素になります\n    - list [ref=s1e137]:\n        - listitem [ref=s1e138]:\n            - strong [ref=s1e139]: 強化学習手法「GRPO」すごい！\n        - listitem [ref=s1e140]:\n            - strong [ref=s1e141]: ルールベース報酬で思考能力が向上するのがすごい！\n        - listitem [ref=s1e142]:\n            - strong [ref=s1e143]: aha moment（ハッとする瞬間）面白い！\n        - listitem [ref=s1e144]:\n            - strong [ref=s1e145]: 蒸留vs強化学習の比較実験から重要な示唆が得られた！\n    - heading "本論文の流れ" [level=1] [ref=s1e146]\n    - heading "前提のモデル構造" [level=3] [ref=s1e147]\n    - paragraph [ref=s1e148]: そもそも、大前提として、この論文は「DeepSeek-V3-Base」という強力なLLMの性能を改善する論文です。\n        これ単体でも、GPT-4oレベルの能力を持っているオープンソースLLMになります。\n    - paragraph [ref=s1e150]: 本論文のモデルアーキテクチャは「DeepSeek-V3-Base」と完全に同じものを利用します。\n        （つまりモデルアーキテクチャをかえずに、学習方法を工夫してLLMの性能を上げる挑戦になります）\n    - heading "DeepSeek-R1-Zeroを強化学習を用いて作る" [level=3] [ref=s1e152]\n    - paragraph [ref=s1e153]: その上で、まずは、「DeepSeek-V3-Base」に対して、大規模な強化学習のみを適応させることで、強化学習の可能性について探究する話が始まります。\n        結果として、「DeepSeek-R1-Zero」ができます。\n        「DeepSeek-R1-Zero」は、SFT（教師付きファインチューニング）をしなくても、強化学習のみで圧倒的な思考能力を取得できることを示す興味深いモデルです。\n    - heading "DeepSeek-R1をSFT+強化学習を用いて作る" [level=3] [ref=s1e156]\n    - paragraph [ref=s1e157]: その後、あまりにも「DeepSeek-R1-Zero」の性能が良かったことから、さらなる探究が進められました。\n    - paragraph [ref=s1e158]: この時点で、「DeepSeek-R1-Zero」には下記の課題がありました。\n    - list [ref=s1e159]:\n        - listitem [ref=s1e160]: 学習初期段階にて、強化学習の学習が不安定であり、収束に時間がかかる（コールドスタート問題）\n        - listitem [ref=s1e161]:\n            - text: 思考能力は高いが、言語が混在（日本語で話していたら急に中国語になるなど）し、人間が読みにくい\n            - list [ref=s1e162]:\n                - listitem [ref=s1e163]: 結果、高度な思考が必要なタスクには強いが、一般的タスク（ロールプレイやライティングなど）には使いにくいという評価\n    - paragraph [ref=s1e164]: そこで、「DeepSeek-V3-Base」に対して、少量のCoT（Chain of\n        Thought）データセットを用いて、SFT（教師付き微調整）を行います。このデータは、著者らが収集・構築したものになります。\n        その後、SFT後のモデルに対して、大規模強化学習を行います。\n        このとき、「DeepSeek-R1-Zero」を作成した際の報酬に加え、「言語一貫性報酬」を導入して、上述した言語の不一致の問題を緩和させます。\n    - paragraph [ref=s1e167]: その後、得られたモデル（チェックポイント）を利用して、さらなるSFT（教師付き微調整）学習データを収集し、追加SFTを行います。\n        さらに、その後、改めて２段階目の大規模強化学習を実施することで、最終的に「DeepSeek-R1」が得られます。\n    - heading "小型モデルへの蒸留を試す" [level=3] [ref=s1e169]\n    - paragraph [ref=s1e170]: 最後に、小型モデルに対する「DeepSeek-R1」による蒸留が試されています。\n        その中で、小型モデルに対して、「DeepSeek-R1」による蒸留を実施するのと、「DeepSeek-R1-Zero」と同様の強化学習手法で思考能力を強化するのと、どちらが性能向上に寄与するのかを調査してくれています。\n    - paragraph [ref=s1e172]: これが非常に興味深いです。\n    - heading "DeepSeek-R1-Zeroを作るまで" [level=1] [ref=s1e173]\n    - paragraph [ref=s1e174]: では、ここから論文の詳細な解説に入っていきます。\n    - paragraph [ref=s1e175]: まずは、「DeepSeek-R1」の前段階である「DeepSeek-R1-Zero」を作るまでの手法を解説します。\n    - paragraph [ref=s1e176]:\n        - strong [ref=s1e177]: 前段階と侮るなかれ、「DeepSeek-R1-Zero」を作るまでの手法と示唆にこそ、本論文の貴重な示唆が含まれています。\n    - heading "元々のアーキテクチャ" [level=2] [ref=s1e178]\n    - paragraph [ref=s1e179]: 今回作る「DeepSeek-R1-Zero」や「DeepSeek-R1」は、新しいモデル構造を採用して、性能を向上させたわけではありません。\n    - paragraph [ref=s1e180]: 元々のアーキテクチャは「DeepSeek-V3-Base」というアーキテクチャになります。\n        「DeepSeek-V3-Base」は、681Bのモデルパラメータを持つ大規模言語モデルで、性能はGPT-4oに匹敵する非常に高性能モデルありながら、モデル重みが完全に公開されているオープンソースモデルです。\n    - paragraph [ref=s1e182]: 「DeepSeek-V3-Base」に対して、Instructionチューニングを施したchatモデルに関しては、APIでも公開されています。\n        非常に安価に使えるにも関わらず、GPT-4oに匹敵する性能を誇ることから、人気のモデルになっております。\n    - complementary [ref=s1e184]:\n        - text: "!"\n        - paragraph [ref=s1e187]: 安価なモデルではありますが、APIの提供元は中国の企業ですので、データの取り扱いなどは十分注意してください。\n            基本的には、API利用の際には、収集されて困るような情報は入力しないようにするのが無難です。\n            （オープンソースモデルをローカルで動かす場合は、おそらく心配ありません）\n    - paragraph [ref=s1e190]: 本論文のすごいところは、モデル構造を変えずに、「DeepSeek-V3-Base」に対する学習方法を工夫しただけで、OpenAI\n        o1に匹敵する性能のLLMを開発していることです。\n        具体的には、大規模強化学習をLLMに対して適用することで、「DeepSeek-R1-Zero」を開発します。\n    - heading "すごいぞ！強化学習" [level=2] [ref=s1e192]\n    - heading "強化学習って何？" [level=3] [ref=s1e193]\n    - paragraph [ref=s1e194]: そもそも「強化学習って何？」という方もいらっしゃると思うので、簡単に説明します。\n        （以前、スイカゲームを強化学習でプレイしたこともあるので、いずれ詳しい解説記事を作る予定ですが、ここではざっくり説明します。）\n    - paragraph [ref=s1e196]: 強化学習とは、一言で言えば「一連の行動の流れ（シークエンス）によって得られる、累積報酬の期待値を最大化する」ことを目指す学習手法です。\n        ある行動をとった結果がすぐにはわからず、時間が経ってから影響が現れるような問題に適しています。\n        専門的に言うと、「マルコフ決定過程（MDP）」が成立する問題で活用されます。\n    - heading "テトリスで考える" [level=3] [ref=s1e199]\n    - paragraph [ref=s1e200]: 例えば、「テトリス」を考えてみましょう。\n    - paragraph [ref=s1e201]: テトリスは、上から「ミノ」と呼ばれる、さまざまな形のブロックを落として、横一列を綺麗に埋めてブロックを消去するゲームです。\n    - paragraph [ref=s1e202]: 横一列がすべて揃うと、その列が消えて、「ポイント」を獲得できます。\n        逆に、列を消せずに、ブロックが一定の高さまで積み上がると「ゲームオーバ」になります。\n    - paragraph [ref=s1e204]: テトリスを強化学習で解こうとすると、「どうすれば最も多くのポイントを得られるか」をAIに学習させることが重要になります。\n        単純に列を揃えて消すだけではなく、「次の手」や「その次の手」を見据えて、より良い結果につながるようにミノを配置することも必要です。\n        しかし、通常の学習方法(よくある入出力を近似させる学習方法)では、目の前の結果だけに注目してしまい、将来的な高得点につながる手を学習させることは難しいです。\n    - paragraph [ref=s1e207]:\n        - strong [ref=s1e208]: そこで役立つのが「強化学習」です。\n    - paragraph [ref=s1e209]: 強化学習では、ゲームの最初から最後までの流れを通して、「どう行動すれば最終的に得られる得点を最大化できるか」を学習できます。\n        このように、途中の行動の良し悪しがすぐにはわからなくても、最終的なゴール（得点の最大化）が設定できる問題に対して、強化学習は非常に効果的です。\n    - paragraph [ref=s1e211]:\n        - img [ref=s1e212]\n    - group [ref=s1e213]: 補足\n    - heading "強化学習の事例「AlphaGo Zero」" [level=3] [ref=s1e215]\n    - paragraph [ref=s1e216]: さらに、強化学習の威力をお伝えする事例を記載します。\n    - paragraph [ref=s1e217]: 2015年にアメリカのDeepMind社（Googleの傘下）が「AlphaGo」というモデルを開発しました。これはAIが囲碁をプレイするモデルで、当時の世界王者をハンディキャップなしで破るという、非常に衝撃的なモデルでした。\n        （当時としては、非常にセンセーショナルな出来事だったため、記憶にある方もいらっしゃるかと思います）\n    - paragraph [ref=s1e219]: この「AlphaGo」は、深層学習と強化学習により学習されています。\n        プロ棋士の棋譜データで深層学習で学習させた上で、強化学習による自己対局（AI同士の対局）を実施したモデルです。\n    - paragraph [ref=s1e221]:\n        - text: さて、ここまでも十分すごいですが、\n        - strong [ref=s1e223]: さらにすごいのはここからです。\n    - paragraph [ref=s1e224]: DeepMind社は後継モデルとして「AlphaGo Zero」というモデルを開発しました。\n        このモデルは、プロ棋士の棋譜データを「使わず」に、完全に「強化学習のみ」で学習されたモデルです。\n        つまり、囲碁のルールだけを与え、あとは最初から最後までAI同士の自己対局のみで学習されています。\n        （この場合、対局に勝利したら報酬、敗北したら罰を与える形で強化学習をし、対局に勝利した時に取っていた手（行動）の選択確率を上げるように学習しています）\n    - paragraph [ref=s1e228]:\n        - strong [ref=s1e229]: そして、その「AlphaGo Zero」は「AlphaGo」に対して、100戦全勝で圧勝しました。\n    - paragraph [ref=s1e230]: AIが囲碁の学習をする際に、人間が持つ定石の情報などは不要だったわけです。\n    - paragraph [ref=s1e231]: これが強化学習の威力です。\n        余談ですが、今回の論文で作成する「DeepSeek-R1-Zero」も（「DeepSeek-V3-Base」をベースにはしますが）強化学習のみで学習されます。\n        名前に入っている「Zero」は同じく強化学習のみで開発された「AlphaGo Zero」のオマージュかなと思います。\n    - heading "そもそもLLMで強化学習とは？" [level=2] [ref=s1e234]\n    - paragraph [ref=s1e235]: 「なんかゲームみたいなものに、強化学習を使うとうまくいきそう」というのは理解していただけたかなと思います。\n    - paragraph [ref=s1e236]:\n        - text: では、LLMに対してはどうか、というのを解説していきます。\n        - strong [ref=s1e238]: 実は、LLMと強化学習は非常に相性がいいです。\n    - paragraph [ref=s1e239]: なぜなら、LLMは一度にすべての文章を出力するAIではなく、1tokenずつ出力するモデルであり、その文章が良い文章か悪い文章かは、すべてのtokenを出力してみないとわからないからです。\n    - paragraph [ref=s1e240]:\n        - text: テトリスでは、ミノを一つずつ落とし、一列が消えたタイミングで、ポイントを取得するという形で評価されます。\n        - img [ref=s1e242]\n    - paragraph [ref=s1e243]:\n        - text: LLMでも、同様にtokenを1tokenずつ出力し、一文全部が出力されたタイミングで、その文章の良し悪しを評価することができれば、強化学習と同じ枠組みを使うことができるというわけです。\n        - img [ref=s1e245]\n    - heading "報酬モデルを作成し、累積報酬を最大化" [level=3] [ref=s1e246]\n    - paragraph [ref=s1e247]:\n        - text: テトリスでは、ゲームシステムが勝手に点数をつけてくれますが、LLMにおいて文章の良し悪しを勝手に判断してくれる便利な機能はありません。そこで我々が、その判断モデルを作ってあげる必要があります。\n            そこで、ニューラルネットワークを利用して、LLMに入力されたプロンプト\n        - math [ref=s1e252]: x\n        - text: に対して、LLMが生成した文章\n        - math [ref=s1e259]: "y"\n        - text: の点数\n        - math [ref=s1e266]: r\n        - text: を出力するようなネットワークを「報酬モデル\n        - math [ref=s1e273]: r θ ( r ∣ y , x )\n        - text: 」として作成することを考えます。\n    - paragraph [ref=s1e286]:\n        - text: まず初めに、データセットとして、「プロンプト\n        - math [ref=s1e290]: x\n        - text: 」「好ましい回答\n        - math [ref=s1e297]: y +\n        - text: 」「好ましくない回答\n        - math [ref=s1e306]: y −\n        - text: 」の3つが組となっているデータセットを用意します。 報酬モデル\n        - math [ref=s1e316]: r θ ( r ∣ y , x )\n        - text: は、「プロンプト\n        - math [ref=s1e332]: x\n        - text: 」と「出力\n        - math [ref=s1e339]: "y"\n        - text: 」の両方を入力することで、報酬（点数）を出力するモデルです。\n    - paragraph [ref=s1e343]:\n        - text: そこで、好ましい回答の報酬推定値\n        - math [ref=s1e347]: r θ ( r ∣ y + , x )\n        - text: と、好ましくない回答の報酬推定値\n        - math [ref=s1e365]: r θ ( r ∣ y − , x )\n        - text: の差を一定以上にするような損失関数を用いて学習を行います。 すると、報酬モデル\n        - math [ref=s1e384]: r θ ( r ∣ y , x )\n        - text: は適切に、好ましい回答\n        - math [ref=s1e400]: y +\n        - text: には高い点数を、好ましくない回答\n        - math [ref=s1e409]: y −\n        - text: には低い点数をつけるようになります。\n    - paragraph [ref=s1e415]:\n        - text: 報酬モデル\n        - math [ref=s1e419]: r θ ( r ∣ y , x )\n        - text: か完成したら、強化学習をするための要素は整います。\n    - paragraph [ref=s1e432]:\n        - text: 状態\n        - math [ref=s1e436]: x\n        - text: は、LLMに入力される「プロンプト」で、LLMの出力が\n        - math [ref=s1e443]: "y"\n        - text: とします。 報酬\n        - math [ref=s1e451]: r\n        - text: は、報酬モデル\n        - math [ref=s1e458]: r θ ( r ∣ y , x )\n        - text: の出力値です。 LLMが全文出力したら、プロンプト\n        - math [ref=s1e475]: x\n        - text: と、LLMの出力\n        - math [ref=s1e482]: "y"\n        - text: を報酬モデルに与えることで、取得します。 方策は、LLMが出力する各tokenごとの確率分布自体です。 このLLM自体を「方策モデル\n        - math [ref=s1e491]: π ϕ ( y t ∣ x , y < t )\n        - text: 」と呼びます。\n    - complementary [ref=s1e510]:\n        - text: "!"\n        - paragraph [ref=s1e513]:\n            - text: LLM（\n            - math [ref=s1e517]: π ϕ ( y t ∣ x , y < t )\n            - text: ）はユーザが入力したプロンプト\n            - math [ref=s1e539]: x\n            - text: と、それまでにLLMが出力した内容\n            - math [ref=s1e546]: y < t\n            - text: をもとに、\n            - math [ref=s1e557]: t\n            - text: ステップ目のtoken\n            - math [ref=s1e564]: y t\n            - text: を出力します。\n        - paragraph [ref=s1e570]: これが、LLMは、「文章の次に出てくる単語を一つずつ予測するモデル」と言われる所以です。\n    - paragraph [ref=s1e571]: これにより、累積報酬を最大化するように方策モデル（LLM）を学習することで、LLMは好ましい回答を出力するようにネットワークを更新できます。\n    - paragraph [ref=s1e572]: これが、LLMを強化学習で学習するということです。 このような手法をRLHF（Reinforcement\n        learning from human feedback）と総称します。\n    - group [ref=s1e574]: 細かい話\n    - heading "つよつよ手法「PPO（Proximal Policy Optimization）」" [level=3] [ref=s1e576]\n    - paragraph [ref=s1e577]: ここまでで、「LLMと強化学習は相性が良い」こと、「強化学習では累積報酬の期待値（状態価値）を最大化することが重要」であることを話しました。\n    - paragraph [ref=s1e578]: さて、実際にLLMに強化学習を組み込む場合、しばしばPPO（Proximal Policy\n        Optimization）という手法が利用されます。\n        （なんなら、LLM以外で強化学習を実用する上でも、最初に選択肢になるベースライン的に強力な手法でもあります）\n    - paragraph [ref=s1e580]: 今回はPPOが主題ではないので、簡単に説明します。\n    - heading "PPOの簡易的な説明" [level=4] [ref=s1e581]\n    - paragraph [ref=s1e582]:\n        - text: PPOでは、行動を確率的に出力する方策モデル（LLM）と、現在の状態価値を推定する状態価値モデルを用意します。 状態価値\n        - math [ref=s1e587]: V t\n        - text: とは、「累積報酬の期待値」として定義できます。 したがって、現時点から、方策に従って行動をした場合の、将来的な報酬総和の期待値が「状態価値\n        - math [ref=s1e597]: V t\n        - text: 」です。\n    - paragraph [ref=s1e603]:\n        - text: 強化学習の枠組みで累積報酬\n        - math [ref=s1e607]: G t\n        - text: が計算されるため、その期待値である状態価値\n        - math [ref=s1e616]: V t\n        - text: も学習が可能です。 一方で、方策モデルを学習する際に、Advantage\n        - math [ref=s1e626]: A t\n        - text: というものを利用します。\n    - paragraph [ref=s1e632]:\n        - text: Advantage\n        - math [ref=s1e636]: A t\n        - text: というのは下記の式で表されます。\n    - math [ref=s1e648]: A t = G t − V t ( v t ∣ y < t , x )\n    - paragraph [ref=s1e675]:\n        - text: 状態価値\n        - math [ref=s1e679]: V t ( v t ∣ y < t , x )\n        - text: は、累積報酬\n        - math [ref=s1e701]: G t\n        - text: の期待値（大雑把にいうと平均）になるように学習されているという前提があるため、Advantage\n        - math [ref=s1e710]: A t\n        - text: が正になるということは、今回LLMが出力したtoken\n        - math [ref=s1e719]: y t\n        - text: は、将来的に高い報酬を得ることができる「良い出力であった」と判断できます。\n    - paragraph [ref=s1e725]:\n        - text: したがって、方策モデル\n        - math [ref=s1e729]: π ϕ ( y t ∣ x , y < t )\n        - text: はAdvantage\n        - math [ref=s1e751]: A t\n        - text: を最大化するように学習します。\n    - paragraph [ref=s1e757]:\n        - text: Advantage\n        - math [ref=s1e761]: A t\n        - text: を考えずに、単純に累積報酬\n        - math [ref=s1e770]: G t\n        - text: を最大化するように考えることもできます。（むしろそれが初期の強化学習です）\n    - complementary [ref=s1e776]:\n        - text: "!"\n        - paragraph [ref=s1e779]: （2025年2月15日 追記） この辺りの表現は、わかりやすくするために若干誤解を招く表現になっています。\n            下に「細かい話」として補足説明を書きました。\n    - paragraph [ref=s1e782]: しかしその場合、そもそもどんな行動を取っても報酬が高くなる状態と、どんな行動を取っても報酬が低くなってしまう状態において、勾配のスケールがばらつくため、学習の安定性が低くなります。\n    - paragraph [ref=s1e783]:\n        - text: PPOでは、Advantage\n        - math [ref=s1e787]: A t\n        - text: を考えることにより、出力したtokenが平均よりも良いか悪いかだけで学習をすることができ、状態ごとの報酬の差をモデルが考慮する必要がなくなります。\n    - complementary [ref=s1e793]:\n        - text: "!"\n        - paragraph [ref=s1e796]: 本来、PPOの貢献は、あるパラメータ更新時において、更新前と更新後の確率分布が大きく変わらないような制約を、少ない計算量で加えることです。\n            ただ、この話は、DeepSeek-R1-Zeroにおける強化学習を理解するためには、必ずしも重要ではありません。\n            したがって、今回の記事では、必要な部分だけに絞り説明をしました。\n    - group [ref=s1e799]: 細かい話（追記）\n    - heading "著者発案「GRPO（Group Relative Policy Optimization）」" [level=3]\n      [ref=s1e801]\n    - paragraph [ref=s1e802]: さて、PPOは強化学習の手法において、高い性能を発揮しますが、LLMに適用することを考えると、状態価値モデルを別途学習しないといけないという点において、計算量的に大変な手法です。\n    - paragraph [ref=s1e803]: "一方で、PPOの状態価値モデルがやっていたことは、Advantage A_t(a_t|y_t,\n        y_{<t},x) を計算し、今回出力したtokenが、平均よりも良いのか悪いのかを判断することです。"\n    - paragraph [ref=s1e806]: "そこで、LLMにおいては、わざわざ状態価値モデルを用意して平均を計算しなくても、実際に複数文章を生成して、そ\\\n        れぞれの報酬を平均することで、Advantage A_t(a_t|y_t, y_{<t},x) を計算しようと考えました。\n        こうすることで、状態価値モデルなしで、Advantage A_t(a_t|y_t, y_{<t},x)\n        を計算できるため、計算量を大幅に削減できることができます。"\n    - paragraph [ref=s1e812]:\n        - text: この手法は「GRPO（Group Relative Policy Optimization）」と呼びます。\n            本論文の著者が、過去に公開した論文で提案した手法です。\n        - iframe [ref=s1e815]: <could not take iframe snapshot>\n    - paragraph [ref=s1e816]:\n        - text: 下記が、論文中で提示されている「PPO」と「GRPO」の違いを説明する図です。\n        - img [ref=s1e818]\n        - text: "このように状態価値モデル（Value model）なしに、Advantage A_t(a_t|y_t, y_{<t},x)\n            が計算できていることがわかります。"\n    - complementary [ref=s1e821]:\n        - text: "!"\n        - paragraph [ref=s1e824]: 本記事でのアルファベットの付け方と、上記の図のアルファベットの付け方が異なることに注意してください。\n        - paragraph [ref=s1e825]: 上記の図（GRPO）では、 プロンプトを q 、LLMの各出力文章を o_i （ i\n            はそれぞれの文章単位でのインデックス）、各文章ごとの報酬モデルの出力を r_i 、報酬 r_i\n            の平均から計算された各文章ごとのAdvantageを A_i としています。\n        - paragraph [ref=s1e839]: 本記事での添え字は、token単位での添え字を採用しています。 例えば A_t なら t token目でのAdvantageです。\n        - paragraph [ref=s1e845]: 上記の図では、文章単位での添え字です。 例えば、 A_i なら、 i 文章目でのAdvantageとなります。\n            従って A_i はベクトルとなっており、中にtokenごとのAdvantageの値も格納されているはずです。\n    - heading "報酬モデルを作らずルールベースで実施" [level=3] [ref=s1e854]\n    - paragraph [ref=s1e855]:\n        - text: 改めて、LLMに強化学習を適用するために必要なものをまとめます。 （下記の図のGRPOの手法を利用します）\n        - img [ref=s1e858]\n    - paragraph [ref=s1e859]: "まず、大前提として方策モデル \\\\pi_\\\\phi(y_t|x,y_{<t}) （LLM）は必要です。"\n    - paragraph [ref=s1e862]: "上の図中における「Reference Model」とは、「強化学習をする前」の方策モデル\n        \\\\pi_{ref}(y_t|x,y_{<t}) （つまり、DeepSeek-V3-Base）です。 また、「Policy\n        Model」というのは、「強化学習中」の現時点での方策モデル \\\\pi_\\\\phi(y_t|x,y_{<t}) を指しています。"\n    - paragraph [ref=s1e868]: そして、「プロンプト」「良い回答」「悪い回答」の3つを組とするデータセットで学習された、報酬モデル\n        r_\\theta(r|y,x) （Reward Model）が必要です。 報酬モデル r_\\theta(r|y,x) は「プロンプト x\n        」と「LLMの出力回答 y 」を入力することで、「報酬 r 」を出力します。\n    - paragraph [ref=s1e880]: この二つがあれば、著者が提案したGRPOという手法で、強化学習が可能になります。 （GRPOでは、状態価値モデルは不要です）\n    - paragraph [ref=s1e882]:\n        - text: しかしながら著者らは、さらに驚くべきことに、「DeepSeek-R1-Zero」を学習する際に、\n        - strong [ref=s1e884]: 報酬モデルを使わずに、ルールベースで報酬を計算しています。\n        - text: それにより、圧倒的な計算効率を達成しています。\n    - paragraph [ref=s1e886]: 著者らが設定しているルールは下記の二つです。\n    - list [ref=s1e887]:\n        - listitem [ref=s1e888]:\n            - strong [ref=s1e889]: 精度報酬\n            - list [ref=s1e890]:\n                - listitem [ref=s1e891]: 応答が正しいかどうかを評価する\n                - listitem [ref=s1e892]: 例えば、数学の問題であれば、最終的な回答が答えと合っているかどうかで報酬を渡す。\n        - listitem [ref=s1e893]:\n            - strong [ref=s1e894]: フォーマット報酬\n            - list [ref=s1e895]:\n                - listitem [ref=s1e896]:\n                    - text: 思考プロセスを\n                    - code [ref=s1e897]: <think>\n                    - text: と\n                    - code [ref=s1e898]: </think>\n                    - text: タグの間に置くことをモデルに強制し、合っていない場合はペナルティを課す。\n    - paragraph [ref=s1e899]:\n        - text: このたった2つの報酬のみをルールベースで設定しており、\n        - strong [ref=s1e901]: ニューラルネットワークベースの報酬モデルは利用していません\n    - paragraph [ref=s1e902]: ニューラルネットワークの報酬モデルを利用せずに、ルールベース報酬だけで強化学習をすることにより、報酬モデル分の計算量を削減しています。\n    - heading "この報酬設計で良い理由" [level=4] [ref=s1e903]\n    - paragraph [ref=s1e904]: フォーマット報酬は、モデルに出力形式を強制するものであるので、思考能力を向上させる直接的な報酬ではありません。\n        であれば、「DeepSeek-R1-Zero」の圧倒的な思考能力を支えているのは、「精度報酬」になります。\n    - paragraph [ref=s1e906]: 本論文での、強化学習の段階で利用しているデータセットは、おそらく数学データセットです。\n    - paragraph [ref=s1e907]:\n        - text: 数学の問題は、基本的に回答が一意に定まるため、ルールベースであっても出力が正しいかどうかは判断できます。\n        - strong [ref=s1e909]: そして、数学の問題に正答するためには、思考能力を強化する必要があります。\n    - complementary [ref=s1e910]:\n        - text: "!"\n        - paragraph [ref=s1e913]: ここが本論文の非常に面白い部分です。\n            DeepSeek-R1-Zeroを学習する際に、より長い長考をさせるようなデータセットによる学習や、思考部分に対して報酬を与えるような仕組みは「全く無く」、ただ、数学の問題の回答が合っているか間違っているかのゴール報酬のみが与えられています。\n        - paragraph [ref=s1e915]: それだけにも関わらず、自動的に数学の問題を正答するために、思考能力を強化するように学習が進んでおり、強化学習の威力を感じます。\n    - paragraph [ref=s1e916]: 強化学習の枠組みにおいて、最終的に報酬を得る（問題に正解する）ために、今、何のtokenを出力しないといけないのか、が最適化されます。\n        結果として、問題の正当に近づくようなtokenを出力する能力、すなわち問題をステップバイステップで解く能力が磨かれることになります。\n    - paragraph [ref=s1e918]: このように、数学という問題の答えを合わせるように報酬設計をすることで、勝手に思考能力が強化される、というのは非常に面白いなと思います。\n    - heading "（補足）データセットの作り方（の考察）" [level=4] [ref=s1e919]\n    - paragraph [ref=s1e920]:\n        - text: 本論文での、強化学習に利用するデータセットは、おそらく\n        - link "上述した論文" [ref=s1e921]:\n            - /url: https://arxiv.org/html/2402.03300v3\n        - text: の前半に記載されている数学データセットです。\n    - complementary [ref=s1e922]:\n        - text: "!"\n        - paragraph [ref=s1e925]: 強化学習に利用したデータセットに関しては、本論文で言及されていなかったと思うので憶測です。間違っていたら教えてください。\n    - paragraph [ref=s1e926]: 本論文の主要部分ではないですが、興味深いため紹介します。\n    - paragraph [ref=s1e927]:\n        - img [ref=s1e928]\n        - text: 引用：\n        - \'link "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models" [ref=s1e929]\':\n            - /url: https://arxiv.org/html/2402.03300v3\n    - paragraph [ref=s1e930]:\n        - strong [ref=s1e931]: 1. Train a FastText Model\n        - text: まず初めに、「OpenWebMath」という高品質の数学のウェブテキストのデータセットを初期のシードコーパスとして利用します。\n            このコーパスから50万のデータを正例、 Common\n            Crawlデータセット（大規模なWebページコーパス）から取得した50万のデータを負例として、fastTextモデルを学習します。\n            こうすることにより、fastTextモデルを利用して、Webページから、それが数学コンテンツか、非数学コンテンツかを分類することができます。\n    - paragraph [ref=s1e935]:\n        - strong [ref=s1e936]: 2. Recall Math-Related Webpages From Common Crawl\n    - paragraph [ref=s1e937]: 学習した、fastTextモデルを利用して、Common\n        Crawlデータセットから数学のWebページを取得します。 このとき、低品質な数学コンテンツを除外するために、 fastText\n        モデルによって予測されたスコアをもとにランク付けした、上位のスコアのページのみを取得します。\n    - paragraph [ref=s1e939]:\n        - strong [ref=s1e940]: 3. Discover Math-Related Domains\n    - paragraph [ref=s1e941]: この方法で取得されたWebページは、数学の問題ではありますが、偏りがあります。\n        なぜなら、初期の学習に利用された「OpenWebMath」データセットには、少数のデータしかなく、さらに上位のスコアのWebコーパス（類似度の高いWebコーパス）しか収集していないため、「OpenWebMath」データセットと似たような数学Webページしか、収集できていないからです。\n    - paragraph [ref=s1e943]: そこで、上記の方法で収集したWebページの親ドメインに着目します。\n        そして、親ドメイン内のWebページのうち、10%以上のWebページが、今回の方法で収集されている場合、親ドメイン自体が数学に関連するドメインであると、考えることができます。\n    - complementary [ref=s1e945]:\n        - text: "!"\n        - paragraph [ref=s1e948]: 例えば、「mathhoge.com」の配下のページの10%以上がfastText\n            モデルによって収集されていた場合は、「mathhoge.com」自体を数学コンテンツであると分類します。\n    - paragraph [ref=s1e949]: あとは、この親ドメイン内の、まだ収集されていないWebページから、高品質な数学Webページを収集することで、幅広い数学のコンテンツを収集できそうです。\n    - paragraph [ref=s1e950]:\n        - strong [ref=s1e951]: 4. Annotate Math-Related URL Path From Labelers\n    - paragraph [ref=s1e952]: 分類されたドメイン内の数学コンテンツに対して、「手動」でURLに注釈をつけ、注釈をつけたもののうち、またシードコーパスに収集されていないWebページを、シードコーパスに追加します。\n    - paragraph [ref=s1e953]: この方法を繰り返すことで、高品質かつ多様な数学データセットを収集することができます。\n        著者らはこのデータ収集を4回繰り返したのちに、3,550万の数学Webページ、合計1200億トークンのコーパスを完成させました。\n    - paragraph [ref=s1e955]: 4回の繰り返しの段階で、データのうち98%が3回目までの繰り返しですでに収集されていたことがわかったため、4回で繰り返し作業を終了したとのことです。\n    - heading "（補足）DPO（Direct Preference Optimization）との違い" [level=3]\n      [ref=s1e956]\n    - paragraph [ref=s1e957]: DPO（Direct Preference Optimization）という手法があります。\n        こちらは、強化学習を利用して、LLMに好ましい回答を学習させるRLHF（Reinforcement learning from human\n        feedback）と同様の効果を、強化学習を使わずに学習させることができる手法です。\n    - paragraph [ref=s1e959]: "強化学習をしないため、報酬モデルも状態価値モデルも必要なく、単純に「プロンプト x 」と「好ましい回答 y^+\n        」「好ましくない回答 y^- 」の3つが組となっているデータセット D を用意すれば、あとは \\\\pi_\\\\phi(y^+|x) 、\n        \\\\pi_\\\\phi(y^-|x) 、 \\\\pi_{ref}(y^+|x) 、 \\\\pi_{ref}(y^+|x)\n        を計算して、損失関数に落とし込むことで、学習が可能になります。"\n    - paragraph [ref=s1e976]: このように、「報酬モデルも状態価値モデルも必要ない」の部分が、本論文で提案している「GRPO（Group\n        Relative Policy Optimization）+ルールベース報酬」の手法の利点と近いです。\n    - paragraph [ref=s1e977]: ただし、DPOでは、上述した通り、「プロンプト x 」と「好ましい回答 y^+ 」「好ましくない回答 y^-\n        」の3つが組となっているデータセットが必要です。 逆に言えば、このデータセットの範囲内でしか学習ができません。\n    - paragraph [ref=s1e985]: 一方で、著者らが提案する「GRPO（Group Relative Policy\n        Optimization）+ルールベース報酬」の手法では、数学などの答えが一意に決まり、ルールベースで正解不正解を判断できるデータセット（つまり、数学データセット）であれば、「プロンプト\n        x 」と「回答 y 」の組だけで、学習が可能です。\n    - paragraph [ref=s1e990]:\n        - strong [ref=s1e991]: したがって、DPOよりもはるかに多くのデータを用いて学習ができます。\n        - text: これが、今回の提案手法の利点になりそうです。\n    - heading "著者らの執念" [level=3] [ref=s1e993]\n    - paragraph [ref=s1e994]: 著者らは、LLMを強化学習に適用する上で、計算量的に障害になる報酬モデルと状態価値モデルを利用しない斬新な手法を提案しました。\n        これにより、圧倒的な計算効率にて、強化学習を適用させました。\n    - paragraph [ref=s1e996]: また、似た効果を持つ手法としてDPOという手法があります。 ただし、DPOでは、利用できるデータセットが制限されるため、大規模な学習は困難です。\n    - paragraph [ref=s1e998]: しかしながら著者らの手法は、ただ数学のように一定の推論の結果、答えが一意に定まるような問題を解かせ続ければ、ルールベース報酬による強化学習で、思考機能を強化させることができてしまっています。\n    - paragraph [ref=s1e999]:\n        - strong [ref=s1e1000]: 絶対に、学習データの量を減らさずに、計算量を徹底的に削減してやろうという著者らの執念を感じるとともに、こういう工夫に積み重ねが、高性能と圧倒的に格安なAPI料金の両立を達成しているのだろうなと感じました。\n    - heading "ZeroのSystemプロンプト" [level=2] [ref=s1e1001]\n    - paragraph [ref=s1e1002]: 続いては、少し話が変わり、「DeepSeek-R1-Zero」に与えているsystemプロンプトについて紹介します。\n        このように、systemプロンプトが公開されているというのは、非常に勉強になりますね。とても嬉しいです\n    - paragraph [ref=s1e1004]: systemプロンプトは下記です。\n    - paragraph [ref=s1e1005]: "A conversation between User and Assistant. The user\n        asks a question, and the Assistant solves it. The assistant first thinks\n        about the reasoning process in the mind and then provides the user with\n        the answer. The reasoning process and answer are enclosed within <think>\n        </think> and <answer> </answer> tags, respectively, i.e., <think>\n        reasoning process here </think> <answer> answer here </answer>. User:\n        prompt. Assistant:"\n    - paragraph [ref=s1e1006]: 訳）\n        ユーザーとアシスタントの会話。ユーザーが質問し、アシスタントがそれを解決する。アシスタントは、まず頭の中で推論プロセスを考え、それからユーザに答えを提供します。推論過程と答えはそれぞれ<think>\n        </think>タグと<answer> </answer>タグで囲まれます。つまり、<think> 推論過程はここ</think>\n        <answer>答えはここ</answer>です。ユーザー：プロンプト アシスタント：\n    - paragraph [ref=s1e1008]: 上記部分の「prompt」の部分は、ユーザが入力したプロンプトの内容に書き換えられます。\n        このように、systemプロンプトにおいて<think> </think>タグを利用して、その中で思考をさせる形になっています。\n    - heading "Zeroの「Aha Moment」" [level=2] [ref=s1e1010]\n    - paragraph [ref=s1e1011]: 「Aha Moment」とは、辞書で調べると「〔問題の答えが突然ひらめいたときなどの〕なるほど！と思う瞬間」とのことです。\n    - paragraph [ref=s1e1012]: 実は、DeepSeek-R1-Zeroの出力において、それが発生したそうです。 数学の問題を解く途中で、下記のような出力が見られました。\n    - paragraph [ref=s1e1014]: "Wait, wait. Wait. That’s an aha moment I can flag\n        here. Let’s reevaluate this step-by-step to identify if the correct sum\n        can be · · · We started with the equation:"\n    - paragraph [ref=s1e1017]: 訳） 待て、待て。待ってくれ。これはハッとさせられる瞬間だ。 正しい和が - - - -\n        になるかどうかを確認するために、ステップ・バイ・ステップで再評価してみよう。\n    - paragraph [ref=s1e1020]: これは恐ろしいことです。\n        モデルは途中まで推論を実施しながら、途中で自身の出力結果を見直し、さらにもう一度ステップバイステップで検討をするという、高度な問題解決能力を獲得しています。\n        （自分の考えを見直して、さらに良い戦略を発見する。これができる人間がどれくらいいるのだろうか）\n    - paragraph [ref=s1e1023]: 何度も記載していますが、DeepSeek-R1-Zeroを学習する際に、問題の解き方を明示的にモデルに教えることや、思考部分を強化させるような報酬は与えていません。\n        ただ、数学の問題に正解することで得られる報酬を与えるだけで、モデルが自律的に高度な問題解決戦略を獲得しています。\n        （もちろん、推論の途中で、検算を行うなどのデータセットで学習もさせていません。もしかしたら、DeepSeek-V3-Baseを学習させるときのSFTデータセットには含まれているかもですが。）\n    - paragraph [ref=s1e1026]: これこそが、本論文で何度も述べている、「強化学習の可能性」です。\n        強化学習は（そのほかの学習手法とは異なり）（人間にとって）予想外で、かつ洗練された結果を導き、人工知能が新たなレベルの知性を手にいれるための核となる技術になりそうです。\n    - heading "DeepSeek-R1-Zeroのまとめ" [level=2] [ref=s1e1028]\n    - paragraph [ref=s1e1029]: DeepSeek-R1-ZeroはDeepSeek-V3-Baseに対して、「GRPO+ルールベース報酬」の強化学習「のみ」を適用し、圧倒的な思考能力を獲得したモデルになります。\n    - paragraph [ref=s1e1030]: DeepSeek-R1-Zeroを学習する際に、問題の解き方を明示的にモデルに教えることや、思考部分を強化させるような報酬は与えていません。\n        ただ、数学の問題に正解することで得られる報酬を与えるだけで、モデルが自律的に高度な問題解決戦略を獲得するという、とても興味深く、恐ろしい結果が示されています。\n    - heading "DeepSeek-R1への道" [level=1] [ref=s1e1032]\n    - paragraph [ref=s1e1033]: さて、ここまで、「DeepSeek-R1-Zero」について、たくさん語ってきましたが、とても魅力的でしたよね？\n        あくまで、「DeepSeek-R1-Zero」は「DeepSeek-V3-Base」に対して、強化学習のみを適用させて、強化学習の可能性を探ったモデルでした。\n    - paragraph [ref=s1e1035]:\n        - text: その結果、こんなにも魅力的なモデル（DeepSeek-R1-Zero）ができてしまったら、、、\n        - strong [ref=s1e1037]: SFTも入れて完全体のモデルを作りたくなりますよね？？？\n    - paragraph [ref=s1e1038]: というモチベーションで「DeepSeek-R1」は作られています。\n    - paragraph [ref=s1e1039]: もし、DeepSeek-R1-Zeroの性能がそんなに高くなかったら、今騒がれているDeepSeek-R1は作ろうという発想にすらならなかったと思います。\n    - heading "DeepSeek-R1-Zeroの課題" [level=2] [ref=s1e1040]\n    - paragraph [ref=s1e1041]: さて、非常に高性能で、示唆に富んだDeepSeek-R1-Zeroでしたが、課題はいくつかあります。 それは下記です。\n    - list [ref=s1e1043]:\n        - listitem [ref=s1e1044]: 学習初期は強化学習の学習が不安定で収束に時間がかかる（コールドスタート問題）\n        - listitem [ref=s1e1045]:\n            - text: 思考能力は高いが、言語が混在（日本語で話していたら急に中国語になるなど）し、人間が理解しにくい\n            - list [ref=s1e1046]:\n                - listitem [ref=s1e1047]: 結果、高度な思考タスクには強いが、（ロールプレイなどの）一般的なタスクには使えないという評価\n    - paragraph [ref=s1e1048]: そこで、上記の課題を解決する旅に出ます。\n    - heading "課題対処への道" [level=2] [ref=s1e1049]\n    - paragraph [ref=s1e1050]: DeepSeek-R1-Zeroでは、ベースモデルであるDeepSeek-V3-Baseに対して、そのまま上述した強化学習を用いて学習をしていました。\n    - paragraph [ref=s1e1051]:\n        - text: しかし、もともとDeepSeek-V3-Baseは長考モデルとして学習されたモデルではないため、いくらsystemプロンプトで長考をさせる制限（\n        - code [ref=s1e1052]: <think>\n        - text: タグ）を入れたとしても、強化学習の初期フェーズは不安定な学習になってしまいます。\n            （例えば、「そもそも適切な形式で思考をしない」、「思考形式は合っているが、意味のある思考をしていない」、その結果として「数学の問題になかなか正答できないまま、報酬が得られず、学習が不安定になる」が考えられます）\n    - paragraph [ref=s1e1054]: そこで、学習初期のコールドスタート問題を防ぐために、DeepSeek-V3-Baseに対して、SFT（Supervised\n        Fine-Tuning）を実施します。 そのために著者らは、少量のCoT（Chain-of-Thought）データセットを構築しています。\n    - heading "CoTデータセットの構築+SFT" [level=3] [ref=s1e1056]\n    - paragraph [ref=s1e1057]: データセットの構築方法は下記が一例として提示されております。\n    - list [ref=s1e1058]:\n        - listitem [ref=s1e1059]: いくつかの長いCoT解答を提示し、モデルに長いCoT形式の回答を生成させてデータセットに追加する\n        - listitem [ref=s1e1060]: モデルに対して、自己反省や検証を行いながら、詳細な回答生成を促すようなプロンプトを送信し、その出力をデータセットに追加する\n        - listitem [ref=s1e1061]: DeepSeek-R1-Zeroの出力を、読みやすい形式に整理してからデータセットに追加する\n        - listitem [ref=s1e1062]: モデルの出力結果を人間が精査して、より適切なCoTデータの形に整理してから、データセットに追加する\n    - paragraph [ref=s1e1063]: 上記の方法で、数千件のCoTデータを構築し、そのデータを用いて、DeepSeek-V3-BaseをSFTしています。\n    - heading "CoTデータセットを用いることの利点" [level=3] [ref=s1e1064]\n    - paragraph [ref=s1e1065]: 著者らは、下記の要素において、CoTデータセットを用いることに利点があると考えています。\n    - paragraph [ref=s1e1066]:\n        - strong [ref=s1e1067]: 読みやすさ\n        - text: DeepSeek-R1-Zeroの一番の課題は、その出力結果が人間が理解するのが難しいことです。\n            回答には、複数の言語が混在していたり、マークダウン形式などの読みやすい形式が適切に使われていないことで、重要な情報が何かわかりにくい状態でした。\n    - paragraph [ref=s1e1070]: そこで、今回のデータセットを作成する際には、各回答の末尾に「要約文」を入れるようにすることで、人間が読みやすい形に出力させるようにしています。\n        具体的には下記のような出力形式で、全てのCoTデータセットは作成されています。\n    - code [ref=s1e1075]: "|special_token|<reasoning_process>|special_token|<summary>"\n    - button "クリップボードにコピー" [ref=s1e1076]:\n        - img [ref=s1e1077]\n    - button "右端で折り返す" [ref=s1e1078]:\n        - img [ref=s1e1079]\n    - paragraph [ref=s1e1080]:\n        - code [ref=s1e1081]: <reasoning_process>\n        - text: の部分に、モデルの思考に該当する部分が、\n        - code [ref=s1e1082]: <summary>\n        - text: の部分に最終的な結論の要約が記載されます。\n    - paragraph [ref=s1e1083]: これにより、「DeepSeek-R1-Zero」にあった、重要な情報が何かわかりにくいという人間が理解するのが難しい問題を緩和します。\n        （言語の混在に関しての対処は後述します）\n    - paragraph [ref=s1e1085]:\n        - strong [ref=s1e1086]: ポテンシャル\n        - text: 今回、強化学習のみを実施するのではなく、人間の事前知識に基づいて作成したCoTデータセットでSFTを実施しています。\n            また、SFTと強化学習を複数回反復して、繰り返し学習をしています。\n            その結果、DeepSeek-R1はDeepSeek-R1-Zeroよりも良い結果を得ることができました。\n    - paragraph [ref=s1e1090]: 著者らは、このような反復的な学習こそが、長考モデル（reasoning models）を学習する上で良い方法であると主張しています。\n    - heading "推論思考の強化学習" [level=3] [ref=s1e1091]\n    - paragraph [ref=s1e1092]: CoTデータセットによりSFTしたDeepSeek-V3-Baseに対して、上述した（DeepSeek-R1-Zeroを学習した）強化学習手法を適用します。\n        これにより、モデルの思考能力が大幅に強化されます。\n    - paragraph [ref=s1e1094]: 一方で、「DeepSeek-R1-Zero」には、言語不一致の問題がありました。\n        そこで、DeepSeek-R1-Zeroのルールベース報酬に、「言語一貫性報酬」を追加で導入しています。\n    - paragraph [ref=s1e1096]: この報酬導入で、実は、モデルの思考能力が若干落ちてしまうことが実験的にわかっています。\n        しかしながら、この報酬があることで、ようやく人間が理解しやすい文章を生成してくれるようになります。\n    - heading "さらなる追加のSFT" [level=3] [ref=s1e1098]\n    - paragraph [ref=s1e1099]: 上記の強化学習が収束したら、さらにデータを追加してSFTをやり直します。\n        ここでは、初期段階の、CoTに重きを置いた、コールドスタートを解消するための学習データだけにとどまらず、ライティング、ロールプレイといった汎用タスクの性能を向上させるため、他ドメインからのデータも合わせて収集します。\n    - heading "Reasoning Data" [level=4] [ref=s1e1101]\n    - paragraph [ref=s1e1102]: まずは、前段階と同様に思考能力を向上させるために、CoT学習データ（Reasoning Data）を取得します。\n    - paragraph [ref=s1e1103]: 取得方法は以下です。\n    - list [ref=s1e1104]:\n        - listitem [ref=s1e1105]:\n            - text: 強化学習済みモデルの出力から、適切なものを収集して学習データに追加\n            - list [ref=s1e1106]:\n                - listitem [ref=s1e1107]: 前段の強化学習では、ルールベースの報酬モデルを利用していたので、それが使える問題（数学など）のみを学習データとしていたが、次の強化学習では報酬モデルを作成するため、もっと広い範囲でCoTができるデータも学習データに追加\n                - listitem [ref=s1e1108]: モデルの出力はまだ読みにくいところがあるため、さまざまフィルタリング\n    - paragraph [ref=s1e1109]: この結果、合計で600kのReasoning Data（学習データ）を収集しています。\n    - heading "Non-Reasoning Data" [level=4] [ref=s1e1110]\n    - paragraph [ref=s1e1111]: ライティングやQ＆A、翻訳などのNon-Reasoning\n        Dataについては、DeepSeek-V3のSFTデータセットの一部を再利用して、学習データを収集しています。 上記のReasoning\n        Data以外のタスク（Non-Reasoning\n        Data）に関しては、DeepSeek-V3によって簡易的なCoTを実施して、回答を生成し、学習データとします。\n        ただし、本当に簡単なタスク（あいさつとか）に関しては、CoTを学習データに追加しません。\n    - paragraph [ref=s1e1114]: こうすることで、合計200kのトレーニングサンプルを最終的に収集したそうです。\n    - paragraph [ref=s1e1115]: 上記の合計800kサンプルの学習データを利用して、DeepSeek-V3-Base（初回強化学習済み）を2epoch分SFTを実施しています。\n    - heading "最後の強化学習" [level=3] [ref=s1e1116]\n    - paragraph [ref=s1e1117]: 最後に、モデルの思考能力をさらに磨くと同時に、モデルの無害性などを向上させることを目的に、強化学習を実施します。\n    - paragraph [ref=s1e1118]: 思考タスクの強化学習に関しては、これまで説明したのと同様に、数学、コード、論理的推論の領域でルールベース報酬を利用して強化学習をします。\n        一方で、思考タスク以外の一般的なデータに対しては、ルールベースで回答の良し悪しを判断することはできないため、報酬モデルを別途DeepSeek-V3パイプラインをベースに作成しています。\n    - heading "DeepSeek-R1のまとめ" [level=2] [ref=s1e1120]\n    - paragraph [ref=s1e1121]: 以上より、DeepSeek-R1では、DeepSeek-V3-Baseに対して、SFTと強化学習を2回実施することにより構築されています。\n    - paragraph [ref=s1e1122]: SFTにより、強化学習のコールドスタート問題の解消と、モデル出力のユーザフレンドリー性の向上、一般的なタスクに対する性能向上に貢献しています。\n    - heading "結果" [level=1] [ref=s1e1123]\n    - paragraph [ref=s1e1124]: 結果に関しては、他の方がさまざま検証していたり、話題になっているので詳しくは書きません。\n        ただ、DeepSeek-R1は、現在のオープンソースモデルの中では、最も高い性能と言ってよく、また、Open\n        AIのo1モデルにも匹敵する性能だと言われています。\n    - heading "蒸留" [level=1] [ref=s1e1126]\n    - paragraph [ref=s1e1127]: 最後に、蒸留モデルについての解説します。（論文でも言及されています）\n        DeepSeek-R1は671Bものモデルパラメータを持つため、一般的な家庭用PCで動作させるのは難しいです。\n    - paragraph [ref=s1e1129]: そこで、より小さなモデルパラメータのモデルに、DeepSeek-R1のような思考能力を与えるために、蒸留を実施しています。\n        （公式が、個人の利用者のことも考えて、家庭用PCで動作する思考モデルを開発してくれるのは非常にありがたいですね）\n    - paragraph [ref=s1e1131]: 具体的には、Qwen2.5-Math-1.5B、Qwen2.5-Math-7B、Qwen2.5-14B、Qwen2.5-32B、Llama-3.1-8B、Llama-3.3-70B-Instructに対して、DeepSeek-R1から取得した800kのトレーニングサンプルを利用して、直接FineTuningする形で、蒸留を実施しています。\n    - paragraph [ref=s1e1132]: 蒸留モデルに対しては、SFTのみを実施し、後段の強化学習は実施しておらず、蒸留モデルに対する追加の強化学習の探究は、著者らが実施する予定はないそうです。\n        （著者らは、強化学習を追加で実施することにより性能が上がると思っているが、その探究は、別のコミュニティに任せると記述しています）\n    - heading "蒸留 vs 強化学習" [level=2] [ref=s1e1134]\n    - paragraph [ref=s1e1135]: 蒸留後の強化学習に関しては、著者らは実験していない、と記述していました。\n        では、小型モデルに対して、蒸留を実施する前に、強化学習を実施したらどうなるのでしょうか？\n        DeepSeek-V3-Baseのように、大幅に思考能力が強化されて、性能向上が期待できるのでしょうか？\n    - paragraph [ref=s1e1138]: それに関しては、著者らが実験してくれています。\n    - paragraph [ref=s1e1139]: ここでは、「Qwen-32B-Base」モデルを利用して、10kステップ以上の大規模強化学習を実施し、「DeepSeek-R1-Zero-Qwen-32B」を構築しました。\n        一方で、「Qwen-32B-Base」モデルに対して、蒸留を実施したモデルを、「DeepSeek-R1Distill-Qwen-32B」モデルとして、この二つのモデルを比較します。\n    - paragraph [ref=s1e1141]: その結果を比較すると、小型モデルに対する強化学習を適用した「DeepSeek-R1-Zero-Qwen-32B」よりも、小型モデルに蒸留を適用した「DeepSeek-R1Distill-Qwen-32B」モデルの方がはるかに高い性能でした。\n    - paragraph [ref=s1e1142]:\n        - strong [ref=s1e1143]: 以下は私の主観ですが、\n        - text: 以上の結果から、\n        - strong [ref=s1e1146]: 強化学習による思考能力の向上は、安易に小型モデルに試していい戦略でない\n        - text: ということが示唆されていると思います。\n    - paragraph [ref=s1e1148]: 強化学習による思考能力の大幅な強化は、おそらくですが「DeepSeek-V3」という元々大規模かつ性能の高いモデルであったからこそ、達成されたのだと思います。\n    - paragraph [ref=s1e1149]: DeepSeek-R1-Zeroの課題でも説明した通り、今回のルールベース報酬では、正しい数学の解答を生成できないと永遠に報酬が獲得できません。\n        小型モデルでは、このコールドスタートの問題が、より致命的なレベルで発生するのだと思います。\n    - paragraph [ref=s1e1151]: したがって、思考モデルを作りたいからといって、小型モデルに対して、安易にルールベース強化学習を試すのはおすすめしません。\n        しかしながら、コールドスタート問題を解決できる戦略があるのであれば、有効な手段になると思われます。\n        （例えば、DeepSeek-R1で蒸留済みの小型モデルを利用する、最初から数学に強い小型モデルで強化学習を試すなどです。重要なのは強化学習のコールドスタートを解消できるかどうかだと思われます。）\n    - paragraph [ref=s1e1154]:\n        - text: （追記） また、同様の実験を小型モデルに対して試したグループもいるようです。 そちらの実験レポートが下記になります。\n        - iframe [ref=s1e1158]: <could not take iframe snapshot>\n        - text: （DeepSeek-R1-Zeroと同様に、「Aha Moment」が見られることもわかっており、非常に興味深いです）\n    - paragraph [ref=s1e1159]: ただ、こちらのレポートにおいても、ベースモデルとしては数学に特化したモデルを利用しています。\n        このことからも、ある程度の数学能力は必要なのだなと思います。\n    - heading "今後の展望" [level=1] [ref=s1e1161]\n    - paragraph [ref=s1e1162]: DeepSeek-R1は、さまざまなタスクにおいて、OpenAI\n        o1に匹敵する性能を達成した、驚異的なオープンソースモデルですが、苦手としている部分もあります。 それが今後の展望に記載されています。\n        （今後の展望を見れば、DeepSeek-R1が苦手な部分を知ることができます）\n    - paragraph [ref=s1e1165]: それは以下の項目です。\n    - list [ref=s1e1166]:\n        - listitem [ref=s1e1167]:\n            - strong [ref=s1e1168]: 一般的な能力\n            - list [ref=s1e1169]:\n                - listitem [ref=s1e1170]: 関数呼び出しやマルチターン対話、複雑なロールプレイング。、json出力において、DeepSeek-V3に及ばない\n        - listitem [ref=s1e1171]:\n            - strong [ref=s1e1172]: 言語の混在\n            - list [ref=s1e1173]:\n                - listitem [ref=s1e1174]: 現在、DeepSeek-R1は英語と中国語に最適化されているため、そのほかの言語を利用した場合、言語の混在が発生する可能性がある\n        - listitem [ref=s1e1175]:\n            - strong [ref=s1e1176]: プロンプトエンジニアリング\n            - list [ref=s1e1177]:\n                - listitem [ref=s1e1178]: DeepSeek-R1はプロンプトに非常に敏感で、特にFewーshotプロンプトを利用すると、パフォーマンスが低下する。\n                - listitem [ref=s1e1179]: 基本的には、問題を直接シンプルに記述し、例を提示しない（Zero-shotプロンプト）を利用することを推奨している\n        - listitem [ref=s1e1180]:\n            - strong [ref=s1e1181]: ソフトウェアエンジニアリングタスク\n            - list [ref=s1e1182]:\n                - listitem [ref=s1e1183]: コード生成やコード保管などのソフトウェアエンジニアリングタスクは、評価に時間がかかるため、強化学習の効率が悪くなることから、現時点では強化学習において、このようなタスクは学習していない。\n                    大々的には学習しておらず、一部のテスト可能な範囲でしか学習できていない。\n                - listitem [ref=s1e1185]: 結果として、このタスクの性能は、DeepSeek-V3からほとんど改善していない。\n    - complementary [ref=s1e1186]:\n        - text: "!"\n        - paragraph [ref=s1e1189]: （追記） masaishiさんのコメントに基づき修正させていただきました。 私の読み間違いで誤解を与えてしまい申し訳ございません。\n        - blockquote [ref=s1e1192]:\n            - paragraph [ref=s1e1193]: 2.2.2. Reward Modeling ・・・ ・Accuracy\n                rewards:・・・Similarly, for LeetCode problems, a compiler can be\n                used to generate feedback based on predefined test cases.\n        - paragraph [ref=s1e1196]: と記載のあるように、全くコード生成などを学習していないわけではなく、一部の、簡単にテストができる問題のみ学習しているようです。\n            ただ、そういう簡易的なタスクだけしか学習できていないため、性能向上は小さめということだと思われます。\n    - paragraph [ref=s1e1198]: 上記の内容が、現時点での「DeepSeek-R1」の課題です。\n        ですが、今後のアップデートで改善していくとも記載されているため、現時点で不満があったとしても、上記に記載されている項目であれば、今後改善する可能性が高いです。\n    - heading "まとめ" [level=1] [ref=s1e1200]\n    - paragraph [ref=s1e1201]: ここまで読んでくださった方、本当にお疲れ様でした。\n        （これでも説明はだいぶ短くした方なんです・・・本当は、もっと強化学習の部分とかを数式をもとに説明したかったです）\n    - paragraph [ref=s1e1203]: DeepSeek-R1の論文いかがでしたか？\n        私にとっては、非常に勉強になるような示唆や知見に溢れており、こんな論文を公開してくれて、かつオープンソースのモデルとしても公開してくれる「DeepSeek-AI」には感謝しかありません。\n    - paragraph [ref=s1e1205]: 本記事が、皆様の理解の助けになり、一人でも多くの方が原論文に興味を持って、読んでくれましたら幸いです。\n        ここまで読んでくださってありがとうございました！\n    - heading "参考文献" [level=1] [ref=s1e1207]\n    - paragraph [ref=s1e1208]: 本論文は、論文の中では読みやすい方だとは思いますが、強化学習に関しては、一定程度の基礎知識は要求されます。\n        そこで、本論文を読む上で必要な知識を収集できるおすすめの書籍に関して記載します。 （少なくとも私はこの書籍で学習しました）\n    - paragraph [ref=s1e1211]: （書籍のリンクはamazonアフィリエイトリンクになります）\n    - heading "強化学習" [level=4] [ref=s1e1212]\n    - paragraph [ref=s1e1213]: 強化学習に関しておすすめの本は、下記の本たちです。\n    - paragraph [ref=s1e1214]:\n        - link "ゼロから作るDeep Learning ❹ ―強化学習編" [ref=s1e1215]:\n            - /url: https://amzn.to/40qFuNg\n        - text: まずは、定番の本です。\n            強化学習に関して、本当に最初の部分（マルコフ決定過程とは何か）から、非常に詳細かつわかりやすく解説されており、深層学習を利用して、どのように強化学習をすることできるのかまで、しっかり書かれているためおすすめです。\n            また、コードと理論のバランスもちょうどよく、理系大学生とかであれば問題なく読み進められると思います。\n            特に、強化学習の基礎の部分が、詳細かつ分かりやすく書かれているため、一冊目におすすめです。\n    - paragraph [ref=s1e1220]:\n        - link "機械学習スタートアップシリーズ Ｐｙｔｈｏｎで学ぶ強化学習 ［改訂第２版］ 入門から実践まで (ＫＳ情報科学専門書)" [ref=s1e1221]:\n            - /url: https://amzn.to/4joKbjg\n    - paragraph [ref=s1e1223]: 普段からpythonを使っており、数式よりも実装に興味がある方にとっては、こちらの書籍もおすすめです。\n        こちらの書籍は、強化学習の基礎よりは、少し応用というか、実用を意識した書籍になっており、実務でよく使われるモデルなどに関しても、詳しく説明されています。\n        強化学習の基礎はわかっている人や、もっと実装をベースに学びたいというかたにおすすめです。\n    - paragraph [ref=s1e1226]:\n        - link "強化学習 (機械学習プロフェッショナルシリーズ)" [ref=s1e1227]:\n            - /url: https://amzn.to/40NX5QM\n        - link "強化学習から信頼できる意思決定へ (AI/データサイエンスライブラリ“基礎から応用へ” 5)" [ref=s1e1229]:\n            - /url: https://amzn.to/4jtFq8d\n    - paragraph [ref=s1e1230]: 名著です。\n        どちらも、理論説明の部分が非常に面白く、正直難易度は高いですが、強化学習の理論をちゃんと理解したいという方におすすめの書籍です。\n        下の本に関しては、2024年の本ということもあり、近年の強化学習の実世界での応用と、理論的な定式化の架け橋になっているような本になっており、非常に面白いです。\n    - heading "AlphaGO Zero" [level=4] [ref=s1e1233]\n    - paragraph [ref=s1e1234]:\n        - link "AlphaZero 深層学習・強化学習・探索 人工知能プログラミング実践入門" [ref=s1e1235]:\n            - /url: https://amzn.to/3Cdnuy1\n    - paragraph [ref=s1e1236]: だいぶ昔に購入させていただいた本ですが、まさか2025年に活躍することがあろうとは。\n        という感じですが、こちらの書籍も参考にさせていただきました。 Alpha\n        Zeroを作る上で核となる技術を余すところなく解説されており、私は非常に勉強になりました。 Alpha\n        Zeroに興味がある方は、ぜひ一度ご一読ください。\n    - heading "LLM、RLHF, DPO" [level=4] [ref=s1e1240]\n    - paragraph [ref=s1e1241]:\n        - link "大規模言語モデル入門Ⅱ〜生成型LLMの実装と評価" [ref=s1e1242]:\n            - /url: https://amzn.to/3Cs3Uhn\n        - text: よく紹介させていただいておりますが、こちらの書籍は、LLMのファインチューニングから、RLHF、RAG、分散学習にかけて、本当に幅広く解説されており、いつも参考にさせていただいております。\n            今回のDPOやRLHFの解説部分でも、とても活躍してくれました。\n            LLMを取り扱っている方は、とりあえず買っておいても損はないと思います。\n            さまざまな章、ページで読めば読むほど新しい発見が得られる、スルメのような本だなといつも思っております。\n    - button "いいね" [ref=s1e1249]:\n        - img [ref=s1e1250]\n    - text: "921"\n    - button "ブックマーク" [ref=s1e1266]:\n        - img [ref=s1e1267]\n    - text: "344"\n    - button "その他の操作" [ref=s1e1272]:\n        - img [ref=s1e1273]\n    - tooltip "X(Twitter)にポスト" [ref=s1e1276]:\n        - img [ref=s1e1277]\n        - text: X(Twitter)にポスト\n    - tooltip "Facebookに投稿" [ref=s1e1283]:\n        - img [ref=s1e1284]\n        - text: Facebookに投稿\n    - tooltip "はてなブックマークに登録" [ref=s1e1290]:\n        - img [ref=s1e1291]\n        - text: はてなブックマークに登録\n    - complementary [ref=s1e1294]:\n        - link "asap" [ref=s1e1296]:\n            - /url: /asap\n            - img "asap" [ref=s1e1297]\n        - link "asap" [ref=s1e1299]:\n            - /url: /asap\n        - paragraph [ref=s1e1301]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n        - button "フォロー" [ref=s1e1305]\n        - tooltip "@personabb" [ref=s1e1306]:\n            - img "GitHub" [ref=s1e1307]\n            - text: "@personabb"\n        - tooltip "@asap2650" [ref=s1e1309]:\n            - img "X(Twitter)" [ref=s1e1310]\n            - text: "@asap2650"\n        - tooltip "yard.tips" [ref=s1e1316]:\n            - img "リンク" [ref=s1e1317]\n            - text: yard.tips\n    - complementary [ref=s1e1320]:\n        - text: バッジを贈って著者を応援しよう\n        - paragraph [ref=s1e1324]: バッジを受け取った著者にはZennから現金やAmazonギフトカードが還元されます。\n        - button "バッジを贈る" [ref=s1e1326]\n        - img [ref=s1e1327]\n    - text: この記事に贈られたバッジ\n    - img "Thank you" [ref=s1e1338]\n    - img [ref=s1e1340]\n    - img "参考になった" [ref=s1e1352]\n    - img [ref=s1e1354]\n    - img "もっと読みたい" [ref=s1e1363]\n    - img [ref=s1e1365]\n    - img "参考になった" [ref=s1e1375]\n    - img [ref=s1e1377]\n    - img "もっと読みたい" [ref=s1e1386]\n    - img [ref=s1e1388]\n    - heading "Discussion" [level=3] [ref=s1e1400]\n    - article [ref=s1e1403]:\n        - link "はまなすなぎさ" [ref=s1e1405]:\n            - /url: /nhamanasu\n            - img "はまなすなぎさ" [ref=s1e1406]\n        - link "はまなすなぎさ" [ref=s1e1407]:\n            - /url: /nhamanasu\n        - link "2025年1月28日 8時33分" [ref=s1e1408]:\n            - /url: "#comment-f230c9d295b3f5"\n            - tooltip "2025年1月28日 8時33分" [ref=s1e1409]: 2ヶ月前 2025年1月28日 8時33分\n        - button "メニューを開く" [ref=s1e1411]:\n            - img [ref=s1e1412]\n        - paragraph [ref=s1e1417]: 非常にためになる骨太な記事、ありがとうございます！とても勉強になります。\n        - paragraph [ref=s1e1418]: ちょっとした誤字かも？の箇所なのですが、冒頭の「強化学習手法「DRPO」すごい！」の部分は「GRPO」ではないかなと思いました。ご報告まで。\n        - button "いいね" [ref=s1e1421]:\n            - img [ref=s1e1422]\n        - text: "1"\n        - button "コメント 1" [ref=s1e1437]:\n            - img "コメント" [ref=s1e1438]\n            - text: "1"\n        - article [ref=s1e1442]:\n            - link "asap" [ref=s1e1444]:\n                - /url: /asap\n                - img "asap" [ref=s1e1445]\n            - link "asap" [ref=s1e1446]:\n                - /url: /asap\n            - link "2025年1月28日 10時23分" [ref=s1e1447]:\n                - /url: "#comment-41cdebd92d8d44"\n                - tooltip "2025年1月28日 10時23分" [ref=s1e1448]: 2ヶ月前 2025年1月28日 10時23分\n            - button "メニューを開く" [ref=s1e1450]:\n                - img [ref=s1e1451]\n            - paragraph [ref=s1e1456]: 読んでくださりありがとうございます！ おっしゃる通りです！修正させていただきました！\n            - button "いいね" [ref=s1e1460]:\n                - img [ref=s1e1461]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1477]\n    - article [ref=s1e1479]:\n        - link "bang" [ref=s1e1481]:\n            - /url: /zenmnm\n            - img "bang" [ref=s1e1482]\n        - link "bang" [ref=s1e1483]:\n            - /url: /zenmnm\n        - link "2025年1月28日 13時46分" [ref=s1e1484]:\n            - /url: "#comment-d0aac9375fbbf7"\n            - tooltip "2025年1月28日 13時46分" [ref=s1e1485]: 2ヶ月前 2025年1月28日 13時46分\n        - button "メニューを開く" [ref=s1e1487]:\n            - img [ref=s1e1488]\n        - paragraph [ref=s1e1493]: AIについてはまだ詳しくありませんが、興味はあります。同じように古いモデルを使って新しいモデルを作るのですが、蒸留方法の代わりにモデルを組み合わせることは可能でしょうか?sakana\n            aiのようにです\n        - button "いいね" [ref=s1e1496]:\n            - img [ref=s1e1497]\n        - text: "1"\n        - button "コメント 2" [ref=s1e1512]:\n            - img "コメント" [ref=s1e1513]\n            - text: "2"\n        - article [ref=s1e1517]:\n            - link "asap" [ref=s1e1519]:\n                - /url: /asap\n                - img "asap" [ref=s1e1520]\n            - link "asap" [ref=s1e1521]:\n                - /url: /asap\n            - link "2025年1月28日 15時33分" [ref=s1e1522]:\n                - /url: "#comment-27e79c6ee51f2a"\n                - tooltip "2025年1月28日 15時33分" [ref=s1e1523]: 2ヶ月前 2025年1月28日 15時33分\n            - button "メニューを開く" [ref=s1e1525]:\n                - img [ref=s1e1526]\n            - paragraph [ref=s1e1531]: モデルマージに関してはあまり詳しくないですが、基本的には同じアーキテクチャのモデルを利用することになると思います。したがってDeepSeek-R1によって蒸留されたLlamaモデルとかであれば、他のLlamaモデル（同じモデルアーキテクチャ）とモデルマージすることで、推論機能を付け加えたり、日本語機能を強化したりすることが期待できると思います。\n            - button "いいね" [ref=s1e1534]:\n                - img [ref=s1e1535]\n        - article [ref=s1e1549]:\n            - link "bang" [ref=s1e1551]:\n                - /url: /zenmnm\n                - img "bang" [ref=s1e1552]\n            - link "bang" [ref=s1e1553]:\n                - /url: /zenmnm\n            - link "2025年1月28日 16時42分" [ref=s1e1554]:\n                - /url: "#comment-0221309240798f"\n                - tooltip "2025年1月28日 16時42分" [ref=s1e1555]: 2ヶ月前 2025年1月28日 16時42分\n            - button "メニューを開く" [ref=s1e1557]:\n                - img [ref=s1e1558]\n            - paragraph [ref=s1e1563]: お返事ありがとうございます。私はまだ初心者なので、これは私の軽い考えです。今では研究のプロセスごとにいくつかの方法があり、それらを組み合わせたり微調整したりすれば、たくさんのモデルができるようになっているでしょう。\n            - button "いいね" [ref=s1e1566]:\n                - img [ref=s1e1567]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1583]\n    - article [ref=s1e1585]:\n        - link "HWSeungwoo" [ref=s1e1587]:\n            - /url: /syoyuu96\n            - img "HWSeungwoo" [ref=s1e1588]\n        - link "HWSeungwoo" [ref=s1e1589]:\n            - /url: /syoyuu96\n        - link "2025年1月28日 19時28分" [ref=s1e1590]:\n            - /url: "#comment-a07230ddbdaebe"\n            - tooltip "2025年1月28日 19時28分" [ref=s1e1591]: 2ヶ月前 2025年1月28日 19時28分\n        - button "メニューを開く" [ref=s1e1593]:\n            - img [ref=s1e1594]\n        - paragraph [ref=s1e1599]: Deep\n            Seekに関する記事やニュースはたくさん目にしますが、ここまで深掘りされた内容には驚きました。とても勉強になりました。ありがとうございます！\n        - button "いいね" [ref=s1e1602]:\n            - img [ref=s1e1603]\n        - text: "1"\n        - button "コメント 1" [ref=s1e1618]:\n            - img "コメント" [ref=s1e1619]\n            - text: "1"\n        - article [ref=s1e1623]:\n            - link "asap" [ref=s1e1625]:\n                - /url: /asap\n                - img "asap" [ref=s1e1626]\n            - link "asap" [ref=s1e1627]:\n                - /url: /asap\n            - link "2025年1月28日 19時32分" [ref=s1e1628]:\n                - /url: "#comment-1e1e3abc86df00"\n                - tooltip "2025年1月28日 19時32分" [ref=s1e1629]: 2ヶ月前 2025年1月28日 19時32分\n            - button "メニューを開く" [ref=s1e1631]:\n                - img [ref=s1e1632]\n            - paragraph [ref=s1e1637]: 読んでくださってありがとうございます！ コメント嬉しいです！ありがとうございます！\n            - button "いいね" [ref=s1e1641]:\n                - img [ref=s1e1642]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1658]\n    - article [ref=s1e1660]:\n        - link "masaishi" [ref=s1e1662]:\n            - /url: /masaishi\n            - img "masaishi" [ref=s1e1663]\n        - link "masaishi" [ref=s1e1664]:\n            - /url: /masaishi\n        - link "2025年1月29日 16時14分" [ref=s1e1665]:\n            - /url: "#comment-a0d75bd96cadfc"\n            - tooltip "2025年1月29日 16時14分" [ref=s1e1666]: 2ヶ月前 2025年1月29日 16時14分\n        - button "メニューを開く" [ref=s1e1668]:\n            - img [ref=s1e1669]\n        - paragraph [ref=s1e1674]: 素晴らしい記事をありがとうございます！\n            GRPOなどの強化学習手法に関する解説も大変勉強になりました。この記事をきっかけに、DeepSeek-R1の論文にも興味を持ち、読んでみようと思えました。\n        - paragraph [ref=s1e1676]: 一点だけ気になったことがあり、もし私の理解が誤っていたら申し訳ありません。\n            "コード生成やコード保管などのソフトウェアエンジニアリングタスク"の強化学習に用いられていないとありますが、論文の\n            2.2.2（Reward Modeling）の\n        - blockquote [ref=s1e1678]:\n            - paragraph [ref=s1e1679]: For LeetCode problems, a compiler can be used to\n                generate feedback based on predefined test cases.\n        - paragraph [ref=s1e1680]: という記述を見ると、少なくともアルゴリズム問題のコード生成には強化学習が用いられているのかなと思います。\n        - paragraph [ref=s1e1681]: ソフトウェアエンジニアリングタスクは強化学習していませんが、コード生成自体は行っているような気がするので、注釈などで少し補足していただけると、読者としては誤解が減って助かるかもしれません。しかし、かなり細かい点ですので、最終的には著者さまの判断にお任せいたします。\n        - button "いいね" [ref=s1e1684]:\n            - img [ref=s1e1685]\n        - text: "2"\n        - button "コメント 2" [ref=s1e1700]:\n            - img "コメント" [ref=s1e1701]\n            - text: "2"\n        - article [ref=s1e1705]:\n            - link "asap" [ref=s1e1707]:\n                - /url: /asap\n                - img "asap" [ref=s1e1708]\n            - link "asap" [ref=s1e1709]:\n                - /url: /asap\n            - link "2025年1月29日 16時37分" [ref=s1e1710]:\n                - /url: "#comment-afb254c15106b7"\n                - tooltip "2025年1月29日 16時37分" [ref=s1e1711]: 2ヶ月前 2025年1月29日 16時37分\n            - button "メニューを開く" [ref=s1e1713]:\n                - img [ref=s1e1714]\n            - paragraph [ref=s1e1719]: コメントありがとうございます！ おっしゃる通りでした！読み間違いです。ご指摘助かります。\n            - blockquote [ref=s1e1721]:\n                - paragraph [ref=s1e1722]: Due to the long evaluation times, which impact the\n                    efficiency of the RL process, large-scale RL has not been\n                    applied extensively in software engineering tasks.\n            - paragraph [ref=s1e1723]: この部分の文章をもとに記載していましたが、大々的には適用されていないだけで、一部は学習に入っているっぽいですね。\n            - paragraph [ref=s1e1724]: 助かります！修正もこれからさせていただきます\n            - button "いいね" [ref=s1e1727]:\n                - img [ref=s1e1728]\n            - text: "2"\n        - article [ref=s1e1743]:\n            - link "masaishi" [ref=s1e1745]:\n                - /url: /masaishi\n                - img "masaishi" [ref=s1e1746]\n            - link "masaishi" [ref=s1e1747]:\n                - /url: /masaishi\n            - link "2025年1月29日 19時5分" [ref=s1e1748]:\n                - /url: "#comment-debff7de1aad74"\n                - tooltip "2025年1月29日 19時5分" [ref=s1e1749]: 2ヶ月前 2025年1月29日 19時5分\n            - button "メニューを開く" [ref=s1e1751]:\n                - img [ref=s1e1752]\n            - paragraph [ref=s1e1757]: ご丁寧にありがとうございます！\n            - button "いいね" [ref=s1e1760]:\n                - img [ref=s1e1761]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1777]\n    - article [ref=s1e1779]:\n        - link "RyotoTanaka" [ref=s1e1781]:\n            - /url: /ryototanaka\n            - img "RyotoTanaka" [ref=s1e1782]\n        - link "RyotoTanaka" [ref=s1e1783]:\n            - /url: /ryototanaka\n        - link "2025年1月30日 8時0分" [ref=s1e1784]:\n            - /url: "#comment-4ae639fe8d1892"\n            - tooltip "2025年1月30日 8時0分" [ref=s1e1785]: 2ヶ月前 2025年1月30日 8時0分\n        - button "メニューを開く" [ref=s1e1787]:\n            - img [ref=s1e1788]\n        - paragraph [ref=s1e1793]: 読み応えがありました。素晴らしい記事をありがとうございます！ 些細な事なのですが、DPOは Direct\n            Preference Optimization ではないでしょうか。\n        - button "いいね" [ref=s1e1797]:\n            - img [ref=s1e1798]\n        - text: "1"\n        - button "コメント 1" [ref=s1e1813]:\n            - img "コメント" [ref=s1e1814]\n            - text: "1"\n        - article [ref=s1e1818]:\n            - link "asap" [ref=s1e1820]:\n                - /url: /asap\n                - img "asap" [ref=s1e1821]\n            - link "asap" [ref=s1e1822]:\n                - /url: /asap\n            - link "2025年1月30日 13時15分" [ref=s1e1823]:\n                - /url: "#comment-ec7e829404f7bc"\n                - tooltip "2025年1月30日 13時15分" [ref=s1e1824]: 2ヶ月前 2025年1月30日 13時15分\n            - button "メニューを開く" [ref=s1e1826]:\n                - img [ref=s1e1827]\n            - paragraph [ref=s1e1832]: ほんとですね！間違えて入力してました ありがとうございます！\n            - button "いいね" [ref=s1e1836]:\n                - img [ref=s1e1837]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1853]\n    - text: ログインするとコメントできます\n    - button "Login" [ref=s1e1857]\n    - complementary [ref=s1e1858]:\n        - link "asap" [ref=s1e1863]:\n            - /url: /asap\n            - img "asap" [ref=s1e1864]\n        - link "asap" [ref=s1e1866]:\n            - /url: /asap\n        - button "フォロー" [ref=s1e1868]\n        - link "@personabb" [ref=s1e1869]:\n            - /url: https://github.com/personabb\n            - tooltip "@personabb" [ref=s1e1870]:\n                - img "GitHub" [ref=s1e1871]\n                - text: "@personabb"\n        - link "@asap2650" [ref=s1e1873]:\n            - /url: https://twitter.com/asap2650\n            - tooltip "@asap2650" [ref=s1e1874]:\n                - img "X(Twitter)" [ref=s1e1875]\n                - text: "@asap2650"\n        - link "yard.tips" [ref=s1e1881]:\n            - /url: https://yard.tips/asap2650\n            - tooltip "yard.tips" [ref=s1e1882]:\n                - img "リンク" [ref=s1e1883]\n                - text: yard.tips\n        - paragraph [ref=s1e1886]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n        - button "バッジを贈る" [ref=s1e1889]\n        - link "バッジを贈るとは" [ref=s1e1891]:\n            - /url: /faq#badges\n            - text: バッジを贈るとは\n            - img [ref=s1e1892]\n        - text: 目次\n        - list [ref=s1e1901]:\n            - listitem [ref=s1e1902]:\n                - link "はじめに" [ref=s1e1903]:\n                    - /url: "#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB"\n            - listitem [ref=s1e1904]:\n                - link "本論文の興味深いところ" [ref=s1e1905]:\n                    - /url: "#%E6%9C%AC%E8%AB%96%E6%96%87%E3%81%AE%E8%88%88%E5%91%B3%E6%B7%B1%E3%81%8\\\n                        4%E3%81%A8%E3%81%93%E3%82%8D"\n            - listitem [ref=s1e1906]:\n                - link "本論文の流れ" [ref=s1e1907]:\n                    - /url: "#%E6%9C%AC%E8%AB%96%E6%96%87%E3%81%AE%E6%B5%81%E3%82%8C"\n                - list [ref=s1e1908]:\n                    - listitem [ref=s1e1909]:\n                        - link "前提のモデル構造" [ref=s1e1910]:\n                            - /url: "#%E5%89%8D%E6%8F%90%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E6%A7%8B%E9%80%A\\\n                                0"\n                    - listitem [ref=s1e1911]:\n                        - link "DeepSeek-R1-Zeroを強化学習を用いて作る" [ref=s1e1912]:\n                            - /url: "#deepseek-r1-zero%E3%82%92%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E3%82%92%\\\n                                E7%94%A8%E3%81%84%E3%81%A6%E4%BD%9C%E3%82%8B"\n                    - listitem [ref=s1e1913]:\n                        - link "DeepSeek-R1をSFT+強化学習を用いて作る" [ref=s1e1914]:\n                            - /url: "#deepseek-r1%E3%82%92sft%2B%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E3%82%92\\\n                                %E7%94%A8%E3%81%84%E3%81%A6%E4%BD%9C%E3%82%8B"\n                    - listitem [ref=s1e1915]:\n                        - link "小型モデルへの蒸留を試す" [ref=s1e1916]:\n                            - /url: "#%E5%B0%8F%E5%9E%8B%E3%83%A2%E3%83%87%E3%83%AB%E3%81%B8%E3%81%AE%E8%92%B\\\n                                8%E7%95%99%E3%82%92%E8%A9%A6%E3%81%99"\n            - listitem [ref=s1e1917]:\n                - link "DeepSeek-R1-Zeroを作るまで" [ref=s1e1918]:\n                    - /url: "#deepseek-r1-zero%E3%82%92%E4%BD%9C%E3%82%8B%E3%81%BE%E3%81%A7"\n                - list [ref=s1e1919]:\n                    - listitem [ref=s1e1920]:\n                        - link "元々のアーキテクチャ" [ref=s1e1921]:\n                            - /url: "#%E5%85%83%E3%80%85%E3%81%AE%E3%82%A2%E3%83%BC%E3%82%AD%E3%83%86%E3%82%A\\\n                                F%E3%83%81%E3%83%A3"\n                    - listitem [ref=s1e1922]:\n                        - link "すごいぞ！強化学習" [ref=s1e1923]:\n                            - /url: "#%E3%81%99%E3%81%94%E3%81%84%E3%81%9E%EF%BC%81%E5%BC%B7%E5%8C%96%E5%AD%A\\\n                                6%E7%BF%92"\n                    - listitem [ref=s1e1924]:\n                        - link "そもそもLLMで強化学習とは？" [ref=s1e1925]:\n                            - /url: "#%E3%81%9D%E3%82%82%E3%81%9D%E3%82%82llm%E3%81%A7%E5%BC%B7%E5%8C%96%E5%A\\\n                                D%A6%E7%BF%92%E3%81%A8%E3%81%AF%EF%BC%9F"\n                    - listitem [ref=s1e1926]:\n                        - link "ZeroのSystemプロンプト" [ref=s1e1927]:\n                            - /url: "#zero%E3%81%AEsystem%E3%83%97%E3%83%AD%E3%83%B3%E3%83%97%E3%83%88"\n                    - listitem [ref=s1e1928]:\n                        - link "Zeroの「Aha Moment」" [ref=s1e1929]:\n                            - /url: "#zero%E3%81%AE%E3%80%8Caha-moment%E3%80%8D"\n                    - listitem [ref=s1e1930]:\n                        - link "DeepSeek-R1-Zeroのまとめ" [ref=s1e1931]:\n                            - /url: "#deepseek-r1-zero%E3%81%AE%E3%81%BE%E3%81%A8%E3%82%81"\n            - listitem [ref=s1e1932]:\n                - link "DeepSeek-R1への道" [ref=s1e1933]:\n                    - /url: "#deepseek-r1%E3%81%B8%E3%81%AE%E9%81%93"\n                - list [ref=s1e1934]:\n                    - listitem [ref=s1e1935]:\n                        - link "DeepSeek-R1-Zeroの課題" [ref=s1e1936]:\n                            - /url: "#deepseek-r1-zero%E3%81%AE%E8%AA%B2%E9%A1%8C"\n                    - listitem [ref=s1e1937]:\n                        - link "課題対処への道" [ref=s1e1938]:\n                            - /url: "#%E8%AA%B2%E9%A1%8C%E5%AF%BE%E5%87%A6%E3%81%B8%E3%81%AE%E9%81%93"\n                    - listitem [ref=s1e1939]:\n                        - link "DeepSeek-R1のまとめ" [ref=s1e1940]:\n                            - /url: "#deepseek-r1%E3%81%AE%E3%81%BE%E3%81%A8%E3%82%81"\n            - listitem [ref=s1e1941]:\n                - link "結果" [ref=s1e1942]:\n                    - /url: "#%E7%B5%90%E6%9E%9C"\n            - listitem [ref=s1e1943]:\n                - link "蒸留" [ref=s1e1944]:\n                    - /url: "#%E8%92%B8%E7%95%99"\n                - list [ref=s1e1945]:\n                    - listitem [ref=s1e1946]:\n                        - link "蒸留 vs 強化学習" [ref=s1e1947]:\n                            - /url: "#%E8%92%B8%E7%95%99-vs-%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92"\n            - listitem [ref=s1e1948]:\n                - link "今後の展望" [ref=s1e1949]:\n                    - /url: "#%E4%BB%8A%E5%BE%8C%E3%81%AE%E5%B1%95%E6%9C%9B"\n            - listitem [ref=s1e1950]:\n                - link "まとめ" [ref=s1e1951]:\n                    - /url: "#%E3%81%BE%E3%81%A8%E3%82%81"\n            - listitem [ref=s1e1952]:\n                - link "参考文献" [ref=s1e1953]:\n                    - /url: "#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"\n        - paragraph [ref=s1e1955]: Zennからのお知らせ\n        - article [ref=s1e1958]:\n            - link "第２回 AI Agent Hackathon with Google Cloud 第２回 AI Agent Hackathon with Google Cloud 4/14登録受付開始！" [ref=s1e1959]:\n                - /url: /hackathons/google-cloud-japan-ai-hackathon-vol2\n                - img "第２回 AI Agent Hackathon with Google Cloud" [ref=s1e1961]\n                - paragraph [ref=s1e1962]: 第２回 AI Agent Hackathon with Google Cloud 4/14登録受付開始！\n        - article [ref=s1e1963]:\n            - link "Zenncafe日比谷#2 生成AIで変わる仕事 Zenncafe日比谷#2 生成AIで変わる仕事" [ref=s1e1964]:\n                - /url: https://zenn.connpass.com/event/351131/\n                - img "Zenncafe日比谷#2 生成AIで変わる仕事" [ref=s1e1966]\n                - paragraph [ref=s1e1967]: Zenncafe日比谷#2 生成AIで変わる仕事\n- contentinfo [ref=s1e1969]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1973]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1974]\n    - paragraph [ref=s1e1982]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e1984]:\n        - heading "About" [level=4] [ref=s1e1985]\n        - list [ref=s1e1986]:\n            - listitem [ref=s1e1987]:\n                - link "Zennについて" [ref=s1e1989]:\n                    - /url: /about\n            - listitem [ref=s1e1990]:\n                - link "運営会社" [ref=s1e1991]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e1992]:\n                - link "お知らせ・リリース" [ref=s1e1993]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e1994]:\n                - link "イベント" [ref=s1e1996]:\n                    - /url: /events\n    - navigation [ref=s1e1997]:\n        - heading "Guides" [level=4] [ref=s1e1998]\n        - list [ref=s1e1999]:\n            - listitem [ref=s1e2000]:\n                - link "使い方" [ref=s1e2002]:\n                    - /url: /manual\n            - listitem [ref=s1e2003]:\n                - link "法人向けメニュー" [ref=s1e2005]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e2007]:\n                - link "Publication / Pro" [ref=s1e2009]:\n                    - /url: /publications\n            - listitem [ref=s1e2010]:\n                - link "よくある質問" [ref=s1e2012]:\n                    - /url: /faq\n    - navigation [ref=s1e2013]:\n        - heading "Links" [level=4] [ref=s1e2014]\n        - list [ref=s1e2015]:\n            - listitem [ref=s1e2016]:\n                - link "X(Twitter)" [ref=s1e2017]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e2018]:\n                - link "GitHub" [ref=s1e2019]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e2020]:\n                - link "メディアキット" [ref=s1e2022]:\n                    - /url: /mediakit\n    - navigation [ref=s1e2023]:\n        - heading "Legal" [level=4] [ref=s1e2024]\n        - list [ref=s1e2025]:\n            - listitem [ref=s1e2026]:\n                - link "利用規約" [ref=s1e2028]:\n                    - /url: /terms\n            - listitem [ref=s1e2029]:\n                - link "プライバシーポリシー" [ref=s1e2031]:\n                    - /url: /privacy\n            - listitem [ref=s1e2032]:\n                - link "特商法表記" [ref=s1e2034]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e2037]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e2038]\n- alert [ref=s1e2057]\n```\n', name='browser_navigate', tool_call_id='d85f8505-9c2a-4960-b17f-9ef88d983d9d'), AIMessage(content='まず、指定されたZennの著者のページにアクセスし、最もいいねが多い記事を特定しました。その結果、「DeepSeek-R1の論文読んだ？【勉強になるよ】」という記事が最も評価されていることがわかりました。\n\n次に、その記事の内容を詳しく調べました。このブログ記事は、DeepSeek-R1という大規模言語モデルに関する論文を解説したものです。記事は、論文の内容をわかりやすく説明することに重点を置いており、特に強化学習の概念や、DeepSeek-R1を開発する上で用いられたGRPO（Group Relative Policy Optimization）という手法について詳しく解説しています。\n\n記事が評価されている理由として、以下の点が考えられます。\n\n1.  **技術的な深さと分かりやすさの両立:** 大規模言語モデルや強化学習といった複雑なトピックを扱いながらも、専門知識がない読者にも理解できるように、かみ砕いた説明がなされています。\n2.  **論文の内容の網羅性:** DeepSeek-R1の論文の重要なポイントを漏れなくカバーしており、論文を読むための良い入門となっています。\n3.  **著者自身の考察:** 論文の内容を解説するだけでなく、著者自身の考察や解釈が加えられており、読者に新たな視点を提供しています。\n4.  **読者への配慮:** 記事の冒頭で、誤りがあるかもしれないことを断り、コメントでの指摘を歓迎するなど、読者とのコミュニケーションを重視する姿勢が見られます。\n5.  **参考文献の紹介:** 記事の最後に、強化学習や関連技術を学ぶための参考文献が紹介されており、読者の学習をサポートする姿勢が評価されていると考えられます。\n\nこれらの要素が組み合わさることで、この記事はDeepSeek-R1に関心のあるエンジニアにとって非常に価値のある情報源となり、高い評価を得ていると考えられます。', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-d5ac6129-cca3-4759-ac6a-eafe68db505f-0', usage_metadata={'input_tokens': 47987, 'output_tokens': 387, 'total_tokens': 48374, 'input_token_details': {'cache_read': 0}}), HumanMessage(content=[{'type': 'text', 'text': 'LLMに強化学習ってどういうこと？GRPOって何？'}], additional_kwargs={}, response_metadata={}), AIMessage(content='LLM（大規模言語モデル）に対する強化学習と、GRPO（Group Relative Policy Optimization）について説明します。\n\n### LLMに対する強化学習\n\nLLMに対する強化学習は、LLMがより良いテキストを生成するように学習させるための手法です。通常のLLMの学習では、大量のテキストデータを用いて、与えられたテキストの続きを予測するように学習します。これに対して、強化学習では、LLMが生成したテキストに対して「報酬」を与え、その報酬を最大化するように学習します。\n\n**なぜ強化学習が必要なのか？**\n\nLLMは、大量のテキストデータから学習することで、自然な文章を生成する能力を獲得します。しかし、必ずしも人間が望むような文章を生成するとは限りません。例えば、以下のような課題があります。\n\n*   **目的の達成:** 特定のタスク（質問応答、要約など）において、必ずしも最適な回答を生成するとは限らない。\n*   **安全性:** 有害な情報や偏見のある情報を生成する可能性がある。\n*   **創造性:** 新しいアイデアや表現を生み出す能力が限られている。\n\n強化学習は、これらの課題を克服するために用いられます。\n\n**強化学習の基本的な流れ**\n\n1.  **LLM（エージェント）:** テキストを生成する主体。\n2.  **環境:** LLMがテキストを生成する対象となるタスクや状況。\n3.  **報酬:** LLMが生成したテキストの良し悪しを評価する指標。\n4.  **学習:** LLMが報酬を最大化するように、テキスト生成の方針（方策）を調整する。\n\n具体的には、以下のような手順で学習が進みます。\n\n1.  LLMがプロンプト（指示）を受け取り、テキストを生成します。\n2.  生成されたテキストに対して、報酬関数が報酬を与えます。\n    *   例えば、質問応答タスクであれば、正解との一致度合いが高いほど高い報酬を与えます。\n    *   安全性に関する報酬であれば、有害な情報を生成した場合に低い報酬を与えます。\n3.  LLMは、その報酬に基づいて、自身のテキスト生成の方針を調整します。\n    *   具体的には、報酬が高くなるような単語やフレーズを選択する確率を高めます。\n4.  このプロセスを繰り返すことで、LLMはより高い報酬を得られるようなテキストを生成するように学習していきます。\n\n### GRPO（Group Relative Policy Optimization）とは？\n\nGRPOは、LLMに対する強化学習の一種で、特に大規模なLLMを効率的に学習させるために開発された手法です。\n\n**PPO（Proximal Policy Optimization）との比較**\n\nLLMの強化学習でよく用いられる手法として、PPO（Proximal Policy Optimization）があります。PPOでは、報酬に加えて、LLMの出力が大きく変化しないように制約を加えながら学習を進めます。しかし、PPOでは、状態価値モデルという追加のモデルを学習する必要があり、計算コストが高くなるという課題がありました。\n\nGRPOは、この状態価値モデルを不要にし、より効率的に学習を進めることを目指します。\n\n**GRPOの仕組み**\n\nGRPOでは、LLMに複数のテキストを生成させ、それらのテキストに対する報酬を比較することで、相対的な評価を行います。具体的には、以下の手順で学習が進みます。\n\n1.  LLMが同じプロンプトに対して、複数のテキスト（例えば4つ）を生成します。\n2.  それぞれのテキストに対して報酬関数が報酬を与えます。\n3.  それぞれのテキストの報酬を比較し、最も高い報酬を得たテキストを「良いテキスト」、それ以外のテキストを「悪いテキスト」とします。\n4.  LLMは、「良いテキスト」を生成する確率を高め、「悪いテキスト」を生成する確率を下げるように、自身のテキスト生成の方針を調整します。\n\nこのように、GRPOでは、テキスト間の相対的な評価を用いることで、状態価値モデルを学習する必要がなくなり、計算コストを削減することができます。\n\n**GRPOの利点**\n\n*   **計算効率:** 状態価値モデルが不要なため、PPOよりも計算コストが低い。\n*   **大規模LLMへの適用:** 大規模なLLMでも効率的に学習できる。\n\n**まとめ**\n\nLLMに対する強化学習は、LLMがより良いテキストを生成するように学習させるための手法であり、GRPOはその効率的な学習を可能にする手法の一つです。GRPOは、テキスト間の相対的な評価を用いることで、状態価値モデルを不要にし、計算コストを削減することができます。', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-4d11a554-e7d5-4420-9ac9-b8efaccf8b11-0', usage_metadata={'input_tokens': 48388, 'output_tokens': 996, 'total_tokens': 49384, 'input_token_details': {'cache_read': 0}}), HumanMessage(content=[{'type': 'text', 'text': '2番目にいいねが多い記事は何？'}], additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-aae69546-e8ee-4891-9040-daee70739fa4-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap'}, 'id': '61c67f9f-6525-4601-af0d-36cd5f4631a2', 'type': 'tool_call'}], usage_metadata={'input_tokens': 49392, 'output_tokens': 13, 'total_tokens': 49405, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap\n\n- Page URL: https://zenn.dev/asap\n- Page Title: asapさんの記事一覧 | Zenn\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- banner [ref=s1e32]:\n    - img "asap" [ref=s1e36]\n    - heading "asap" [level=1] [ref=s1e39]\n    - button "フォロー" [ref=s1e41]\n    - paragraph [ref=s1e43]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n    - button "2,214 Likes" [ref=s1e46]\n    - button "310 Followers" [ref=s1e48]\n    - link "@personabb" [ref=s1e51]:\n        - /url: https://github.com/personabb\n        - tooltip "@personabb" [ref=s1e52]:\n            - img "GitHub" [ref=s1e53]\n            - text: "@personabb"\n    - link "@asap2650" [ref=s1e55]:\n        - /url: https://twitter.com/asap2650\n        - tooltip "@asap2650" [ref=s1e56]:\n            - img "X(Twitter)" [ref=s1e57]\n            - text: "@asap2650"\n    - link "yard.tips" [ref=s1e63]:\n        - /url: https://yard.tips/asap2650\n        - tooltip "yard.tips" [ref=s1e64]:\n            - img [ref=s1e65]\n            - text: yard.tips\n    - link "RSS" [ref=s1e67]:\n        - /url: /asap/feed\n        - tooltip "RSS" [ref=s1e68]:\n            - img [ref=s1e69]\n            - text: RSS\n- link "Articles 72" [ref=s1e80]:\n    - /url: /asap\n- link "Scraps 5" [ref=s1e82]:\n    - /url: /asap?tab=scraps\n- link "Comments" [ref=s1e84]:\n    - /url: /asap?tab=comments\n- contentinfo [ref=s1e88]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e92]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e93]\n    - paragraph [ref=s1e101]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e103]:\n        - heading "About" [level=4] [ref=s1e104]\n        - list [ref=s1e105]:\n            - listitem [ref=s1e106]:\n                - link "Zennについて" [ref=s1e108]:\n                    - /url: /about\n            - listitem [ref=s1e109]:\n                - link "運営会社" [ref=s1e110]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e111]:\n                - link "お知らせ・リリース" [ref=s1e112]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e113]:\n                - link "イベント" [ref=s1e115]:\n                    - /url: /events\n    - navigation [ref=s1e116]:\n        - heading "Guides" [level=4] [ref=s1e117]\n        - list [ref=s1e118]:\n            - listitem [ref=s1e119]:\n                - link "使い方" [ref=s1e121]:\n                    - /url: /manual\n            - listitem [ref=s1e122]:\n                - link "法人向けメニュー" [ref=s1e124]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e126]:\n                - link "Publication / Pro" [ref=s1e128]:\n                    - /url: /publications\n            - listitem [ref=s1e129]:\n                - link "よくある質問" [ref=s1e131]:\n                    - /url: /faq\n    - navigation [ref=s1e132]:\n        - heading "Links" [level=4] [ref=s1e133]\n        - list [ref=s1e134]:\n            - listitem [ref=s1e135]:\n                - link "X(Twitter)" [ref=s1e136]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e137]:\n                - link "GitHub" [ref=s1e138]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e139]:\n                - link "メディアキット" [ref=s1e141]:\n                    - /url: /mediakit\n    - navigation [ref=s1e142]:\n        - heading "Legal" [level=4] [ref=s1e143]\n        - list [ref=s1e144]:\n            - listitem [ref=s1e145]:\n                - link "利用規約" [ref=s1e147]:\n                    - /url: /terms\n            - listitem [ref=s1e148]:\n                - link "プライバシーポリシー" [ref=s1e150]:\n                    - /url: /privacy\n            - listitem [ref=s1e151]:\n                - link "特商法表記" [ref=s1e153]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e156]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e157]\n- alert [ref=s1e176]\n```\n', name='browser_navigate', tool_call_id='61c67f9f-6525-4601-af0d-36cd5f4631a2'), AIMessage(content='asapさんのZennの記事の中で2番目にいいねが多い記事は、「VAEって結局何者なの？」で、いいね数は104です。', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-7c7bf5a2-7650-41d4-9cc5-c1fb3e0de2f1-0', usage_metadata={'input_tokens': 51212, 'output_tokens': 32, 'total_tokens': 51244, 'input_token_details': {'cache_read': 0}}), HumanMessage(content=[{'type': 'text', 'text': 'どんな記事ですか？解説してください。'}], additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap/articles/6caa9043276424"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-093f5ac0-5315-41a0-aaf8-f4fef2243382-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap/articles/6caa9043276424'}, 'id': '45a87836-eb09-4499-bf44-a2bcdbd3ec2b', 'type': 'tool_call'}], usage_metadata={'input_tokens': 51251, 'output_tokens': 28, 'total_tokens': 51279, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap/articles/6caa9043276424\n\n- Page URL: https://zenn.dev/asap/articles/6caa9043276424\n- Page Title: VAEって結局何者なの？\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- article [ref=s1e32]:\n    - text: 📖\n    - heading "VAEって結局何者なの？" [level=1] [ref=s1e38]\n    - text: 2025/01/20に公開\n    - link "ディープラーニング" [ref=s1e52]:\n        - /url: /topics/deeplearning\n        - img [ref=s1e54]\n        - text: ディープラーニング\n    - link "VAE" [ref=s1e56]:\n        - /url: /topics/vae\n        - img [ref=s1e58]\n        - text: VAE\n    - link "Stable Diffusion" [ref=s1e60]:\n        - /url: /topics/stablediffusion\n        - img [ref=s1e62]\n        - text: Stable Diffusion\n    - link "生成 AI" [ref=s1e64]:\n        - /url: /topics/%E7%94%9F%E6%88%90ai\n        - img [ref=s1e66]\n        - text: 生成 AI\n    - link "elbo" [ref=s1e68]:\n        - /url: /topics/elbo\n        - img [ref=s1e70]\n        - text: elbo\n    - link "tech" [ref=s1e72]:\n        - /url: /tech-or-idea\n        - img [ref=s1e74]\n        - text: tech\n    - heading "はじめに" [level=1] [ref=s1e78]\n    - paragraph [ref=s1e79]: 今回は、満を持してVAE（Variational Auto Encoder）をちゃんと理解していこうと思います。\n    - paragraph [ref=s1e80]: VAEに関しては、だいたい知っていますが、MusicGenという音楽生成AIを理解しようと思った時に、関連してRVQ-GANが出てきたので、再勉強をしています。\n    - paragraph [ref=s1e81]: 今後、下記の流れで記事を書いていく予定です。\n    - paragraph [ref=s1e82]: VAE（今回） ↓ VQ-VAE ↓ RQ-VAE,RVQ-GAN ↓ MusicGen\n    - paragraph [ref=s1e89]: 今回は、流れの最初であり、現在でも非常に重要な概念であるVAEに関して記事を書きます。\n    - paragraph [ref=s1e90]: VAEは、Stable\n        Diffusionの中でも中核として使われていたりと、比較的古い概念でありながら、まだまだ活躍が期待できる汎用ネットワークです。\n        本記事が、みなさまの理解の手助けになれば、これほど嬉しいことはございません。\n    - heading "参考文献" [level=1] [ref=s1e92]\n    - paragraph [ref=s1e93]:\n        - iframe [ref=s1e95]:\n            - link "Auto-Encoding Variational Bayes How can we perform efficient inference and learning in directed probabilistic models, in the presenc arxiv.org favicon image arxiv.org arxiv.org thumbnail image" [ref=f1s1e6]:\n                - /url: https://arxiv.org/abs/1312.6114\n                - heading "Auto-Encoding Variational Bayes" [level=1]\n                  [ref=f1s1e8]\n                - text: How can we perform efficient inference and learning in directed\n                    probabilistic models, in the presenc\n                - img "arxiv.org favicon image" [ref=f1s1e11]\n                - text: arxiv.org\n                - img "arxiv.org thumbnail image" [ref=f1s1e13]\n            - alert [ref=f1s1e15]\n        - text: VAEの論文です。\n    - paragraph [ref=s1e96]:\n        - link "ゼロから作るDeep Learning ❺ ―生成モデル編" [ref=s1e97]:\n            - /url: https://amzn.to/3Aw9vCt\n    - paragraph [ref=s1e98]: 圧倒的な名著シリーズである「ゼロから作るDeep Learning」シリーズの5作目である、生成モデル編です。\n        この本では、最終的に拡散モデルを理解することを目的に、VAEや混合ガウスモデル、正規分布まで遡り、本当にゼロから中身を理解しながら実装をすることができる、大変素晴らしい書籍になっています。\n    - paragraph [ref=s1e100]:\n        - link "妥協しないデータ分析のための 微積分+線形代数入門" [ref=s1e101]:\n            - /url: https://amzn.to/4hE7ftx\n    - paragraph [ref=s1e102]: 本書の後半にもVAEやELBOについての簡単な記載があり、本記事を書くうえで表現を参考にさせていただきました。\n        確率分布のどの部分が計算可能で、どの部分が計算不可能なのかが常に明示されながら進んでいくので、初学者には非常に分かりやすい書籍でした！\n    - paragraph [ref=s1e104]: 特に、VAEなどの生成AIをただ使うだけでなく、ちゃんと理論を理解したいと思った場合、論文や理論書を理解するために「線形代数」や「微積分学」の事前知識が必要になります。\n        本書は、初学者が一番最初に読んで必要な基礎知識が習得できる良本になります。\n    - paragraph [ref=s1e106]:\n        - link "生成 Deep Learning 第2版 ―絵を描き、物語や音楽を作り、ゲームをプレイする" [ref=s1e107]:\n            - /url: https://amzn.to/4g8hjJg\n    - paragraph [ref=s1e108]: 本書の序盤で、Auto\n        EncoderとVAEの違いを説明してくれています。しかもそれぞれの潜在空間の分布を可視化してくれているので、違いが分かりやすく、理解もしやすいです。\n        さらに、本書はVAEだけでなくGANや画像生成、音楽生成、テキスト生成など、ありとあらゆる生成AIの紹介と解説をしてくれており、必読の一冊となっております。\n    - paragraph [ref=s1e110]: （書籍のリンクはAmazonアフィリエイトリンクになっております）\n    - heading "関連記事" [level=1] [ref=s1e111]\n    - paragraph [ref=s1e112]: VAEに関連した記事は過去のも書いております。 こちらの記事もご覧いただけると嬉しいです！\n    - paragraph [ref=s1e114]:\n        - iframe [ref=s1e116]:\n            - link "【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】 asapさんによる記事 zenn.dev favicon image zenn.dev zenn.dev thumbnail image" [ref=f2s1e6]:\n                - /url: https://zenn.dev/asap/articles/ea9f42edff6573\n                - heading\n                  "【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】"\n                  [level=1] [ref=f2s1e8]\n                - text: asapさんによる記事\n                - img "zenn.dev favicon image" [ref=f2s1e11]\n                - text: zenn.dev\n                - img "zenn.dev thumbnail image" [ref=f2s1e13]\n            - alert [ref=f2s1e15]\n    - heading "Auto Encoder" [level=1] [ref=s1e117]\n    - heading "Auto Encoderの紹介" [level=2] [ref=s1e118]\n    - paragraph [ref=s1e119]: VAEを語る上で欠かせないのは、Auto Encoderの存在です。 Auto\n        Encoderというのは、ニューラルネットワークにより構築されたネットワーク構造の一つで、入力と出力を一致させるように学習させるネットワークです。\n    - paragraph [ref=s1e121]:\n        - img [ref=s1e122]\n    - paragraph [ref=s1e123]: 最も単純なAuto Encoderは上記のような構造をしています。\n        前段は、入力データから、それより次元の小さな潜在表現を生成するEncoderです。\n        上の図では、入力データを重みデータによる線形変換（+活性化層における非線形変換）で成り立っています。\n        Encoderでは、次元の大きな入力データを、次元の小さい潜在表現に圧縮する処理を行います。\n    - paragraph [ref=s1e127]: 後段は、潜在表現から、それより次元の大きな出力データを生成するDecoderです。\n        出力データは入力と一致するように学習がなされています。\n        このとき、Decoderでは、次元の小さな潜在表現から、出力データ（入力データ）を復元する処理を行います。\n        この「圧縮(符号化)」と「復元(複合化)」の構造から、「Encoder」「Decoder」と名付けられており、この圧縮・復元をデータを与えれば自動で実施するニューラルネットワークなので、「Auto\n        Encoder」と呼ばれます。\n    - heading "Auto Encoderの本質" [level=2] [ref=s1e131]\n    - paragraph [ref=s1e132]: ここでの本質は、次元の大きい入力データを、潜在表現に圧縮し、その潜在表現から、入力データを再構成することです。\n    - heading "小さな潜在表現から元データを復元できる理由" [level=3] [ref=s1e133]\n    - paragraph [ref=s1e134]: これが達成される理由は2つあります。\n    - paragraph [ref=s1e135]: 1つ目は、世の中にあるデータは冗長であることです。\n        世の中に存在するデータを冗長なデータが多いです。例えば画像データであれば、生の01でデータを保持するよりも、pngなどの圧縮形式を利用してデータを保存した方が、データ量を抑えることができます。\n        これは、画像中の「隣接する画素は、近い値が多い」という事前知識を用いることで、データを効率的に圧縮することができるからです。\n    - complementary [ref=s1e138]:\n        - text: "!"\n        - paragraph [ref=s1e141]: 話が脱線しますが、pngは可逆圧縮と呼ばれる手法であるため、この手法を利用してデータ量を削減した場合であっても、元の画像データを「完全に」復元することができます。これはzipなども同様です。\n        - paragraph [ref=s1e142]: 一方、画像でよく使われている、jpgやHEIFなどの規格は、非可逆圧縮と呼ばれる手法です。\n            この手法でデータ量を削減した場合は、元の画像から劣化することになります。\n            ただし、その劣化具合はみなさんご存知の通り非常に小さく、かつデータ削減量も可逆圧縮手法と比較して大幅に削減できます。\n    - paragraph [ref=s1e145]: 2つ目は、Auto Encoderの重みデータに、入力データを復元するための情報を学習しているからです。\n    - paragraph [ref=s1e146]: 例えば画像であれば、Auto\n        Encoderを利用する際に、事前に大規模画像データセットを利用して学習が行われます。その学習を通して、Decoderは「どうやったら少ない潜在表現から画像データを品質高く復元できるのか」、Encoderは「大量の画像データのうち、どの情報を潜在表現に埋め込めば、Decoderが元の画像を復元できるのか」を学習しています。\n        その結果、ちょうどpngで説明した通り、「隣接する画素は、近い値が多い」のような自然画像全般に通ずる事前知識を獲得し、その情報を用いることで、少ないデータ量から、元の入力データを復元するようなネットワークを構築することができるというわけです。\n    - heading "構造" [level=3] [ref=s1e148]\n    - paragraph [ref=s1e149]: 先ほど記載した通り、Auto\n        Encoderの本質は、次元の大きい入力データを、潜在表現に圧縮し、小さな潜在表現から、入力データを再構成することです。\n        そして、これが達成できるのであれば、どんなEncoder,Decoderの構造をしていたとしても、それはAuto\n        Encoderの一種であると考えることができます。\n    - paragraph [ref=s1e151]: 例えば画像を処理する際には、しばしば畳み込みニューラルネットワークやTransformerなどが使われます。\n        当然、これらを利用したAuto Encoderも考えることができます。\n    - paragraph [ref=s1e153]: 潜在表現も単純な1次元データだけではなく、画像のような3次元（縦, 横,\n        チャネル）データを考えることができます。 （ResNetなどのGAP前の層の出力結果をEncoderの出力として利用することもできます）\n    - paragraph [ref=s1e155]: したがって、以降ではEncoder Decoder構造を記載する際には、下記のように記載することにします。\n    - paragraph [ref=s1e156]:\n        - img [ref=s1e157]\n    - heading "Auto Encoerの課題" [level=2] [ref=s1e158]\n    - paragraph [ref=s1e159]: Auto Encoerはそれ単体では、それなりに優れた情報圧縮器ではありますが、それ以外での活用可能性が難しいネットワークではありました。\n    - paragraph [ref=s1e160]: 入力データと形状は同じな、出力データを出力する際には、ネットワーク構造が使われることも多いです（例えばセマンティックセグメンテーションタスクでU-Netが利用されるなど）が、同じ入出力になるように学習することというのは、情報圧縮以外の文脈で利用するのは難しいです。\n    - paragraph [ref=s1e161]: そこで、生成モデルとしての活用可能性が考えられました、\n        例えば、潜在表現を何らかの形で得ることができれば、その潜在表現をDecoderに入力することで、対応した出力データ（画像など）を得ることができます。\n        これが達成されれば、AIに絵を描かせたり、音楽を生成させたりすることが可能です。\n    - paragraph [ref=s1e164]: ただし、Auto Encoderでは、その潜在表現をEncoderを利用すること以外で取得することは困難です。\n        ここで、Auto Encoderによる画像生成の夢は途絶えたかのように見えました。\n    - heading "VAE（Variational Auto Encoder）" [level=1] [ref=s1e166]\n    - heading "VAEと生成モデル" [level=2] [ref=s1e167]\n    - heading "生成モデルの観点から見たVAE" [level=3] [ref=s1e168]\n    - paragraph [ref=s1e169]:\n        - text: VAEというのは、生成モデルとして重要な「対数尤度の最大化」問題を解くために必要な構造を用意したら、\n        - strong [ref=s1e171]: 結果的にAuto Encoderのような形になったというモデルです。\n    - paragraph [ref=s1e172]: すなわち、Auto\n        Encoderの構造があって、それをこうしたら生成モデルとして使えるようになった。というよりは、生成モデルを作る上で重要な「対数尤度を最大化する」という方向性で、理論的に式変形を繰り返し、計算不可能な部分を取り除き、計算が難しい部分をニューラルネットで近似することで、結果としてAuto\n        Encoderの構造が現れたという解釈があっているような気がします。\n    - heading "対数尤度の最大化からモデル構造を考える" [level=3] [ref=s1e173]\n    - heading "対数尤度の最大化に関しての軽い復習" [level=4] [ref=s1e174]\n    - paragraph [ref=s1e175]:\n        - text: 「対数尤度の最大化」に関しては下記の記事で解説しておりますので、そちらをご覧ください。\n        - iframe [ref=s1e177]: <could not take iframe snapshot>\n    - paragraph [ref=s1e178]:\n        - text: 上記の記事では詳細に記載していますが、簡単に解説すると、生成モデルにおいて重要なのは、対数尤度\n        - math [ref=s1e182]: p θ ( x )\n        - text: を最大化することです。 ここで、 x は実際のデータで、 \\theta は生成モデルのパラメータです。 したがって、 p_\\theta(x)\n            は対数尤度の確率分布になります。\n    - paragraph [ref=s1e199]:\n        - text: 生成モデルのパラメータ自体にはランダム性がないため、様々なバリエーションのデータを生成するためには、入力にランダムにサンプリングされたデータや定数を入力し、生成モデルがデータを再構成します。\n            イメージとしては下記のイメージです。\n        - img [ref=s1e202]\n    - complementary [ref=s1e203]:\n        - text: "!"\n        - paragraph [ref=s1e206]:\n            - text: ネットワークが確率分布を生成するというのは違和感があるかと思います。 その辺りの詳細は、下記の記事でも解説しているので参考にしてみてください。\n            - link "https://zenn.dev/asap/articles/4092ab60570b05" [ref=s1e209]:\n                - /url: https://zenn.dev/asap/articles/4092ab60570b05\n        - paragraph [ref=s1e210]: "ここでは、ネットワークが出力したデータ y を「平均」、単位行列 I を「分散」とする多変数正規分布\n            \\\\mathcal{N}(y, I) を考えることで、Decoderの出力を確率分布として計算しています。\n            実用上は、ネットワークが出力した平均 y の値を、そのまま生成したデータ x\'=y として扱います。"\n    - paragraph [ref=s1e222]: "しかし、上記で計算しているのは、尤度 p_{\\\\theta}(x|z) です。\n        （なぜなら、Decoderはパラメータ \\\\theta と潜在表現 z によって条件付けられたネットワークであるためです。）"\n    - paragraph [ref=s1e230]: これを用いて対数尤度 p_\\theta(x) を計算するには、下記の式を最適化する必要があります。\n    - text: "\\\\log p_{\\\\theta}(x) = \\\\mathbb{E}_{q_{\\\\psi}(z|x)}[\\\\log\n        p_{\\\\theta}(x|z)] - \\\\mathrm{KL}\\\\left(q_{\\\\psi}(z|x) \\\\parallel\n        p(z)\\\\right) + \\\\mathrm{KL}\\\\left(q_{\\\\psi}(z|x) \\\\parallel\n        p_{\\\\theta}(z|x)\\\\right)"\n    - paragraph [ref=s1e236]: "この中で、事後分布 p_{\\\\theta}(z|x) は計算不可能なため、近似事後分布\n        q_{\\\\psi}(z|x) を用意して、KLダイバージェンスに押し込みます。\n        そうすることで、第三項のKLダイバージェンスは非負であるため、この項を除いた部分(ELBO)を最大化することで、間接的に対数尤度を最大化するこ\\\n        とができます。"\n    - paragraph [ref=s1e242]: "このとき必要な、近似事後分布 q_{\\\\psi}(z|x) を計算するために、パラメータ \\\\psi\n        を持ち、データ x を入力して、潜在表現 z の確率分布を出力するようなネットワークとして、Encoderが必要になります。"\n    - paragraph [ref=s1e251]:\n        - text: 詳しくは\n        - link "上述した記事" [ref=s1e252]:\n            - /url: https://zenn.dev/asap/articles/ea9f42edff6573\n        - text: をご覧ください。\n    - heading "Encoderが平均と分散を出力する理由" [level=4] [ref=s1e253]\n    - paragraph [ref=s1e254]:\n        - text: ここまでの議論を考慮すると、下記のようなモデル構造になります。\n        - img [ref=s1e256]\n    - paragraph [ref=s1e257]: "上述した通り、Decoderの場合は、Decoderが出力するデータ y を「平均」、単位行列 I\n        を「分散」とする多変数正規分布 \\\\mathcal{N}(y, I) を確率分布として持ちますが、実用上は、Decoderが出力した平均 y\n        の値を、そのまま生成データ x\'=y と考えることができます。"\n    - paragraph [ref=s1e268]: 一方で、Encoderは確率分布を生成する必要があります。\n        さらにいうと、ELBOの式の中で、Encoderが出力する確率分布を用いて下記の式を計算する必要があります。\n    - text: "\\\\mathrm{KL}\\\\left(q_{\\\\psi}(z|x) \\\\parallel p(z)\\\\right)"\n    - paragraph [ref=s1e273]: ここで、 p(z) は潜在表現 z の事前分布なので、ベイズ推定等と同様に、こちらが設定することができます。\n        ネットワークは、こちらが設定した事前分布が成立するように学習します。\n        今回の場合、真の事前分布がどんな形状になるのかの事前情報を持っていないので、あまりバイアス情報が入らないような分布を設定することが望ましいです。\n    - paragraph [ref=s1e280]: "問題は、 q_{\\\\psi}(z|x) です。"\n    - paragraph [ref=s1e283]: この分布は特殊な分布になってはいけません。 なぜなら、KLダイバージェンスを計算できる分布の種類はあまり多くないからです。\n    - complementary [ref=s1e285]:\n        - text: "!"\n        - paragraph [ref=s1e288]: 一般的な連続確率分布同士のKLダイバージェンスを計算するのは、積分計算が入るため困難です。\n            しかしながら、指数型分布のような確率分布であれば計算できる余地があります。\n        - paragraph [ref=s1e290]: 例えば、正規分布同士のKLダイバージェンスであれば、比較的簡単に計算をすることができます。\n    - paragraph [ref=s1e291]: したがって、Encoderが生成する確率分布に、何らかの制限を加える必要があります。 今回は、確率分布が正規分布になるように制限をかけます。\n    - paragraph [ref=s1e293]: "その場合、Encoderは、平均 \\\\mu と分散 \\\\sigma^2\n        のパラメータを出力するような構造を考えることで、Encoderが正規分布を表現することができます。 そして、Encoderが出力した、平均\n        \\\\mu ・分散 \\\\sigma^2 パラメータから得られる正規分布 \\\\mathcal{N}(\\\\mu, \\\\sigma)\n        からサンプリングされた値を潜在表現 z とすることで、ELBOを計算することができます。 この場合、計算効率化のため、事前分布 p(z)\n        には平均 0 、分散 I の標準正規分布 \\\\mathcal{N}(0, I) を設定します。 標準正規分布 \\\\mathcal{N}(0,\n        I) は、正規分布とのKLダイバージェンスの計算が簡単であることと、平均 0 、分散 I\n        の条件下におけるエントロピーが最大になる分布（つまり、そのほかの事前情報を含まない分布）であるため、採用されています。"\n    - paragraph [ref=s1e323]: これを達成するためのモデル構造は下記になります。\n    - paragraph [ref=s1e324]:\n        - img [ref=s1e325]\n    - paragraph [ref=s1e326]:\n        - text: これでVAEを構築することができました。 上の図をよく見ると、\n        - strong [ref=s1e329]: まるでAuto Encoderのような構造になっていると思います。\n    - heading "VAEの挙動・振る舞い" [level=2] [ref=s1e330]\n    - heading "VAEの目的関数" [level=3] [ref=s1e331]\n    - paragraph [ref=s1e332]: 改めてVAEの目的関数は下記になります。\n    - complementary [ref=s1e333]:\n        - text: "!"\n        - paragraph [ref=s1e336]: ELBO\n        - text: "\\\\mathrm{ELBO} = \\\\mathbb{E}_{q_{\\\\psi}(z|x)}[\\\\log p_{\\\\theta}(x|z)] -\n            \\\\mathrm{KL}\\\\left(q_{\\\\psi}(z|x) \\\\parallel p(z)\\\\right)"\n    - paragraph [ref=s1e340]: このELBOを最大化するようにVAEのネットワークパラメータ( \\theta 、 \\psi )が最適化されます。\n    - paragraph [ref=s1e345]:\n        - text: ここで、\n        - link "先日書いた記事" [ref=s1e346]:\n            - /url: https://zenn.dev/asap/articles/ea9f42edff6573#%E7%AC%AC%E4%B8%80%E9%A0%85%E3%82%92%E8%80%83%E3%81%88%E3%82%8B\n        - text: にも記載しましたが、第1項は「再構成誤差」、第2項は「正則化項」になります。\n    - paragraph [ref=s1e347]: 式変形を行うと、第1項はDecoderの出力 y と、正解データ x との2乗誤差の負数になります。\n        したがってELBOを最大化する際には、2乗誤差を最小化する方向にVAEのネットワークパラメータ( \\theta 、 \\psi\n        )が最適化されます。\n    - complementary [ref=s1e357]:\n        - text: "!"\n        - paragraph [ref=s1e360]: なお、ここで再構成誤差が2乗誤差となるのは、VAEの出力の確率分布を正規分布と仮定したためです。\n            もし、出力の分布を「ベルヌーイ分布」と仮定した場合、再構成誤差項は下記のようにクロスエントロピー損失と等価になります。\n        - paragraph [ref=s1e362]: ベルヌーイ分布は、データ内のあるサンプル i を考えると、下記で表せます。\n        - text: "p_{\\\\theta}(x_i|z_i) = y_i^{x_i} (1 - y_i)^{1 - x_i}"\n        - paragraph [ref=s1e368]: "この時、ELBOの第1項は、期待値計算において、テンプルサイズ1で近似すると、 \\\\log\n            p_{\\\\theta}(x|z) に近似できるため、これを計算すると、"\n        - text: "\\\\log p_{\\\\theta}(x_i|z_i) = x_i \\\\log y_i + (1 - x_i) \\\\log (1 - y_i)"\n        - paragraph [ref=s1e374]: となるため、まさしくクロスエントロピー項となります。\n        - paragraph [ref=s1e375]: ただし、ベルヌーイ分布と仮定するためには、生成するデータ x が[0,1]である必要があります。\n    - paragraph [ref=s1e378]: "第2項は、Encoderが出力する q_{\\\\psi}(z|x) と平均 0 、分散 I の標準正規分布\n        \\\\mathcal{N}(0, I) とのKLダイバージェンスの負数になります。\n        したがってELBOを最大化する際には、KLダイバージェンス自体を最小化する方向にVAEのEncoderパラメータ( \\\\psi\n        )が最適化されます。 KLダイバージェンスは、2つの分布が一致しているときに最小値 0 をとります。したがって、Encoderが出力する確率分布\n        q_{\\\\psi}(z|x) が標準正規分布 \\\\mathcal{N}(0, I) に近づくように最適化されます。"\n    - heading "ELBOの第2項" [level=3] [ref=s1e397]\n    - paragraph [ref=s1e398]: 第1項は再構成誤差のため、分かりやすいと思いますが、第2項がどう言う意味を含むのかが分かりにくいと思います。\n    - complementary [ref=s1e399]:\n        - text: "!"\n        - paragraph [ref=s1e402]: ELBOの第2項\n        - text: "- \\\\mathrm{KL}\\\\left(q_{\\\\psi}(z|x) \\\\parallel p(z)\\\\right)"\n    - paragraph [ref=s1e406]: "ここでは、潜在表現 z の事前分布が標準正規分布 \\\\mathcal{N}(0, I) に従うと仮定していました。"\n    - paragraph [ref=s1e411]:\n        - text: "また、実は、Encoderが出力する近似事後分布 q_{\\\\psi}(z|x) は、ELBOに関係なく、実際の計算不能な真の事後分布\n            p_{\\\\theta}(z|x) に近づくように学習されています。 この詳細は、"\n        - link "以前書いた記事" [ref=s1e417]:\n            - /url: https://zenn.dev/asap/articles/ea9f42edff6573#elbo%E3%81%8C%E5%AF%BE%E6%95%B0%E5%B0%A4%E5%BA%A6%E3%81%AB%E8%BF%91%E3%81%A5%E3%81%8F%E6%9D%A1%E4%BB%B6\n        - text: "をご覧ください。 これは主に、Encoderが出力する近似事後分布 q_{\\\\psi}(z|x) が、真の事後分布\n            p_{\\\\theta}(z|x) に近づくことで、Decoderの再構成誤差が小さくなることに由来します。"\n    - paragraph [ref=s1e423]: "その上で、ELBOの第2項では、Encoderが出力する近似事後分布 q_{\\\\psi}(z|x)\n        は、標準正規分布 \\\\mathcal{N}(0, I) に近づくようにも学習されることを示しています。"\n    - paragraph [ref=s1e428]: "したがって、上記の議論をまとめると、Encoderが出力する近似事後分布 q_{\\\\psi}(z|x)\n        は、標準正規分布 \\\\mathcal{N}(0, I)\n        に近づくようにも学習されますが、その上で、Decoderの再構成誤差とバランスをとりながら、真の事後分布 p_{\\\\theta}(z|x)\n        にも近づくように学習されます。"\n    - paragraph [ref=s1e435]: "さらに言うと、真の事後分布 p_{\\\\theta}(z|x) は、Decoderパラメータ \\\\theta\n        依存して変化します。（計算できないですが、必ず存在し、 \\\\theta と x に依存します） この場合、真の事後分布\n        p_{\\\\theta}(z|x) もEncoderが出力する近似事後分布 q_{\\\\psi}(z|x)\n        に近づくような力学が多少なりとも働くことになります。"\n    - paragraph [ref=s1e449]: したがってDecoder側は、過度に複雑な事後分布をEncoder側に近似することを要求せず、ある程度、真の事後分布が標準正規分布に近づくように、（すなわち標準正規分布からデータを再構成できるように）学習が協力して進んでいく挙動をします。\n    - paragraph [ref=s1e450]: "以上を考慮すると、Decoderは、Encoderがないときには、事前分布として仮定した、標準正規分布\n        \\\\mathcal{N}(0, I) からサンプリングされた潜在表現 z を利用することで、そこそこ意味のあるデータを生成することができます。\n        この特性から、VAEは「生成モデル」として利用できることがわかります。"\n    - complementary [ref=s1e456]:\n        - text: "!"\n        - paragraph [ref=s1e459]: ただし、この関係は非常に難しいバランスの上で成立しています。\n        - paragraph [ref=s1e460]: 潜在表現の次元が増えたり、目的関数の項ごとの比率が変わるなどして、再構成誤差項と正則化項のバランスが崩れた際には、上記が成立しなくなります。\n            それについては後述します。\n        - paragraph [ref=s1e462]: しかし、その場合であっても、上記のような力学はある程度働きながら、学習が進むことになります。\n    - heading "Decoderが学習する潜在表現のランダム性" [level=3] [ref=s1e463]\n    - paragraph [ref=s1e464]: VAEの全体の構造を改めて下記に提示します。\n    - paragraph [ref=s1e465]:\n        - img [ref=s1e466]\n    - paragraph [ref=s1e467]: "こちらを見ると、Decoderに入力される潜在表現 z というのは、平均 \\\\mu ・分散 \\\\sigma^2\n        パラメータから得られる正規分布 \\\\mathcal{N}(\\\\mu, \\\\sigma) からサンプリングされた値を用いていることがわかります。\n        すなわち、学習時にDecoderが見る潜在表現 z には多少のランダム性がございます。\n        これは、Encoderに入力されるデータが同じデータであっても、decoderはランダムに異なる潜在表現 z から入力データ x\n        を再構成することを示します。"\n    - paragraph [ref=s1e484]: "そしてこのサンプリングは、平均ベクトル \\\\mu を中心として、正規分布 \\\\mathcal{N}(\\\\mu,\n        \\\\sigma) としてサンプリングされるため、中心から若干だけズレた値がサンプリングされることになります。"\n    - paragraph [ref=s1e489]: このことを考えると、Decoderは毎回若干異なる潜在表現 z から、入力データ x\n        を再構成することになります。 したがって、Decoderが入力データ x を再構成する際の潜在表現 z の許容範囲が広くなるように学習されます。\n        これを大量の入力 x に対して学習を実施すると、似たようなデータ同士では、似たような潜在表現 z\n        をEncoderがサンプリングするようになり、大きく異なるデータ同士では、異なる潜在表現 z をEncoderがサンプリングするようになります。\n    - paragraph [ref=s1e506]: これが達成されると、潜在表現 z が少し変わると、再構成データ y も少しだけ変わるという滑らかさをDecoderは得ることができます。\n    - paragraph [ref=s1e511]: この特性のおかげで、VAEは、2種類のデータ（例えば、男性の顔写真と女性の顔写真）を構成する潜在表現 z\n        の間を動かすことで、出力データ y は男性の顔から女性の顔に滑らかに変化するような挙動をするようになります。\n        これはよくVAEの解説サイトなどで見ることのできる挙動だと思います。\n    - heading "VAEはどうやって学習しているのか" [level=3] [ref=s1e517]\n    - paragraph [ref=s1e518]: もう一度VAEの構成図を表示します。\n    - paragraph [ref=s1e519]:\n        - img [ref=s1e520]\n    - paragraph [ref=s1e521]: ここまで読んで、勘のいい読者は、下記のように考えるかもしれません。\n        「確率分布からのSampling処理は微分不可能なため、学習できないのでないか」と\n    - paragraph [ref=s1e523]: その疑問はおっしゃる通りです。 ではここから、VAEの構造を損なわない形で、上記の図中からSampling構造を取り除くことを考えます。\n    - heading "Decoder出力部分" [level=4] [ref=s1e525]\n    - paragraph [ref=s1e526]: "図を見ると、Decoderの出力部分にてSampling構造があります。\n        生成モデルは、「対数尤度を最大化する」という目的のため、確率分布を出力することが求められますが、Decoderの出力はベクトルです。したがって\\\n        、出力ベクトル y を平均、単位行列 I を分散とする正規分布 \\\\mathcal{N}(y, I)\n        を確率分布として考え、そこからサンプリングされた値 x\' を生成データだと考えます。"\n    - paragraph [ref=s1e536]: この形で、目的関数ELBOを計算すると、出力ベクトル y と、入力データ x\n        の2乗誤差を最小化することが最終的な目的関数になることがわかります。\n        したがって、Sampling処理をなくすため、Decoderが出力するベクトル y 自体を、生成したデータ x\'=y と見なすことが可能です。\n    - complementary [ref=s1e546]:\n        - text: "!"\n        - paragraph [ref=s1e549]: 可能と言っているのは、 x\'=y と解釈したところで、目的関数は変わらず、パラメータを更新する微分値が変わらないことを指しています。\n    - paragraph [ref=s1e552]:\n        - text: したがって、これを考慮すると下記のような図になります。\n        - img [ref=s1e554]\n    - heading "Encoder出力部分" [level=4] [ref=s1e555]\n    - paragraph [ref=s1e556]: "続いて、Samplingが残っている箇所は、Encoderの出力する平均 \\\\mu ・分散 \\\\sigma^2\n        パラメータから得られる正規分布 \\\\mathcal{N}(\\\\mu, \\\\sigma) からサンプリングして、潜在表現 z\n        を得る部分になります。"\n    - paragraph [ref=s1e565]: この部分では、微分可能な計算グラフにするために、「Reparameterization\n        Trick」という手法を利用しています。 統計学に詳しい方には、「標準化の逆」という説明がしっくりくるかもしれません。\n    - complementary [ref=s1e567]:\n        - text: "!"\n        - paragraph [ref=s1e570]: 標準化とは、平均 \\mu 、分散 \\sigma^2 の複数データ z に対して、下記の処理を施した後のデータ\n            \\epsilon は、平均 0 、分散 1 となる変換です。\n        - text: "\\\\epsilon = \\\\dfrac{z - \\\\mu}{\\\\sigma}"\n    - paragraph [ref=s1e586]: "VAEでは、Encoderが平均 \\\\mu ・分散 \\\\sigma^2 を出力し、そこから得られる正規分布\n        \\\\mathcal{N}(\\\\mu, \\\\sigma) から潜在表現 z をサンプリングしています。"\n    - paragraph [ref=s1e595]: "このようなサンプリングの操作を、計算グラフの外に出すことで計算可能にする手法をReparameterizet\\\n        ion Trickと呼びます。 具体的には、計算グラフの外であらかじめ、平均 0 ・分散 1 の標準正規分布 \\\\mathcal{N}(0,\n        1) から乱数 \\\\epsilon を取得しておきます。"\n    - paragraph [ref=s1e605]: "その後、潜在表現 z は下記の計算を行うことで、まるで「標準化の逆」を行ったように、平均 \\\\mu ・分散\n        \\\\sigma^2 の正規分布 \\\\mathcal{N}(\\\\mu, \\\\sigma) からサンプリングされたような値に変換できます。"\n    - text: z = \\mu + \\sigma \\epsilon\n    - paragraph [ref=s1e617]: この処理を反映させると下記のような図になります。\n    - paragraph [ref=s1e618]:\n        - img [ref=s1e619]\n    - paragraph [ref=s1e620]: この処理により、DeocderとEncoderを繋ぐ部分にはSampling処理が現れなくなったため、Decoder側からEncoder側に勾配が正しく伝播するようになるため、学習が可能なネットワークになりました。\n    - heading "VAEを生成モデルとして利用する" [level=2] [ref=s1e621]\n    - heading "VAEの特徴" [level=3] [ref=s1e622]\n    - paragraph [ref=s1e623]: さて、ここまでの議論からVAEには下記の特徴を持つことが言えます。\n    - list [ref=s1e624]:\n        - listitem [ref=s1e625]:\n            - text: VAEの潜在表現 z には滑らかさが存在し、 z を少し変化させると生成されるデータ x\' が少しだけ変化する\n            - list [ref=s1e632]:\n                - listitem [ref=s1e633]:\n                    - text: この特徴より、潜在空間内において、2地点を行き来することで、滑らかに画像を変化させることができる\n                    - list [ref=s1e634]:\n                        - listitem [ref=s1e635]: 例えば、男性の顔写真から女性の顔写真へと滑らかに生成データを変化させることができる\n        - listitem [ref=s1e636]:\n            - text: "VAEの潜在表現の事前分布 p(z) は標準正規分布 \\\\mathcal{N}(0, 1) を仮定している"\n            - list [ref=s1e641]:\n                - listitem [ref=s1e642]: "標準正規分布 \\\\mathcal{N}(0, 1) から潜在表現 z\n                    をサンプリングすることができ、それを用いて「ある程度意味のある」データ x\' を生成できる"\n    - paragraph [ref=s1e649]: したがって、新しいデータを生成できるという観点で、生成モデルと呼ぶことができます。\n    - heading "落とし穴" [level=3] [ref=s1e650]\n    - paragraph [ref=s1e651]: と、私も昔は思っておりました。 いや、完全に間違っている訳ではないのですが、時には理論通りに進まないものです。\n    - paragraph [ref=s1e653]: 正確には、VAEの特徴は場合によっては正しくて、場合によっては正しくないです。\n        「目的関数の再構成誤差と正則化項のバランス」が崩れた場合に、VAEの特徴は機能しなくなります。\n        （正確には、ある程度その特徴は持っているが、だんだん機能しなくなってくるというイメージ）\n    - paragraph [ref=s1e656]: これから、説明させていただきます。 ここで、VAEの目的関数であるELBOを再提示します。\n    - complementary [ref=s1e658]: "! \\\\mathrm{ELBO} =\n        \\\\mathbb{E}_{q_{\\\\psi}(z|x)}[\\\\log p_{\\\\theta}(x|z)] -\n        \\\\mathrm{KL}\\\\left(q_{\\\\psi}(z|x) \\\\parallel p(z)\\\\right)"\n    - paragraph [ref=s1e664]: この式は、第1項の再構成誤差項と、第2項の正則化項の「バランス」をとって学習が進むことがわかっています。\n        このバランスは、例えば、第2項に対してハイパーパラメータを導入し、その寄与割合を変化させることによっても変わります。 （これを\n        β-VAE（ベータVAE）と呼びます）\n    - paragraph [ref=s1e667]:\n        - strong [ref=s1e668]: そしてもう一つ、潜在表現の次元数によっても変わります。\n    - paragraph [ref=s1e669]: よく見るVAEの解説記事では、MNISTなどの簡単な画像に対して、潜在表現の次元を2次元に設定して検証している記事が多いと思います。\n        これは、VAEの潜在表現の分布と、手描き文字の数字を対応付けて可視化するために非常に優れた例示の仕方だと思っております。\n        一方で、「VAEであれば、必ず、潜在表現の事前分布が正規分布に落ち着く」というようなミスリードを読者に引き起こす危険性もはらんでいると思っております。\n    - heading "潜在表現の次元が小さい場合" [level=3] [ref=s1e672]\n    - paragraph [ref=s1e673]: 他記事のように、VAEの潜在表現 z が2次元の場合、潜在表現 z はたった二つの値しか持たないことになります。\n        2次元の潜在表現 z では、画像の情報を保持することは困難です。\n    - paragraph [ref=s1e681]: この場合、VAEは、潜在空間 z に情報を埋め込むのは諦めます。 そして、潜在空間 z\n        はあくまで「乱数のサンプリング+おおよそのラベル情報」と割り切り、画像の再構成情報をDecoderに埋め込むように学習します。\n    - paragraph [ref=s1e687]: "すると、潜在表現 z は画像情報などの追加情報をほとんど持たないため、事前分布\n        p(z)=\\\\mathcal{N}(0, 1) に従って分布されるようになります。"\n    - paragraph [ref=s1e692]: この場合は、上記の「VAEの特徴」で記載した生成モデルとしての特徴を強く持つことになります。\n    - paragraph [ref=s1e693]: これは、ELBOの正則化項の寄与率を大きくしても発生します。\n    - heading "潜在変数の次元が多い場合" [level=3] [ref=s1e694]\n    - paragraph [ref=s1e695]: 一方で、潜在変数の次元数が多い場合はどうか。\n        潜在変数の次元数が多い場合は、あまり解説に向かないため、解説記事は少ないです。\n        しかし、潜在変数の次元数が多いVAEが使われており、比較的解析しやすい技術はわかります。\n    - paragraph [ref=s1e698]:\n        - strong [ref=s1e699]: みなさんご存知、Stable Diffusionです。\n    - heading "Stable Diffusionのモデル構造" [level=4] [ref=s1e700]\n    - paragraph [ref=s1e701]:\n        - img [ref=s1e702]\n        - text: （引用：\n        - link "https://github.com/CompVis/latent-diffusion/blob/main/assets/modelfigure.png）" [ref=s1e703]:\n            - /url: https://github.com/CompVis/latent-diffusion/blob/main/assets/modelfigure.png%EF%BC%89\n    - paragraph [ref=s1e704]: Stable Diffusionのモデル構造は上記のようになっております。\n    - paragraph [ref=s1e705]:\n        - strong [ref=s1e706]: Conditioningブロック\n    - paragraph [ref=s1e707]: 一番右の「Conditioning」ブロックは、画像の条件付けに使われる部分です。 例えば Stable\n        Diffusionにおける、Text to Imageモデルの場合、プロンプトと呼ばれる呪文で、生成される画像のテーマを制御することができます。\n    - paragraph [ref=s1e709]: なぜかというと、まず、「Conditioning」ブロックで使われいてる、CLIPやT5\n        Encoderと呼ばれるtext Encoderにより、プロンプトを特徴量に変換しています。\n        このとき、CLIPは、「画像とテキストの特徴量が一致」するように学習されているため、プロンプトからCLIPを通じて得られた特徴量は、画像の特徴量と同様に取り扱うことができます。\n        そのため、プロンプトの特徴量を、画像特徴量として、後述する「クロスアテンション」にて、計算することができるようになるからです。\n    - paragraph [ref=s1e712]:\n        - strong [ref=s1e713]: Latent Spaceブロック\n        - text: 真ん中の「Latent Space」ブロックでは、潜在表現 z を生成するブロックになります。\n            ここでは、「拡散モデル」というモデルを利用して、潜在表現 z を生成します。 （拡散モデルに関しては\n        - link "こちらの記事も" [ref=s1e721]:\n            - /url: https://zenn.dev/asap/articles/4092ab60570b05\n        - text: ご覧ください）\n    - paragraph [ref=s1e722]: また、拡散モデルでは、TransformerやU-netというモデルが利用されていますが、そのどちらにおいても、CLIPから得られたプロンプト特徴量を「クロスアテンション」により注入しています。\n    - paragraph [ref=s1e723]:\n        - text: クロスアテンションとは、処理中の画像特徴量と、プロンプト特徴量の類似度を計算し、類似度が高い画像特徴量を増幅させるような処理が行われており、この処理により、プロンプトに適した画像になるように潜在表現\n            z が生成されていきます。 （アテンションに関しては、\n        - link "こちらの記事" [ref=s1e727]:\n            - /url: https://zenn.dev/asap/articles/6bdf89d8e964ec#self-attension%E3%81%A8%E3%81%AF\n        - text: でも解説しております。）\n    - paragraph [ref=s1e728]:\n        - strong [ref=s1e729]: Pixel Spaceブロック\n        - text: 最後に、一番右側の「Pixel Space」ブロックが、VAEが担当する部分です。 上の段の x は生成したい入力データで、 \\varepsilon\n            と書かれている部分がEncoderです。\n    - paragraph [ref=s1e736]: "そして、下の段の \\\\tilde{x} が生成データで、 D と書かれているのが、Decoderになります。"\n    - paragraph [ref=s1e741]: "すなわち、拡散モデルが生成した潜在表現 z から、VAE decoderが出力画像 \\\\tilde{x} を生成していることになります。"\n    - complementary [ref=s1e746]:\n        - text: "!"\n        - paragraph [ref=s1e749]: ここでは、VAEは完全畳み込み層になっており、入力が3次元の画像（縦x横xチャネル）データの場合、潜在表現 z\n            は(縦/8 x 横/8 x 4)の形になっています。\n    - paragraph [ref=s1e752]: "さて、ここで重要なのは、Stable\n        Diffusionでは、VAEが使われてるはずですが、画像を生成する際に標準正規分布 \\\\mathcal{N}(0, 1)\n        からサンプリングした潜在表現 z を利用していないことです。"\n    - paragraph [ref=s1e757]: ちなみに、(詳しい方向けに補足しておくと)Stable Diffusion\n        XLで使われているのは、VQ-VAEではなく、VAEです。\n        （これはDiffusersライブラリでSDXLの部分を見ればわかります。Diffusersライブラリにて実装されているのは、通常のVAEです。）\n    - group [ref=s1e759]: 該当コード\n    - paragraph [ref=s1e761]: "Stable Diffusionでは何をやっているのかというと、標準正規分布 \\\\mathcal{N}(0,\n        1) から得られた潜在表現を、拡散モデルにより分布を変換し、変換後の潜在表現 z\n        をVAEのdecoderに入力することで、画像を生成しています。"\n    - paragraph [ref=s1e766]: "すなわち、上述した「VAEの特徴：標準正規分布 \\\\mathcal{N}(0, 1) から潜在表現 z\n        をサンプリングすることができ、それを用いて「ある程度意味のある」データ x\' を生成できる」が崩壊していることがわかります。"\n    - heading "Stable Diffusion内部のVAEの特徴" [level=4] [ref=s1e773]\n    - paragraph [ref=s1e774]:\n        - text: "実際、単純な標準正規分布 \\\\mathcal{N}(0, 1) から得られた潜在表現を、そのままVAE\n            decoderに入力すると、ノイズ画像が生成されます。 そのあたりは下記の記事などもご参照ください。"\n        - iframe [ref=s1e779]: <could not take iframe snapshot>\n    - paragraph [ref=s1e780]: 上記の記事中のgifを見ると、最初の方は砂嵐のようなノイズ画像が表示されており、だんだん拡散モデルにより潜在表現\n        z の分布が変化していくと、decoderが画像として出力できるようになっているのがわかると思います。\n    - heading "そのときVAEに何が起きているのか！？" [level=4] [ref=s1e783]\n    - paragraph [ref=s1e784]: "これは何が起きているのかというと、第一に、VAEの潜在表現 z の事前分布 p(z) が標準正規分布\n        \\\\mathcal{N}(0, 1) から大きくずれてしまっています。"\n    - paragraph [ref=s1e791]: これは、学習時に、ELBO第2項の正則化項が入っているのにも関わらず・・・です。\n    - paragraph [ref=s1e792]:\n        - text: 正則化項は、下記のコードの\n        - code [ref=s1e793]: DiagonalGaussianDistribution.kl\n        - text: メソッドにて計算がされています。\n        - iframe [ref=s1e795]: <could not take iframe snapshot>\n    - paragraph [ref=s1e796]: "では、なぜVAEの潜在表現 z の事前分布 p(z) が標準正規分布 \\\\mathcal{N}(0, 1)\n        から大きくずれるのかというと、VAEの潜在表現 z の次元数の多さが原因です。"\n    - paragraph [ref=s1e805]: 上述した通り、Stable Diffusionの潜在表現 z のサイズは「縦/8 x 横/8 x\n        4」になっています。 Stable Diffusion XLでは、学習される画像のサイズは「1024x1024」画素が一般的です。\n        このとき、潜在表現 z のサイズは、「128 x 128 x 4」となり、潜在表現 z の要素数は65536となります。\n        65536という数字は、MNISTの実験で使っていた2次元の潜在表現の次元数と比較して、非常に大きな次元数であることがわかります。\n    - paragraph [ref=s1e815]: このレベルで潜在表現 z の次元が大きくなると、潜在表現 z 自体にも画像の情報を埋め込めるようになります。\n        2次元しか潜在表現 z\n        がない場合、潜在表現に情報を埋め込むのは困難なため、画像の再構成情報はDecoderに全て埋め込まれるような挙動になると、記載しました。\n        しかし、今回の場合は、潜在表現 z とDecoderの協業で、画像を再構成することになります。\n    - paragraph [ref=s1e826]: これは、すなわち正則化項よりも、再構成損失項の方が強いバランスになります。\n        潜在表現に画像情報を埋め込むことによって、正則化項の損失が大きくなったとしても、その分大幅に再構成損失項の損失を減少させることができるからです。\n    - paragraph [ref=s1e828]: "その結果、潜在表現 z の事前分布 p(z) が、標準正規分布 \\\\mathcal{N}(0, 1)\n        から大きく離れるようになります。 その代わり、Stable Diffusionでは、より高品質な画像が生成できるようになっているわけです。"\n    - heading "だから拡散モデルがいる" [level=4] [ref=s1e836]\n    - paragraph [ref=s1e837]: ただし、VAEが正則化項を無視し始めると、画質が良くなる代わりに、潜在表現 z の事前分布 p(z)\n        は意味がわからない分布になります。 この意味のわからない分布を、人間は把握することはできず、その分布からサンプリングした潜在表現 z\n        をEncoderなしでは取得できません。\n    - paragraph [ref=s1e845]: そこで、その潜在表現の事前分布 p(z) を、拡散モデルに学習してもらいます\n        そして、その学習ずみ拡散モデルが、VAEの潜在表現 z\n        を生成することで、高品質なVAEの生成画像を維持しながら、生成モデルとして0から新しいデータを生み出しつつ、さらにプロンプトによる条件付けまで達成しています。\n        （だから、これだけ騒がれて、めちゃくちゃ使われているわけですね）\n    - complementary [ref=s1e852]:\n        - text: "!"\n        - paragraph [ref=s1e855]: この場合、VAEの特徴はかなり少なくなっており、どちらかというと Auto Encoderに近い挙動をします。\n            しかし、正則化項が完全になくなったわけではないため、ある程度、潜在表現の滑らかさなどは残っており、結果として、Auto\n            Encoderの潜在表現 z よりも、VAEの潜在表現 z の方が、拡散モデルが学習しやすくなっていると思われます。\n    - heading "最後にVAEの課題" [level=1] [ref=s1e861]\n    - paragraph [ref=s1e862]: ここまでVAEのいいところを説明してきましたが、VAEを単体で利用する場合は、まだまだ課題があります。\n    - heading "Posterior Collapse" [level=2] [ref=s1e863]\n    - paragraph [ref=s1e864]: "一つは「Posterior Collapse（事後崩壊）」の問題です。\n        これは、VAEの目的関数ELBOの正則化項が強すぎる場合、Encoderが出力する近似事後分布 q_{\\\\psi}(z|x) と、事前分布（\n        p(z)=\\\\mathcal{N}(0, 1) )が完全に一致してしまうことで発生します。\n        この場合、画像の再構成情報が完全にDecoderに入ってしまい、潜在表現 z\n        を無視した再構成が行われてしまいます。なまじDecoderの性能が高いと発生しやすい問題です。"\n    - paragraph [ref=s1e873]: posterior collapseの対策として、学習初期は正則化項の寄与率を下げて学習させ、だんだん寄与率を大きくしていくことが挙げられます。\n    - heading "画像がボケやすい" [level=2] [ref=s1e874]\n    - paragraph [ref=s1e875]: もう一つは、生成される画像がボケやすい点です。 Stable\n        Diffusionに搭載されているVAEのように、事前分布 p(z) の形状を完全に気にしないように学習する（つまり潜在表現 z\n        の次元数が非常に大きい）場合は起こりにくい問題ではありますが、ある程度正則化項による正則化を入れていきたい場合（つまり、事前分布 p(z)\n        に正規分布を仮定したい状況）では、この問題が発生しやすいです。\n    - paragraph [ref=s1e883]: これは、上述した通り、Decoderに入力される潜在表現 z\n        は、乱数のサンプリング後の値であるため、例え全く同じ画像を学習していたとしても、潜在表現 z の値は学習のたびに異なります。\n    - paragraph [ref=s1e888]: すると、似たような画像 y を生成する潜在表現 z は、潜在表現空間において、近づきやすくなります。\n        一方で、decoder側は、潜在表現 z が密集している空間の場合、ある潜在表現 z が、どの生成画像 x\n        に対応しているのか、そのランダム性のために判断することができなくなってしまうため、なるべく最終的な損失を小さくするために、密集している潜在表現\n        z から生成されうる画像 x の平均のような画像を学習してしまいます。 これは、潜在表現 z\n        の次元が小さい方が、潜在表現空間における密集度が高まるため、発生しやすくなります。（一般に低次元空間の方が2点間の距離は小さく、高次元空間の方が2点間の距離が広くなる）\n    - paragraph [ref=s1e907]: "これの対処法は、VAEの潜在表現の次元を増やすことか、第2項の寄与率を下げるくらいしかありませんでした。\n        しかし、これを行うと、どうしても生成モデルとしての特徴である、「潜在表現zの事前分布 p(z) が標準正規分布 \\\\mathcal{N}(0,\n        1) である」という仮定が消失しやすくなります。"\n    - paragraph [ref=s1e913]: したがって、バランス調整が難しい問題で、VAEが持つ悪い特徴の一つでしたが、後日記事を書くVQ-VAEでは、この問題を解消しています。\n    - heading "まとめ" [level=1] [ref=s1e914]\n    - paragraph [ref=s1e915]: ここまで読んでくださってありがとうございました！\n        生成モデルと言えば、Transformerや拡散モデルが主流ですが、非常に長い系列長（音声など）を生成する場合には、まだまだVAEが使われいてるように見えます。\n    - paragraph [ref=s1e917]: VAEは一度に全てのデータを生成できるため、速度が必要なアプリケーションにも適していますし、圧縮器としての機能もあるため、大きなデータを潜在表現に圧縮し、その潜在表現をTransformerや拡散モデルによって再構成するといった使い方がよくなされており、古い概念ではありますが、まだまだVAEには価値があるなと感じております。\n    - paragraph [ref=s1e918]: わかった気になっていましたが、VAEに関して詳細に色々調べていると、まだまだ理解があやふやな部分もあり、記事を書きながら理解度を高めることができました！\n    - paragraph [ref=s1e919]: この記事が、何らかの形でみなさまの手助けになれば幸いです！\n    - paragraph [ref=s1e920]: ここまで読んでくださり、ありがとうございました！\n    - heading "参考文献" [level=1] [ref=s1e921]\n    - paragraph [ref=s1e922]:\n        - iframe [ref=s1e924]: <could not take iframe snapshot>\n        - text: VAEの論文です。\n    - paragraph [ref=s1e925]:\n        - link "ゼロから作るDeep Learning ❺ ―生成モデル編" [ref=s1e926]:\n            - /url: https://amzn.to/3Aw9vCt\n    - paragraph [ref=s1e927]: 圧倒的な名著シリーズである「ゼロから作るDeep Learning」シリーズの5作目である、生成モデル編です。\n        この本では、最終的に拡散モデルを理解することを目的に、VAEや混合ガウスモデル、正規分布まで遡り、本当にゼロから中身を理解しながら実装をすることができる、大変素晴らしい書籍になっています。\n    - paragraph [ref=s1e929]:\n        - link "妥協しないデータ分析のための 微積分+線形代数入門" [ref=s1e930]:\n            - /url: https://amzn.to/4hE7ftx\n    - paragraph [ref=s1e931]: 本書の後半にもVAEやELBOについての簡単な記載があり、本記事を書くうえで表現を参考にさせていただきました。\n        確率分布のどの部分が計算可能で、どの部分が計算不可能なのかが常に明示されながら進んでいくので、初学者には非常に分かりやすい書籍でした！\n    - paragraph [ref=s1e933]: 特に、VAEなどの生成AIをただ使うだけでなく、ちゃんと理論を理解したいと思った場合、論文や理論書を理解するために「線形代数」や「微積分学」の事前知識が必要になります。\n        本書は、初学者が一番最初に読んで必要な基礎知識が習得できる良本になります。\n    - paragraph [ref=s1e935]:\n        - link "生成 Deep Learning 第2版 ―絵を描き、物語や音楽を作り、ゲームをプレイする" [ref=s1e936]:\n            - /url: https://amzn.to/4g8hjJg\n    - paragraph [ref=s1e937]: 本書の序盤で、Auto\n        EncoderとVAEの違いを説明してくれています。しかもそれぞれの潜在空間の分布を可視化してくれているので、違いが分かりやすく、理解もしやすいです。\n        さらに、本書はVAEだけでなくGANや画像生成、音楽生成、テキスト生成など、ありとあらゆる生成AIの紹介と解説をしてくれており、必読の一冊となっております。\n    - paragraph [ref=s1e939]: （書籍のリンクはAmazonアフィリエイトリンクになっております）\n    - button "いいね" [ref=s1e942]:\n        - img [ref=s1e943]\n    - text: "104"\n    - button "ブックマーク" [ref=s1e959]:\n        - img [ref=s1e960]\n    - text: "43"\n    - button "その他の操作" [ref=s1e965]:\n        - img [ref=s1e966]\n    - tooltip "X(Twitter)にポスト" [ref=s1e969]:\n        - img [ref=s1e970]\n        - text: X(Twitter)にポスト\n    - tooltip "Facebookに投稿" [ref=s1e976]:\n        - img [ref=s1e977]\n        - text: Facebookに投稿\n    - tooltip "はてなブックマークに登録" [ref=s1e983]:\n        - img [ref=s1e984]\n        - text: はてなブックマークに登録\n    - complementary [ref=s1e987]:\n        - link "asap" [ref=s1e989]:\n            - /url: /asap\n            - img "asap" [ref=s1e990]\n        - link "asap" [ref=s1e992]:\n            - /url: /asap\n        - paragraph [ref=s1e994]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n        - button "フォロー" [ref=s1e998]\n        - tooltip "@personabb" [ref=s1e999]:\n            - img "GitHub" [ref=s1e1000]\n            - text: "@personabb"\n        - tooltip "@asap2650" [ref=s1e1002]:\n            - img "X(Twitter)" [ref=s1e1003]\n            - text: "@asap2650"\n        - tooltip "yard.tips" [ref=s1e1009]:\n            - img "リンク" [ref=s1e1010]\n            - text: yard.tips\n    - complementary [ref=s1e1013]:\n        - text: バッジを贈って著者を応援しよう\n        - paragraph [ref=s1e1017]: バッジを受け取った著者にはZennから現金やAmazonギフトカードが還元されます。\n        - button "バッジを贈る" [ref=s1e1019]\n        - img [ref=s1e1020]\n    - heading "Discussion" [level=3] [ref=s1e1029]\n    - img [ref=s1e1030]\n    - text: ログインするとコメントできます\n    - button "Login" [ref=s1e1034]\n    - complementary [ref=s1e1035]:\n        - link "asap" [ref=s1e1040]:\n            - /url: /asap\n            - img "asap" [ref=s1e1041]\n        - link "asap" [ref=s1e1043]:\n            - /url: /asap\n        - button "フォロー" [ref=s1e1045]\n        - link "@personabb" [ref=s1e1046]:\n            - /url: https://github.com/personabb\n            - tooltip "@personabb" [ref=s1e1047]:\n                - img "GitHub" [ref=s1e1048]\n                - text: "@personabb"\n        - link "@asap2650" [ref=s1e1050]:\n            - /url: https://twitter.com/asap2650\n            - tooltip "@asap2650" [ref=s1e1051]:\n                - img "X(Twitter)" [ref=s1e1052]\n                - text: "@asap2650"\n        - link "yard.tips" [ref=s1e1058]:\n            - /url: https://yard.tips/asap2650\n            - tooltip "yard.tips" [ref=s1e1059]:\n                - img "リンク" [ref=s1e1060]\n                - text: yard.tips\n        - paragraph [ref=s1e1063]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n        - button "バッジを贈る" [ref=s1e1066]\n        - link "バッジを贈るとは" [ref=s1e1068]:\n            - /url: /faq#badges\n            - text: バッジを贈るとは\n            - img [ref=s1e1069]\n        - text: 目次\n        - list [ref=s1e1078]:\n            - listitem [ref=s1e1079]:\n                - link "はじめに" [ref=s1e1080]:\n                    - /url: "#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB"\n            - listitem [ref=s1e1081]:\n                - link "参考文献" [ref=s1e1082]:\n                    - /url: "#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"\n            - listitem [ref=s1e1083]:\n                - link "関連記事" [ref=s1e1084]:\n                    - /url: "#%E9%96%A2%E9%80%A3%E8%A8%98%E4%BA%8B"\n            - listitem [ref=s1e1085]:\n                - link "Auto Encoder" [ref=s1e1086]:\n                    - /url: "#auto-encoder"\n                - list [ref=s1e1087]:\n                    - listitem [ref=s1e1088]:\n                        - link "Auto Encoderの紹介" [ref=s1e1089]:\n                            - /url: "#auto-encoder%E3%81%AE%E7%B4%B9%E4%BB%8B"\n                    - listitem [ref=s1e1090]:\n                        - link "Auto Encoderの本質" [ref=s1e1091]:\n                            - /url: "#auto-encoder%E3%81%AE%E6%9C%AC%E8%B3%AA"\n                    - listitem [ref=s1e1092]:\n                        - link "Auto Encoerの課題" [ref=s1e1093]:\n                            - /url: "#auto-encoer%E3%81%AE%E8%AA%B2%E9%A1%8C"\n            - listitem [ref=s1e1094]:\n                - link "VAE（Variational Auto Encoder）" [ref=s1e1095]:\n                    - /url: "#vae%EF%BC%88variational-auto-encoder%EF%BC%89"\n                - list [ref=s1e1096]:\n                    - listitem [ref=s1e1097]:\n                        - link "VAEと生成モデル" [ref=s1e1098]:\n                            - /url: "#vae%E3%81%A8%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB"\n                    - listitem [ref=s1e1099]:\n                        - link "VAEの挙動・振る舞い" [ref=s1e1100]:\n                            - /url: "#vae%E3%81%AE%E6%8C%99%E5%8B%95%E3%83%BB%E6%8C%AF%E3%82%8B%E8%88%9E%E3%8\\\n                                1%84"\n                    - listitem [ref=s1e1101]:\n                        - link "VAEを生成モデルとして利用する" [ref=s1e1102]:\n                            - /url: "#vae%E3%82%92%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB%E3%81%A8%E3%8\\\n                                1%97%E3%81%A6%E5%88%A9%E7%94%A8%E3%81%99%E3%82%\\\n                                8B"\n            - listitem [ref=s1e1103]:\n                - link "最後にVAEの課題" [ref=s1e1104]:\n                    - /url: "#%E6%9C%80%E5%BE%8C%E3%81%ABvae%E3%81%AE%E8%AA%B2%E9%A1%8C"\n                - list [ref=s1e1105]:\n                    - listitem [ref=s1e1106]:\n                        - link "Posterior Collapse" [ref=s1e1107]:\n                            - /url: "#posterior-collapse"\n                    - listitem [ref=s1e1108]:\n                        - link "画像がボケやすい" [ref=s1e1109]:\n                            - /url: "#%E7%94%BB%E5%83%8F%E3%81%8C%E3%83%9C%E3%82%B1%E3%82%84%E3%81%99%E3%81%8\\\n                                4"\n            - listitem [ref=s1e1110]:\n                - link "まとめ" [ref=s1e1111]:\n                    - /url: "#%E3%81%BE%E3%81%A8%E3%82%81"\n            - listitem [ref=s1e1112]:\n                - link "参考文献" [ref=s1e1113]:\n                    - /url: "#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE-1"\n        - paragraph [ref=s1e1115]: Zennからのお知らせ\n        - article [ref=s1e1118]:\n            - link "第２回 AI Agent Hackathon with Google Cloud 第２回 AI Agent Hackathon with Google Cloud 4/14登録受付開始！" [ref=s1e1119]:\n                - /url: /hackathons/google-cloud-japan-ai-hackathon-vol2\n                - img "第２回 AI Agent Hackathon with Google Cloud" [ref=s1e1121]\n                - paragraph [ref=s1e1122]: 第２回 AI Agent Hackathon with Google Cloud 4/14登録受付開始！\n        - article [ref=s1e1123]:\n            - link "Zenncafe日比谷#2 生成AIで変わる仕事 Zenncafe日比谷#2 生成AIで変わる仕事" [ref=s1e1124]:\n                - /url: https://zenn.connpass.com/event/351131/\n                - img "Zenncafe日比谷#2 生成AIで変わる仕事" [ref=s1e1126]\n                - paragraph [ref=s1e1127]: Zenncafe日比谷#2 生成AIで変わる仕事\n- contentinfo [ref=s1e1129]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1133]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1134]\n    - paragraph [ref=s1e1142]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e1144]:\n        - heading "About" [level=4] [ref=s1e1145]\n        - list [ref=s1e1146]:\n            - listitem [ref=s1e1147]:\n                - link "Zennについて" [ref=s1e1149]:\n                    - /url: /about\n            - listitem [ref=s1e1150]:\n                - link "運営会社" [ref=s1e1151]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e1152]:\n                - link "お知らせ・リリース" [ref=s1e1153]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e1154]:\n                - link "イベント" [ref=s1e1156]:\n                    - /url: /events\n    - navigation [ref=s1e1157]:\n        - heading "Guides" [level=4] [ref=s1e1158]\n        - list [ref=s1e1159]:\n            - listitem [ref=s1e1160]:\n                - link "使い方" [ref=s1e1162]:\n                    - /url: /manual\n            - listitem [ref=s1e1163]:\n                - link "法人向けメニュー" [ref=s1e1165]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e1167]:\n                - link "Publication / Pro" [ref=s1e1169]:\n                    - /url: /publications\n            - listitem [ref=s1e1170]:\n                - link "よくある質問" [ref=s1e1172]:\n                    - /url: /faq\n    - navigation [ref=s1e1173]:\n        - heading "Links" [level=4] [ref=s1e1174]\n        - list [ref=s1e1175]:\n            - listitem [ref=s1e1176]:\n                - link "X(Twitter)" [ref=s1e1177]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e1178]:\n                - link "GitHub" [ref=s1e1179]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e1180]:\n                - link "メディアキット" [ref=s1e1182]:\n                    - /url: /mediakit\n    - navigation [ref=s1e1183]:\n        - heading "Legal" [level=4] [ref=s1e1184]\n        - list [ref=s1e1185]:\n            - listitem [ref=s1e1186]:\n                - link "利用規約" [ref=s1e1188]:\n                    - /url: /terms\n            - listitem [ref=s1e1189]:\n                - link "プライバシーポリシー" [ref=s1e1191]:\n                    - /url: /privacy\n            - listitem [ref=s1e1192]:\n                - link "特商法表記" [ref=s1e1194]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e1197]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e1198]\n- alert [ref=s1e1217]\n```\n', name='browser_navigate', tool_call_id='45a87836-eb09-4499-bf44-a2bcdbd3ec2b'), AIMessage(content='「VAEって結局何者なの？」という記事は、生成モデルの一つであるVAE（Variational Autoencoder）について解説した記事です。\n\n記事の内容をまとめると、以下のようになります。\n\n*   **VAEとは:** VAEは、生成モデルとして重要な「対数尤度の最大化」問題を解くために必要な構造を用意したら、結果的にAuto Encoderのような形になったというモデルです。\n*   **Auto Encoderとの違い:** VAEを理解するためには、まずAuto Encoderを理解する必要があります。Auto Encoderは、入力データと出力を一致させるように学習させるネットワークですが、それ自体では生成モデルとして活用することが難しいという課題があります。\n*   **VAEの仕組み:** VAEは、Encoderで入力データを潜在表現と呼ばれる低次元のデータに圧縮し、Decoderでその潜在表現から元のデータを復元します。この際、Encoderは潜在表現の平均と分散を出力し、その分布からサンプリングされた値をDecoderに入力します。これにより、VAEは潜在空間に滑らかさを持ち、生成モデルとして機能することができます。\n*   **VAEの学習:** VAEは、ELBO（Evidence Lower Bound）と呼ばれる目的関数を最大化するように学習されます。ELBOは、再構成誤差と正則化項の和で表され、VAEはこのバランスをとりながら学習を進めます。\n*   **VAEの課題:** VAEには、Posterior Collapse（事後崩壊）や生成される画像がボケやすいといった課題があります。これらの課題は、潜在表現の次元数や正則化項の強さを調整することで、ある程度緩和することができます。\n*   **Stable DiffusionにおけるVAE:** Stable Diffusionでは、VAEが画像のエンコード・デコードを担当しています。ただし、Stable Diffusionでは、VAEの潜在表現の事前分布が標準正規分布から大きくずれており、VAEはAuto Encoderに近い挙動をしています。\n\nこの記事は、VAEの基本的な仕組みから、学習方法、課題、そしてStable DiffusionにおけるVAEの役割まで、幅広く解説しています。特に、VAEの潜在空間の性質や、Stable DiffusionにおけるVAEの挙動について詳しく解説している点が特徴です。', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-bf93ed43-18df-4eb1-bd5a-f1423de25a27-0', usage_metadata={'input_tokens': 70378, 'output_tokens': 467, 'total_tokens': 70845, 'input_token_details': {'cache_read': 0}})]}
=================================
「VAEって結局何者なの？」という記事は、生成モデルの一つであるVAE（Variational Autoencoder）について解説した記事です。

記事の内容をまとめると、以下のようになります。

*   **VAEとは:** VAEは、生成モデルとして重要な「対数尤度の最大化」問題を解くために必要な構造を用意したら、結果的にAuto Encoderのような形になったというモデルです。
*   **Auto Encoderとの違い:** VAEを理解するためには、まずAuto Encoderを理解する必要があります。Auto Encoderは、入力データと出力を一致させるように学習させるネットワークですが、それ自体では生成モデルとして活用することが難しいという課題があります。
*   **VAEの仕組み:** VAEは、Encoderで入力データを潜在表現と呼ばれる低次元のデータに圧縮し、Decoderでその潜在表現から元のデータを復元します。この際、Encoderは潜在表現の平均と分散を出力し、その分布からサンプリングされた値をDecoderに入力します。これにより、VAEは潜在空間に滑らかさを持ち、生成モデルとして機能することができます。
*   **VAEの学習:** VAEは、ELBO（Evidence Lower Bound）と呼ばれる目的関数を最大化するように学習されます。ELBOは、再構成誤差と正則化項の和で表され、VAEはこのバランスをとりながら学習を進めます。
*   **VAEの課題:** VAEには、Posterior Collapse（事後崩壊）や生成される画像がボケやすいといった課題があります。これらの課題は、潜在表現の次元数や正則化項の強さを調整することで、ある程度緩和することができます。
*   **Stable DiffusionにおけるVAE:** Stable Diffusionでは、VAEが画像のエンコード・デコードを担当しています。ただし、Stable Diffusionでは、VAEの潜在表現の事前分布が標準正規分布から大きくずれており、VAEはAuto Encoderに近い挙動をしています。

この記事は、VAEの基本的な仕組みから、学習方法、課題、そしてStable DiffusionにおけるVAEの役割まで、幅広く解説しています。特に、VAEの潜在空間の性質や、Stable DiffusionにおけるVAEの挙動について詳しく解説している点が特徴です。








入力してください: 3番目のいいねが多い記事は？
response:  {'messages': [HumanMessage(content=[{'type': 'text', 'text': '「https://zenn.dev/asap」この執筆者のもっともいいねがついている記事を調査して、その記事の中身を取得し、その記事がなぜ評価されているのかを分析してください。'}], additional_kwargs={}, response_metadata={}), AIMessage(content='zenn.devのasap氏の最もいいねが多い記事を調べ、その内容を取得し、評価されている理由を分析する、というご要望ですね。\n\nまず、zenn.dev/asapにアクセスして、最もいいねが多い記事のタイトルとURLを特定する必要があります。その後、そのURLにアクセスして記事の内容を取得します。最後に、記事の内容から評価されている理由を推測します。\n\nでは、始めましょう。\n\n1.  zenn.dev/asapにアクセスし、最もいいねが多い記事の情報を取得します。', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-989fe5ca-2637-4757-878f-db26e13526b8-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap'}, 'id': '58fad86e-0b01-4fc6-a8cc-57deb5c48b41', 'type': 'tool_call'}], usage_metadata={'input_tokens': 778, 'output_tokens': 128, 'total_tokens': 906, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap\n\n- Page URL: https://zenn.dev/asap\n- Page Title: asapさんの記事一覧 | Zenn\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- banner [ref=s1e32]:\n    - img "asap" [ref=s1e36]\n    - heading "asap" [level=1] [ref=s1e39]\n    - button "フォロー" [ref=s1e41]\n    - paragraph [ref=s1e43]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n    - button "2,214 Likes" [ref=s1e46]\n    - button "310 Followers" [ref=s1e48]\n    - link "@personabb" [ref=s1e51]:\n        - /url: https://github.com/personabb\n        - tooltip "@personabb" [ref=s1e52]:\n            - img "GitHub" [ref=s1e53]\n            - text: "@personabb"\n    - link "@asap2650" [ref=s1e55]:\n        - /url: https://twitter.com/asap2650\n        - tooltip "@asap2650" [ref=s1e56]:\n            - img "X(Twitter)" [ref=s1e57]\n            - text: "@asap2650"\n    - link "yard.tips" [ref=s1e63]:\n        - /url: https://yard.tips/asap2650\n        - tooltip "yard.tips" [ref=s1e64]:\n            - img [ref=s1e65]\n            - text: yard.tips\n    - link "RSS" [ref=s1e67]:\n        - /url: /asap/feed\n        - tooltip "RSS" [ref=s1e68]:\n            - img [ref=s1e69]\n            - text: RSS\n- link "Articles 72" [ref=s1e80]:\n    - /url: /asap\n- link "Scraps 5" [ref=s1e82]:\n    - /url: /asap?tab=scraps\n- link "Comments" [ref=s1e84]:\n    - /url: /asap?tab=comments\n- article [ref=s1e89]:\n    - link "tech" [ref=s1e90]:\n        - /url: /tech-or-idea\n    - link "❄️ LangChainでGemini-2.0 APIの画像編集を実行する方法 6日前 いいねされた数 18" [ref=s1e91]:\n        - /url: /asap/articles/1d5e6ec06c380f\n        - text: ❄️\n        - heading "LangChainでGemini-2.0 APIの画像編集を実行する方法" [level=3] [ref=s1e95]\n        - time [ref=s1e98]: 6日前\n        - img "いいねされた数" [ref=s1e100]\n        - text: "18"\n- article [ref=s1e102]:\n    - link "tech" [ref=s1e103]:\n        - /url: /tech-or-idea\n    - link "🙌 完全自律型AIエージェントOSS【LangManus】完全に理解した！ 13日前 いいねされた数 48" [ref=s1e104]:\n        - /url: /asap/articles/b27e832c2a49b7\n        - text: 🙌\n        - heading "完全自律型AIエージェントOSS【LangManus】完全に理解した！" [level=3] [ref=s1e108]\n        - time [ref=s1e111]: 13日前\n        - img "いいねされた数" [ref=s1e113]\n        - text: "48"\n- article [ref=s1e115]:\n    - link "tech" [ref=s1e116]:\n        - /url: /tech-or-idea\n    - link "💫 LangChainでOpenAI gpt-4o / o1にPDFを入力する方法 20日前 いいねされた数 9" [ref=s1e117]:\n        - /url: /asap/articles/74885fab863c9e\n        - text: 💫\n        - heading "LangChainでOpenAI gpt-4o / o1にPDFを入力する方法" [level=3]\n          [ref=s1e121]\n        - time [ref=s1e124]: 20日前\n        - img "いいねされた数" [ref=s1e126]\n        - text: "9"\n- article [ref=s1e128]:\n    - link "tech" [ref=s1e129]:\n        - /url: /tech-or-idea\n    - link "👣 Ollama + LangChainで画像を入力する方法【Gemma3 27B】 27日前 いいねされた数 14" [ref=s1e130]:\n        - /url: /asap/articles/18b110d551b811\n        - text: 👣\n        - heading "Ollama + LangChainで画像を入力する方法【Gemma3 27B】" [level=3]\n          [ref=s1e134]\n        - time [ref=s1e137]: 27日前\n        - img "いいねされた数" [ref=s1e139]\n        - text: "14"\n- article [ref=s1e141]:\n    - link "tech" [ref=s1e142]:\n        - /url: /tech-or-idea\n    - link "🦄 Google CloudのVector StoreでRAG Agentsを構築する【Big Querty編】 28日前 いいねされた数 12" [ref=s1e143]:\n        - /url: /asap/articles/3d258861c6c528\n        - text: 🦄\n        - heading "Google CloudのVector StoreでRAG Agentsを構築する【Big Querty編】"\n          [level=3] [ref=s1e147]\n        - time [ref=s1e150]: 28日前\n        - img "いいねされた数" [ref=s1e152]\n        - text: "12"\n- article [ref=s1e154]:\n    - link "tech" [ref=s1e155]:\n        - /url: /tech-or-idea\n    - link "🎶 最新音楽生成AI 【DiffRhythm】をGoogle Colabで動かせるようにしました！（簡易版） 1ヶ月前 いいねされた数 39" [ref=s1e156]:\n        - /url: /asap/articles/4edc4a71de8961\n        - text: 🎶\n        - heading "最新音楽生成AI 【DiffRhythm】をGoogle Colabで動かせるようにしました！（簡易版）"\n          [level=3] [ref=s1e160]\n        - time [ref=s1e163]: 1ヶ月前\n        - img "いいねされた数" [ref=s1e165]\n        - text: "39"\n- article [ref=s1e167]:\n    - link "tech" [ref=s1e168]:\n        - /url: /tech-or-idea\n    - link "🐤 LangGraphのTool Callingを利用して、RAG Agentsを構築する（後編） 1ヶ月前 いいねされた数 12" [ref=s1e169]:\n        - /url: /asap/articles/c58cfab9c1362a\n        - text: 🐤\n        - heading "LangGraphのTool Callingを利用して、RAG Agentsを構築する（後編）" [level=3]\n          [ref=s1e173]\n        - time [ref=s1e176]: 1ヶ月前\n        - img "いいねされた数" [ref=s1e178]\n        - text: "12"\n- article [ref=s1e180]:\n    - link "idea" [ref=s1e181]:\n        - /url: /tech-or-idea\n    - link "🏆 イベントに登壇してきた話【資料もこちら】 1ヶ月前 いいねされた数 20" [ref=s1e182]:\n        - /url: /asap/articles/89d2a13bdc75a3\n        - text: 🏆\n        - heading "イベントに登壇してきた話【資料もこちら】" [level=3] [ref=s1e186]\n        - time [ref=s1e189]: 1ヶ月前\n        - img "いいねされた数" [ref=s1e191]\n        - text: "20"\n- article [ref=s1e193]:\n    - link "tech" [ref=s1e194]:\n        - /url: /tech-or-idea\n    - link "👌 Grok API（Grok 2）で遊んでみた！【Elyza Tasks 100】 2ヶ月前 いいねされた数 11" [ref=s1e195]:\n        - /url: /asap/articles/6aa01bde32163d\n        - text: 👌\n        - heading "Grok API（Grok 2）で遊んでみた！【Elyza Tasks 100】" [level=3]\n          [ref=s1e199]\n        - time [ref=s1e202]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e204]\n        - text: "11"\n- article [ref=s1e206]:\n    - link "tech" [ref=s1e207]:\n        - /url: /tech-or-idea\n    - link "📑 Multi-head Latent Attention (MLA)の詳細【be absorbedとは？】 2ヶ月前 いいねされた数 22" [ref=s1e208]:\n        - /url: /asap/articles/be3d4b60d8ac92\n        - text: 📑\n        - heading "Multi-head Latent Attention (MLA)の詳細【be absorbedとは？】"\n          [level=3] [ref=s1e212]\n        - time [ref=s1e215]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e217]\n        - text: "22"\n- article [ref=s1e219]:\n    - link "tech" [ref=s1e220]:\n        - /url: /tech-or-idea\n    - link "⛳ LangGraphのTool Callingを利用して、RAG Agentsを構築する（前編） 2ヶ月前 いいねされた数 46" [ref=s1e221]:\n        - /url: /asap/articles/2ad7eb5e7560cf\n        - text: ⛳\n        - heading "LangGraphのTool Callingを利用して、RAG Agentsを構築する（前編）" [level=3]\n          [ref=s1e225]\n        - time [ref=s1e228]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e230]\n        - text: "46"\n- article [ref=s1e232]:\n    - link "tech" [ref=s1e233]:\n        - /url: /tech-or-idea\n    - link "🫠 【LangChain】で【Transformer】を利用する場合に【chat_template】を設定する方法【エラー対処記事】 2ヶ月前 いいねされた数 13" [ref=s1e234]:\n        - /url: /asap/articles/985f3580ea476d\n        - text: 🫠\n        - heading\n          "【LangChain】で【Transformer】を利用する場合に【chat_template】を設定する方法【エラー対処記事】"\n          [level=3] [ref=s1e238]\n        - time [ref=s1e241]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e243]\n        - text: "13"\n- article [ref=s1e245]:\n    - link "tech" [ref=s1e246]:\n        - /url: /tech-or-idea\n    - link "🌊 採点理由を出力しながらローカルLLMを評価する【Elyza Tasks 100】 2ヶ月前 いいねされた数 26" [ref=s1e247]:\n        - /url: /asap/articles/3a99c34fc1d4cb\n        - text: 🌊\n        - heading "採点理由を出力しながらローカルLLMを評価する【Elyza Tasks 100】" [level=3]\n          [ref=s1e251]\n        - time [ref=s1e254]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e256]\n        - text: "26"\n- article [ref=s1e258]:\n    - link "tech" [ref=s1e259]:\n        - /url: /tech-or-idea\n    - link "📑 DeepSeek-R1の論文読んだ？【勉強になるよ】 2ヶ月前 いいねされた数 921" [ref=s1e260]:\n        - /url: /asap/articles/34237ad87f8511\n        - text: 📑\n        - heading "DeepSeek-R1の論文読んだ？【勉強になるよ】" [level=3] [ref=s1e264]\n        - time [ref=s1e267]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e269]\n        - text: "921"\n- article [ref=s1e271]:\n    - link "tech" [ref=s1e272]:\n        - /url: /tech-or-idea\n    - link "📖 VAEって結局何者なの？ 3ヶ月前 いいねされた数 104" [ref=s1e273]:\n        - /url: /asap/articles/6caa9043276424\n        - text: 📖\n        - heading "VAEって結局何者なの？" [level=3] [ref=s1e277]\n        - time [ref=s1e280]: 3ヶ月前\n        - img "いいねされた数" [ref=s1e282]\n        - text: "104"\n- article [ref=s1e284]:\n    - link "tech" [ref=s1e285]:\n        - /url: /tech-or-idea\n    - link "🦔 初めてのSageMaker AIでSFTTrainerによるLLMのフルパラメータファインチューニングを試す 3ヶ月前 いいねされた数 9" [ref=s1e286]:\n        - /url: /asap/articles/3f31b77010624c\n        - text: 🦔\n        - heading "初めてのSageMaker AIでSFTTrainerによるLLMのフルパラメータファインチューニングを試す"\n          [level=3] [ref=s1e290]\n        - time [ref=s1e293]: 3ヶ月前\n        - img "いいねされた数" [ref=s1e295]\n        - text: "9"\n- article [ref=s1e297]:\n    - link "tech" [ref=s1e298]:\n        - /url: /tech-or-idea\n    - link "😺 【動画翻訳】OpenAIの研究者によるo1やLLMに対するディスカッションの翻訳 3ヶ月前 いいねされた数 12" [ref=s1e299]:\n        - /url: /asap/articles/2d2832bf3288aa\n        - text: 😺\n        - heading "【動画翻訳】OpenAIの研究者によるo1やLLMに対するディスカッションの翻訳" [level=3]\n          [ref=s1e303]\n        - time [ref=s1e306]: 3ヶ月前\n        - img "いいねされた数" [ref=s1e308]\n        - text: "12"\n- article [ref=s1e310]:\n    - link "tech" [ref=s1e311]:\n        - /url: /tech-or-idea\n    - link "🧚\u200d♀️ AIでオリジナル楽曲を作り、自作のかわいい声で歌ってもらう方法 3ヶ月前 いいねされた数 28" [ref=s1e312]:\n        - /url: /asap/articles/886af0fe48dda3\n        - text: 🧚\u200d♀️\n        - heading "AIでオリジナル楽曲を作り、自作のかわいい声で歌ってもらう方法" [level=3] [ref=s1e316]\n        - time [ref=s1e319]: 3ヶ月前\n        - img "いいねされた数" [ref=s1e321]\n        - text: "28"\n- article [ref=s1e323]:\n    - link "tech" [ref=s1e324]:\n        - /url: /tech-or-idea\n    - link "🎮 Google ColabでSeed-VCによる声色変換を試す 4ヶ月前 いいねされた数 27" [ref=s1e325]:\n        - /url: /asap/articles/9c54aef739a6ed\n        - text: 🎮\n        - heading "Google ColabでSeed-VCによる声色変換を試す" [level=3] [ref=s1e329]\n        - time [ref=s1e332]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e334]\n        - text: "27"\n- article [ref=s1e336]:\n    - link "tech" [ref=s1e337]:\n        - /url: /tech-or-idea\n    - link "🐡 LLMの性能における重要補題【Johnson–Lindenstrauss lemma】のお気持ち〜Attentionの数式理解を添えて〜 4ヶ月前 いいねされた数 21" [ref=s1e338]:\n        - /url: /asap/articles/6bdf89d8e964ec\n        - text: 🐡\n        - heading "LLMの性能における重要補題【Johnson–Lindenstrauss\n          lemma】のお気持ち〜Attentionの数式理解を添えて〜" [level=3] [ref=s1e342]\n        - time [ref=s1e345]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e347]\n        - text: "21"\n- article [ref=s1e349]:\n    - link "tech" [ref=s1e350]:\n        - /url: /tech-or-idea\n    - link "📏 【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】 4ヶ月前 いいねされた数 50" [ref=s1e351]:\n        - /url: /asap/articles/ea9f42edff6573\n        - text: 📏\n        - heading "【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】"\n          [level=3] [ref=s1e355]\n        - time [ref=s1e358]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e360]\n        - text: "50"\n- article [ref=s1e362]:\n    - link "tech" [ref=s1e363]:\n        - /url: /tech-or-idea\n    - link "🎙️ 高品質合成音声モデル【Style-Bert-VITS2】をGoogle Colabで学習し、【Aivis Speech】で利用する方法 4ヶ月前 いいねされた数 24" [ref=s1e364]:\n        - /url: /asap/articles/5c437ce2943ed7\n        - text: 🎙️\n        - heading "高品質合成音声モデル【Style-Bert-VITS2】をGoogle Colabで学習し、【Aivis\n          Speech】で利用する方法" [level=3] [ref=s1e368]\n        - time [ref=s1e371]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e373]\n        - text: "24"\n- article [ref=s1e375]:\n    - link "tech" [ref=s1e376]:\n        - /url: /tech-or-idea\n    - link "🎄 【LangChain】LCEL ステップバイステップ ハンズオン 〜なろう短編小説の自動生成＆評価・改善を行う「AIエージェント」を添えて〜 4ヶ月前 いいねされた数 19" [ref=s1e377]:\n        - /url: /asap/articles/aa587ad8d76105\n        - text: 🎄\n        - heading "【LangChain】LCEL ステップバイステップ ハンズオン\n          〜なろう短編小説の自動生成＆評価・改善を行う「AIエージェント」を添えて〜" [level=3] [ref=s1e381]\n        - time [ref=s1e384]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e386]\n        - text: "19"\n- article [ref=s1e388]:\n    - link "tech" [ref=s1e389]:\n        - /url: /tech-or-idea\n    - link "💻 LangGraphによる「AIエージェントWebアプリ」を作成する【Next.js】 4ヶ月前 いいねされた数 45" [ref=s1e390]:\n        - /url: /asap/articles/a350fe4ac00343\n        - text: 💻\n        - heading "LangGraphによる「AIエージェントWebアプリ」を作成する【Next.js】" [level=3]\n          [ref=s1e394]\n        - time [ref=s1e397]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e399]\n        - text: "45"\n- article [ref=s1e401]:\n    - link "tech" [ref=s1e402]:\n        - /url: /tech-or-idea\n    - link "🤔 LangGraphにて入力情報が不十分な場合に、ユーザへ追加質問をするAIエージェントを作る方法 5ヶ月前 いいねされた数 41" [ref=s1e403]:\n        - /url: /asap/articles/03548f844efe6f\n        - text: 🤔\n        - heading "LangGraphにて入力情報が不十分な場合に、ユーザへ追加質問をするAIエージェントを作る方法" [level=3]\n          [ref=s1e407]\n        - time [ref=s1e410]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e412]\n        - text: "41"\n- article [ref=s1e414]:\n    - link "tech" [ref=s1e415]:\n        - /url: /tech-or-idea\n    - link "🤖 高品質動画生成AI【Allegro】をローカルで実行する【VRAM12GB】 5ヶ月前 いいねされた数 10" [ref=s1e416]:\n        - /url: /asap/articles/dac57655eb0314\n        - text: 🤖\n        - heading "高品質動画生成AI【Allegro】をローカルで実行する【VRAM12GB】" [level=3] [ref=s1e420]\n        - time [ref=s1e423]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e425]\n        - text: "10"\n- article [ref=s1e427]:\n    - link "tech" [ref=s1e428]:\n        - /url: /tech-or-idea\n    - link "🌊 フレーム補間技術【RIFE】を利用してCogVideoXのFPSを改善しよう 5ヶ月前 いいねされた数 2" [ref=s1e429]:\n        - /url: /asap/articles/dbeef72b7bfdc3\n        - text: 🌊\n        - heading "フレーム補間技術【RIFE】を利用してCogVideoXのFPSを改善しよう" [level=3] [ref=s1e433]\n        - time [ref=s1e436]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e438]\n        - text: "2"\n- article [ref=s1e440]:\n    - link "tech" [ref=s1e441]:\n        - /url: /tech-or-idea\n    - link "🔥 LangGraphを使ってAIエージェントを実装する方法 5ヶ月前 いいねされた数 34" [ref=s1e442]:\n        - /url: /asap/articles/5da9cf01703a47\n        - text: 🔥\n        - heading "LangGraphを使ってAIエージェントを実装する方法" [level=3] [ref=s1e446]\n        - time [ref=s1e449]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e451]\n        - text: "34"\n- article [ref=s1e453]:\n    - link "tech" [ref=s1e454]:\n        - /url: /tech-or-idea\n    - link "🌈 動画生成AI【CogVideoX】をローカルで実行する 5ヶ月前 いいねされた数 9" [ref=s1e455]:\n        - /url: /asap/articles/a4077e8b9ddf1a\n        - text: 🌈\n        - heading "動画生成AI【CogVideoX】をローカルで実行する" [level=3] [ref=s1e459]\n        - time [ref=s1e462]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e464]\n        - text: "9"\n- article [ref=s1e466]:\n    - link "tech" [ref=s1e467]:\n        - /url: /tech-or-idea\n    - link "🛎️ ローカルでDIfyを使ってAIエージェントを実装する方法 5ヶ月前 いいねされた数 55" [ref=s1e468]:\n        - /url: /asap/articles/a35254020fbbae\n        - text: 🛎️\n        - heading "ローカルでDIfyを使ってAIエージェントを実装する方法" [level=3] [ref=s1e472]\n        - time [ref=s1e475]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e477]\n        - text: "55"\n- article [ref=s1e479]:\n    - link "tech" [ref=s1e480]:\n        - /url: /tech-or-idea\n    - link "💫 高品質動画生成AI【Pyramid Flow】をローカルで実行する 6ヶ月前 いいねされた数 12" [ref=s1e481]:\n        - /url: /asap/articles/e1880903625cb5\n        - text: 💫\n        - heading "高品質動画生成AI【Pyramid Flow】をローカルで実行する" [level=3] [ref=s1e485]\n        - time [ref=s1e488]: 6ヶ月前\n        - img "いいねされた数" [ref=s1e490]\n        - text: "12"\n- article [ref=s1e492]:\n    - link "tech" [ref=s1e493]:\n        - /url: /tech-or-idea\n    - link "📖 不偏分散で N-1 で割る理由 を自力で導出する 6ヶ月前 いいねされた数 5" [ref=s1e494]:\n        - /url: /asap/articles/f0cba34c2ac2ac\n        - text: 📖\n        - heading "不偏分散で N-1 で割る理由 を自力で導出する" [level=3] [ref=s1e498]\n        - time [ref=s1e501]: 6ヶ月前\n        - img "いいねされた数" [ref=s1e503]\n        - text: "5"\n- article [ref=s1e505]:\n    - link "tech" [ref=s1e506]:\n        - /url: /tech-or-idea\n    - link "✍️ 勾配降下法 ・ 誤差逆伝播法 を自力で導出する 6ヶ月前 いいねされた数 25" [ref=s1e507]:\n        - /url: /asap/articles/643f604d444813\n        - text: ✍️\n        - heading "勾配降下法 ・ 誤差逆伝播法 を自力で導出する" [level=3] [ref=s1e511]\n        - time [ref=s1e514]: 6ヶ月前\n        - img "いいねされた数" [ref=s1e516]\n        - text: "25"\n- article [ref=s1e518]:\n    - link "tech" [ref=s1e519]:\n        - /url: /tech-or-idea\n    - link "💰 Open AI Realtime APIで、会話履歴を削除して高額請求を回避したい！【Pythonサンプルコード】 2024/10/07 いいねされた数 25" [ref=s1e520]:\n        - /url: /asap/articles/af07fcedbbef61\n        - text: 💰\n        - heading "Open AI Realtime APIで、会話履歴を削除して高額請求を回避したい！【Pythonサンプルコード】"\n          [level=3] [ref=s1e524]\n        - time [ref=s1e527]: 2024/10/07\n        - img "いいねされた数" [ref=s1e529]\n        - text: "25"\n- article [ref=s1e531]:\n    - link "tech" [ref=s1e532]:\n        - /url: /tech-or-idea\n    - link "✈️ Open AI Realtime APIのPythonサンプルコードで、AI発話にカットイン（割り込み）する方法 2024/10/06 いいねされた数 24" [ref=s1e533]:\n        - /url: /asap/articles/563500af4649da\n        - text: ✈️\n        - heading "Open AI Realtime APIのPythonサンプルコードで、AI発話にカットイン（割り込み）する方法"\n          [level=3] [ref=s1e537]\n        - time [ref=s1e540]: 2024/10/06\n        - img "いいねされた数" [ref=s1e542]\n        - text: "24"\n- article [ref=s1e544]:\n    - link "tech" [ref=s1e545]:\n        - /url: /tech-or-idea\n    - link "🥰 Open AI Realtime APIのPythonサンプルコードを作成 2024/10/04 いいねされた数 79" [ref=s1e546]:\n        - /url: /asap/articles/4368fd306b592a\n        - text: 🥰\n        - heading "Open AI Realtime APIのPythonサンプルコードを作成" [level=3] [ref=s1e550]\n        - time [ref=s1e553]: 2024/10/04\n        - img "いいねされた数" [ref=s1e555]\n        - text: "79"\n- article [ref=s1e557]:\n    - link "tech" [ref=s1e558]:\n        - /url: /tech-or-idea\n    - link "📗 主成分分析を自力で導出する 2024/10/03 いいねされた数 6" [ref=s1e559]:\n        - /url: /asap/articles/ff8f34d19ca6a4\n        - text: 📗\n        - heading "主成分分析を自力で導出する" [level=3] [ref=s1e563]\n        - time [ref=s1e566]: 2024/10/03\n        - img "いいねされた数" [ref=s1e568]\n        - text: "6"\n- article [ref=s1e570]:\n    - link "tech" [ref=s1e571]:\n        - /url: /tech-or-idea\n    - link "😍 FLUX.1-dev で ControlNet-depth と LoRA【QTCanimatedFeeling-FLUX】 を試す 2024/10/02 いいねされた数 3" [ref=s1e572]:\n        - /url: /asap/articles/9192c2363dd96a\n        - text: 😍\n        - heading "FLUX.1-dev で ControlNet-depth と LoRA【QTCanimatedFeeling-FLUX】\n          を試す" [level=3] [ref=s1e576]\n        - time [ref=s1e579]: 2024/10/02\n        - img "いいねされた数" [ref=s1e581]\n        - text: "3"\n- article [ref=s1e583]:\n    - link "tech" [ref=s1e584]:\n        - /url: /tech-or-idea\n    - link "📗 拡散モデルの理論について（拡散過程の条件付き確率・逆拡散過程のスケジューラ） 2024/09/23 いいねされた数 8" [ref=s1e585]:\n        - /url: /asap/articles/8aaa896a02f168\n        - text: 📗\n        - heading "拡散モデルの理論について（拡散過程の条件付き確率・逆拡散過程のスケジューラ）" [level=3] [ref=s1e589]\n        - time [ref=s1e592]: 2024/09/23\n        - img "いいねされた数" [ref=s1e594]\n        - text: "8"\n- article [ref=s1e596]:\n    - link "tech" [ref=s1e597]:\n        - /url: /tech-or-idea\n    - link "📖 分散共分散行列の魅力 2024/09/20 いいねされた数 14" [ref=s1e598]:\n        - /url: /asap/articles/2269cd74c5ddc2\n        - text: 📖\n        - heading "分散共分散行列の魅力" [level=3] [ref=s1e602]\n        - time [ref=s1e605]: 2024/09/20\n        - img "いいねされた数" [ref=s1e607]\n        - text: "14"\n- article [ref=s1e609]:\n    - link "tech" [ref=s1e610]:\n        - /url: /tech-or-idea\n    - link "😍 超強力汎用ControlNet「AnyTest v4」を利用する方法【diffusers】 2024/09/17 いいねされた数 9" [ref=s1e611]:\n        - /url: /asap/articles/c08dffa41bec03\n        - text: 😍\n        - heading "超強力汎用ControlNet「AnyTest v4」を利用する方法【diffusers】" [level=3]\n          [ref=s1e615]\n        - time [ref=s1e618]: 2024/09/17\n        - img "いいねされた数" [ref=s1e620]\n        - text: "9"\n- article [ref=s1e622]:\n    - link "tech" [ref=s1e623]:\n        - /url: /tech-or-idea\n    - link "🌍 FLUX.1-devでControlNetを利用したら画像の質が下がったのでImg2Imgを使って改善してみた【Diffusers】 2024/09/13 いいねされた数 2" [ref=s1e624]:\n        - /url: /asap/articles/912d6de879d006\n        - text: 🌍\n        - heading\n          "FLUX.1-devでControlNetを利用したら画像の質が下がったのでImg2Imgを使って改善してみた【Diffusers】"\n          [level=3] [ref=s1e628]\n        - time [ref=s1e631]: 2024/09/13\n        - img "いいねされた数" [ref=s1e633]\n        - text: "2"\n- article [ref=s1e635]:\n    - link "tech" [ref=s1e636]:\n        - /url: /tech-or-idea\n    - link "🎮 【Next.js】ポートレートの表情を編集する自作UIを作ってみた【ComfyUI-AdvancedLivePortrait】 2024/09/11 いいねされた数 8" [ref=s1e637]:\n        - /url: /asap/articles/c49551ff14d5e9\n        - text: 🎮\n        - heading\n          "【Next.js】ポートレートの表情を編集する自作UIを作ってみた【ComfyUI-AdvancedLivePortrait】"\n          [level=3] [ref=s1e641]\n        - time [ref=s1e644]: 2024/09/11\n        - img "いいねされた数" [ref=s1e646]\n        - text: "8"\n- article [ref=s1e648]:\n    - link "tech" [ref=s1e649]:\n        - /url: /tech-or-idea\n    - link "📖 ComfyUI-AdvancedLivePortraitを使ってみる（No WebUI）（Google Colab） 2024/09/10 いいねされた数 7" [ref=s1e650]:\n        - /url: /asap/articles/c0a1b82a10f0e2\n        - text: 📖\n        - heading "ComfyUI-AdvancedLivePortraitを使ってみる（No WebUI）（Google Colab）"\n          [level=3] [ref=s1e654]\n        - time [ref=s1e657]: 2024/09/10\n        - img "いいねされた数" [ref=s1e659]\n        - text: "7"\n- article [ref=s1e661]:\n    - link "tech" [ref=s1e662]:\n        - /url: /tech-or-idea\n    - link "⛳ websocketsでリアルタイム音声処理 2024/09/06 いいねされた数 9" [ref=s1e663]:\n        - /url: /asap/articles/47f839550e5535\n        - text: ⛳\n        - heading "websocketsでリアルタイム音声処理" [level=3] [ref=s1e667]\n        - time [ref=s1e670]: 2024/09/06\n        - img "いいねされた数" [ref=s1e672]\n        - text: "9"\n- article [ref=s1e674]:\n    - link "tech" [ref=s1e675]:\n        - /url: /tech-or-idea\n    - link "😍 生成AIをGoogle Colabで簡単に 【FLUX.1-dev + ControlNet + LoRA】【Diffusers】 2024/08/31 いいねされた数 9" [ref=s1e676]:\n        - /url: /asap/articles/e4c199cecf1836\n        - text: 😍\n        - heading "生成AIをGoogle Colabで簡単に 【FLUX.1-dev + ControlNet +\n          LoRA】【Diffusers】" [level=3] [ref=s1e680]\n        - time [ref=s1e683]: 2024/08/31\n        - img "いいねされた数" [ref=s1e685]\n        - text: "9"\n- article [ref=s1e687]:\n    - link "tech" [ref=s1e688]:\n        - /url: /tech-or-idea\n    - link "👋 Style-Bert-VITS2のAPI化 2024/08/28 いいねされた数 6" [ref=s1e689]:\n        - /url: /asap/articles/12b0f31625a90f\n        - text: 👋\n        - heading "Style-Bert-VITS2のAPI化" [level=3] [ref=s1e693]\n        - time [ref=s1e696]: 2024/08/28\n        - img "いいねされた数" [ref=s1e698]\n        - text: "6"\n- article [ref=s1e700]:\n    - link "tech" [ref=s1e701]:\n        - /url: /tech-or-idea\n    - link "😊 【超簡単】1枚の画像と音声から口パク動画を作りたい！【EchoMimic】 2024/08/25 いいねされた数 6" [ref=s1e702]:\n        - /url: /asap/articles/db1ed88f291b98\n        - text: 😊\n        - heading "【超簡単】1枚の画像と音声から口パク動画を作りたい！【EchoMimic】" [level=3] [ref=s1e706]\n        - time [ref=s1e709]: 2024/08/25\n        - img "いいねされた数" [ref=s1e711]\n        - text: "6"\n- article [ref=s1e713]:\n    - link "tech" [ref=s1e714]:\n        - /url: /tech-or-idea\n    - link "✍️ 【Next.js】生成AIによる「お絵描きサポートWebアプリ」を作りたい！【全くわからない人】 2024/08/20 いいねされた数 8" [ref=s1e715]:\n        - /url: /asap/articles/8f51ffd4fc29b8\n        - text: ✍️\n        - heading "【Next.js】生成AIによる「お絵描きサポートWebアプリ」を作りたい！【全くわからない人】" [level=3]\n          [ref=s1e719]\n        - time [ref=s1e722]: 2024/08/20\n        - img "いいねされた数" [ref=s1e724]\n        - text: "8"\n- article [ref=s1e726]:\n    - link "tech" [ref=s1e727]:\n        - /url: /tech-or-idea\n    - link "🔥 GPT-SoVITS ver2を学習する【WebUI使わない】 2024/08/15 いいねされた数 8" [ref=s1e728]:\n        - /url: /asap/articles/e64c6fce28e0f0\n        - text: 🔥\n        - heading "GPT-SoVITS ver2を学習する【WebUI使わない】" [level=3] [ref=s1e732]\n        - time [ref=s1e735]: 2024/08/15\n        - img "いいねされた数" [ref=s1e737]\n        - text: "8"\n- article [ref=s1e739]:\n    - link "tech" [ref=s1e740]:\n        - /url: /tech-or-idea\n    - link "🔥 GPT-soVITS ver2をモジュールとして動かす 2024/08/08 いいねされた数 9" [ref=s1e741]:\n        - /url: /asap/articles/964a7d863d8896\n        - text: 🔥\n        - heading "GPT-soVITS ver2をモジュールとして動かす" [level=3] [ref=s1e745]\n        - time [ref=s1e748]: 2024/08/08\n        - img "いいねされた数" [ref=s1e750]\n        - text: "9"\n- article [ref=s1e752]:\n    - link "tech" [ref=s1e753]:\n        - /url: /tech-or-idea\n    - link "💯 拡散モデルの理論って面白いよね 2024/08/07 いいねされた数 87" [ref=s1e754]:\n        - /url: /asap/articles/4092ab60570b05\n        - text: 💯\n        - heading "拡散モデルの理論って面白いよね" [level=3] [ref=s1e758]\n        - time [ref=s1e761]: 2024/08/07\n        - img "いいねされた数" [ref=s1e763]\n        - text: "87"\n- article [ref=s1e765]:\n    - link "tech" [ref=s1e766]:\n        - /url: /tech-or-idea\n    - link "🙆\u200d♀️ 画像生成AIの生成途中経過を可視化したい！【Diffusers】 2024/08/06 いいねされた数 4" [ref=s1e767]:\n        - /url: /asap/articles/7940b17be86da7\n        - text: 🙆\u200d♀️\n        - heading "画像生成AIの生成途中経過を可視化したい！【Diffusers】" [level=3] [ref=s1e771]\n        - time [ref=s1e774]: 2024/08/06\n        - img "いいねされた数" [ref=s1e776]\n        - text: "4"\n- article [ref=s1e778]:\n    - link "tech" [ref=s1e779]:\n        - /url: /tech-or-idea\n    - link "🚀 生成AIをGoogle Colabで簡単に 【Part10 SDXL+ANIMAGINE +ControlNet+LoRA編（全部載せ）】 2024/08/03 いいねされた数 3" [ref=s1e780]:\n        - /url: /asap/articles/aa856819e2a722\n        - text: 🚀\n        - heading "生成AIをGoogle Colabで簡単に 【Part10 SDXL+ANIMAGINE\n          +ControlNet+LoRA編（全部載せ）】" [level=3] [ref=s1e784]\n        - time [ref=s1e787]: 2024/08/03\n        - img "いいねされた数" [ref=s1e789]\n        - text: "3"\n- article [ref=s1e791]:\n    - link "tech" [ref=s1e792]:\n        - /url: /tech-or-idea\n    - link "😸 生成AIをGoogle Colaboratoryで簡単に 【Part6.5 画像生成AI SDXL+ANIMAGINE XL 3.1編】 2024/07/31 いいねされた数 3" [ref=s1e793]:\n        - /url: /asap/articles/f9ac2e83c820ff\n        - text: 😸\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part6.5 画像生成AI SDXL+ANIMAGINE\n          XL 3.1編】" [level=3] [ref=s1e797]\n        - time [ref=s1e800]: 2024/07/31\n        - img "いいねされた数" [ref=s1e802]\n        - text: "3"\n- article [ref=s1e804]:\n    - link "tech" [ref=s1e805]:\n        - /url: /tech-or-idea\n    - link "🎃 生成AIをGoogle Colaboratoryで簡単に 【Part9 画像生成AI SD3+ControlNet編】 2024/07/29 いいねされた数 3" [ref=s1e806]:\n        - /url: /asap/articles/53560859d273e4\n        - text: 🎃\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part9 画像生成AI SD3+ControlNet編】"\n          [level=3] [ref=s1e810]\n        - time [ref=s1e813]: 2024/07/29\n        - img "いいねされた数" [ref=s1e815]\n        - text: "3"\n- article [ref=s1e817]:\n    - link "tech" [ref=s1e818]:\n        - /url: /tech-or-idea\n    - link "✨ 生成AIをGoogle Colaboratoryで簡単に 【Part8 画像生成AI Stable Diffusion 3 Medium編】 2024/07/25 いいねされた数 4" [ref=s1e819]:\n        - /url: /asap/articles/6f6ca9aa0513d8\n        - text: ✨\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part8 画像生成AI Stable Diffusion 3\n          Medium編】" [level=3] [ref=s1e823]\n        - time [ref=s1e826]: 2024/07/25\n        - img "いいねされた数" [ref=s1e828]\n        - text: "4"\n- article [ref=s1e830]:\n    - link "tech" [ref=s1e831]:\n        - /url: /tech-or-idea\n    - link "🌟 生成AIをGoogle Colaboratoryで簡単に 【Part7 画像生成AI SDXL Turbo編】 2024/07/23 いいねされた数 2" [ref=s1e832]:\n        - /url: /asap/articles/ed58a6899c1c26\n        - text: 🌟\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part7 画像生成AI SDXL Turbo編】"\n          [level=3] [ref=s1e836]\n        - time [ref=s1e839]: 2024/07/23\n        - img "いいねされた数" [ref=s1e841]\n        - text: "2"\n- article [ref=s1e843]:\n    - link "tech" [ref=s1e844]:\n        - /url: /tech-or-idea\n    - link "🌟 生成AIをGoogle Colaboratoryで簡単に 【Part6 画像生成AI Stable Diffusion XL編】 2024/07/19 いいねされた数 4" [ref=s1e845]:\n        - /url: /asap/articles/e4791be54f90ce\n        - text: 🌟\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part6 画像生成AI Stable Diffusion\n          XL編】" [level=3] [ref=s1e849]\n        - time [ref=s1e852]: 2024/07/19\n        - img "いいねされた数" [ref=s1e854]\n        - text: "4"\n- article [ref=s1e856]:\n    - link "tech" [ref=s1e857]:\n        - /url: /tech-or-idea\n    - link "🎤 生成AIをローカルで簡単に 【Part5.5 faster-whisper+マイク録音編】 2024/07/16 いいねされた数 7" [ref=s1e858]:\n        - /url: /asap/articles/2c0d421e68ef16\n        - text: 🎤\n        - heading "生成AIをローカルで簡単に 【Part5.5 faster-whisper+マイク録音編】" [level=3]\n          [ref=s1e862]\n        - time [ref=s1e865]: 2024/07/16\n        - img "いいねされた数" [ref=s1e867]\n        - text: "7"\n- article [ref=s1e869]:\n    - link "tech" [ref=s1e870]:\n        - /url: /tech-or-idea\n    - link "📖 生成AIをGoogle Colaboratoryで簡単に 【Part5 音声認識AI faster-whisper編】 2024/07/13 いいねされた数 2" [ref=s1e871]:\n        - /url: /asap/articles/ba8fcb1880165e\n        - text: 📖\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part5 音声認識AI faster-whisper編】"\n          [level=3] [ref=s1e875]\n        - time [ref=s1e878]: 2024/07/13\n        - img "いいねされた数" [ref=s1e880]\n        - text: "2"\n- article [ref=s1e882]:\n    - link "tech" [ref=s1e883]:\n        - /url: /tech-or-idea\n    - link "😸 生成AIをGoogle Colaboratoryで簡単に 【Part4 音声認識AI Whisper API編】 2024/07/11 いいねされた数 2" [ref=s1e884]:\n        - /url: /asap/articles/ecaa5631e42195\n        - text: 😸\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part4 音声認識AI Whisper API編】"\n          [level=3] [ref=s1e888]\n        - time [ref=s1e891]: 2024/07/11\n        - img "いいねされた数" [ref=s1e893]\n        - text: "2"\n- article [ref=s1e895]:\n    - link "tech" [ref=s1e896]:\n        - /url: /tech-or-idea\n    - link "😊 生成AIをGoogle Colaboratoryで簡単に 【Part3 テキスト生成 ローカルLLM編】 2024/07/08 いいねされた数 4" [ref=s1e897]:\n        - /url: /asap/articles/8c3d5164d14f6e\n        - text: 😊\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part3 テキスト生成 ローカルLLM編】"\n          [level=3] [ref=s1e901]\n        - time [ref=s1e904]: 2024/07/08\n        - img "いいねされた数" [ref=s1e906]\n        - text: "4"\n- article [ref=s1e908]:\n    - link "tech" [ref=s1e909]:\n        - /url: /tech-or-idea\n    - link "🐷 生成AIをGoogle Colaboratoryで簡単に 【Part2 テキスト生成ChatGPT API編】 2024/07/05 いいねされた数 2" [ref=s1e910]:\n        - /url: /asap/articles/f410dd5d407652\n        - text: 🐷\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part2 テキスト生成ChatGPT API編】"\n          [level=3] [ref=s1e914]\n        - time [ref=s1e917]: 2024/07/05\n        - img "いいねされた数" [ref=s1e919]\n        - text: "2"\n- article [ref=s1e921]:\n    - link "tech" [ref=s1e922]:\n        - /url: /tech-or-idea\n    - link "🗂 生成AIをGoogle Colaboratoryで簡単に 【Part1 合成音声Style-Bert-VITS2編】 2024/07/02 いいねされた数 3" [ref=s1e923]:\n        - /url: /asap/articles/7bdb20dcb5cc56\n        - text: 🗂\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part1 合成音声Style-Bert-VITS2編】"\n          [level=3] [ref=s1e927]\n        - time [ref=s1e930]: 2024/07/02\n        - img "いいねされた数" [ref=s1e932]\n        - text: "3"\n- article [ref=s1e934]:\n    - link "tech" [ref=s1e935]:\n        - /url: /tech-or-idea\n    - link "👏 Remdisの環境構築で沼った話（＋使い方の紹介） 2024/06/26 いいねされた数 14" [ref=s1e936]:\n        - /url: /asap/articles/101379d8951b08\n        - text: 👏\n        - heading "Remdisの環境構築で沼った話（＋使い方の紹介）" [level=3] [ref=s1e940]\n        - time [ref=s1e943]: 2024/06/26\n        - img "いいねされた数" [ref=s1e945]\n        - text: "14"\n- article [ref=s1e947]:\n    - link "tech" [ref=s1e948]:\n        - /url: /tech-or-idea\n    - link "🚀 Style-Bert-VITS2の使い方メモ 2024/06/17 いいねされた数 34" [ref=s1e949]:\n        - /url: /asap/articles/f8c0621cdd74cc\n        - text: 🚀\n        - heading "Style-Bert-VITS2の使い方メモ" [level=3] [ref=s1e953]\n        - time [ref=s1e956]: 2024/06/17\n        - img "いいねされた数" [ref=s1e958]\n        - text: "34"\n- article [ref=s1e960]:\n    - link "tech" [ref=s1e961]:\n        - /url: /tech-or-idea\n    - link "💭 Style-Bert-VITS2のVoice Conversion機能の復活に挑戦してみた（前編） 2024/06/09 いいねされた数 9" [ref=s1e962]:\n        - /url: /asap/articles/df2c610d1389e4\n        - text: 💭\n        - heading "Style-Bert-VITS2のVoice Conversion機能の復活に挑戦してみた（前編）" [level=3]\n          [ref=s1e966]\n        - time [ref=s1e969]: 2024/06/09\n        - img "いいねされた数" [ref=s1e971]\n        - text: "9"\n- article [ref=s1e973]:\n    - link "tech" [ref=s1e974]:\n        - /url: /tech-or-idea\n    - link "😸 OpenAI Assistants API v2を色々いじってみた（使い方）（File Search編） 2024/05/28 いいねされた数 10" [ref=s1e975]:\n        - /url: /asap/articles/5c2f8ffe2a46ce\n        - text: 😸\n        - heading "OpenAI Assistants API v2を色々いじってみた（使い方）（File Search編）"\n          [level=3] [ref=s1e979]\n        - time [ref=s1e982]: 2024/05/28\n        - img "いいねされた数" [ref=s1e984]\n        - text: "10"\n- article [ref=s1e986]:\n    - link "tech" [ref=s1e987]:\n        - /url: /tech-or-idea\n    - link "👻 OpenAI Assistants API v2を色々いじってみた（使い方）（code interpreter編） 2024/05/27 いいねされた数 5" [ref=s1e988]:\n        - /url: /asap/articles/b0ce16ea1832ac\n        - text: 👻\n        - heading "OpenAI Assistants API v2を色々いじってみた（使い方）（code interpreter編）"\n          [level=3] [ref=s1e992]\n        - time [ref=s1e995]: 2024/05/27\n        - img "いいねされた数" [ref=s1e997]\n        - text: "5"\n- article [ref=s1e999]:\n    - link "tech" [ref=s1e1000]:\n        - /url: /tech-or-idea\n    - link "😑 【自分用】「AI後輩ちゃん」の個人的つまづきポイントまとめ 2024/05/09 いいねされた数 3" [ref=s1e1001]:\n        - /url: /asap/articles/98b04c5ea82bd9\n        - text: 😑\n        - heading "【自分用】「AI後輩ちゃん」の個人的つまづきポイントまとめ" [level=3] [ref=s1e1005]\n        - time [ref=s1e1008]: 2024/05/09\n        - img "いいねされた数" [ref=s1e1010]\n        - text: "3"\n- article [ref=s1e1012]:\n    - link "tech" [ref=s1e1013]:\n        - /url: /tech-or-idea\n    - link "😍 音声対話システム「AI後輩ちゃん」を作ってみた 2024/05/04 いいねされた数 17" [ref=s1e1014]:\n        - /url: /asap/articles/5b1b7553fcaa76\n        - text: 😍\n        - heading "音声対話システム「AI後輩ちゃん」を作ってみた" [level=3] [ref=s1e1018]\n        - time [ref=s1e1021]: 2024/05/04\n        - img "いいねされた数" [ref=s1e1023]\n        - text: "17"\n- contentinfo [ref=s1e1026]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1030]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1031]\n    - paragraph [ref=s1e1039]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e1041]:\n        - heading "About" [level=4] [ref=s1e1042]\n        - list [ref=s1e1043]:\n            - listitem [ref=s1e1044]:\n                - link "Zennについて" [ref=s1e1046]:\n                    - /url: /about\n            - listitem [ref=s1e1047]:\n                - link "運営会社" [ref=s1e1048]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e1049]:\n                - link "お知らせ・リリース" [ref=s1e1050]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e1051]:\n                - link "イベント" [ref=s1e1053]:\n                    - /url: /events\n    - navigation [ref=s1e1054]:\n        - heading "Guides" [level=4] [ref=s1e1055]\n        - list [ref=s1e1056]:\n            - listitem [ref=s1e1057]:\n                - link "使い方" [ref=s1e1059]:\n                    - /url: /manual\n            - listitem [ref=s1e1060]:\n                - link "法人向けメニュー" [ref=s1e1062]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e1064]:\n                - link "Publication / Pro" [ref=s1e1066]:\n                    - /url: /publications\n            - listitem [ref=s1e1067]:\n                - link "よくある質問" [ref=s1e1069]:\n                    - /url: /faq\n    - navigation [ref=s1e1070]:\n        - heading "Links" [level=4] [ref=s1e1071]\n        - list [ref=s1e1072]:\n            - listitem [ref=s1e1073]:\n                - link "X(Twitter)" [ref=s1e1074]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e1075]:\n                - link "GitHub" [ref=s1e1076]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e1077]:\n                - link "メディアキット" [ref=s1e1079]:\n                    - /url: /mediakit\n    - navigation [ref=s1e1080]:\n        - heading "Legal" [level=4] [ref=s1e1081]\n        - list [ref=s1e1082]:\n            - listitem [ref=s1e1083]:\n                - link "利用規約" [ref=s1e1085]:\n                    - /url: /terms\n            - listitem [ref=s1e1086]:\n                - link "プライバシーポリシー" [ref=s1e1088]:\n                    - /url: /privacy\n            - listitem [ref=s1e1089]:\n                - link "特商法表記" [ref=s1e1091]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e1094]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e1095]\n- alert [ref=s1e1114]\n```\n', name='browser_navigate', tool_call_id='58fad86e-0b01-4fc6-a8cc-57deb5c48b41'), AIMessage(content='`https://zenn.dev/asap`にアクセスしました。\n\n執筆者の記事一覧から、いいねの数が多い記事を探します。\n\n- 「DeepSeek-R1の論文読んだ？【勉強になるよ】」 が921いいね\n- 「拡散モデルの理論って面白いよね」 が87いいね\n- 「Open AI Realtime APIのPythonサンプルコードを作成」 が79いいね\n- 「📏 【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】」 が50いいね\n- 「🙌 完全自律型AIエージェントOSS【LangManus】完全に理解した！」 が48いいね\n\n最もいいねが多い記事は「DeepSeek-R1の論文読んだ？【勉強になるよ】」です。\n記事の中身を取得します。', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap/articles/34237ad87f8511"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-27104099-cd7e-4de1-8c89-51060ed9c6f1-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap/articles/34237ad87f8511'}, 'id': 'd85f8505-9c2a-4960-b17f-9ef88d983d9d', 'type': 'tool_call'}], usage_metadata={'input_tokens': 17995, 'output_tokens': 216, 'total_tokens': 18211, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap/articles/34237ad87f8511\n\n- Page URL: https://zenn.dev/asap/articles/34237ad87f8511\n- Page Title: DeepSeek-R1の論文読んだ？【勉強になるよ】\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- article [ref=s1e32]:\n    - text: 📑\n    - heading "DeepSeek-R1の論文読んだ？【勉強になるよ】" [level=1] [ref=s1e38]\n    - text: 2025/01/27に公開\n    - tooltip "更新日" [ref=s1e45]:\n        - img [ref=s1e46]\n        - text: 2025/02/22 更新日\n    - link "12件" [ref=s1e51]:\n        - /url: "#discuss"\n        - img [ref=s1e52]\n        - text: 12件\n    - link "強化学習" [ref=s1e63]:\n        - /url: /topics/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92\n        - img [ref=s1e65]\n        - text: 強化学習\n    - link "蒸留" [ref=s1e67]:\n        - /url: /topics/%E8%92%B8%E7%95%99\n        - img [ref=s1e69]\n        - text: 蒸留\n    - link "LLM" [ref=s1e71]:\n        - /url: /topics/llm\n        - img [ref=s1e73]\n        - text: LLM\n    - link "生成 AI" [ref=s1e75]:\n        - /url: /topics/%E7%94%9F%E6%88%90ai\n        - img [ref=s1e77]\n        - text: 生成 AI\n    - link "DeepSeek" [ref=s1e79]:\n        - /url: /topics/deepseek\n        - img [ref=s1e81]\n        - text: DeepSeek\n    - link "tech" [ref=s1e83]:\n        - /url: /tech-or-idea\n        - img [ref=s1e85]\n        - text: tech\n    - heading "はじめに" [level=1] [ref=s1e89]\n    - paragraph [ref=s1e90]: タイトル通りDeepSeek-R1の論文を読んだので、少しでもわかりやすく説明できればと思います。\n        まだ論文を読んでいない方は、ぜひ原文も読んでみてください！\n    - paragraph [ref=s1e92]:\n        - text: 難しい数式や概念がほとんど使われていないため、論文の中では全然読みやすい方だと思います。読み物として読めるレベルです。\n        - iframe [ref=s1e94]:\n            - text: Loading...\n            - alert [ref=f1s1e8]\n    - paragraph [ref=s1e95]: とはいえ、本記事では細かい部分など間違っている部分もあるかもしれないので、ぜひ誤りがあったら、こそっと教えてください！\n    - complementary [ref=s1e96]:\n        - text: "!"\n        - paragraph [ref=s1e99]: 実は、本論文を解説する上で、著者らが公開しているもう一つの論文が大きく関連しています。 そちらの論文の内容も若干解説を入れております。\n        - paragraph [ref=s1e101]:\n            - text: 「\n            - \'link "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models" [ref=s1e102]\':\n                - /url: https://arxiv.org/html/2402.03300v3\n            - text: 」\n        - paragraph [ref=s1e103]: 本記事でも少し触れますが、上記の論文も非常に勉強になるため、ぜひ読んでみてください。\n    - complementary [ref=s1e104]:\n        - text: "!"\n        - paragraph [ref=s1e107]:\n            - text: 本記事は、\n            - link "DeepSeek-R1の論文" [ref=s1e108]:\n                - /url: https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\n            - text: と\n            - link "DeepSeekMathの論文" [ref=s1e109]:\n                - /url: https://arxiv.org/html/2402.03300v3\n            - text: を読んだ私の理解をもとに記載しています。\n        - paragraph [ref=s1e110]: 本論文で使われている技術に関しては、ある程度の知識を持っているので、大きくは外していないとは思いますが、私の主観も入っている部分もありますので、ご了承ください。\n    - paragraph [ref=s1e111]:\n        - text: また、DeepSeek-R1の論文が公開される前に、小型モデルに対して同様の実験（強化学習）をしていたグループがあるようです。\n            そちらのレポートは下記になります。\n        - iframe [ref=s1e114]:\n            - \'link "7B Model and 8K Examples: Emerging Reasoning with Reinforcement Learning is Both Effective and Effic A replication of DeepSeek-R1 training on small models with limited data hkust-nlp.notion.site favicon image hkust-nlp.notion.site hkust-nlp.notion.site thumbnail image" [ref=f2s1e6]\':\n                - /url: https://hkust-nlp.notion.site/simplerl-reason\n                - \'heading "7B Model and 8K Examples: Emerging Reasoning with\n                  Reinforcement Learning is Both Effective and Effic" [level=1]\n                  [ref=f2s1e8]\'\n                - text: A replication of DeepSeek-R1 training on small models with limited data\n                - img "hkust-nlp.notion.site favicon image" [ref=f2s1e11]\n                - text: hkust-nlp.notion.site\n                - img "hkust-nlp.notion.site thumbnail image" [ref=f2s1e13]\n            - alert [ref=f2s1e15]\n    - paragraph [ref=s1e115]: 意図せず、DeepSeek-R1-Zeroの再現実験のようなレポートになっていますが、レポートの著者はDeepSeek-R1論文の公開前から実験していると主張しています。\n        こちらも非常に興味深かったため紹介です。\n    - heading "本論文の興味深いところ" [level=1] [ref=s1e117]\n    - paragraph [ref=s1e118]: 本論文は、大きく分けて3つの構成でできています\n    - list [ref=s1e119]:\n        - listitem [ref=s1e120]:\n            - strong [ref=s1e121]: 強化学習による思考能力の強化\n            - list [ref=s1e122]:\n                - listitem [ref=s1e123]: LLM（DeepSeek-V3-Base）に対して「強化学習のみ」を適用させたところ、強力な思考能力を獲得した「DeepSeek-R1-Zero」が作成されました。\n                - listitem [ref=s1e124]: 本論文のメインストリームであり、非常に興味深い示唆が得られる部分です。\n        - listitem [ref=s1e125]:\n            - strong [ref=s1e126]: SFTによるさらなる強化\n            - list [ref=s1e127]:\n                - listitem [ref=s1e128]: DeepSeek-V3-Baseに対して、コールドスタート問題を解消するために、CoT（Chain of\n                    Thought）データセットによるSFT（教師付きファインチューニング）を実施\n                - listitem [ref=s1e129]: その後、強化学習を適用させることで、o1レベルの性能を獲得した「DeepSeek-R1」が作成されました。\n                - listitem [ref=s1e130]: 非常に強力なLLMを獲得するまでの反復学習などの試行錯誤などを学ぶことができます。\n        - listitem [ref=s1e131]:\n            - strong [ref=s1e132]: 蒸留による小型モデルの性能強化\n            - list [ref=s1e133]:\n                - listitem [ref=s1e134]: 上記で作成されたDeepSeek-R1により、他社の小型オープンソースLLMを蒸留しました。\n                - listitem [ref=s1e135]: その結果、元の性能を大幅に上回る、非常に高い性能の小型LLMが得られました。\n    - paragraph [ref=s1e136]: その中でも私が特に面白いなと思った要素は下記の要素になります\n    - list [ref=s1e137]:\n        - listitem [ref=s1e138]:\n            - strong [ref=s1e139]: 強化学習手法「GRPO」すごい！\n        - listitem [ref=s1e140]:\n            - strong [ref=s1e141]: ルールベース報酬で思考能力が向上するのがすごい！\n        - listitem [ref=s1e142]:\n            - strong [ref=s1e143]: aha moment（ハッとする瞬間）面白い！\n        - listitem [ref=s1e144]:\n            - strong [ref=s1e145]: 蒸留vs強化学習の比較実験から重要な示唆が得られた！\n    - heading "本論文の流れ" [level=1] [ref=s1e146]\n    - heading "前提のモデル構造" [level=3] [ref=s1e147]\n    - paragraph [ref=s1e148]: そもそも、大前提として、この論文は「DeepSeek-V3-Base」という強力なLLMの性能を改善する論文です。\n        これ単体でも、GPT-4oレベルの能力を持っているオープンソースLLMになります。\n    - paragraph [ref=s1e150]: 本論文のモデルアーキテクチャは「DeepSeek-V3-Base」と完全に同じものを利用します。\n        （つまりモデルアーキテクチャをかえずに、学習方法を工夫してLLMの性能を上げる挑戦になります）\n    - heading "DeepSeek-R1-Zeroを強化学習を用いて作る" [level=3] [ref=s1e152]\n    - paragraph [ref=s1e153]: その上で、まずは、「DeepSeek-V3-Base」に対して、大規模な強化学習のみを適応させることで、強化学習の可能性について探究する話が始まります。\n        結果として、「DeepSeek-R1-Zero」ができます。\n        「DeepSeek-R1-Zero」は、SFT（教師付きファインチューニング）をしなくても、強化学習のみで圧倒的な思考能力を取得できることを示す興味深いモデルです。\n    - heading "DeepSeek-R1をSFT+強化学習を用いて作る" [level=3] [ref=s1e156]\n    - paragraph [ref=s1e157]: その後、あまりにも「DeepSeek-R1-Zero」の性能が良かったことから、さらなる探究が進められました。\n    - paragraph [ref=s1e158]: この時点で、「DeepSeek-R1-Zero」には下記の課題がありました。\n    - list [ref=s1e159]:\n        - listitem [ref=s1e160]: 学習初期段階にて、強化学習の学習が不安定であり、収束に時間がかかる（コールドスタート問題）\n        - listitem [ref=s1e161]:\n            - text: 思考能力は高いが、言語が混在（日本語で話していたら急に中国語になるなど）し、人間が読みにくい\n            - list [ref=s1e162]:\n                - listitem [ref=s1e163]: 結果、高度な思考が必要なタスクには強いが、一般的タスク（ロールプレイやライティングなど）には使いにくいという評価\n    - paragraph [ref=s1e164]: そこで、「DeepSeek-V3-Base」に対して、少量のCoT（Chain of\n        Thought）データセットを用いて、SFT（教師付き微調整）を行います。このデータは、著者らが収集・構築したものになります。\n        その後、SFT後のモデルに対して、大規模強化学習を行います。\n        このとき、「DeepSeek-R1-Zero」を作成した際の報酬に加え、「言語一貫性報酬」を導入して、上述した言語の不一致の問題を緩和させます。\n    - paragraph [ref=s1e167]: その後、得られたモデル（チェックポイント）を利用して、さらなるSFT（教師付き微調整）学習データを収集し、追加SFTを行います。\n        さらに、その後、改めて２段階目の大規模強化学習を実施することで、最終的に「DeepSeek-R1」が得られます。\n    - heading "小型モデルへの蒸留を試す" [level=3] [ref=s1e169]\n    - paragraph [ref=s1e170]: 最後に、小型モデルに対する「DeepSeek-R1」による蒸留が試されています。\n        その中で、小型モデルに対して、「DeepSeek-R1」による蒸留を実施するのと、「DeepSeek-R1-Zero」と同様の強化学習手法で思考能力を強化するのと、どちらが性能向上に寄与するのかを調査してくれています。\n    - paragraph [ref=s1e172]: これが非常に興味深いです。\n    - heading "DeepSeek-R1-Zeroを作るまで" [level=1] [ref=s1e173]\n    - paragraph [ref=s1e174]: では、ここから論文の詳細な解説に入っていきます。\n    - paragraph [ref=s1e175]: まずは、「DeepSeek-R1」の前段階である「DeepSeek-R1-Zero」を作るまでの手法を解説します。\n    - paragraph [ref=s1e176]:\n        - strong [ref=s1e177]: 前段階と侮るなかれ、「DeepSeek-R1-Zero」を作るまでの手法と示唆にこそ、本論文の貴重な示唆が含まれています。\n    - heading "元々のアーキテクチャ" [level=2] [ref=s1e178]\n    - paragraph [ref=s1e179]: 今回作る「DeepSeek-R1-Zero」や「DeepSeek-R1」は、新しいモデル構造を採用して、性能を向上させたわけではありません。\n    - paragraph [ref=s1e180]: 元々のアーキテクチャは「DeepSeek-V3-Base」というアーキテクチャになります。\n        「DeepSeek-V3-Base」は、681Bのモデルパラメータを持つ大規模言語モデルで、性能はGPT-4oに匹敵する非常に高性能モデルありながら、モデル重みが完全に公開されているオープンソースモデルです。\n    - paragraph [ref=s1e182]: 「DeepSeek-V3-Base」に対して、Instructionチューニングを施したchatモデルに関しては、APIでも公開されています。\n        非常に安価に使えるにも関わらず、GPT-4oに匹敵する性能を誇ることから、人気のモデルになっております。\n    - complementary [ref=s1e184]:\n        - text: "!"\n        - paragraph [ref=s1e187]: 安価なモデルではありますが、APIの提供元は中国の企業ですので、データの取り扱いなどは十分注意してください。\n            基本的には、API利用の際には、収集されて困るような情報は入力しないようにするのが無難です。\n            （オープンソースモデルをローカルで動かす場合は、おそらく心配ありません）\n    - paragraph [ref=s1e190]: 本論文のすごいところは、モデル構造を変えずに、「DeepSeek-V3-Base」に対する学習方法を工夫しただけで、OpenAI\n        o1に匹敵する性能のLLMを開発していることです。\n        具体的には、大規模強化学習をLLMに対して適用することで、「DeepSeek-R1-Zero」を開発します。\n    - heading "すごいぞ！強化学習" [level=2] [ref=s1e192]\n    - heading "強化学習って何？" [level=3] [ref=s1e193]\n    - paragraph [ref=s1e194]: そもそも「強化学習って何？」という方もいらっしゃると思うので、簡単に説明します。\n        （以前、スイカゲームを強化学習でプレイしたこともあるので、いずれ詳しい解説記事を作る予定ですが、ここではざっくり説明します。）\n    - paragraph [ref=s1e196]: 強化学習とは、一言で言えば「一連の行動の流れ（シークエンス）によって得られる、累積報酬の期待値を最大化する」ことを目指す学習手法です。\n        ある行動をとった結果がすぐにはわからず、時間が経ってから影響が現れるような問題に適しています。\n        専門的に言うと、「マルコフ決定過程（MDP）」が成立する問題で活用されます。\n    - heading "テトリスで考える" [level=3] [ref=s1e199]\n    - paragraph [ref=s1e200]: 例えば、「テトリス」を考えてみましょう。\n    - paragraph [ref=s1e201]: テトリスは、上から「ミノ」と呼ばれる、さまざまな形のブロックを落として、横一列を綺麗に埋めてブロックを消去するゲームです。\n    - paragraph [ref=s1e202]: 横一列がすべて揃うと、その列が消えて、「ポイント」を獲得できます。\n        逆に、列を消せずに、ブロックが一定の高さまで積み上がると「ゲームオーバ」になります。\n    - paragraph [ref=s1e204]: テトリスを強化学習で解こうとすると、「どうすれば最も多くのポイントを得られるか」をAIに学習させることが重要になります。\n        単純に列を揃えて消すだけではなく、「次の手」や「その次の手」を見据えて、より良い結果につながるようにミノを配置することも必要です。\n        しかし、通常の学習方法(よくある入出力を近似させる学習方法)では、目の前の結果だけに注目してしまい、将来的な高得点につながる手を学習させることは難しいです。\n    - paragraph [ref=s1e207]:\n        - strong [ref=s1e208]: そこで役立つのが「強化学習」です。\n    - paragraph [ref=s1e209]: 強化学習では、ゲームの最初から最後までの流れを通して、「どう行動すれば最終的に得られる得点を最大化できるか」を学習できます。\n        このように、途中の行動の良し悪しがすぐにはわからなくても、最終的なゴール（得点の最大化）が設定できる問題に対して、強化学習は非常に効果的です。\n    - paragraph [ref=s1e211]:\n        - img [ref=s1e212]\n    - group [ref=s1e213]: 補足\n    - heading "強化学習の事例「AlphaGo Zero」" [level=3] [ref=s1e215]\n    - paragraph [ref=s1e216]: さらに、強化学習の威力をお伝えする事例を記載します。\n    - paragraph [ref=s1e217]: 2015年にアメリカのDeepMind社（Googleの傘下）が「AlphaGo」というモデルを開発しました。これはAIが囲碁をプレイするモデルで、当時の世界王者をハンディキャップなしで破るという、非常に衝撃的なモデルでした。\n        （当時としては、非常にセンセーショナルな出来事だったため、記憶にある方もいらっしゃるかと思います）\n    - paragraph [ref=s1e219]: この「AlphaGo」は、深層学習と強化学習により学習されています。\n        プロ棋士の棋譜データで深層学習で学習させた上で、強化学習による自己対局（AI同士の対局）を実施したモデルです。\n    - paragraph [ref=s1e221]:\n        - text: さて、ここまでも十分すごいですが、\n        - strong [ref=s1e223]: さらにすごいのはここからです。\n    - paragraph [ref=s1e224]: DeepMind社は後継モデルとして「AlphaGo Zero」というモデルを開発しました。\n        このモデルは、プロ棋士の棋譜データを「使わず」に、完全に「強化学習のみ」で学習されたモデルです。\n        つまり、囲碁のルールだけを与え、あとは最初から最後までAI同士の自己対局のみで学習されています。\n        （この場合、対局に勝利したら報酬、敗北したら罰を与える形で強化学習をし、対局に勝利した時に取っていた手（行動）の選択確率を上げるように学習しています）\n    - paragraph [ref=s1e228]:\n        - strong [ref=s1e229]: そして、その「AlphaGo Zero」は「AlphaGo」に対して、100戦全勝で圧勝しました。\n    - paragraph [ref=s1e230]: AIが囲碁の学習をする際に、人間が持つ定石の情報などは不要だったわけです。\n    - paragraph [ref=s1e231]: これが強化学習の威力です。\n        余談ですが、今回の論文で作成する「DeepSeek-R1-Zero」も（「DeepSeek-V3-Base」をベースにはしますが）強化学習のみで学習されます。\n        名前に入っている「Zero」は同じく強化学習のみで開発された「AlphaGo Zero」のオマージュかなと思います。\n    - heading "そもそもLLMで強化学習とは？" [level=2] [ref=s1e234]\n    - paragraph [ref=s1e235]: 「なんかゲームみたいなものに、強化学習を使うとうまくいきそう」というのは理解していただけたかなと思います。\n    - paragraph [ref=s1e236]:\n        - text: では、LLMに対してはどうか、というのを解説していきます。\n        - strong [ref=s1e238]: 実は、LLMと強化学習は非常に相性がいいです。\n    - paragraph [ref=s1e239]: なぜなら、LLMは一度にすべての文章を出力するAIではなく、1tokenずつ出力するモデルであり、その文章が良い文章か悪い文章かは、すべてのtokenを出力してみないとわからないからです。\n    - paragraph [ref=s1e240]:\n        - text: テトリスでは、ミノを一つずつ落とし、一列が消えたタイミングで、ポイントを取得するという形で評価されます。\n        - img [ref=s1e242]\n    - paragraph [ref=s1e243]:\n        - text: LLMでも、同様にtokenを1tokenずつ出力し、一文全部が出力されたタイミングで、その文章の良し悪しを評価することができれば、強化学習と同じ枠組みを使うことができるというわけです。\n        - img [ref=s1e245]\n    - heading "報酬モデルを作成し、累積報酬を最大化" [level=3] [ref=s1e246]\n    - paragraph [ref=s1e247]:\n        - text: テトリスでは、ゲームシステムが勝手に点数をつけてくれますが、LLMにおいて文章の良し悪しを勝手に判断してくれる便利な機能はありません。そこで我々が、その判断モデルを作ってあげる必要があります。\n            そこで、ニューラルネットワークを利用して、LLMに入力されたプロンプト\n        - math [ref=s1e252]: x\n        - text: に対して、LLMが生成した文章\n        - math [ref=s1e259]: "y"\n        - text: の点数\n        - math [ref=s1e266]: r\n        - text: を出力するようなネットワークを「報酬モデル\n        - math [ref=s1e273]: r θ ( r ∣ y , x )\n        - text: 」として作成することを考えます。\n    - paragraph [ref=s1e286]:\n        - text: まず初めに、データセットとして、「プロンプト\n        - math [ref=s1e290]: x\n        - text: 」「好ましい回答\n        - math [ref=s1e297]: y +\n        - text: 」「好ましくない回答\n        - math [ref=s1e306]: y −\n        - text: 」の3つが組となっているデータセットを用意します。 報酬モデル\n        - math [ref=s1e316]: r θ ( r ∣ y , x )\n        - text: は、「プロンプト\n        - math [ref=s1e332]: x\n        - text: 」と「出力\n        - math [ref=s1e339]: "y"\n        - text: 」の両方を入力することで、報酬（点数）を出力するモデルです。\n    - paragraph [ref=s1e343]:\n        - text: そこで、好ましい回答の報酬推定値\n        - math [ref=s1e347]: r θ ( r ∣ y + , x )\n        - text: と、好ましくない回答の報酬推定値\n        - math [ref=s1e365]: r θ ( r ∣ y − , x )\n        - text: の差を一定以上にするような損失関数を用いて学習を行います。 すると、報酬モデル\n        - math [ref=s1e384]: r θ ( r ∣ y , x )\n        - text: は適切に、好ましい回答\n        - math [ref=s1e400]: y +\n        - text: には高い点数を、好ましくない回答\n        - math [ref=s1e409]: y −\n        - text: には低い点数をつけるようになります。\n    - paragraph [ref=s1e415]:\n        - text: 報酬モデル\n        - math [ref=s1e419]: r θ ( r ∣ y , x )\n        - text: か完成したら、強化学習をするための要素は整います。\n    - paragraph [ref=s1e432]:\n        - text: 状態\n        - math [ref=s1e436]: x\n        - text: は、LLMに入力される「プロンプト」で、LLMの出力が\n        - math [ref=s1e443]: "y"\n        - text: とします。 報酬\n        - math [ref=s1e451]: r\n        - text: は、報酬モデル\n        - math [ref=s1e458]: r θ ( r ∣ y , x )\n        - text: の出力値です。 LLMが全文出力したら、プロンプト\n        - math [ref=s1e475]: x\n        - text: と、LLMの出力\n        - math [ref=s1e482]: "y"\n        - text: を報酬モデルに与えることで、取得します。 方策は、LLMが出力する各tokenごとの確率分布自体です。 このLLM自体を「方策モデル\n        - math [ref=s1e491]: π ϕ ( y t ∣ x , y < t )\n        - text: 」と呼びます。\n    - complementary [ref=s1e510]:\n        - text: "!"\n        - paragraph [ref=s1e513]:\n            - text: LLM（\n            - math [ref=s1e517]: π ϕ ( y t ∣ x , y < t )\n            - text: ）はユーザが入力したプロンプト\n            - math [ref=s1e539]: x\n            - text: と、それまでにLLMが出力した内容\n            - math [ref=s1e546]: y < t\n            - text: をもとに、\n            - math [ref=s1e557]: t\n            - text: ステップ目のtoken\n            - math [ref=s1e564]: y t\n            - text: を出力します。\n        - paragraph [ref=s1e570]: これが、LLMは、「文章の次に出てくる単語を一つずつ予測するモデル」と言われる所以です。\n    - paragraph [ref=s1e571]: これにより、累積報酬を最大化するように方策モデル（LLM）を学習することで、LLMは好ましい回答を出力するようにネットワークを更新できます。\n    - paragraph [ref=s1e572]: これが、LLMを強化学習で学習するということです。 このような手法をRLHF（Reinforcement\n        learning from human feedback）と総称します。\n    - group [ref=s1e574]: 細かい話\n    - heading "つよつよ手法「PPO（Proximal Policy Optimization）」" [level=3] [ref=s1e576]\n    - paragraph [ref=s1e577]: ここまでで、「LLMと強化学習は相性が良い」こと、「強化学習では累積報酬の期待値（状態価値）を最大化することが重要」であることを話しました。\n    - paragraph [ref=s1e578]: さて、実際にLLMに強化学習を組み込む場合、しばしばPPO（Proximal Policy\n        Optimization）という手法が利用されます。\n        （なんなら、LLM以外で強化学習を実用する上でも、最初に選択肢になるベースライン的に強力な手法でもあります）\n    - paragraph [ref=s1e580]: 今回はPPOが主題ではないので、簡単に説明します。\n    - heading "PPOの簡易的な説明" [level=4] [ref=s1e581]\n    - paragraph [ref=s1e582]:\n        - text: PPOでは、行動を確率的に出力する方策モデル（LLM）と、現在の状態価値を推定する状態価値モデルを用意します。 状態価値\n        - math [ref=s1e587]: V t\n        - text: とは、「累積報酬の期待値」として定義できます。 したがって、現時点から、方策に従って行動をした場合の、将来的な報酬総和の期待値が「状態価値\n        - math [ref=s1e597]: V t\n        - text: 」です。\n    - paragraph [ref=s1e603]:\n        - text: 強化学習の枠組みで累積報酬\n        - math [ref=s1e607]: G t\n        - text: が計算されるため、その期待値である状態価値\n        - math [ref=s1e616]: V t\n        - text: も学習が可能です。 一方で、方策モデルを学習する際に、Advantage\n        - math [ref=s1e626]: A t\n        - text: というものを利用します。\n    - paragraph [ref=s1e632]:\n        - text: Advantage\n        - math [ref=s1e636]: A t\n        - text: というのは下記の式で表されます。\n    - math [ref=s1e648]: A t = G t − V t ( v t ∣ y < t , x )\n    - paragraph [ref=s1e675]:\n        - text: 状態価値\n        - math [ref=s1e679]: V t ( v t ∣ y < t , x )\n        - text: は、累積報酬\n        - math [ref=s1e701]: G t\n        - text: の期待値（大雑把にいうと平均）になるように学習されているという前提があるため、Advantage\n        - math [ref=s1e710]: A t\n        - text: が正になるということは、今回LLMが出力したtoken\n        - math [ref=s1e719]: y t\n        - text: は、将来的に高い報酬を得ることができる「良い出力であった」と判断できます。\n    - paragraph [ref=s1e725]:\n        - text: したがって、方策モデル\n        - math [ref=s1e729]: π ϕ ( y t ∣ x , y < t )\n        - text: はAdvantage\n        - math [ref=s1e751]: A t\n        - text: を最大化するように学習します。\n    - paragraph [ref=s1e757]:\n        - text: Advantage\n        - math [ref=s1e761]: A t\n        - text: を考えずに、単純に累積報酬\n        - math [ref=s1e770]: G t\n        - text: を最大化するように考えることもできます。（むしろそれが初期の強化学習です）\n    - complementary [ref=s1e776]:\n        - text: "!"\n        - paragraph [ref=s1e779]: （2025年2月15日 追記） この辺りの表現は、わかりやすくするために若干誤解を招く表現になっています。\n            下に「細かい話」として補足説明を書きました。\n    - paragraph [ref=s1e782]: しかしその場合、そもそもどんな行動を取っても報酬が高くなる状態と、どんな行動を取っても報酬が低くなってしまう状態において、勾配のスケールがばらつくため、学習の安定性が低くなります。\n    - paragraph [ref=s1e783]:\n        - text: PPOでは、Advantage\n        - math [ref=s1e787]: A t\n        - text: を考えることにより、出力したtokenが平均よりも良いか悪いかだけで学習をすることができ、状態ごとの報酬の差をモデルが考慮する必要がなくなります。\n    - complementary [ref=s1e793]:\n        - text: "!"\n        - paragraph [ref=s1e796]: 本来、PPOの貢献は、あるパラメータ更新時において、更新前と更新後の確率分布が大きく変わらないような制約を、少ない計算量で加えることです。\n            ただ、この話は、DeepSeek-R1-Zeroにおける強化学習を理解するためには、必ずしも重要ではありません。\n            したがって、今回の記事では、必要な部分だけに絞り説明をしました。\n    - group [ref=s1e799]: 細かい話（追記）\n    - heading "著者発案「GRPO（Group Relative Policy Optimization）」" [level=3]\n      [ref=s1e801]\n    - paragraph [ref=s1e802]: さて、PPOは強化学習の手法において、高い性能を発揮しますが、LLMに適用することを考えると、状態価値モデルを別途学習しないといけないという点において、計算量的に大変な手法です。\n    - paragraph [ref=s1e803]: "一方で、PPOの状態価値モデルがやっていたことは、Advantage A_t(a_t|y_t,\n        y_{<t},x) を計算し、今回出力したtokenが、平均よりも良いのか悪いのかを判断することです。"\n    - paragraph [ref=s1e806]: "そこで、LLMにおいては、わざわざ状態価値モデルを用意して平均を計算しなくても、実際に複数文章を生成して、そ\\\n        れぞれの報酬を平均することで、Advantage A_t(a_t|y_t, y_{<t},x) を計算しようと考えました。\n        こうすることで、状態価値モデルなしで、Advantage A_t(a_t|y_t, y_{<t},x)\n        を計算できるため、計算量を大幅に削減できることができます。"\n    - paragraph [ref=s1e812]:\n        - text: この手法は「GRPO（Group Relative Policy Optimization）」と呼びます。\n            本論文の著者が、過去に公開した論文で提案した手法です。\n        - iframe [ref=s1e815]: <could not take iframe snapshot>\n    - paragraph [ref=s1e816]:\n        - text: 下記が、論文中で提示されている「PPO」と「GRPO」の違いを説明する図です。\n        - img [ref=s1e818]\n        - text: "このように状態価値モデル（Value model）なしに、Advantage A_t(a_t|y_t, y_{<t},x)\n            が計算できていることがわかります。"\n    - complementary [ref=s1e821]:\n        - text: "!"\n        - paragraph [ref=s1e824]: 本記事でのアルファベットの付け方と、上記の図のアルファベットの付け方が異なることに注意してください。\n        - paragraph [ref=s1e825]: 上記の図（GRPO）では、 プロンプトを q 、LLMの各出力文章を o_i （ i\n            はそれぞれの文章単位でのインデックス）、各文章ごとの報酬モデルの出力を r_i 、報酬 r_i\n            の平均から計算された各文章ごとのAdvantageを A_i としています。\n        - paragraph [ref=s1e839]: 本記事での添え字は、token単位での添え字を採用しています。 例えば A_t なら t token目でのAdvantageです。\n        - paragraph [ref=s1e845]: 上記の図では、文章単位での添え字です。 例えば、 A_i なら、 i 文章目でのAdvantageとなります。\n            従って A_i はベクトルとなっており、中にtokenごとのAdvantageの値も格納されているはずです。\n    - heading "報酬モデルを作らずルールベースで実施" [level=3] [ref=s1e854]\n    - paragraph [ref=s1e855]:\n        - text: 改めて、LLMに強化学習を適用するために必要なものをまとめます。 （下記の図のGRPOの手法を利用します）\n        - img [ref=s1e858]\n    - paragraph [ref=s1e859]: "まず、大前提として方策モデル \\\\pi_\\\\phi(y_t|x,y_{<t}) （LLM）は必要です。"\n    - paragraph [ref=s1e862]: "上の図中における「Reference Model」とは、「強化学習をする前」の方策モデル\n        \\\\pi_{ref}(y_t|x,y_{<t}) （つまり、DeepSeek-V3-Base）です。 また、「Policy\n        Model」というのは、「強化学習中」の現時点での方策モデル \\\\pi_\\\\phi(y_t|x,y_{<t}) を指しています。"\n    - paragraph [ref=s1e868]: そして、「プロンプト」「良い回答」「悪い回答」の3つを組とするデータセットで学習された、報酬モデル\n        r_\\theta(r|y,x) （Reward Model）が必要です。 報酬モデル r_\\theta(r|y,x) は「プロンプト x\n        」と「LLMの出力回答 y 」を入力することで、「報酬 r 」を出力します。\n    - paragraph [ref=s1e880]: この二つがあれば、著者が提案したGRPOという手法で、強化学習が可能になります。 （GRPOでは、状態価値モデルは不要です）\n    - paragraph [ref=s1e882]:\n        - text: しかしながら著者らは、さらに驚くべきことに、「DeepSeek-R1-Zero」を学習する際に、\n        - strong [ref=s1e884]: 報酬モデルを使わずに、ルールベースで報酬を計算しています。\n        - text: それにより、圧倒的な計算効率を達成しています。\n    - paragraph [ref=s1e886]: 著者らが設定しているルールは下記の二つです。\n    - list [ref=s1e887]:\n        - listitem [ref=s1e888]:\n            - strong [ref=s1e889]: 精度報酬\n            - list [ref=s1e890]:\n                - listitem [ref=s1e891]: 応答が正しいかどうかを評価する\n                - listitem [ref=s1e892]: 例えば、数学の問題であれば、最終的な回答が答えと合っているかどうかで報酬を渡す。\n        - listitem [ref=s1e893]:\n            - strong [ref=s1e894]: フォーマット報酬\n            - list [ref=s1e895]:\n                - listitem [ref=s1e896]:\n                    - text: 思考プロセスを\n                    - code [ref=s1e897]: <think>\n                    - text: と\n                    - code [ref=s1e898]: </think>\n                    - text: タグの間に置くことをモデルに強制し、合っていない場合はペナルティを課す。\n    - paragraph [ref=s1e899]:\n        - text: このたった2つの報酬のみをルールベースで設定しており、\n        - strong [ref=s1e901]: ニューラルネットワークベースの報酬モデルは利用していません\n    - paragraph [ref=s1e902]: ニューラルネットワークの報酬モデルを利用せずに、ルールベース報酬だけで強化学習をすることにより、報酬モデル分の計算量を削減しています。\n    - heading "この報酬設計で良い理由" [level=4] [ref=s1e903]\n    - paragraph [ref=s1e904]: フォーマット報酬は、モデルに出力形式を強制するものであるので、思考能力を向上させる直接的な報酬ではありません。\n        であれば、「DeepSeek-R1-Zero」の圧倒的な思考能力を支えているのは、「精度報酬」になります。\n    - paragraph [ref=s1e906]: 本論文での、強化学習の段階で利用しているデータセットは、おそらく数学データセットです。\n    - paragraph [ref=s1e907]:\n        - text: 数学の問題は、基本的に回答が一意に定まるため、ルールベースであっても出力が正しいかどうかは判断できます。\n        - strong [ref=s1e909]: そして、数学の問題に正答するためには、思考能力を強化する必要があります。\n    - complementary [ref=s1e910]:\n        - text: "!"\n        - paragraph [ref=s1e913]: ここが本論文の非常に面白い部分です。\n            DeepSeek-R1-Zeroを学習する際に、より長い長考をさせるようなデータセットによる学習や、思考部分に対して報酬を与えるような仕組みは「全く無く」、ただ、数学の問題の回答が合っているか間違っているかのゴール報酬のみが与えられています。\n        - paragraph [ref=s1e915]: それだけにも関わらず、自動的に数学の問題を正答するために、思考能力を強化するように学習が進んでおり、強化学習の威力を感じます。\n    - paragraph [ref=s1e916]: 強化学習の枠組みにおいて、最終的に報酬を得る（問題に正解する）ために、今、何のtokenを出力しないといけないのか、が最適化されます。\n        結果として、問題の正当に近づくようなtokenを出力する能力、すなわち問題をステップバイステップで解く能力が磨かれることになります。\n    - paragraph [ref=s1e918]: このように、数学という問題の答えを合わせるように報酬設計をすることで、勝手に思考能力が強化される、というのは非常に面白いなと思います。\n    - heading "（補足）データセットの作り方（の考察）" [level=4] [ref=s1e919]\n    - paragraph [ref=s1e920]:\n        - text: 本論文での、強化学習に利用するデータセットは、おそらく\n        - link "上述した論文" [ref=s1e921]:\n            - /url: https://arxiv.org/html/2402.03300v3\n        - text: の前半に記載されている数学データセットです。\n    - complementary [ref=s1e922]:\n        - text: "!"\n        - paragraph [ref=s1e925]: 強化学習に利用したデータセットに関しては、本論文で言及されていなかったと思うので憶測です。間違っていたら教えてください。\n    - paragraph [ref=s1e926]: 本論文の主要部分ではないですが、興味深いため紹介します。\n    - paragraph [ref=s1e927]:\n        - img [ref=s1e928]\n        - text: 引用：\n        - \'link "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models" [ref=s1e929]\':\n            - /url: https://arxiv.org/html/2402.03300v3\n    - paragraph [ref=s1e930]:\n        - strong [ref=s1e931]: 1. Train a FastText Model\n        - text: まず初めに、「OpenWebMath」という高品質の数学のウェブテキストのデータセットを初期のシードコーパスとして利用します。\n            このコーパスから50万のデータを正例、 Common\n            Crawlデータセット（大規模なWebページコーパス）から取得した50万のデータを負例として、fastTextモデルを学習します。\n            こうすることにより、fastTextモデルを利用して、Webページから、それが数学コンテンツか、非数学コンテンツかを分類することができます。\n    - paragraph [ref=s1e935]:\n        - strong [ref=s1e936]: 2. Recall Math-Related Webpages From Common Crawl\n    - paragraph [ref=s1e937]: 学習した、fastTextモデルを利用して、Common\n        Crawlデータセットから数学のWebページを取得します。 このとき、低品質な数学コンテンツを除外するために、 fastText\n        モデルによって予測されたスコアをもとにランク付けした、上位のスコアのページのみを取得します。\n    - paragraph [ref=s1e939]:\n        - strong [ref=s1e940]: 3. Discover Math-Related Domains\n    - paragraph [ref=s1e941]: この方法で取得されたWebページは、数学の問題ではありますが、偏りがあります。\n        なぜなら、初期の学習に利用された「OpenWebMath」データセットには、少数のデータしかなく、さらに上位のスコアのWebコーパス（類似度の高いWebコーパス）しか収集していないため、「OpenWebMath」データセットと似たような数学Webページしか、収集できていないからです。\n    - paragraph [ref=s1e943]: そこで、上記の方法で収集したWebページの親ドメインに着目します。\n        そして、親ドメイン内のWebページのうち、10%以上のWebページが、今回の方法で収集されている場合、親ドメイン自体が数学に関連するドメインであると、考えることができます。\n    - complementary [ref=s1e945]:\n        - text: "!"\n        - paragraph [ref=s1e948]: 例えば、「mathhoge.com」の配下のページの10%以上がfastText\n            モデルによって収集されていた場合は、「mathhoge.com」自体を数学コンテンツであると分類します。\n    - paragraph [ref=s1e949]: あとは、この親ドメイン内の、まだ収集されていないWebページから、高品質な数学Webページを収集することで、幅広い数学のコンテンツを収集できそうです。\n    - paragraph [ref=s1e950]:\n        - strong [ref=s1e951]: 4. Annotate Math-Related URL Path From Labelers\n    - paragraph [ref=s1e952]: 分類されたドメイン内の数学コンテンツに対して、「手動」でURLに注釈をつけ、注釈をつけたもののうち、またシードコーパスに収集されていないWebページを、シードコーパスに追加します。\n    - paragraph [ref=s1e953]: この方法を繰り返すことで、高品質かつ多様な数学データセットを収集することができます。\n        著者らはこのデータ収集を4回繰り返したのちに、3,550万の数学Webページ、合計1200億トークンのコーパスを完成させました。\n    - paragraph [ref=s1e955]: 4回の繰り返しの段階で、データのうち98%が3回目までの繰り返しですでに収集されていたことがわかったため、4回で繰り返し作業を終了したとのことです。\n    - heading "（補足）DPO（Direct Preference Optimization）との違い" [level=3]\n      [ref=s1e956]\n    - paragraph [ref=s1e957]: DPO（Direct Preference Optimization）という手法があります。\n        こちらは、強化学習を利用して、LLMに好ましい回答を学習させるRLHF（Reinforcement learning from human\n        feedback）と同様の効果を、強化学習を使わずに学習させることができる手法です。\n    - paragraph [ref=s1e959]: "強化学習をしないため、報酬モデルも状態価値モデルも必要なく、単純に「プロンプト x 」と「好ましい回答 y^+\n        」「好ましくない回答 y^- 」の3つが組となっているデータセット D を用意すれば、あとは \\\\pi_\\\\phi(y^+|x) 、\n        \\\\pi_\\\\phi(y^-|x) 、 \\\\pi_{ref}(y^+|x) 、 \\\\pi_{ref}(y^+|x)\n        を計算して、損失関数に落とし込むことで、学習が可能になります。"\n    - paragraph [ref=s1e976]: このように、「報酬モデルも状態価値モデルも必要ない」の部分が、本論文で提案している「GRPO（Group\n        Relative Policy Optimization）+ルールベース報酬」の手法の利点と近いです。\n    - paragraph [ref=s1e977]: ただし、DPOでは、上述した通り、「プロンプト x 」と「好ましい回答 y^+ 」「好ましくない回答 y^-\n        」の3つが組となっているデータセットが必要です。 逆に言えば、このデータセットの範囲内でしか学習ができません。\n    - paragraph [ref=s1e985]: 一方で、著者らが提案する「GRPO（Group Relative Policy\n        Optimization）+ルールベース報酬」の手法では、数学などの答えが一意に決まり、ルールベースで正解不正解を判断できるデータセット（つまり、数学データセット）であれば、「プロンプト\n        x 」と「回答 y 」の組だけで、学習が可能です。\n    - paragraph [ref=s1e990]:\n        - strong [ref=s1e991]: したがって、DPOよりもはるかに多くのデータを用いて学習ができます。\n        - text: これが、今回の提案手法の利点になりそうです。\n    - heading "著者らの執念" [level=3] [ref=s1e993]\n    - paragraph [ref=s1e994]: 著者らは、LLMを強化学習に適用する上で、計算量的に障害になる報酬モデルと状態価値モデルを利用しない斬新な手法を提案しました。\n        これにより、圧倒的な計算効率にて、強化学習を適用させました。\n    - paragraph [ref=s1e996]: また、似た効果を持つ手法としてDPOという手法があります。 ただし、DPOでは、利用できるデータセットが制限されるため、大規模な学習は困難です。\n    - paragraph [ref=s1e998]: しかしながら著者らの手法は、ただ数学のように一定の推論の結果、答えが一意に定まるような問題を解かせ続ければ、ルールベース報酬による強化学習で、思考機能を強化させることができてしまっています。\n    - paragraph [ref=s1e999]:\n        - strong [ref=s1e1000]: 絶対に、学習データの量を減らさずに、計算量を徹底的に削減してやろうという著者らの執念を感じるとともに、こういう工夫に積み重ねが、高性能と圧倒的に格安なAPI料金の両立を達成しているのだろうなと感じました。\n    - heading "ZeroのSystemプロンプト" [level=2] [ref=s1e1001]\n    - paragraph [ref=s1e1002]: 続いては、少し話が変わり、「DeepSeek-R1-Zero」に与えているsystemプロンプトについて紹介します。\n        このように、systemプロンプトが公開されているというのは、非常に勉強になりますね。とても嬉しいです\n    - paragraph [ref=s1e1004]: systemプロンプトは下記です。\n    - paragraph [ref=s1e1005]: "A conversation between User and Assistant. The user\n        asks a question, and the Assistant solves it. The assistant first thinks\n        about the reasoning process in the mind and then provides the user with\n        the answer. The reasoning process and answer are enclosed within <think>\n        </think> and <answer> </answer> tags, respectively, i.e., <think>\n        reasoning process here </think> <answer> answer here </answer>. User:\n        prompt. Assistant:"\n    - paragraph [ref=s1e1006]: 訳）\n        ユーザーとアシスタントの会話。ユーザーが質問し、アシスタントがそれを解決する。アシスタントは、まず頭の中で推論プロセスを考え、それからユーザに答えを提供します。推論過程と答えはそれぞれ<think>\n        </think>タグと<answer> </answer>タグで囲まれます。つまり、<think> 推論過程はここ</think>\n        <answer>答えはここ</answer>です。ユーザー：プロンプト アシスタント：\n    - paragraph [ref=s1e1008]: 上記部分の「prompt」の部分は、ユーザが入力したプロンプトの内容に書き換えられます。\n        このように、systemプロンプトにおいて<think> </think>タグを利用して、その中で思考をさせる形になっています。\n    - heading "Zeroの「Aha Moment」" [level=2] [ref=s1e1010]\n    - paragraph [ref=s1e1011]: 「Aha Moment」とは、辞書で調べると「〔問題の答えが突然ひらめいたときなどの〕なるほど！と思う瞬間」とのことです。\n    - paragraph [ref=s1e1012]: 実は、DeepSeek-R1-Zeroの出力において、それが発生したそうです。 数学の問題を解く途中で、下記のような出力が見られました。\n    - paragraph [ref=s1e1014]: "Wait, wait. Wait. That’s an aha moment I can flag\n        here. Let’s reevaluate this step-by-step to identify if the correct sum\n        can be · · · We started with the equation:"\n    - paragraph [ref=s1e1017]: 訳） 待て、待て。待ってくれ。これはハッとさせられる瞬間だ。 正しい和が - - - -\n        になるかどうかを確認するために、ステップ・バイ・ステップで再評価してみよう。\n    - paragraph [ref=s1e1020]: これは恐ろしいことです。\n        モデルは途中まで推論を実施しながら、途中で自身の出力結果を見直し、さらにもう一度ステップバイステップで検討をするという、高度な問題解決能力を獲得しています。\n        （自分の考えを見直して、さらに良い戦略を発見する。これができる人間がどれくらいいるのだろうか）\n    - paragraph [ref=s1e1023]: 何度も記載していますが、DeepSeek-R1-Zeroを学習する際に、問題の解き方を明示的にモデルに教えることや、思考部分を強化させるような報酬は与えていません。\n        ただ、数学の問題に正解することで得られる報酬を与えるだけで、モデルが自律的に高度な問題解決戦略を獲得しています。\n        （もちろん、推論の途中で、検算を行うなどのデータセットで学習もさせていません。もしかしたら、DeepSeek-V3-Baseを学習させるときのSFTデータセットには含まれているかもですが。）\n    - paragraph [ref=s1e1026]: これこそが、本論文で何度も述べている、「強化学習の可能性」です。\n        強化学習は（そのほかの学習手法とは異なり）（人間にとって）予想外で、かつ洗練された結果を導き、人工知能が新たなレベルの知性を手にいれるための核となる技術になりそうです。\n    - heading "DeepSeek-R1-Zeroのまとめ" [level=2] [ref=s1e1028]\n    - paragraph [ref=s1e1029]: DeepSeek-R1-ZeroはDeepSeek-V3-Baseに対して、「GRPO+ルールベース報酬」の強化学習「のみ」を適用し、圧倒的な思考能力を獲得したモデルになります。\n    - paragraph [ref=s1e1030]: DeepSeek-R1-Zeroを学習する際に、問題の解き方を明示的にモデルに教えることや、思考部分を強化させるような報酬は与えていません。\n        ただ、数学の問題に正解することで得られる報酬を与えるだけで、モデルが自律的に高度な問題解決戦略を獲得するという、とても興味深く、恐ろしい結果が示されています。\n    - heading "DeepSeek-R1への道" [level=1] [ref=s1e1032]\n    - paragraph [ref=s1e1033]: さて、ここまで、「DeepSeek-R1-Zero」について、たくさん語ってきましたが、とても魅力的でしたよね？\n        あくまで、「DeepSeek-R1-Zero」は「DeepSeek-V3-Base」に対して、強化学習のみを適用させて、強化学習の可能性を探ったモデルでした。\n    - paragraph [ref=s1e1035]:\n        - text: その結果、こんなにも魅力的なモデル（DeepSeek-R1-Zero）ができてしまったら、、、\n        - strong [ref=s1e1037]: SFTも入れて完全体のモデルを作りたくなりますよね？？？\n    - paragraph [ref=s1e1038]: というモチベーションで「DeepSeek-R1」は作られています。\n    - paragraph [ref=s1e1039]: もし、DeepSeek-R1-Zeroの性能がそんなに高くなかったら、今騒がれているDeepSeek-R1は作ろうという発想にすらならなかったと思います。\n    - heading "DeepSeek-R1-Zeroの課題" [level=2] [ref=s1e1040]\n    - paragraph [ref=s1e1041]: さて、非常に高性能で、示唆に富んだDeepSeek-R1-Zeroでしたが、課題はいくつかあります。 それは下記です。\n    - list [ref=s1e1043]:\n        - listitem [ref=s1e1044]: 学習初期は強化学習の学習が不安定で収束に時間がかかる（コールドスタート問題）\n        - listitem [ref=s1e1045]:\n            - text: 思考能力は高いが、言語が混在（日本語で話していたら急に中国語になるなど）し、人間が理解しにくい\n            - list [ref=s1e1046]:\n                - listitem [ref=s1e1047]: 結果、高度な思考タスクには強いが、（ロールプレイなどの）一般的なタスクには使えないという評価\n    - paragraph [ref=s1e1048]: そこで、上記の課題を解決する旅に出ます。\n    - heading "課題対処への道" [level=2] [ref=s1e1049]\n    - paragraph [ref=s1e1050]: DeepSeek-R1-Zeroでは、ベースモデルであるDeepSeek-V3-Baseに対して、そのまま上述した強化学習を用いて学習をしていました。\n    - paragraph [ref=s1e1051]:\n        - text: しかし、もともとDeepSeek-V3-Baseは長考モデルとして学習されたモデルではないため、いくらsystemプロンプトで長考をさせる制限（\n        - code [ref=s1e1052]: <think>\n        - text: タグ）を入れたとしても、強化学習の初期フェーズは不安定な学習になってしまいます。\n            （例えば、「そもそも適切な形式で思考をしない」、「思考形式は合っているが、意味のある思考をしていない」、その結果として「数学の問題になかなか正答できないまま、報酬が得られず、学習が不安定になる」が考えられます）\n    - paragraph [ref=s1e1054]: そこで、学習初期のコールドスタート問題を防ぐために、DeepSeek-V3-Baseに対して、SFT（Supervised\n        Fine-Tuning）を実施します。 そのために著者らは、少量のCoT（Chain-of-Thought）データセットを構築しています。\n    - heading "CoTデータセットの構築+SFT" [level=3] [ref=s1e1056]\n    - paragraph [ref=s1e1057]: データセットの構築方法は下記が一例として提示されております。\n    - list [ref=s1e1058]:\n        - listitem [ref=s1e1059]: いくつかの長いCoT解答を提示し、モデルに長いCoT形式の回答を生成させてデータセットに追加する\n        - listitem [ref=s1e1060]: モデルに対して、自己反省や検証を行いながら、詳細な回答生成を促すようなプロンプトを送信し、その出力をデータセットに追加する\n        - listitem [ref=s1e1061]: DeepSeek-R1-Zeroの出力を、読みやすい形式に整理してからデータセットに追加する\n        - listitem [ref=s1e1062]: モデルの出力結果を人間が精査して、より適切なCoTデータの形に整理してから、データセットに追加する\n    - paragraph [ref=s1e1063]: 上記の方法で、数千件のCoTデータを構築し、そのデータを用いて、DeepSeek-V3-BaseをSFTしています。\n    - heading "CoTデータセットを用いることの利点" [level=3] [ref=s1e1064]\n    - paragraph [ref=s1e1065]: 著者らは、下記の要素において、CoTデータセットを用いることに利点があると考えています。\n    - paragraph [ref=s1e1066]:\n        - strong [ref=s1e1067]: 読みやすさ\n        - text: DeepSeek-R1-Zeroの一番の課題は、その出力結果が人間が理解するのが難しいことです。\n            回答には、複数の言語が混在していたり、マークダウン形式などの読みやすい形式が適切に使われていないことで、重要な情報が何かわかりにくい状態でした。\n    - paragraph [ref=s1e1070]: そこで、今回のデータセットを作成する際には、各回答の末尾に「要約文」を入れるようにすることで、人間が読みやすい形に出力させるようにしています。\n        具体的には下記のような出力形式で、全てのCoTデータセットは作成されています。\n    - code [ref=s1e1075]: "|special_token|<reasoning_process>|special_token|<summary>"\n    - button "クリップボードにコピー" [ref=s1e1076]:\n        - img [ref=s1e1077]\n    - button "右端で折り返す" [ref=s1e1078]:\n        - img [ref=s1e1079]\n    - paragraph [ref=s1e1080]:\n        - code [ref=s1e1081]: <reasoning_process>\n        - text: の部分に、モデルの思考に該当する部分が、\n        - code [ref=s1e1082]: <summary>\n        - text: の部分に最終的な結論の要約が記載されます。\n    - paragraph [ref=s1e1083]: これにより、「DeepSeek-R1-Zero」にあった、重要な情報が何かわかりにくいという人間が理解するのが難しい問題を緩和します。\n        （言語の混在に関しての対処は後述します）\n    - paragraph [ref=s1e1085]:\n        - strong [ref=s1e1086]: ポテンシャル\n        - text: 今回、強化学習のみを実施するのではなく、人間の事前知識に基づいて作成したCoTデータセットでSFTを実施しています。\n            また、SFTと強化学習を複数回反復して、繰り返し学習をしています。\n            その結果、DeepSeek-R1はDeepSeek-R1-Zeroよりも良い結果を得ることができました。\n    - paragraph [ref=s1e1090]: 著者らは、このような反復的な学習こそが、長考モデル（reasoning models）を学習する上で良い方法であると主張しています。\n    - heading "推論思考の強化学習" [level=3] [ref=s1e1091]\n    - paragraph [ref=s1e1092]: CoTデータセットによりSFTしたDeepSeek-V3-Baseに対して、上述した（DeepSeek-R1-Zeroを学習した）強化学習手法を適用します。\n        これにより、モデルの思考能力が大幅に強化されます。\n    - paragraph [ref=s1e1094]: 一方で、「DeepSeek-R1-Zero」には、言語不一致の問題がありました。\n        そこで、DeepSeek-R1-Zeroのルールベース報酬に、「言語一貫性報酬」を追加で導入しています。\n    - paragraph [ref=s1e1096]: この報酬導入で、実は、モデルの思考能力が若干落ちてしまうことが実験的にわかっています。\n        しかしながら、この報酬があることで、ようやく人間が理解しやすい文章を生成してくれるようになります。\n    - heading "さらなる追加のSFT" [level=3] [ref=s1e1098]\n    - paragraph [ref=s1e1099]: 上記の強化学習が収束したら、さらにデータを追加してSFTをやり直します。\n        ここでは、初期段階の、CoTに重きを置いた、コールドスタートを解消するための学習データだけにとどまらず、ライティング、ロールプレイといった汎用タスクの性能を向上させるため、他ドメインからのデータも合わせて収集します。\n    - heading "Reasoning Data" [level=4] [ref=s1e1101]\n    - paragraph [ref=s1e1102]: まずは、前段階と同様に思考能力を向上させるために、CoT学習データ（Reasoning Data）を取得します。\n    - paragraph [ref=s1e1103]: 取得方法は以下です。\n    - list [ref=s1e1104]:\n        - listitem [ref=s1e1105]:\n            - text: 強化学習済みモデルの出力から、適切なものを収集して学習データに追加\n            - list [ref=s1e1106]:\n                - listitem [ref=s1e1107]: 前段の強化学習では、ルールベースの報酬モデルを利用していたので、それが使える問題（数学など）のみを学習データとしていたが、次の強化学習では報酬モデルを作成するため、もっと広い範囲でCoTができるデータも学習データに追加\n                - listitem [ref=s1e1108]: モデルの出力はまだ読みにくいところがあるため、さまざまフィルタリング\n    - paragraph [ref=s1e1109]: この結果、合計で600kのReasoning Data（学習データ）を収集しています。\n    - heading "Non-Reasoning Data" [level=4] [ref=s1e1110]\n    - paragraph [ref=s1e1111]: ライティングやQ＆A、翻訳などのNon-Reasoning\n        Dataについては、DeepSeek-V3のSFTデータセットの一部を再利用して、学習データを収集しています。 上記のReasoning\n        Data以外のタスク（Non-Reasoning\n        Data）に関しては、DeepSeek-V3によって簡易的なCoTを実施して、回答を生成し、学習データとします。\n        ただし、本当に簡単なタスク（あいさつとか）に関しては、CoTを学習データに追加しません。\n    - paragraph [ref=s1e1114]: こうすることで、合計200kのトレーニングサンプルを最終的に収集したそうです。\n    - paragraph [ref=s1e1115]: 上記の合計800kサンプルの学習データを利用して、DeepSeek-V3-Base（初回強化学習済み）を2epoch分SFTを実施しています。\n    - heading "最後の強化学習" [level=3] [ref=s1e1116]\n    - paragraph [ref=s1e1117]: 最後に、モデルの思考能力をさらに磨くと同時に、モデルの無害性などを向上させることを目的に、強化学習を実施します。\n    - paragraph [ref=s1e1118]: 思考タスクの強化学習に関しては、これまで説明したのと同様に、数学、コード、論理的推論の領域でルールベース報酬を利用して強化学習をします。\n        一方で、思考タスク以外の一般的なデータに対しては、ルールベースで回答の良し悪しを判断することはできないため、報酬モデルを別途DeepSeek-V3パイプラインをベースに作成しています。\n    - heading "DeepSeek-R1のまとめ" [level=2] [ref=s1e1120]\n    - paragraph [ref=s1e1121]: 以上より、DeepSeek-R1では、DeepSeek-V3-Baseに対して、SFTと強化学習を2回実施することにより構築されています。\n    - paragraph [ref=s1e1122]: SFTにより、強化学習のコールドスタート問題の解消と、モデル出力のユーザフレンドリー性の向上、一般的なタスクに対する性能向上に貢献しています。\n    - heading "結果" [level=1] [ref=s1e1123]\n    - paragraph [ref=s1e1124]: 結果に関しては、他の方がさまざま検証していたり、話題になっているので詳しくは書きません。\n        ただ、DeepSeek-R1は、現在のオープンソースモデルの中では、最も高い性能と言ってよく、また、Open\n        AIのo1モデルにも匹敵する性能だと言われています。\n    - heading "蒸留" [level=1] [ref=s1e1126]\n    - paragraph [ref=s1e1127]: 最後に、蒸留モデルについての解説します。（論文でも言及されています）\n        DeepSeek-R1は671Bものモデルパラメータを持つため、一般的な家庭用PCで動作させるのは難しいです。\n    - paragraph [ref=s1e1129]: そこで、より小さなモデルパラメータのモデルに、DeepSeek-R1のような思考能力を与えるために、蒸留を実施しています。\n        （公式が、個人の利用者のことも考えて、家庭用PCで動作する思考モデルを開発してくれるのは非常にありがたいですね）\n    - paragraph [ref=s1e1131]: 具体的には、Qwen2.5-Math-1.5B、Qwen2.5-Math-7B、Qwen2.5-14B、Qwen2.5-32B、Llama-3.1-8B、Llama-3.3-70B-Instructに対して、DeepSeek-R1から取得した800kのトレーニングサンプルを利用して、直接FineTuningする形で、蒸留を実施しています。\n    - paragraph [ref=s1e1132]: 蒸留モデルに対しては、SFTのみを実施し、後段の強化学習は実施しておらず、蒸留モデルに対する追加の強化学習の探究は、著者らが実施する予定はないそうです。\n        （著者らは、強化学習を追加で実施することにより性能が上がると思っているが、その探究は、別のコミュニティに任せると記述しています）\n    - heading "蒸留 vs 強化学習" [level=2] [ref=s1e1134]\n    - paragraph [ref=s1e1135]: 蒸留後の強化学習に関しては、著者らは実験していない、と記述していました。\n        では、小型モデルに対して、蒸留を実施する前に、強化学習を実施したらどうなるのでしょうか？\n        DeepSeek-V3-Baseのように、大幅に思考能力が強化されて、性能向上が期待できるのでしょうか？\n    - paragraph [ref=s1e1138]: それに関しては、著者らが実験してくれています。\n    - paragraph [ref=s1e1139]: ここでは、「Qwen-32B-Base」モデルを利用して、10kステップ以上の大規模強化学習を実施し、「DeepSeek-R1-Zero-Qwen-32B」を構築しました。\n        一方で、「Qwen-32B-Base」モデルに対して、蒸留を実施したモデルを、「DeepSeek-R1Distill-Qwen-32B」モデルとして、この二つのモデルを比較します。\n    - paragraph [ref=s1e1141]: その結果を比較すると、小型モデルに対する強化学習を適用した「DeepSeek-R1-Zero-Qwen-32B」よりも、小型モデルに蒸留を適用した「DeepSeek-R1Distill-Qwen-32B」モデルの方がはるかに高い性能でした。\n    - paragraph [ref=s1e1142]:\n        - strong [ref=s1e1143]: 以下は私の主観ですが、\n        - text: 以上の結果から、\n        - strong [ref=s1e1146]: 強化学習による思考能力の向上は、安易に小型モデルに試していい戦略でない\n        - text: ということが示唆されていると思います。\n    - paragraph [ref=s1e1148]: 強化学習による思考能力の大幅な強化は、おそらくですが「DeepSeek-V3」という元々大規模かつ性能の高いモデルであったからこそ、達成されたのだと思います。\n    - paragraph [ref=s1e1149]: DeepSeek-R1-Zeroの課題でも説明した通り、今回のルールベース報酬では、正しい数学の解答を生成できないと永遠に報酬が獲得できません。\n        小型モデルでは、このコールドスタートの問題が、より致命的なレベルで発生するのだと思います。\n    - paragraph [ref=s1e1151]: したがって、思考モデルを作りたいからといって、小型モデルに対して、安易にルールベース強化学習を試すのはおすすめしません。\n        しかしながら、コールドスタート問題を解決できる戦略があるのであれば、有効な手段になると思われます。\n        （例えば、DeepSeek-R1で蒸留済みの小型モデルを利用する、最初から数学に強い小型モデルで強化学習を試すなどです。重要なのは強化学習のコールドスタートを解消できるかどうかだと思われます。）\n    - paragraph [ref=s1e1154]:\n        - text: （追記） また、同様の実験を小型モデルに対して試したグループもいるようです。 そちらの実験レポートが下記になります。\n        - iframe [ref=s1e1158]: <could not take iframe snapshot>\n        - text: （DeepSeek-R1-Zeroと同様に、「Aha Moment」が見られることもわかっており、非常に興味深いです）\n    - paragraph [ref=s1e1159]: ただ、こちらのレポートにおいても、ベースモデルとしては数学に特化したモデルを利用しています。\n        このことからも、ある程度の数学能力は必要なのだなと思います。\n    - heading "今後の展望" [level=1] [ref=s1e1161]\n    - paragraph [ref=s1e1162]: DeepSeek-R1は、さまざまなタスクにおいて、OpenAI\n        o1に匹敵する性能を達成した、驚異的なオープンソースモデルですが、苦手としている部分もあります。 それが今後の展望に記載されています。\n        （今後の展望を見れば、DeepSeek-R1が苦手な部分を知ることができます）\n    - paragraph [ref=s1e1165]: それは以下の項目です。\n    - list [ref=s1e1166]:\n        - listitem [ref=s1e1167]:\n            - strong [ref=s1e1168]: 一般的な能力\n            - list [ref=s1e1169]:\n                - listitem [ref=s1e1170]: 関数呼び出しやマルチターン対話、複雑なロールプレイング。、json出力において、DeepSeek-V3に及ばない\n        - listitem [ref=s1e1171]:\n            - strong [ref=s1e1172]: 言語の混在\n            - list [ref=s1e1173]:\n                - listitem [ref=s1e1174]: 現在、DeepSeek-R1は英語と中国語に最適化されているため、そのほかの言語を利用した場合、言語の混在が発生する可能性がある\n        - listitem [ref=s1e1175]:\n            - strong [ref=s1e1176]: プロンプトエンジニアリング\n            - list [ref=s1e1177]:\n                - listitem [ref=s1e1178]: DeepSeek-R1はプロンプトに非常に敏感で、特にFewーshotプロンプトを利用すると、パフォーマンスが低下する。\n                - listitem [ref=s1e1179]: 基本的には、問題を直接シンプルに記述し、例を提示しない（Zero-shotプロンプト）を利用することを推奨している\n        - listitem [ref=s1e1180]:\n            - strong [ref=s1e1181]: ソフトウェアエンジニアリングタスク\n            - list [ref=s1e1182]:\n                - listitem [ref=s1e1183]: コード生成やコード保管などのソフトウェアエンジニアリングタスクは、評価に時間がかかるため、強化学習の効率が悪くなることから、現時点では強化学習において、このようなタスクは学習していない。\n                    大々的には学習しておらず、一部のテスト可能な範囲でしか学習できていない。\n                - listitem [ref=s1e1185]: 結果として、このタスクの性能は、DeepSeek-V3からほとんど改善していない。\n    - complementary [ref=s1e1186]:\n        - text: "!"\n        - paragraph [ref=s1e1189]: （追記） masaishiさんのコメントに基づき修正させていただきました。 私の読み間違いで誤解を与えてしまい申し訳ございません。\n        - blockquote [ref=s1e1192]:\n            - paragraph [ref=s1e1193]: 2.2.2. Reward Modeling ・・・ ・Accuracy\n                rewards:・・・Similarly, for LeetCode problems, a compiler can be\n                used to generate feedback based on predefined test cases.\n        - paragraph [ref=s1e1196]: と記載のあるように、全くコード生成などを学習していないわけではなく、一部の、簡単にテストができる問題のみ学習しているようです。\n            ただ、そういう簡易的なタスクだけしか学習できていないため、性能向上は小さめということだと思われます。\n    - paragraph [ref=s1e1198]: 上記の内容が、現時点での「DeepSeek-R1」の課題です。\n        ですが、今後のアップデートで改善していくとも記載されているため、現時点で不満があったとしても、上記に記載されている項目であれば、今後改善する可能性が高いです。\n    - heading "まとめ" [level=1] [ref=s1e1200]\n    - paragraph [ref=s1e1201]: ここまで読んでくださった方、本当にお疲れ様でした。\n        （これでも説明はだいぶ短くした方なんです・・・本当は、もっと強化学習の部分とかを数式をもとに説明したかったです）\n    - paragraph [ref=s1e1203]: DeepSeek-R1の論文いかがでしたか？\n        私にとっては、非常に勉強になるような示唆や知見に溢れており、こんな論文を公開してくれて、かつオープンソースのモデルとしても公開してくれる「DeepSeek-AI」には感謝しかありません。\n    - paragraph [ref=s1e1205]: 本記事が、皆様の理解の助けになり、一人でも多くの方が原論文に興味を持って、読んでくれましたら幸いです。\n        ここまで読んでくださってありがとうございました！\n    - heading "参考文献" [level=1] [ref=s1e1207]\n    - paragraph [ref=s1e1208]: 本論文は、論文の中では読みやすい方だとは思いますが、強化学習に関しては、一定程度の基礎知識は要求されます。\n        そこで、本論文を読む上で必要な知識を収集できるおすすめの書籍に関して記載します。 （少なくとも私はこの書籍で学習しました）\n    - paragraph [ref=s1e1211]: （書籍のリンクはamazonアフィリエイトリンクになります）\n    - heading "強化学習" [level=4] [ref=s1e1212]\n    - paragraph [ref=s1e1213]: 強化学習に関しておすすめの本は、下記の本たちです。\n    - paragraph [ref=s1e1214]:\n        - link "ゼロから作るDeep Learning ❹ ―強化学習編" [ref=s1e1215]:\n            - /url: https://amzn.to/40qFuNg\n        - text: まずは、定番の本です。\n            強化学習に関して、本当に最初の部分（マルコフ決定過程とは何か）から、非常に詳細かつわかりやすく解説されており、深層学習を利用して、どのように強化学習をすることできるのかまで、しっかり書かれているためおすすめです。\n            また、コードと理論のバランスもちょうどよく、理系大学生とかであれば問題なく読み進められると思います。\n            特に、強化学習の基礎の部分が、詳細かつ分かりやすく書かれているため、一冊目におすすめです。\n    - paragraph [ref=s1e1220]:\n        - link "機械学習スタートアップシリーズ Ｐｙｔｈｏｎで学ぶ強化学習 ［改訂第２版］ 入門から実践まで (ＫＳ情報科学専門書)" [ref=s1e1221]:\n            - /url: https://amzn.to/4joKbjg\n    - paragraph [ref=s1e1223]: 普段からpythonを使っており、数式よりも実装に興味がある方にとっては、こちらの書籍もおすすめです。\n        こちらの書籍は、強化学習の基礎よりは、少し応用というか、実用を意識した書籍になっており、実務でよく使われるモデルなどに関しても、詳しく説明されています。\n        強化学習の基礎はわかっている人や、もっと実装をベースに学びたいというかたにおすすめです。\n    - paragraph [ref=s1e1226]:\n        - link "強化学習 (機械学習プロフェッショナルシリーズ)" [ref=s1e1227]:\n            - /url: https://amzn.to/40NX5QM\n        - link "強化学習から信頼できる意思決定へ (AI/データサイエンスライブラリ“基礎から応用へ” 5)" [ref=s1e1229]:\n            - /url: https://amzn.to/4jtFq8d\n    - paragraph [ref=s1e1230]: 名著です。\n        どちらも、理論説明の部分が非常に面白く、正直難易度は高いですが、強化学習の理論をちゃんと理解したいという方におすすめの書籍です。\n        下の本に関しては、2024年の本ということもあり、近年の強化学習の実世界での応用と、理論的な定式化の架け橋になっているような本になっており、非常に面白いです。\n    - heading "AlphaGO Zero" [level=4] [ref=s1e1233]\n    - paragraph [ref=s1e1234]:\n        - link "AlphaZero 深層学習・強化学習・探索 人工知能プログラミング実践入門" [ref=s1e1235]:\n            - /url: https://amzn.to/3Cdnuy1\n    - paragraph [ref=s1e1236]: だいぶ昔に購入させていただいた本ですが、まさか2025年に活躍することがあろうとは。\n        という感じですが、こちらの書籍も参考にさせていただきました。 Alpha\n        Zeroを作る上で核となる技術を余すところなく解説されており、私は非常に勉強になりました。 Alpha\n        Zeroに興味がある方は、ぜひ一度ご一読ください。\n    - heading "LLM、RLHF, DPO" [level=4] [ref=s1e1240]\n    - paragraph [ref=s1e1241]:\n        - link "大規模言語モデル入門Ⅱ〜生成型LLMの実装と評価" [ref=s1e1242]:\n            - /url: https://amzn.to/3Cs3Uhn\n        - text: よく紹介させていただいておりますが、こちらの書籍は、LLMのファインチューニングから、RLHF、RAG、分散学習にかけて、本当に幅広く解説されており、いつも参考にさせていただいております。\n            今回のDPOやRLHFの解説部分でも、とても活躍してくれました。\n            LLMを取り扱っている方は、とりあえず買っておいても損はないと思います。\n            さまざまな章、ページで読めば読むほど新しい発見が得られる、スルメのような本だなといつも思っております。\n    - button "いいね" [ref=s1e1249]:\n        - img [ref=s1e1250]\n    - text: "921"\n    - button "ブックマーク" [ref=s1e1266]:\n        - img [ref=s1e1267]\n    - text: "344"\n    - button "その他の操作" [ref=s1e1272]:\n        - img [ref=s1e1273]\n    - tooltip "X(Twitter)にポスト" [ref=s1e1276]:\n        - img [ref=s1e1277]\n        - text: X(Twitter)にポスト\n    - tooltip "Facebookに投稿" [ref=s1e1283]:\n        - img [ref=s1e1284]\n        - text: Facebookに投稿\n    - tooltip "はてなブックマークに登録" [ref=s1e1290]:\n        - img [ref=s1e1291]\n        - text: はてなブックマークに登録\n    - complementary [ref=s1e1294]:\n        - link "asap" [ref=s1e1296]:\n            - /url: /asap\n            - img "asap" [ref=s1e1297]\n        - link "asap" [ref=s1e1299]:\n            - /url: /asap\n        - paragraph [ref=s1e1301]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n        - button "フォロー" [ref=s1e1305]\n        - tooltip "@personabb" [ref=s1e1306]:\n            - img "GitHub" [ref=s1e1307]\n            - text: "@personabb"\n        - tooltip "@asap2650" [ref=s1e1309]:\n            - img "X(Twitter)" [ref=s1e1310]\n            - text: "@asap2650"\n        - tooltip "yard.tips" [ref=s1e1316]:\n            - img "リンク" [ref=s1e1317]\n            - text: yard.tips\n    - complementary [ref=s1e1320]:\n        - text: バッジを贈って著者を応援しよう\n        - paragraph [ref=s1e1324]: バッジを受け取った著者にはZennから現金やAmazonギフトカードが還元されます。\n        - button "バッジを贈る" [ref=s1e1326]\n        - img [ref=s1e1327]\n    - text: この記事に贈られたバッジ\n    - img "Thank you" [ref=s1e1338]\n    - img [ref=s1e1340]\n    - img "参考になった" [ref=s1e1352]\n    - img [ref=s1e1354]\n    - img "もっと読みたい" [ref=s1e1363]\n    - img [ref=s1e1365]\n    - img "参考になった" [ref=s1e1375]\n    - img [ref=s1e1377]\n    - img "もっと読みたい" [ref=s1e1386]\n    - img [ref=s1e1388]\n    - heading "Discussion" [level=3] [ref=s1e1400]\n    - article [ref=s1e1403]:\n        - link "はまなすなぎさ" [ref=s1e1405]:\n            - /url: /nhamanasu\n            - img "はまなすなぎさ" [ref=s1e1406]\n        - link "はまなすなぎさ" [ref=s1e1407]:\n            - /url: /nhamanasu\n        - link "2025年1月28日 8時33分" [ref=s1e1408]:\n            - /url: "#comment-f230c9d295b3f5"\n            - tooltip "2025年1月28日 8時33分" [ref=s1e1409]: 2ヶ月前 2025年1月28日 8時33分\n        - button "メニューを開く" [ref=s1e1411]:\n            - img [ref=s1e1412]\n        - paragraph [ref=s1e1417]: 非常にためになる骨太な記事、ありがとうございます！とても勉強になります。\n        - paragraph [ref=s1e1418]: ちょっとした誤字かも？の箇所なのですが、冒頭の「強化学習手法「DRPO」すごい！」の部分は「GRPO」ではないかなと思いました。ご報告まで。\n        - button "いいね" [ref=s1e1421]:\n            - img [ref=s1e1422]\n        - text: "1"\n        - button "コメント 1" [ref=s1e1437]:\n            - img "コメント" [ref=s1e1438]\n            - text: "1"\n        - article [ref=s1e1442]:\n            - link "asap" [ref=s1e1444]:\n                - /url: /asap\n                - img "asap" [ref=s1e1445]\n            - link "asap" [ref=s1e1446]:\n                - /url: /asap\n            - link "2025年1月28日 10時23分" [ref=s1e1447]:\n                - /url: "#comment-41cdebd92d8d44"\n                - tooltip "2025年1月28日 10時23分" [ref=s1e1448]: 2ヶ月前 2025年1月28日 10時23分\n            - button "メニューを開く" [ref=s1e1450]:\n                - img [ref=s1e1451]\n            - paragraph [ref=s1e1456]: 読んでくださりありがとうございます！ おっしゃる通りです！修正させていただきました！\n            - button "いいね" [ref=s1e1460]:\n                - img [ref=s1e1461]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1477]\n    - article [ref=s1e1479]:\n        - link "bang" [ref=s1e1481]:\n            - /url: /zenmnm\n            - img "bang" [ref=s1e1482]\n        - link "bang" [ref=s1e1483]:\n            - /url: /zenmnm\n        - link "2025年1月28日 13時46分" [ref=s1e1484]:\n            - /url: "#comment-d0aac9375fbbf7"\n            - tooltip "2025年1月28日 13時46分" [ref=s1e1485]: 2ヶ月前 2025年1月28日 13時46分\n        - button "メニューを開く" [ref=s1e1487]:\n            - img [ref=s1e1488]\n        - paragraph [ref=s1e1493]: AIについてはまだ詳しくありませんが、興味はあります。同じように古いモデルを使って新しいモデルを作るのですが、蒸留方法の代わりにモデルを組み合わせることは可能でしょうか?sakana\n            aiのようにです\n        - button "いいね" [ref=s1e1496]:\n            - img [ref=s1e1497]\n        - text: "1"\n        - button "コメント 2" [ref=s1e1512]:\n            - img "コメント" [ref=s1e1513]\n            - text: "2"\n        - article [ref=s1e1517]:\n            - link "asap" [ref=s1e1519]:\n                - /url: /asap\n                - img "asap" [ref=s1e1520]\n            - link "asap" [ref=s1e1521]:\n                - /url: /asap\n            - link "2025年1月28日 15時33分" [ref=s1e1522]:\n                - /url: "#comment-27e79c6ee51f2a"\n                - tooltip "2025年1月28日 15時33分" [ref=s1e1523]: 2ヶ月前 2025年1月28日 15時33分\n            - button "メニューを開く" [ref=s1e1525]:\n                - img [ref=s1e1526]\n            - paragraph [ref=s1e1531]: モデルマージに関してはあまり詳しくないですが、基本的には同じアーキテクチャのモデルを利用することになると思います。したがってDeepSeek-R1によって蒸留されたLlamaモデルとかであれば、他のLlamaモデル（同じモデルアーキテクチャ）とモデルマージすることで、推論機能を付け加えたり、日本語機能を強化したりすることが期待できると思います。\n            - button "いいね" [ref=s1e1534]:\n                - img [ref=s1e1535]\n        - article [ref=s1e1549]:\n            - link "bang" [ref=s1e1551]:\n                - /url: /zenmnm\n                - img "bang" [ref=s1e1552]\n            - link "bang" [ref=s1e1553]:\n                - /url: /zenmnm\n            - link "2025年1月28日 16時42分" [ref=s1e1554]:\n                - /url: "#comment-0221309240798f"\n                - tooltip "2025年1月28日 16時42分" [ref=s1e1555]: 2ヶ月前 2025年1月28日 16時42分\n            - button "メニューを開く" [ref=s1e1557]:\n                - img [ref=s1e1558]\n            - paragraph [ref=s1e1563]: お返事ありがとうございます。私はまだ初心者なので、これは私の軽い考えです。今では研究のプロセスごとにいくつかの方法があり、それらを組み合わせたり微調整したりすれば、たくさんのモデルができるようになっているでしょう。\n            - button "いいね" [ref=s1e1566]:\n                - img [ref=s1e1567]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1583]\n    - article [ref=s1e1585]:\n        - link "HWSeungwoo" [ref=s1e1587]:\n            - /url: /syoyuu96\n            - img "HWSeungwoo" [ref=s1e1588]\n        - link "HWSeungwoo" [ref=s1e1589]:\n            - /url: /syoyuu96\n        - link "2025年1月28日 19時28分" [ref=s1e1590]:\n            - /url: "#comment-a07230ddbdaebe"\n            - tooltip "2025年1月28日 19時28分" [ref=s1e1591]: 2ヶ月前 2025年1月28日 19時28分\n        - button "メニューを開く" [ref=s1e1593]:\n            - img [ref=s1e1594]\n        - paragraph [ref=s1e1599]: Deep\n            Seekに関する記事やニュースはたくさん目にしますが、ここまで深掘りされた内容には驚きました。とても勉強になりました。ありがとうございます！\n        - button "いいね" [ref=s1e1602]:\n            - img [ref=s1e1603]\n        - text: "1"\n        - button "コメント 1" [ref=s1e1618]:\n            - img "コメント" [ref=s1e1619]\n            - text: "1"\n        - article [ref=s1e1623]:\n            - link "asap" [ref=s1e1625]:\n                - /url: /asap\n                - img "asap" [ref=s1e1626]\n            - link "asap" [ref=s1e1627]:\n                - /url: /asap\n            - link "2025年1月28日 19時32分" [ref=s1e1628]:\n                - /url: "#comment-1e1e3abc86df00"\n                - tooltip "2025年1月28日 19時32分" [ref=s1e1629]: 2ヶ月前 2025年1月28日 19時32分\n            - button "メニューを開く" [ref=s1e1631]:\n                - img [ref=s1e1632]\n            - paragraph [ref=s1e1637]: 読んでくださってありがとうございます！ コメント嬉しいです！ありがとうございます！\n            - button "いいね" [ref=s1e1641]:\n                - img [ref=s1e1642]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1658]\n    - article [ref=s1e1660]:\n        - link "masaishi" [ref=s1e1662]:\n            - /url: /masaishi\n            - img "masaishi" [ref=s1e1663]\n        - link "masaishi" [ref=s1e1664]:\n            - /url: /masaishi\n        - link "2025年1月29日 16時14分" [ref=s1e1665]:\n            - /url: "#comment-a0d75bd96cadfc"\n            - tooltip "2025年1月29日 16時14分" [ref=s1e1666]: 2ヶ月前 2025年1月29日 16時14分\n        - button "メニューを開く" [ref=s1e1668]:\n            - img [ref=s1e1669]\n        - paragraph [ref=s1e1674]: 素晴らしい記事をありがとうございます！\n            GRPOなどの強化学習手法に関する解説も大変勉強になりました。この記事をきっかけに、DeepSeek-R1の論文にも興味を持ち、読んでみようと思えました。\n        - paragraph [ref=s1e1676]: 一点だけ気になったことがあり、もし私の理解が誤っていたら申し訳ありません。\n            "コード生成やコード保管などのソフトウェアエンジニアリングタスク"の強化学習に用いられていないとありますが、論文の\n            2.2.2（Reward Modeling）の\n        - blockquote [ref=s1e1678]:\n            - paragraph [ref=s1e1679]: For LeetCode problems, a compiler can be used to\n                generate feedback based on predefined test cases.\n        - paragraph [ref=s1e1680]: という記述を見ると、少なくともアルゴリズム問題のコード生成には強化学習が用いられているのかなと思います。\n        - paragraph [ref=s1e1681]: ソフトウェアエンジニアリングタスクは強化学習していませんが、コード生成自体は行っているような気がするので、注釈などで少し補足していただけると、読者としては誤解が減って助かるかもしれません。しかし、かなり細かい点ですので、最終的には著者さまの判断にお任せいたします。\n        - button "いいね" [ref=s1e1684]:\n            - img [ref=s1e1685]\n        - text: "2"\n        - button "コメント 2" [ref=s1e1700]:\n            - img "コメント" [ref=s1e1701]\n            - text: "2"\n        - article [ref=s1e1705]:\n            - link "asap" [ref=s1e1707]:\n                - /url: /asap\n                - img "asap" [ref=s1e1708]\n            - link "asap" [ref=s1e1709]:\n                - /url: /asap\n            - link "2025年1月29日 16時37分" [ref=s1e1710]:\n                - /url: "#comment-afb254c15106b7"\n                - tooltip "2025年1月29日 16時37分" [ref=s1e1711]: 2ヶ月前 2025年1月29日 16時37分\n            - button "メニューを開く" [ref=s1e1713]:\n                - img [ref=s1e1714]\n            - paragraph [ref=s1e1719]: コメントありがとうございます！ おっしゃる通りでした！読み間違いです。ご指摘助かります。\n            - blockquote [ref=s1e1721]:\n                - paragraph [ref=s1e1722]: Due to the long evaluation times, which impact the\n                    efficiency of the RL process, large-scale RL has not been\n                    applied extensively in software engineering tasks.\n            - paragraph [ref=s1e1723]: この部分の文章をもとに記載していましたが、大々的には適用されていないだけで、一部は学習に入っているっぽいですね。\n            - paragraph [ref=s1e1724]: 助かります！修正もこれからさせていただきます\n            - button "いいね" [ref=s1e1727]:\n                - img [ref=s1e1728]\n            - text: "2"\n        - article [ref=s1e1743]:\n            - link "masaishi" [ref=s1e1745]:\n                - /url: /masaishi\n                - img "masaishi" [ref=s1e1746]\n            - link "masaishi" [ref=s1e1747]:\n                - /url: /masaishi\n            - link "2025年1月29日 19時5分" [ref=s1e1748]:\n                - /url: "#comment-debff7de1aad74"\n                - tooltip "2025年1月29日 19時5分" [ref=s1e1749]: 2ヶ月前 2025年1月29日 19時5分\n            - button "メニューを開く" [ref=s1e1751]:\n                - img [ref=s1e1752]\n            - paragraph [ref=s1e1757]: ご丁寧にありがとうございます！\n            - button "いいね" [ref=s1e1760]:\n                - img [ref=s1e1761]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1777]\n    - article [ref=s1e1779]:\n        - link "RyotoTanaka" [ref=s1e1781]:\n            - /url: /ryototanaka\n            - img "RyotoTanaka" [ref=s1e1782]\n        - link "RyotoTanaka" [ref=s1e1783]:\n            - /url: /ryototanaka\n        - link "2025年1月30日 8時0分" [ref=s1e1784]:\n            - /url: "#comment-4ae639fe8d1892"\n            - tooltip "2025年1月30日 8時0分" [ref=s1e1785]: 2ヶ月前 2025年1月30日 8時0分\n        - button "メニューを開く" [ref=s1e1787]:\n            - img [ref=s1e1788]\n        - paragraph [ref=s1e1793]: 読み応えがありました。素晴らしい記事をありがとうございます！ 些細な事なのですが、DPOは Direct\n            Preference Optimization ではないでしょうか。\n        - button "いいね" [ref=s1e1797]:\n            - img [ref=s1e1798]\n        - text: "1"\n        - button "コメント 1" [ref=s1e1813]:\n            - img "コメント" [ref=s1e1814]\n            - text: "1"\n        - article [ref=s1e1818]:\n            - link "asap" [ref=s1e1820]:\n                - /url: /asap\n                - img "asap" [ref=s1e1821]\n            - link "asap" [ref=s1e1822]:\n                - /url: /asap\n            - link "2025年1月30日 13時15分" [ref=s1e1823]:\n                - /url: "#comment-ec7e829404f7bc"\n                - tooltip "2025年1月30日 13時15分" [ref=s1e1824]: 2ヶ月前 2025年1月30日 13時15分\n            - button "メニューを開く" [ref=s1e1826]:\n                - img [ref=s1e1827]\n            - paragraph [ref=s1e1832]: ほんとですね！間違えて入力してました ありがとうございます！\n            - button "いいね" [ref=s1e1836]:\n                - img [ref=s1e1837]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1853]\n    - text: ログインするとコメントできます\n    - button "Login" [ref=s1e1857]\n    - complementary [ref=s1e1858]:\n        - link "asap" [ref=s1e1863]:\n            - /url: /asap\n            - img "asap" [ref=s1e1864]\n        - link "asap" [ref=s1e1866]:\n            - /url: /asap\n        - button "フォロー" [ref=s1e1868]\n        - link "@personabb" [ref=s1e1869]:\n            - /url: https://github.com/personabb\n            - tooltip "@personabb" [ref=s1e1870]:\n                - img "GitHub" [ref=s1e1871]\n                - text: "@personabb"\n        - link "@asap2650" [ref=s1e1873]:\n            - /url: https://twitter.com/asap2650\n            - tooltip "@asap2650" [ref=s1e1874]:\n                - img "X(Twitter)" [ref=s1e1875]\n                - text: "@asap2650"\n        - link "yard.tips" [ref=s1e1881]:\n            - /url: https://yard.tips/asap2650\n            - tooltip "yard.tips" [ref=s1e1882]:\n                - img "リンク" [ref=s1e1883]\n                - text: yard.tips\n        - paragraph [ref=s1e1886]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n        - button "バッジを贈る" [ref=s1e1889]\n        - link "バッジを贈るとは" [ref=s1e1891]:\n            - /url: /faq#badges\n            - text: バッジを贈るとは\n            - img [ref=s1e1892]\n        - text: 目次\n        - list [ref=s1e1901]:\n            - listitem [ref=s1e1902]:\n                - link "はじめに" [ref=s1e1903]:\n                    - /url: "#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB"\n            - listitem [ref=s1e1904]:\n                - link "本論文の興味深いところ" [ref=s1e1905]:\n                    - /url: "#%E6%9C%AC%E8%AB%96%E6%96%87%E3%81%AE%E8%88%88%E5%91%B3%E6%B7%B1%E3%81%8\\\n                        4%E3%81%A8%E3%81%93%E3%82%8D"\n            - listitem [ref=s1e1906]:\n                - link "本論文の流れ" [ref=s1e1907]:\n                    - /url: "#%E6%9C%AC%E8%AB%96%E6%96%87%E3%81%AE%E6%B5%81%E3%82%8C"\n                - list [ref=s1e1908]:\n                    - listitem [ref=s1e1909]:\n                        - link "前提のモデル構造" [ref=s1e1910]:\n                            - /url: "#%E5%89%8D%E6%8F%90%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E6%A7%8B%E9%80%A\\\n                                0"\n                    - listitem [ref=s1e1911]:\n                        - link "DeepSeek-R1-Zeroを強化学習を用いて作る" [ref=s1e1912]:\n                            - /url: "#deepseek-r1-zero%E3%82%92%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E3%82%92%\\\n                                E7%94%A8%E3%81%84%E3%81%A6%E4%BD%9C%E3%82%8B"\n                    - listitem [ref=s1e1913]:\n                        - link "DeepSeek-R1をSFT+強化学習を用いて作る" [ref=s1e1914]:\n                            - /url: "#deepseek-r1%E3%82%92sft%2B%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E3%82%92\\\n                                %E7%94%A8%E3%81%84%E3%81%A6%E4%BD%9C%E3%82%8B"\n                    - listitem [ref=s1e1915]:\n                        - link "小型モデルへの蒸留を試す" [ref=s1e1916]:\n                            - /url: "#%E5%B0%8F%E5%9E%8B%E3%83%A2%E3%83%87%E3%83%AB%E3%81%B8%E3%81%AE%E8%92%B\\\n                                8%E7%95%99%E3%82%92%E8%A9%A6%E3%81%99"\n            - listitem [ref=s1e1917]:\n                - link "DeepSeek-R1-Zeroを作るまで" [ref=s1e1918]:\n                    - /url: "#deepseek-r1-zero%E3%82%92%E4%BD%9C%E3%82%8B%E3%81%BE%E3%81%A7"\n                - list [ref=s1e1919]:\n                    - listitem [ref=s1e1920]:\n                        - link "元々のアーキテクチャ" [ref=s1e1921]:\n                            - /url: "#%E5%85%83%E3%80%85%E3%81%AE%E3%82%A2%E3%83%BC%E3%82%AD%E3%83%86%E3%82%A\\\n                                F%E3%83%81%E3%83%A3"\n                    - listitem [ref=s1e1922]:\n                        - link "すごいぞ！強化学習" [ref=s1e1923]:\n                            - /url: "#%E3%81%99%E3%81%94%E3%81%84%E3%81%9E%EF%BC%81%E5%BC%B7%E5%8C%96%E5%AD%A\\\n                                6%E7%BF%92"\n                    - listitem [ref=s1e1924]:\n                        - link "そもそもLLMで強化学習とは？" [ref=s1e1925]:\n                            - /url: "#%E3%81%9D%E3%82%82%E3%81%9D%E3%82%82llm%E3%81%A7%E5%BC%B7%E5%8C%96%E5%A\\\n                                D%A6%E7%BF%92%E3%81%A8%E3%81%AF%EF%BC%9F"\n                    - listitem [ref=s1e1926]:\n                        - link "ZeroのSystemプロンプト" [ref=s1e1927]:\n                            - /url: "#zero%E3%81%AEsystem%E3%83%97%E3%83%AD%E3%83%B3%E3%83%97%E3%83%88"\n                    - listitem [ref=s1e1928]:\n                        - link "Zeroの「Aha Moment」" [ref=s1e1929]:\n                            - /url: "#zero%E3%81%AE%E3%80%8Caha-moment%E3%80%8D"\n                    - listitem [ref=s1e1930]:\n                        - link "DeepSeek-R1-Zeroのまとめ" [ref=s1e1931]:\n                            - /url: "#deepseek-r1-zero%E3%81%AE%E3%81%BE%E3%81%A8%E3%82%81"\n            - listitem [ref=s1e1932]:\n                - link "DeepSeek-R1への道" [ref=s1e1933]:\n                    - /url: "#deepseek-r1%E3%81%B8%E3%81%AE%E9%81%93"\n                - list [ref=s1e1934]:\n                    - listitem [ref=s1e1935]:\n                        - link "DeepSeek-R1-Zeroの課題" [ref=s1e1936]:\n                            - /url: "#deepseek-r1-zero%E3%81%AE%E8%AA%B2%E9%A1%8C"\n                    - listitem [ref=s1e1937]:\n                        - link "課題対処への道" [ref=s1e1938]:\n                            - /url: "#%E8%AA%B2%E9%A1%8C%E5%AF%BE%E5%87%A6%E3%81%B8%E3%81%AE%E9%81%93"\n                    - listitem [ref=s1e1939]:\n                        - link "DeepSeek-R1のまとめ" [ref=s1e1940]:\n                            - /url: "#deepseek-r1%E3%81%AE%E3%81%BE%E3%81%A8%E3%82%81"\n            - listitem [ref=s1e1941]:\n                - link "結果" [ref=s1e1942]:\n                    - /url: "#%E7%B5%90%E6%9E%9C"\n            - listitem [ref=s1e1943]:\n                - link "蒸留" [ref=s1e1944]:\n                    - /url: "#%E8%92%B8%E7%95%99"\n                - list [ref=s1e1945]:\n                    - listitem [ref=s1e1946]:\n                        - link "蒸留 vs 強化学習" [ref=s1e1947]:\n                            - /url: "#%E8%92%B8%E7%95%99-vs-%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92"\n            - listitem [ref=s1e1948]:\n                - link "今後の展望" [ref=s1e1949]:\n                    - /url: "#%E4%BB%8A%E5%BE%8C%E3%81%AE%E5%B1%95%E6%9C%9B"\n            - listitem [ref=s1e1950]:\n                - link "まとめ" [ref=s1e1951]:\n                    - /url: "#%E3%81%BE%E3%81%A8%E3%82%81"\n            - listitem [ref=s1e1952]:\n                - link "参考文献" [ref=s1e1953]:\n                    - /url: "#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"\n        - paragraph [ref=s1e1955]: Zennからのお知らせ\n        - article [ref=s1e1958]:\n            - link "第２回 AI Agent Hackathon with Google Cloud 第２回 AI Agent Hackathon with Google Cloud 4/14登録受付開始！" [ref=s1e1959]:\n                - /url: /hackathons/google-cloud-japan-ai-hackathon-vol2\n                - img "第２回 AI Agent Hackathon with Google Cloud" [ref=s1e1961]\n                - paragraph [ref=s1e1962]: 第２回 AI Agent Hackathon with Google Cloud 4/14登録受付開始！\n        - article [ref=s1e1963]:\n            - link "Zenncafe日比谷#2 生成AIで変わる仕事 Zenncafe日比谷#2 生成AIで変わる仕事" [ref=s1e1964]:\n                - /url: https://zenn.connpass.com/event/351131/\n                - img "Zenncafe日比谷#2 生成AIで変わる仕事" [ref=s1e1966]\n                - paragraph [ref=s1e1967]: Zenncafe日比谷#2 生成AIで変わる仕事\n- contentinfo [ref=s1e1969]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1973]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1974]\n    - paragraph [ref=s1e1982]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e1984]:\n        - heading "About" [level=4] [ref=s1e1985]\n        - list [ref=s1e1986]:\n            - listitem [ref=s1e1987]:\n                - link "Zennについて" [ref=s1e1989]:\n                    - /url: /about\n            - listitem [ref=s1e1990]:\n                - link "運営会社" [ref=s1e1991]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e1992]:\n                - link "お知らせ・リリース" [ref=s1e1993]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e1994]:\n                - link "イベント" [ref=s1e1996]:\n                    - /url: /events\n    - navigation [ref=s1e1997]:\n        - heading "Guides" [level=4] [ref=s1e1998]\n        - list [ref=s1e1999]:\n            - listitem [ref=s1e2000]:\n                - link "使い方" [ref=s1e2002]:\n                    - /url: /manual\n            - listitem [ref=s1e2003]:\n                - link "法人向けメニュー" [ref=s1e2005]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e2007]:\n                - link "Publication / Pro" [ref=s1e2009]:\n                    - /url: /publications\n            - listitem [ref=s1e2010]:\n                - link "よくある質問" [ref=s1e2012]:\n                    - /url: /faq\n    - navigation [ref=s1e2013]:\n        - heading "Links" [level=4] [ref=s1e2014]\n        - list [ref=s1e2015]:\n            - listitem [ref=s1e2016]:\n                - link "X(Twitter)" [ref=s1e2017]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e2018]:\n                - link "GitHub" [ref=s1e2019]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e2020]:\n                - link "メディアキット" [ref=s1e2022]:\n                    - /url: /mediakit\n    - navigation [ref=s1e2023]:\n        - heading "Legal" [level=4] [ref=s1e2024]\n        - list [ref=s1e2025]:\n            - listitem [ref=s1e2026]:\n                - link "利用規約" [ref=s1e2028]:\n                    - /url: /terms\n            - listitem [ref=s1e2029]:\n                - link "プライバシーポリシー" [ref=s1e2031]:\n                    - /url: /privacy\n            - listitem [ref=s1e2032]:\n                - link "特商法表記" [ref=s1e2034]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e2037]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e2038]\n- alert [ref=s1e2057]\n```\n', name='browser_navigate', tool_call_id='d85f8505-9c2a-4960-b17f-9ef88d983d9d'), AIMessage(content='まず、指定されたZennの著者のページにアクセスし、最もいいねが多い記事を特定しました。その結果、「DeepSeek-R1の論文読んだ？【勉強になるよ】」という記事が最も評価されていることがわかりました。\n\n次に、その記事の内容を詳しく調べました。このブログ記事は、DeepSeek-R1という大規模言語モデルに関する論文を解説したものです。記事は、論文の内容をわかりやすく説明することに重点を置いており、特に強化学習の概念や、DeepSeek-R1を開発する上で用いられたGRPO（Group Relative Policy Optimization）という手法について詳しく解説しています。\n\n記事が評価されている理由として、以下の点が考えられます。\n\n1.  **技術的な深さと分かりやすさの両立:** 大規模言語モデルや強化学習といった複雑なトピックを扱いながらも、専門知識がない読者にも理解できるように、かみ砕いた説明がなされています。\n2.  **論文の内容の網羅性:** DeepSeek-R1の論文の重要なポイントを漏れなくカバーしており、論文を読むための良い入門となっています。\n3.  **著者自身の考察:** 論文の内容を解説するだけでなく、著者自身の考察や解釈が加えられており、読者に新たな視点を提供しています。\n4.  **読者への配慮:** 記事の冒頭で、誤りがあるかもしれないことを断り、コメントでの指摘を歓迎するなど、読者とのコミュニケーションを重視する姿勢が見られます。\n5.  **参考文献の紹介:** 記事の最後に、強化学習や関連技術を学ぶための参考文献が紹介されており、読者の学習をサポートする姿勢が評価されていると考えられます。\n\nこれらの要素が組み合わさることで、この記事はDeepSeek-R1に関心のあるエンジニアにとって非常に価値のある情報源となり、高い評価を得ていると考えられます。', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-d5ac6129-cca3-4759-ac6a-eafe68db505f-0', usage_metadata={'input_tokens': 47987, 'output_tokens': 387, 'total_tokens': 48374, 'input_token_details': {'cache_read': 0}}), HumanMessage(content=[{'type': 'text', 'text': 'LLMに強化学習ってどういうこと？GRPOって何？'}], additional_kwargs={}, response_metadata={}), AIMessage(content='LLM（大規模言語モデル）に対する強化学習と、GRPO（Group Relative Policy Optimization）について説明します。\n\n### LLMに対する強化学習\n\nLLMに対する強化学習は、LLMがより良いテキストを生成するように学習させるための手法です。通常のLLMの学習では、大量のテキストデータを用いて、与えられたテキストの続きを予測するように学習します。これに対して、強化学習では、LLMが生成したテキストに対して「報酬」を与え、その報酬を最大化するように学習します。\n\n**なぜ強化学習が必要なのか？**\n\nLLMは、大量のテキストデータから学習することで、自然な文章を生成する能力を獲得します。しかし、必ずしも人間が望むような文章を生成するとは限りません。例えば、以下のような課題があります。\n\n*   **目的の達成:** 特定のタスク（質問応答、要約など）において、必ずしも最適な回答を生成するとは限らない。\n*   **安全性:** 有害な情報や偏見のある情報を生成する可能性がある。\n*   **創造性:** 新しいアイデアや表現を生み出す能力が限られている。\n\n強化学習は、これらの課題を克服するために用いられます。\n\n**強化学習の基本的な流れ**\n\n1.  **LLM（エージェント）:** テキストを生成する主体。\n2.  **環境:** LLMがテキストを生成する対象となるタスクや状況。\n3.  **報酬:** LLMが生成したテキストの良し悪しを評価する指標。\n4.  **学習:** LLMが報酬を最大化するように、テキスト生成の方針（方策）を調整する。\n\n具体的には、以下のような手順で学習が進みます。\n\n1.  LLMがプロンプト（指示）を受け取り、テキストを生成します。\n2.  生成されたテキストに対して、報酬関数が報酬を与えます。\n    *   例えば、質問応答タスクであれば、正解との一致度合いが高いほど高い報酬を与えます。\n    *   安全性に関する報酬であれば、有害な情報を生成した場合に低い報酬を与えます。\n3.  LLMは、その報酬に基づいて、自身のテキスト生成の方針を調整します。\n    *   具体的には、報酬が高くなるような単語やフレーズを選択する確率を高めます。\n4.  このプロセスを繰り返すことで、LLMはより高い報酬を得られるようなテキストを生成するように学習していきます。\n\n### GRPO（Group Relative Policy Optimization）とは？\n\nGRPOは、LLMに対する強化学習の一種で、特に大規模なLLMを効率的に学習させるために開発された手法です。\n\n**PPO（Proximal Policy Optimization）との比較**\n\nLLMの強化学習でよく用いられる手法として、PPO（Proximal Policy Optimization）があります。PPOでは、報酬に加えて、LLMの出力が大きく変化しないように制約を加えながら学習を進めます。しかし、PPOでは、状態価値モデルという追加のモデルを学習する必要があり、計算コストが高くなるという課題がありました。\n\nGRPOは、この状態価値モデルを不要にし、より効率的に学習を進めることを目指します。\n\n**GRPOの仕組み**\n\nGRPOでは、LLMに複数のテキストを生成させ、それらのテキストに対する報酬を比較することで、相対的な評価を行います。具体的には、以下の手順で学習が進みます。\n\n1.  LLMが同じプロンプトに対して、複数のテキスト（例えば4つ）を生成します。\n2.  それぞれのテキストに対して報酬関数が報酬を与えます。\n3.  それぞれのテキストの報酬を比較し、最も高い報酬を得たテキストを「良いテキスト」、それ以外のテキストを「悪いテキスト」とします。\n4.  LLMは、「良いテキスト」を生成する確率を高め、「悪いテキスト」を生成する確率を下げるように、自身のテキスト生成の方針を調整します。\n\nこのように、GRPOでは、テキスト間の相対的な評価を用いることで、状態価値モデルを学習する必要がなくなり、計算コストを削減することができます。\n\n**GRPOの利点**\n\n*   **計算効率:** 状態価値モデルが不要なため、PPOよりも計算コストが低い。\n*   **大規模LLMへの適用:** 大規模なLLMでも効率的に学習できる。\n\n**まとめ**\n\nLLMに対する強化学習は、LLMがより良いテキストを生成するように学習させるための手法であり、GRPOはその効率的な学習を可能にする手法の一つです。GRPOは、テキスト間の相対的な評価を用いることで、状態価値モデルを不要にし、計算コストを削減することができます。', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-4d11a554-e7d5-4420-9ac9-b8efaccf8b11-0', usage_metadata={'input_tokens': 48388, 'output_tokens': 996, 'total_tokens': 49384, 'input_token_details': {'cache_read': 0}}), HumanMessage(content=[{'type': 'text', 'text': '2番目にいいねが多い記事は何？'}], additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-aae69546-e8ee-4891-9040-daee70739fa4-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap'}, 'id': '61c67f9f-6525-4601-af0d-36cd5f4631a2', 'type': 'tool_call'}], usage_metadata={'input_tokens': 49392, 'output_tokens': 13, 'total_tokens': 49405, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap\n\n- Page URL: https://zenn.dev/asap\n- Page Title: asapさんの記事一覧 | Zenn\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- banner [ref=s1e32]:\n    - img "asap" [ref=s1e36]\n    - heading "asap" [level=1] [ref=s1e39]\n    - button "フォロー" [ref=s1e41]\n    - paragraph [ref=s1e43]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n    - button "2,214 Likes" [ref=s1e46]\n    - button "310 Followers" [ref=s1e48]\n    - link "@personabb" [ref=s1e51]:\n        - /url: https://github.com/personabb\n        - tooltip "@personabb" [ref=s1e52]:\n            - img "GitHub" [ref=s1e53]\n            - text: "@personabb"\n    - link "@asap2650" [ref=s1e55]:\n        - /url: https://twitter.com/asap2650\n        - tooltip "@asap2650" [ref=s1e56]:\n            - img "X(Twitter)" [ref=s1e57]\n            - text: "@asap2650"\n    - link "yard.tips" [ref=s1e63]:\n        - /url: https://yard.tips/asap2650\n        - tooltip "yard.tips" [ref=s1e64]:\n            - img [ref=s1e65]\n            - text: yard.tips\n    - link "RSS" [ref=s1e67]:\n        - /url: /asap/feed\n        - tooltip "RSS" [ref=s1e68]:\n            - img [ref=s1e69]\n            - text: RSS\n- link "Articles 72" [ref=s1e80]:\n    - /url: /asap\n- link "Scraps 5" [ref=s1e82]:\n    - /url: /asap?tab=scraps\n- link "Comments" [ref=s1e84]:\n    - /url: /asap?tab=comments\n- contentinfo [ref=s1e88]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e92]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e93]\n    - paragraph [ref=s1e101]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e103]:\n        - heading "About" [level=4] [ref=s1e104]\n        - list [ref=s1e105]:\n            - listitem [ref=s1e106]:\n                - link "Zennについて" [ref=s1e108]:\n                    - /url: /about\n            - listitem [ref=s1e109]:\n                - link "運営会社" [ref=s1e110]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e111]:\n                - link "お知らせ・リリース" [ref=s1e112]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e113]:\n                - link "イベント" [ref=s1e115]:\n                    - /url: /events\n    - navigation [ref=s1e116]:\n        - heading "Guides" [level=4] [ref=s1e117]\n        - list [ref=s1e118]:\n            - listitem [ref=s1e119]:\n                - link "使い方" [ref=s1e121]:\n                    - /url: /manual\n            - listitem [ref=s1e122]:\n                - link "法人向けメニュー" [ref=s1e124]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e126]:\n                - link "Publication / Pro" [ref=s1e128]:\n                    - /url: /publications\n            - listitem [ref=s1e129]:\n                - link "よくある質問" [ref=s1e131]:\n                    - /url: /faq\n    - navigation [ref=s1e132]:\n        - heading "Links" [level=4] [ref=s1e133]\n        - list [ref=s1e134]:\n            - listitem [ref=s1e135]:\n                - link "X(Twitter)" [ref=s1e136]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e137]:\n                - link "GitHub" [ref=s1e138]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e139]:\n                - link "メディアキット" [ref=s1e141]:\n                    - /url: /mediakit\n    - navigation [ref=s1e142]:\n        - heading "Legal" [level=4] [ref=s1e143]\n        - list [ref=s1e144]:\n            - listitem [ref=s1e145]:\n                - link "利用規約" [ref=s1e147]:\n                    - /url: /terms\n            - listitem [ref=s1e148]:\n                - link "プライバシーポリシー" [ref=s1e150]:\n                    - /url: /privacy\n            - listitem [ref=s1e151]:\n                - link "特商法表記" [ref=s1e153]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e156]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e157]\n- alert [ref=s1e176]\n```\n', name='browser_navigate', tool_call_id='61c67f9f-6525-4601-af0d-36cd5f4631a2'), AIMessage(content='asapさんのZennの記事の中で2番目にいいねが多い記事は、「VAEって結局何者なの？」で、いいね数は104です。', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-7c7bf5a2-7650-41d4-9cc5-c1fb3e0de2f1-0', usage_metadata={'input_tokens': 51212, 'output_tokens': 32, 'total_tokens': 51244, 'input_token_details': {'cache_read': 0}}), HumanMessage(content=[{'type': 'text', 'text': 'どんな記事ですか？解説してください。'}], additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap/articles/6caa9043276424"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-093f5ac0-5315-41a0-aaf8-f4fef2243382-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap/articles/6caa9043276424'}, 'id': '45a87836-eb09-4499-bf44-a2bcdbd3ec2b', 'type': 'tool_call'}], usage_metadata={'input_tokens': 51251, 'output_tokens': 28, 'total_tokens': 51279, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap/articles/6caa9043276424\n\n- Page URL: https://zenn.dev/asap/articles/6caa9043276424\n- Page Title: VAEって結局何者なの？\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- article [ref=s1e32]:\n    - text: 📖\n    - heading "VAEって結局何者なの？" [level=1] [ref=s1e38]\n    - text: 2025/01/20に公開\n    - link "ディープラーニング" [ref=s1e52]:\n        - /url: /topics/deeplearning\n        - img [ref=s1e54]\n        - text: ディープラーニング\n    - link "VAE" [ref=s1e56]:\n        - /url: /topics/vae\n        - img [ref=s1e58]\n        - text: VAE\n    - link "Stable Diffusion" [ref=s1e60]:\n        - /url: /topics/stablediffusion\n        - img [ref=s1e62]\n        - text: Stable Diffusion\n    - link "生成 AI" [ref=s1e64]:\n        - /url: /topics/%E7%94%9F%E6%88%90ai\n        - img [ref=s1e66]\n        - text: 生成 AI\n    - link "elbo" [ref=s1e68]:\n        - /url: /topics/elbo\n        - img [ref=s1e70]\n        - text: elbo\n    - link "tech" [ref=s1e72]:\n        - /url: /tech-or-idea\n        - img [ref=s1e74]\n        - text: tech\n    - heading "はじめに" [level=1] [ref=s1e78]\n    - paragraph [ref=s1e79]: 今回は、満を持してVAE（Variational Auto Encoder）をちゃんと理解していこうと思います。\n    - paragraph [ref=s1e80]: VAEに関しては、だいたい知っていますが、MusicGenという音楽生成AIを理解しようと思った時に、関連してRVQ-GANが出てきたので、再勉強をしています。\n    - paragraph [ref=s1e81]: 今後、下記の流れで記事を書いていく予定です。\n    - paragraph [ref=s1e82]: VAE（今回） ↓ VQ-VAE ↓ RQ-VAE,RVQ-GAN ↓ MusicGen\n    - paragraph [ref=s1e89]: 今回は、流れの最初であり、現在でも非常に重要な概念であるVAEに関して記事を書きます。\n    - paragraph [ref=s1e90]: VAEは、Stable\n        Diffusionの中でも中核として使われていたりと、比較的古い概念でありながら、まだまだ活躍が期待できる汎用ネットワークです。\n        本記事が、みなさまの理解の手助けになれば、これほど嬉しいことはございません。\n    - heading "参考文献" [level=1] [ref=s1e92]\n    - paragraph [ref=s1e93]:\n        - iframe [ref=s1e95]:\n            - link "Auto-Encoding Variational Bayes How can we perform efficient inference and learning in directed probabilistic models, in the presenc arxiv.org favicon image arxiv.org arxiv.org thumbnail image" [ref=f1s1e6]:\n                - /url: https://arxiv.org/abs/1312.6114\n                - heading "Auto-Encoding Variational Bayes" [level=1]\n                  [ref=f1s1e8]\n                - text: How can we perform efficient inference and learning in directed\n                    probabilistic models, in the presenc\n                - img "arxiv.org favicon image" [ref=f1s1e11]\n                - text: arxiv.org\n                - img "arxiv.org thumbnail image" [ref=f1s1e13]\n            - alert [ref=f1s1e15]\n        - text: VAEの論文です。\n    - paragraph [ref=s1e96]:\n        - link "ゼロから作るDeep Learning ❺ ―生成モデル編" [ref=s1e97]:\n            - /url: https://amzn.to/3Aw9vCt\n    - paragraph [ref=s1e98]: 圧倒的な名著シリーズである「ゼロから作るDeep Learning」シリーズの5作目である、生成モデル編です。\n        この本では、最終的に拡散モデルを理解することを目的に、VAEや混合ガウスモデル、正規分布まで遡り、本当にゼロから中身を理解しながら実装をすることができる、大変素晴らしい書籍になっています。\n    - paragraph [ref=s1e100]:\n        - link "妥協しないデータ分析のための 微積分+線形代数入門" [ref=s1e101]:\n            - /url: https://amzn.to/4hE7ftx\n    - paragraph [ref=s1e102]: 本書の後半にもVAEやELBOについての簡単な記載があり、本記事を書くうえで表現を参考にさせていただきました。\n        確率分布のどの部分が計算可能で、どの部分が計算不可能なのかが常に明示されながら進んでいくので、初学者には非常に分かりやすい書籍でした！\n    - paragraph [ref=s1e104]: 特に、VAEなどの生成AIをただ使うだけでなく、ちゃんと理論を理解したいと思った場合、論文や理論書を理解するために「線形代数」や「微積分学」の事前知識が必要になります。\n        本書は、初学者が一番最初に読んで必要な基礎知識が習得できる良本になります。\n    - paragraph [ref=s1e106]:\n        - link "生成 Deep Learning 第2版 ―絵を描き、物語や音楽を作り、ゲームをプレイする" [ref=s1e107]:\n            - /url: https://amzn.to/4g8hjJg\n    - paragraph [ref=s1e108]: 本書の序盤で、Auto\n        EncoderとVAEの違いを説明してくれています。しかもそれぞれの潜在空間の分布を可視化してくれているので、違いが分かりやすく、理解もしやすいです。\n        さらに、本書はVAEだけでなくGANや画像生成、音楽生成、テキスト生成など、ありとあらゆる生成AIの紹介と解説をしてくれており、必読の一冊となっております。\n    - paragraph [ref=s1e110]: （書籍のリンクはAmazonアフィリエイトリンクになっております）\n    - heading "関連記事" [level=1] [ref=s1e111]\n    - paragraph [ref=s1e112]: VAEに関連した記事は過去のも書いております。 こちらの記事もご覧いただけると嬉しいです！\n    - paragraph [ref=s1e114]:\n        - iframe [ref=s1e116]:\n            - link "【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】 asapさんによる記事 zenn.dev favicon image zenn.dev zenn.dev thumbnail image" [ref=f2s1e6]:\n                - /url: https://zenn.dev/asap/articles/ea9f42edff6573\n                - heading\n                  "【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】"\n                  [level=1] [ref=f2s1e8]\n                - text: asapさんによる記事\n                - img "zenn.dev favicon image" [ref=f2s1e11]\n                - text: zenn.dev\n                - img "zenn.dev thumbnail image" [ref=f2s1e13]\n            - alert [ref=f2s1e15]\n    - heading "Auto Encoder" [level=1] [ref=s1e117]\n    - heading "Auto Encoderの紹介" [level=2] [ref=s1e118]\n    - paragraph [ref=s1e119]: VAEを語る上で欠かせないのは、Auto Encoderの存在です。 Auto\n        Encoderというのは、ニューラルネットワークにより構築されたネットワーク構造の一つで、入力と出力を一致させるように学習させるネットワークです。\n    - paragraph [ref=s1e121]:\n        - img [ref=s1e122]\n    - paragraph [ref=s1e123]: 最も単純なAuto Encoderは上記のような構造をしています。\n        前段は、入力データから、それより次元の小さな潜在表現を生成するEncoderです。\n        上の図では、入力データを重みデータによる線形変換（+活性化層における非線形変換）で成り立っています。\n        Encoderでは、次元の大きな入力データを、次元の小さい潜在表現に圧縮する処理を行います。\n    - paragraph [ref=s1e127]: 後段は、潜在表現から、それより次元の大きな出力データを生成するDecoderです。\n        出力データは入力と一致するように学習がなされています。\n        このとき、Decoderでは、次元の小さな潜在表現から、出力データ（入力データ）を復元する処理を行います。\n        この「圧縮(符号化)」と「復元(複合化)」の構造から、「Encoder」「Decoder」と名付けられており、この圧縮・復元をデータを与えれば自動で実施するニューラルネットワークなので、「Auto\n        Encoder」と呼ばれます。\n    - heading "Auto Encoderの本質" [level=2] [ref=s1e131]\n    - paragraph [ref=s1e132]: ここでの本質は、次元の大きい入力データを、潜在表現に圧縮し、その潜在表現から、入力データを再構成することです。\n    - heading "小さな潜在表現から元データを復元できる理由" [level=3] [ref=s1e133]\n    - paragraph [ref=s1e134]: これが達成される理由は2つあります。\n    - paragraph [ref=s1e135]: 1つ目は、世の中にあるデータは冗長であることです。\n        世の中に存在するデータを冗長なデータが多いです。例えば画像データであれば、生の01でデータを保持するよりも、pngなどの圧縮形式を利用してデータを保存した方が、データ量を抑えることができます。\n        これは、画像中の「隣接する画素は、近い値が多い」という事前知識を用いることで、データを効率的に圧縮することができるからです。\n    - complementary [ref=s1e138]:\n        - text: "!"\n        - paragraph [ref=s1e141]: 話が脱線しますが、pngは可逆圧縮と呼ばれる手法であるため、この手法を利用してデータ量を削減した場合であっても、元の画像データを「完全に」復元することができます。これはzipなども同様です。\n        - paragraph [ref=s1e142]: 一方、画像でよく使われている、jpgやHEIFなどの規格は、非可逆圧縮と呼ばれる手法です。\n            この手法でデータ量を削減した場合は、元の画像から劣化することになります。\n            ただし、その劣化具合はみなさんご存知の通り非常に小さく、かつデータ削減量も可逆圧縮手法と比較して大幅に削減できます。\n    - paragraph [ref=s1e145]: 2つ目は、Auto Encoderの重みデータに、入力データを復元するための情報を学習しているからです。\n    - paragraph [ref=s1e146]: 例えば画像であれば、Auto\n        Encoderを利用する際に、事前に大規模画像データセットを利用して学習が行われます。その学習を通して、Decoderは「どうやったら少ない潜在表現から画像データを品質高く復元できるのか」、Encoderは「大量の画像データのうち、どの情報を潜在表現に埋め込めば、Decoderが元の画像を復元できるのか」を学習しています。\n        その結果、ちょうどpngで説明した通り、「隣接する画素は、近い値が多い」のような自然画像全般に通ずる事前知識を獲得し、その情報を用いることで、少ないデータ量から、元の入力データを復元するようなネットワークを構築することができるというわけです。\n    - heading "構造" [level=3] [ref=s1e148]\n    - paragraph [ref=s1e149]: 先ほど記載した通り、Auto\n        Encoderの本質は、次元の大きい入力データを、潜在表現に圧縮し、小さな潜在表現から、入力データを再構成することです。\n        そして、これが達成できるのであれば、どんなEncoder,Decoderの構造をしていたとしても、それはAuto\n        Encoderの一種であると考えることができます。\n    - paragraph [ref=s1e151]: 例えば画像を処理する際には、しばしば畳み込みニューラルネットワークやTransformerなどが使われます。\n        当然、これらを利用したAuto Encoderも考えることができます。\n    - paragraph [ref=s1e153]: 潜在表現も単純な1次元データだけではなく、画像のような3次元（縦, 横,\n        チャネル）データを考えることができます。 （ResNetなどのGAP前の層の出力結果をEncoderの出力として利用することもできます）\n    - paragraph [ref=s1e155]: したがって、以降ではEncoder Decoder構造を記載する際には、下記のように記載することにします。\n    - paragraph [ref=s1e156]:\n        - img [ref=s1e157]\n    - heading "Auto Encoerの課題" [level=2] [ref=s1e158]\n    - paragraph [ref=s1e159]: Auto Encoerはそれ単体では、それなりに優れた情報圧縮器ではありますが、それ以外での活用可能性が難しいネットワークではありました。\n    - paragraph [ref=s1e160]: 入力データと形状は同じな、出力データを出力する際には、ネットワーク構造が使われることも多いです（例えばセマンティックセグメンテーションタスクでU-Netが利用されるなど）が、同じ入出力になるように学習することというのは、情報圧縮以外の文脈で利用するのは難しいです。\n    - paragraph [ref=s1e161]: そこで、生成モデルとしての活用可能性が考えられました、\n        例えば、潜在表現を何らかの形で得ることができれば、その潜在表現をDecoderに入力することで、対応した出力データ（画像など）を得ることができます。\n        これが達成されれば、AIに絵を描かせたり、音楽を生成させたりすることが可能です。\n    - paragraph [ref=s1e164]: ただし、Auto Encoderでは、その潜在表現をEncoderを利用すること以外で取得することは困難です。\n        ここで、Auto Encoderによる画像生成の夢は途絶えたかのように見えました。\n    - heading "VAE（Variational Auto Encoder）" [level=1] [ref=s1e166]\n    - heading "VAEと生成モデル" [level=2] [ref=s1e167]\n    - heading "生成モデルの観点から見たVAE" [level=3] [ref=s1e168]\n    - paragraph [ref=s1e169]:\n        - text: VAEというのは、生成モデルとして重要な「対数尤度の最大化」問題を解くために必要な構造を用意したら、\n        - strong [ref=s1e171]: 結果的にAuto Encoderのような形になったというモデルです。\n    - paragraph [ref=s1e172]: すなわち、Auto\n        Encoderの構造があって、それをこうしたら生成モデルとして使えるようになった。というよりは、生成モデルを作る上で重要な「対数尤度を最大化する」という方向性で、理論的に式変形を繰り返し、計算不可能な部分を取り除き、計算が難しい部分をニューラルネットで近似することで、結果としてAuto\n        Encoderの構造が現れたという解釈があっているような気がします。\n    - heading "対数尤度の最大化からモデル構造を考える" [level=3] [ref=s1e173]\n    - heading "対数尤度の最大化に関しての軽い復習" [level=4] [ref=s1e174]\n    - paragraph [ref=s1e175]:\n        - text: 「対数尤度の最大化」に関しては下記の記事で解説しておりますので、そちらをご覧ください。\n        - iframe [ref=s1e177]: <could not take iframe snapshot>\n    - paragraph [ref=s1e178]:\n        - text: 上記の記事では詳細に記載していますが、簡単に解説すると、生成モデルにおいて重要なのは、対数尤度\n        - math [ref=s1e182]: p θ ( x )\n        - text: を最大化することです。 ここで、 x は実際のデータで、 \\theta は生成モデルのパラメータです。 したがって、 p_\\theta(x)\n            は対数尤度の確率分布になります。\n    - paragraph [ref=s1e199]:\n        - text: 生成モデルのパラメータ自体にはランダム性がないため、様々なバリエーションのデータを生成するためには、入力にランダムにサンプリングされたデータや定数を入力し、生成モデルがデータを再構成します。\n            イメージとしては下記のイメージです。\n        - img [ref=s1e202]\n    - complementary [ref=s1e203]:\n        - text: "!"\n        - paragraph [ref=s1e206]:\n            - text: ネットワークが確率分布を生成するというのは違和感があるかと思います。 その辺りの詳細は、下記の記事でも解説しているので参考にしてみてください。\n            - link "https://zenn.dev/asap/articles/4092ab60570b05" [ref=s1e209]:\n                - /url: https://zenn.dev/asap/articles/4092ab60570b05\n        - paragraph [ref=s1e210]: "ここでは、ネットワークが出力したデータ y を「平均」、単位行列 I を「分散」とする多変数正規分布\n            \\\\mathcal{N}(y, I) を考えることで、Decoderの出力を確率分布として計算しています。\n            実用上は、ネットワークが出力した平均 y の値を、そのまま生成したデータ x\'=y として扱います。"\n    - paragraph [ref=s1e222]: "しかし、上記で計算しているのは、尤度 p_{\\\\theta}(x|z) です。\n        （なぜなら、Decoderはパラメータ \\\\theta と潜在表現 z によって条件付けられたネットワークであるためです。）"\n    - paragraph [ref=s1e230]: これを用いて対数尤度 p_\\theta(x) を計算するには、下記の式を最適化する必要があります。\n    - text: "\\\\log p_{\\\\theta}(x) = \\\\mathbb{E}_{q_{\\\\psi}(z|x)}[\\\\log\n        p_{\\\\theta}(x|z)] - \\\\mathrm{KL}\\\\left(q_{\\\\psi}(z|x) \\\\parallel\n        p(z)\\\\right) + \\\\mathrm{KL}\\\\left(q_{\\\\psi}(z|x) \\\\parallel\n        p_{\\\\theta}(z|x)\\\\right)"\n    - paragraph [ref=s1e236]: "この中で、事後分布 p_{\\\\theta}(z|x) は計算不可能なため、近似事後分布\n        q_{\\\\psi}(z|x) を用意して、KLダイバージェンスに押し込みます。\n        そうすることで、第三項のKLダイバージェンスは非負であるため、この項を除いた部分(ELBO)を最大化することで、間接的に対数尤度を最大化するこ\\\n        とができます。"\n    - paragraph [ref=s1e242]: "このとき必要な、近似事後分布 q_{\\\\psi}(z|x) を計算するために、パラメータ \\\\psi\n        を持ち、データ x を入力して、潜在表現 z の確率分布を出力するようなネットワークとして、Encoderが必要になります。"\n    - paragraph [ref=s1e251]:\n        - text: 詳しくは\n        - link "上述した記事" [ref=s1e252]:\n            - /url: https://zenn.dev/asap/articles/ea9f42edff6573\n        - text: をご覧ください。\n    - heading "Encoderが平均と分散を出力する理由" [level=4] [ref=s1e253]\n    - paragraph [ref=s1e254]:\n        - text: ここまでの議論を考慮すると、下記のようなモデル構造になります。\n        - img [ref=s1e256]\n    - paragraph [ref=s1e257]: "上述した通り、Decoderの場合は、Decoderが出力するデータ y を「平均」、単位行列 I\n        を「分散」とする多変数正規分布 \\\\mathcal{N}(y, I) を確率分布として持ちますが、実用上は、Decoderが出力した平均 y\n        の値を、そのまま生成データ x\'=y と考えることができます。"\n    - paragraph [ref=s1e268]: 一方で、Encoderは確率分布を生成する必要があります。\n        さらにいうと、ELBOの式の中で、Encoderが出力する確率分布を用いて下記の式を計算する必要があります。\n    - text: "\\\\mathrm{KL}\\\\left(q_{\\\\psi}(z|x) \\\\parallel p(z)\\\\right)"\n    - paragraph [ref=s1e273]: ここで、 p(z) は潜在表現 z の事前分布なので、ベイズ推定等と同様に、こちらが設定することができます。\n        ネットワークは、こちらが設定した事前分布が成立するように学習します。\n        今回の場合、真の事前分布がどんな形状になるのかの事前情報を持っていないので、あまりバイアス情報が入らないような分布を設定することが望ましいです。\n    - paragraph [ref=s1e280]: "問題は、 q_{\\\\psi}(z|x) です。"\n    - paragraph [ref=s1e283]: この分布は特殊な分布になってはいけません。 なぜなら、KLダイバージェンスを計算できる分布の種類はあまり多くないからです。\n    - complementary [ref=s1e285]:\n        - text: "!"\n        - paragraph [ref=s1e288]: 一般的な連続確率分布同士のKLダイバージェンスを計算するのは、積分計算が入るため困難です。\n            しかしながら、指数型分布のような確率分布であれば計算できる余地があります。\n        - paragraph [ref=s1e290]: 例えば、正規分布同士のKLダイバージェンスであれば、比較的簡単に計算をすることができます。\n    - paragraph [ref=s1e291]: したがって、Encoderが生成する確率分布に、何らかの制限を加える必要があります。 今回は、確率分布が正規分布になるように制限をかけます。\n    - paragraph [ref=s1e293]: "その場合、Encoderは、平均 \\\\mu と分散 \\\\sigma^2\n        のパラメータを出力するような構造を考えることで、Encoderが正規分布を表現することができます。 そして、Encoderが出力した、平均\n        \\\\mu ・分散 \\\\sigma^2 パラメータから得られる正規分布 \\\\mathcal{N}(\\\\mu, \\\\sigma)\n        からサンプリングされた値を潜在表現 z とすることで、ELBOを計算することができます。 この場合、計算効率化のため、事前分布 p(z)\n        には平均 0 、分散 I の標準正規分布 \\\\mathcal{N}(0, I) を設定します。 標準正規分布 \\\\mathcal{N}(0,\n        I) は、正規分布とのKLダイバージェンスの計算が簡単であることと、平均 0 、分散 I\n        の条件下におけるエントロピーが最大になる分布（つまり、そのほかの事前情報を含まない分布）であるため、採用されています。"\n    - paragraph [ref=s1e323]: これを達成するためのモデル構造は下記になります。\n    - paragraph [ref=s1e324]:\n        - img [ref=s1e325]\n    - paragraph [ref=s1e326]:\n        - text: これでVAEを構築することができました。 上の図をよく見ると、\n        - strong [ref=s1e329]: まるでAuto Encoderのような構造になっていると思います。\n    - heading "VAEの挙動・振る舞い" [level=2] [ref=s1e330]\n    - heading "VAEの目的関数" [level=3] [ref=s1e331]\n    - paragraph [ref=s1e332]: 改めてVAEの目的関数は下記になります。\n    - complementary [ref=s1e333]:\n        - text: "!"\n        - paragraph [ref=s1e336]: ELBO\n        - text: "\\\\mathrm{ELBO} = \\\\mathbb{E}_{q_{\\\\psi}(z|x)}[\\\\log p_{\\\\theta}(x|z)] -\n            \\\\mathrm{KL}\\\\left(q_{\\\\psi}(z|x) \\\\parallel p(z)\\\\right)"\n    - paragraph [ref=s1e340]: このELBOを最大化するようにVAEのネットワークパラメータ( \\theta 、 \\psi )が最適化されます。\n    - paragraph [ref=s1e345]:\n        - text: ここで、\n        - link "先日書いた記事" [ref=s1e346]:\n            - /url: https://zenn.dev/asap/articles/ea9f42edff6573#%E7%AC%AC%E4%B8%80%E9%A0%85%E3%82%92%E8%80%83%E3%81%88%E3%82%8B\n        - text: にも記載しましたが、第1項は「再構成誤差」、第2項は「正則化項」になります。\n    - paragraph [ref=s1e347]: 式変形を行うと、第1項はDecoderの出力 y と、正解データ x との2乗誤差の負数になります。\n        したがってELBOを最大化する際には、2乗誤差を最小化する方向にVAEのネットワークパラメータ( \\theta 、 \\psi\n        )が最適化されます。\n    - complementary [ref=s1e357]:\n        - text: "!"\n        - paragraph [ref=s1e360]: なお、ここで再構成誤差が2乗誤差となるのは、VAEの出力の確率分布を正規分布と仮定したためです。\n            もし、出力の分布を「ベルヌーイ分布」と仮定した場合、再構成誤差項は下記のようにクロスエントロピー損失と等価になります。\n        - paragraph [ref=s1e362]: ベルヌーイ分布は、データ内のあるサンプル i を考えると、下記で表せます。\n        - text: "p_{\\\\theta}(x_i|z_i) = y_i^{x_i} (1 - y_i)^{1 - x_i}"\n        - paragraph [ref=s1e368]: "この時、ELBOの第1項は、期待値計算において、テンプルサイズ1で近似すると、 \\\\log\n            p_{\\\\theta}(x|z) に近似できるため、これを計算すると、"\n        - text: "\\\\log p_{\\\\theta}(x_i|z_i) = x_i \\\\log y_i + (1 - x_i) \\\\log (1 - y_i)"\n        - paragraph [ref=s1e374]: となるため、まさしくクロスエントロピー項となります。\n        - paragraph [ref=s1e375]: ただし、ベルヌーイ分布と仮定するためには、生成するデータ x が[0,1]である必要があります。\n    - paragraph [ref=s1e378]: "第2項は、Encoderが出力する q_{\\\\psi}(z|x) と平均 0 、分散 I の標準正規分布\n        \\\\mathcal{N}(0, I) とのKLダイバージェンスの負数になります。\n        したがってELBOを最大化する際には、KLダイバージェンス自体を最小化する方向にVAEのEncoderパラメータ( \\\\psi\n        )が最適化されます。 KLダイバージェンスは、2つの分布が一致しているときに最小値 0 をとります。したがって、Encoderが出力する確率分布\n        q_{\\\\psi}(z|x) が標準正規分布 \\\\mathcal{N}(0, I) に近づくように最適化されます。"\n    - heading "ELBOの第2項" [level=3] [ref=s1e397]\n    - paragraph [ref=s1e398]: 第1項は再構成誤差のため、分かりやすいと思いますが、第2項がどう言う意味を含むのかが分かりにくいと思います。\n    - complementary [ref=s1e399]:\n        - text: "!"\n        - paragraph [ref=s1e402]: ELBOの第2項\n        - text: "- \\\\mathrm{KL}\\\\left(q_{\\\\psi}(z|x) \\\\parallel p(z)\\\\right)"\n    - paragraph [ref=s1e406]: "ここでは、潜在表現 z の事前分布が標準正規分布 \\\\mathcal{N}(0, I) に従うと仮定していました。"\n    - paragraph [ref=s1e411]:\n        - text: "また、実は、Encoderが出力する近似事後分布 q_{\\\\psi}(z|x) は、ELBOに関係なく、実際の計算不能な真の事後分布\n            p_{\\\\theta}(z|x) に近づくように学習されています。 この詳細は、"\n        - link "以前書いた記事" [ref=s1e417]:\n            - /url: https://zenn.dev/asap/articles/ea9f42edff6573#elbo%E3%81%8C%E5%AF%BE%E6%95%B0%E5%B0%A4%E5%BA%A6%E3%81%AB%E8%BF%91%E3%81%A5%E3%81%8F%E6%9D%A1%E4%BB%B6\n        - text: "をご覧ください。 これは主に、Encoderが出力する近似事後分布 q_{\\\\psi}(z|x) が、真の事後分布\n            p_{\\\\theta}(z|x) に近づくことで、Decoderの再構成誤差が小さくなることに由来します。"\n    - paragraph [ref=s1e423]: "その上で、ELBOの第2項では、Encoderが出力する近似事後分布 q_{\\\\psi}(z|x)\n        は、標準正規分布 \\\\mathcal{N}(0, I) に近づくようにも学習されることを示しています。"\n    - paragraph [ref=s1e428]: "したがって、上記の議論をまとめると、Encoderが出力する近似事後分布 q_{\\\\psi}(z|x)\n        は、標準正規分布 \\\\mathcal{N}(0, I)\n        に近づくようにも学習されますが、その上で、Decoderの再構成誤差とバランスをとりながら、真の事後分布 p_{\\\\theta}(z|x)\n        にも近づくように学習されます。"\n    - paragraph [ref=s1e435]: "さらに言うと、真の事後分布 p_{\\\\theta}(z|x) は、Decoderパラメータ \\\\theta\n        依存して変化します。（計算できないですが、必ず存在し、 \\\\theta と x に依存します） この場合、真の事後分布\n        p_{\\\\theta}(z|x) もEncoderが出力する近似事後分布 q_{\\\\psi}(z|x)\n        に近づくような力学が多少なりとも働くことになります。"\n    - paragraph [ref=s1e449]: したがってDecoder側は、過度に複雑な事後分布をEncoder側に近似することを要求せず、ある程度、真の事後分布が標準正規分布に近づくように、（すなわち標準正規分布からデータを再構成できるように）学習が協力して進んでいく挙動をします。\n    - paragraph [ref=s1e450]: "以上を考慮すると、Decoderは、Encoderがないときには、事前分布として仮定した、標準正規分布\n        \\\\mathcal{N}(0, I) からサンプリングされた潜在表現 z を利用することで、そこそこ意味のあるデータを生成することができます。\n        この特性から、VAEは「生成モデル」として利用できることがわかります。"\n    - complementary [ref=s1e456]:\n        - text: "!"\n        - paragraph [ref=s1e459]: ただし、この関係は非常に難しいバランスの上で成立しています。\n        - paragraph [ref=s1e460]: 潜在表現の次元が増えたり、目的関数の項ごとの比率が変わるなどして、再構成誤差項と正則化項のバランスが崩れた際には、上記が成立しなくなります。\n            それについては後述します。\n        - paragraph [ref=s1e462]: しかし、その場合であっても、上記のような力学はある程度働きながら、学習が進むことになります。\n    - heading "Decoderが学習する潜在表現のランダム性" [level=3] [ref=s1e463]\n    - paragraph [ref=s1e464]: VAEの全体の構造を改めて下記に提示します。\n    - paragraph [ref=s1e465]:\n        - img [ref=s1e466]\n    - paragraph [ref=s1e467]: "こちらを見ると、Decoderに入力される潜在表現 z というのは、平均 \\\\mu ・分散 \\\\sigma^2\n        パラメータから得られる正規分布 \\\\mathcal{N}(\\\\mu, \\\\sigma) からサンプリングされた値を用いていることがわかります。\n        すなわち、学習時にDecoderが見る潜在表現 z には多少のランダム性がございます。\n        これは、Encoderに入力されるデータが同じデータであっても、decoderはランダムに異なる潜在表現 z から入力データ x\n        を再構成することを示します。"\n    - paragraph [ref=s1e484]: "そしてこのサンプリングは、平均ベクトル \\\\mu を中心として、正規分布 \\\\mathcal{N}(\\\\mu,\n        \\\\sigma) としてサンプリングされるため、中心から若干だけズレた値がサンプリングされることになります。"\n    - paragraph [ref=s1e489]: このことを考えると、Decoderは毎回若干異なる潜在表現 z から、入力データ x\n        を再構成することになります。 したがって、Decoderが入力データ x を再構成する際の潜在表現 z の許容範囲が広くなるように学習されます。\n        これを大量の入力 x に対して学習を実施すると、似たようなデータ同士では、似たような潜在表現 z\n        をEncoderがサンプリングするようになり、大きく異なるデータ同士では、異なる潜在表現 z をEncoderがサンプリングするようになります。\n    - paragraph [ref=s1e506]: これが達成されると、潜在表現 z が少し変わると、再構成データ y も少しだけ変わるという滑らかさをDecoderは得ることができます。\n    - paragraph [ref=s1e511]: この特性のおかげで、VAEは、2種類のデータ（例えば、男性の顔写真と女性の顔写真）を構成する潜在表現 z\n        の間を動かすことで、出力データ y は男性の顔から女性の顔に滑らかに変化するような挙動をするようになります。\n        これはよくVAEの解説サイトなどで見ることのできる挙動だと思います。\n    - heading "VAEはどうやって学習しているのか" [level=3] [ref=s1e517]\n    - paragraph [ref=s1e518]: もう一度VAEの構成図を表示します。\n    - paragraph [ref=s1e519]:\n        - img [ref=s1e520]\n    - paragraph [ref=s1e521]: ここまで読んで、勘のいい読者は、下記のように考えるかもしれません。\n        「確率分布からのSampling処理は微分不可能なため、学習できないのでないか」と\n    - paragraph [ref=s1e523]: その疑問はおっしゃる通りです。 ではここから、VAEの構造を損なわない形で、上記の図中からSampling構造を取り除くことを考えます。\n    - heading "Decoder出力部分" [level=4] [ref=s1e525]\n    - paragraph [ref=s1e526]: "図を見ると、Decoderの出力部分にてSampling構造があります。\n        生成モデルは、「対数尤度を最大化する」という目的のため、確率分布を出力することが求められますが、Decoderの出力はベクトルです。したがって\\\n        、出力ベクトル y を平均、単位行列 I を分散とする正規分布 \\\\mathcal{N}(y, I)\n        を確率分布として考え、そこからサンプリングされた値 x\' を生成データだと考えます。"\n    - paragraph [ref=s1e536]: この形で、目的関数ELBOを計算すると、出力ベクトル y と、入力データ x\n        の2乗誤差を最小化することが最終的な目的関数になることがわかります。\n        したがって、Sampling処理をなくすため、Decoderが出力するベクトル y 自体を、生成したデータ x\'=y と見なすことが可能です。\n    - complementary [ref=s1e546]:\n        - text: "!"\n        - paragraph [ref=s1e549]: 可能と言っているのは、 x\'=y と解釈したところで、目的関数は変わらず、パラメータを更新する微分値が変わらないことを指しています。\n    - paragraph [ref=s1e552]:\n        - text: したがって、これを考慮すると下記のような図になります。\n        - img [ref=s1e554]\n    - heading "Encoder出力部分" [level=4] [ref=s1e555]\n    - paragraph [ref=s1e556]: "続いて、Samplingが残っている箇所は、Encoderの出力する平均 \\\\mu ・分散 \\\\sigma^2\n        パラメータから得られる正規分布 \\\\mathcal{N}(\\\\mu, \\\\sigma) からサンプリングして、潜在表現 z\n        を得る部分になります。"\n    - paragraph [ref=s1e565]: この部分では、微分可能な計算グラフにするために、「Reparameterization\n        Trick」という手法を利用しています。 統計学に詳しい方には、「標準化の逆」という説明がしっくりくるかもしれません。\n    - complementary [ref=s1e567]:\n        - text: "!"\n        - paragraph [ref=s1e570]: 標準化とは、平均 \\mu 、分散 \\sigma^2 の複数データ z に対して、下記の処理を施した後のデータ\n            \\epsilon は、平均 0 、分散 1 となる変換です。\n        - text: "\\\\epsilon = \\\\dfrac{z - \\\\mu}{\\\\sigma}"\n    - paragraph [ref=s1e586]: "VAEでは、Encoderが平均 \\\\mu ・分散 \\\\sigma^2 を出力し、そこから得られる正規分布\n        \\\\mathcal{N}(\\\\mu, \\\\sigma) から潜在表現 z をサンプリングしています。"\n    - paragraph [ref=s1e595]: "このようなサンプリングの操作を、計算グラフの外に出すことで計算可能にする手法をReparameterizet\\\n        ion Trickと呼びます。 具体的には、計算グラフの外であらかじめ、平均 0 ・分散 1 の標準正規分布 \\\\mathcal{N}(0,\n        1) から乱数 \\\\epsilon を取得しておきます。"\n    - paragraph [ref=s1e605]: "その後、潜在表現 z は下記の計算を行うことで、まるで「標準化の逆」を行ったように、平均 \\\\mu ・分散\n        \\\\sigma^2 の正規分布 \\\\mathcal{N}(\\\\mu, \\\\sigma) からサンプリングされたような値に変換できます。"\n    - text: z = \\mu + \\sigma \\epsilon\n    - paragraph [ref=s1e617]: この処理を反映させると下記のような図になります。\n    - paragraph [ref=s1e618]:\n        - img [ref=s1e619]\n    - paragraph [ref=s1e620]: この処理により、DeocderとEncoderを繋ぐ部分にはSampling処理が現れなくなったため、Decoder側からEncoder側に勾配が正しく伝播するようになるため、学習が可能なネットワークになりました。\n    - heading "VAEを生成モデルとして利用する" [level=2] [ref=s1e621]\n    - heading "VAEの特徴" [level=3] [ref=s1e622]\n    - paragraph [ref=s1e623]: さて、ここまでの議論からVAEには下記の特徴を持つことが言えます。\n    - list [ref=s1e624]:\n        - listitem [ref=s1e625]:\n            - text: VAEの潜在表現 z には滑らかさが存在し、 z を少し変化させると生成されるデータ x\' が少しだけ変化する\n            - list [ref=s1e632]:\n                - listitem [ref=s1e633]:\n                    - text: この特徴より、潜在空間内において、2地点を行き来することで、滑らかに画像を変化させることができる\n                    - list [ref=s1e634]:\n                        - listitem [ref=s1e635]: 例えば、男性の顔写真から女性の顔写真へと滑らかに生成データを変化させることができる\n        - listitem [ref=s1e636]:\n            - text: "VAEの潜在表現の事前分布 p(z) は標準正規分布 \\\\mathcal{N}(0, 1) を仮定している"\n            - list [ref=s1e641]:\n                - listitem [ref=s1e642]: "標準正規分布 \\\\mathcal{N}(0, 1) から潜在表現 z\n                    をサンプリングすることができ、それを用いて「ある程度意味のある」データ x\' を生成できる"\n    - paragraph [ref=s1e649]: したがって、新しいデータを生成できるという観点で、生成モデルと呼ぶことができます。\n    - heading "落とし穴" [level=3] [ref=s1e650]\n    - paragraph [ref=s1e651]: と、私も昔は思っておりました。 いや、完全に間違っている訳ではないのですが、時には理論通りに進まないものです。\n    - paragraph [ref=s1e653]: 正確には、VAEの特徴は場合によっては正しくて、場合によっては正しくないです。\n        「目的関数の再構成誤差と正則化項のバランス」が崩れた場合に、VAEの特徴は機能しなくなります。\n        （正確には、ある程度その特徴は持っているが、だんだん機能しなくなってくるというイメージ）\n    - paragraph [ref=s1e656]: これから、説明させていただきます。 ここで、VAEの目的関数であるELBOを再提示します。\n    - complementary [ref=s1e658]: "! \\\\mathrm{ELBO} =\n        \\\\mathbb{E}_{q_{\\\\psi}(z|x)}[\\\\log p_{\\\\theta}(x|z)] -\n        \\\\mathrm{KL}\\\\left(q_{\\\\psi}(z|x) \\\\parallel p(z)\\\\right)"\n    - paragraph [ref=s1e664]: この式は、第1項の再構成誤差項と、第2項の正則化項の「バランス」をとって学習が進むことがわかっています。\n        このバランスは、例えば、第2項に対してハイパーパラメータを導入し、その寄与割合を変化させることによっても変わります。 （これを\n        β-VAE（ベータVAE）と呼びます）\n    - paragraph [ref=s1e667]:\n        - strong [ref=s1e668]: そしてもう一つ、潜在表現の次元数によっても変わります。\n    - paragraph [ref=s1e669]: よく見るVAEの解説記事では、MNISTなどの簡単な画像に対して、潜在表現の次元を2次元に設定して検証している記事が多いと思います。\n        これは、VAEの潜在表現の分布と、手描き文字の数字を対応付けて可視化するために非常に優れた例示の仕方だと思っております。\n        一方で、「VAEであれば、必ず、潜在表現の事前分布が正規分布に落ち着く」というようなミスリードを読者に引き起こす危険性もはらんでいると思っております。\n    - heading "潜在表現の次元が小さい場合" [level=3] [ref=s1e672]\n    - paragraph [ref=s1e673]: 他記事のように、VAEの潜在表現 z が2次元の場合、潜在表現 z はたった二つの値しか持たないことになります。\n        2次元の潜在表現 z では、画像の情報を保持することは困難です。\n    - paragraph [ref=s1e681]: この場合、VAEは、潜在空間 z に情報を埋め込むのは諦めます。 そして、潜在空間 z\n        はあくまで「乱数のサンプリング+おおよそのラベル情報」と割り切り、画像の再構成情報をDecoderに埋め込むように学習します。\n    - paragraph [ref=s1e687]: "すると、潜在表現 z は画像情報などの追加情報をほとんど持たないため、事前分布\n        p(z)=\\\\mathcal{N}(0, 1) に従って分布されるようになります。"\n    - paragraph [ref=s1e692]: この場合は、上記の「VAEの特徴」で記載した生成モデルとしての特徴を強く持つことになります。\n    - paragraph [ref=s1e693]: これは、ELBOの正則化項の寄与率を大きくしても発生します。\n    - heading "潜在変数の次元が多い場合" [level=3] [ref=s1e694]\n    - paragraph [ref=s1e695]: 一方で、潜在変数の次元数が多い場合はどうか。\n        潜在変数の次元数が多い場合は、あまり解説に向かないため、解説記事は少ないです。\n        しかし、潜在変数の次元数が多いVAEが使われており、比較的解析しやすい技術はわかります。\n    - paragraph [ref=s1e698]:\n        - strong [ref=s1e699]: みなさんご存知、Stable Diffusionです。\n    - heading "Stable Diffusionのモデル構造" [level=4] [ref=s1e700]\n    - paragraph [ref=s1e701]:\n        - img [ref=s1e702]\n        - text: （引用：\n        - link "https://github.com/CompVis/latent-diffusion/blob/main/assets/modelfigure.png）" [ref=s1e703]:\n            - /url: https://github.com/CompVis/latent-diffusion/blob/main/assets/modelfigure.png%EF%BC%89\n    - paragraph [ref=s1e704]: Stable Diffusionのモデル構造は上記のようになっております。\n    - paragraph [ref=s1e705]:\n        - strong [ref=s1e706]: Conditioningブロック\n    - paragraph [ref=s1e707]: 一番右の「Conditioning」ブロックは、画像の条件付けに使われる部分です。 例えば Stable\n        Diffusionにおける、Text to Imageモデルの場合、プロンプトと呼ばれる呪文で、生成される画像のテーマを制御することができます。\n    - paragraph [ref=s1e709]: なぜかというと、まず、「Conditioning」ブロックで使われいてる、CLIPやT5\n        Encoderと呼ばれるtext Encoderにより、プロンプトを特徴量に変換しています。\n        このとき、CLIPは、「画像とテキストの特徴量が一致」するように学習されているため、プロンプトからCLIPを通じて得られた特徴量は、画像の特徴量と同様に取り扱うことができます。\n        そのため、プロンプトの特徴量を、画像特徴量として、後述する「クロスアテンション」にて、計算することができるようになるからです。\n    - paragraph [ref=s1e712]:\n        - strong [ref=s1e713]: Latent Spaceブロック\n        - text: 真ん中の「Latent Space」ブロックでは、潜在表現 z を生成するブロックになります。\n            ここでは、「拡散モデル」というモデルを利用して、潜在表現 z を生成します。 （拡散モデルに関しては\n        - link "こちらの記事も" [ref=s1e721]:\n            - /url: https://zenn.dev/asap/articles/4092ab60570b05\n        - text: ご覧ください）\n    - paragraph [ref=s1e722]: また、拡散モデルでは、TransformerやU-netというモデルが利用されていますが、そのどちらにおいても、CLIPから得られたプロンプト特徴量を「クロスアテンション」により注入しています。\n    - paragraph [ref=s1e723]:\n        - text: クロスアテンションとは、処理中の画像特徴量と、プロンプト特徴量の類似度を計算し、類似度が高い画像特徴量を増幅させるような処理が行われており、この処理により、プロンプトに適した画像になるように潜在表現\n            z が生成されていきます。 （アテンションに関しては、\n        - link "こちらの記事" [ref=s1e727]:\n            - /url: https://zenn.dev/asap/articles/6bdf89d8e964ec#self-attension%E3%81%A8%E3%81%AF\n        - text: でも解説しております。）\n    - paragraph [ref=s1e728]:\n        - strong [ref=s1e729]: Pixel Spaceブロック\n        - text: 最後に、一番右側の「Pixel Space」ブロックが、VAEが担当する部分です。 上の段の x は生成したい入力データで、 \\varepsilon\n            と書かれている部分がEncoderです。\n    - paragraph [ref=s1e736]: "そして、下の段の \\\\tilde{x} が生成データで、 D と書かれているのが、Decoderになります。"\n    - paragraph [ref=s1e741]: "すなわち、拡散モデルが生成した潜在表現 z から、VAE decoderが出力画像 \\\\tilde{x} を生成していることになります。"\n    - complementary [ref=s1e746]:\n        - text: "!"\n        - paragraph [ref=s1e749]: ここでは、VAEは完全畳み込み層になっており、入力が3次元の画像（縦x横xチャネル）データの場合、潜在表現 z\n            は(縦/8 x 横/8 x 4)の形になっています。\n    - paragraph [ref=s1e752]: "さて、ここで重要なのは、Stable\n        Diffusionでは、VAEが使われてるはずですが、画像を生成する際に標準正規分布 \\\\mathcal{N}(0, 1)\n        からサンプリングした潜在表現 z を利用していないことです。"\n    - paragraph [ref=s1e757]: ちなみに、(詳しい方向けに補足しておくと)Stable Diffusion\n        XLで使われているのは、VQ-VAEではなく、VAEです。\n        （これはDiffusersライブラリでSDXLの部分を見ればわかります。Diffusersライブラリにて実装されているのは、通常のVAEです。）\n    - group [ref=s1e759]: 該当コード\n    - paragraph [ref=s1e761]: "Stable Diffusionでは何をやっているのかというと、標準正規分布 \\\\mathcal{N}(0,\n        1) から得られた潜在表現を、拡散モデルにより分布を変換し、変換後の潜在表現 z\n        をVAEのdecoderに入力することで、画像を生成しています。"\n    - paragraph [ref=s1e766]: "すなわち、上述した「VAEの特徴：標準正規分布 \\\\mathcal{N}(0, 1) から潜在表現 z\n        をサンプリングすることができ、それを用いて「ある程度意味のある」データ x\' を生成できる」が崩壊していることがわかります。"\n    - heading "Stable Diffusion内部のVAEの特徴" [level=4] [ref=s1e773]\n    - paragraph [ref=s1e774]:\n        - text: "実際、単純な標準正規分布 \\\\mathcal{N}(0, 1) から得られた潜在表現を、そのままVAE\n            decoderに入力すると、ノイズ画像が生成されます。 そのあたりは下記の記事などもご参照ください。"\n        - iframe [ref=s1e779]: <could not take iframe snapshot>\n    - paragraph [ref=s1e780]: 上記の記事中のgifを見ると、最初の方は砂嵐のようなノイズ画像が表示されており、だんだん拡散モデルにより潜在表現\n        z の分布が変化していくと、decoderが画像として出力できるようになっているのがわかると思います。\n    - heading "そのときVAEに何が起きているのか！？" [level=4] [ref=s1e783]\n    - paragraph [ref=s1e784]: "これは何が起きているのかというと、第一に、VAEの潜在表現 z の事前分布 p(z) が標準正規分布\n        \\\\mathcal{N}(0, 1) から大きくずれてしまっています。"\n    - paragraph [ref=s1e791]: これは、学習時に、ELBO第2項の正則化項が入っているのにも関わらず・・・です。\n    - paragraph [ref=s1e792]:\n        - text: 正則化項は、下記のコードの\n        - code [ref=s1e793]: DiagonalGaussianDistribution.kl\n        - text: メソッドにて計算がされています。\n        - iframe [ref=s1e795]: <could not take iframe snapshot>\n    - paragraph [ref=s1e796]: "では、なぜVAEの潜在表現 z の事前分布 p(z) が標準正規分布 \\\\mathcal{N}(0, 1)\n        から大きくずれるのかというと、VAEの潜在表現 z の次元数の多さが原因です。"\n    - paragraph [ref=s1e805]: 上述した通り、Stable Diffusionの潜在表現 z のサイズは「縦/8 x 横/8 x\n        4」になっています。 Stable Diffusion XLでは、学習される画像のサイズは「1024x1024」画素が一般的です。\n        このとき、潜在表現 z のサイズは、「128 x 128 x 4」となり、潜在表現 z の要素数は65536となります。\n        65536という数字は、MNISTの実験で使っていた2次元の潜在表現の次元数と比較して、非常に大きな次元数であることがわかります。\n    - paragraph [ref=s1e815]: このレベルで潜在表現 z の次元が大きくなると、潜在表現 z 自体にも画像の情報を埋め込めるようになります。\n        2次元しか潜在表現 z\n        がない場合、潜在表現に情報を埋め込むのは困難なため、画像の再構成情報はDecoderに全て埋め込まれるような挙動になると、記載しました。\n        しかし、今回の場合は、潜在表現 z とDecoderの協業で、画像を再構成することになります。\n    - paragraph [ref=s1e826]: これは、すなわち正則化項よりも、再構成損失項の方が強いバランスになります。\n        潜在表現に画像情報を埋め込むことによって、正則化項の損失が大きくなったとしても、その分大幅に再構成損失項の損失を減少させることができるからです。\n    - paragraph [ref=s1e828]: "その結果、潜在表現 z の事前分布 p(z) が、標準正規分布 \\\\mathcal{N}(0, 1)\n        から大きく離れるようになります。 その代わり、Stable Diffusionでは、より高品質な画像が生成できるようになっているわけです。"\n    - heading "だから拡散モデルがいる" [level=4] [ref=s1e836]\n    - paragraph [ref=s1e837]: ただし、VAEが正則化項を無視し始めると、画質が良くなる代わりに、潜在表現 z の事前分布 p(z)\n        は意味がわからない分布になります。 この意味のわからない分布を、人間は把握することはできず、その分布からサンプリングした潜在表現 z\n        をEncoderなしでは取得できません。\n    - paragraph [ref=s1e845]: そこで、その潜在表現の事前分布 p(z) を、拡散モデルに学習してもらいます\n        そして、その学習ずみ拡散モデルが、VAEの潜在表現 z\n        を生成することで、高品質なVAEの生成画像を維持しながら、生成モデルとして0から新しいデータを生み出しつつ、さらにプロンプトによる条件付けまで達成しています。\n        （だから、これだけ騒がれて、めちゃくちゃ使われているわけですね）\n    - complementary [ref=s1e852]:\n        - text: "!"\n        - paragraph [ref=s1e855]: この場合、VAEの特徴はかなり少なくなっており、どちらかというと Auto Encoderに近い挙動をします。\n            しかし、正則化項が完全になくなったわけではないため、ある程度、潜在表現の滑らかさなどは残っており、結果として、Auto\n            Encoderの潜在表現 z よりも、VAEの潜在表現 z の方が、拡散モデルが学習しやすくなっていると思われます。\n    - heading "最後にVAEの課題" [level=1] [ref=s1e861]\n    - paragraph [ref=s1e862]: ここまでVAEのいいところを説明してきましたが、VAEを単体で利用する場合は、まだまだ課題があります。\n    - heading "Posterior Collapse" [level=2] [ref=s1e863]\n    - paragraph [ref=s1e864]: "一つは「Posterior Collapse（事後崩壊）」の問題です。\n        これは、VAEの目的関数ELBOの正則化項が強すぎる場合、Encoderが出力する近似事後分布 q_{\\\\psi}(z|x) と、事前分布（\n        p(z)=\\\\mathcal{N}(0, 1) )が完全に一致してしまうことで発生します。\n        この場合、画像の再構成情報が完全にDecoderに入ってしまい、潜在表現 z\n        を無視した再構成が行われてしまいます。なまじDecoderの性能が高いと発生しやすい問題です。"\n    - paragraph [ref=s1e873]: posterior collapseの対策として、学習初期は正則化項の寄与率を下げて学習させ、だんだん寄与率を大きくしていくことが挙げられます。\n    - heading "画像がボケやすい" [level=2] [ref=s1e874]\n    - paragraph [ref=s1e875]: もう一つは、生成される画像がボケやすい点です。 Stable\n        Diffusionに搭載されているVAEのように、事前分布 p(z) の形状を完全に気にしないように学習する（つまり潜在表現 z\n        の次元数が非常に大きい）場合は起こりにくい問題ではありますが、ある程度正則化項による正則化を入れていきたい場合（つまり、事前分布 p(z)\n        に正規分布を仮定したい状況）では、この問題が発生しやすいです。\n    - paragraph [ref=s1e883]: これは、上述した通り、Decoderに入力される潜在表現 z\n        は、乱数のサンプリング後の値であるため、例え全く同じ画像を学習していたとしても、潜在表現 z の値は学習のたびに異なります。\n    - paragraph [ref=s1e888]: すると、似たような画像 y を生成する潜在表現 z は、潜在表現空間において、近づきやすくなります。\n        一方で、decoder側は、潜在表現 z が密集している空間の場合、ある潜在表現 z が、どの生成画像 x\n        に対応しているのか、そのランダム性のために判断することができなくなってしまうため、なるべく最終的な損失を小さくするために、密集している潜在表現\n        z から生成されうる画像 x の平均のような画像を学習してしまいます。 これは、潜在表現 z\n        の次元が小さい方が、潜在表現空間における密集度が高まるため、発生しやすくなります。（一般に低次元空間の方が2点間の距離は小さく、高次元空間の方が2点間の距離が広くなる）\n    - paragraph [ref=s1e907]: "これの対処法は、VAEの潜在表現の次元を増やすことか、第2項の寄与率を下げるくらいしかありませんでした。\n        しかし、これを行うと、どうしても生成モデルとしての特徴である、「潜在表現zの事前分布 p(z) が標準正規分布 \\\\mathcal{N}(0,\n        1) である」という仮定が消失しやすくなります。"\n    - paragraph [ref=s1e913]: したがって、バランス調整が難しい問題で、VAEが持つ悪い特徴の一つでしたが、後日記事を書くVQ-VAEでは、この問題を解消しています。\n    - heading "まとめ" [level=1] [ref=s1e914]\n    - paragraph [ref=s1e915]: ここまで読んでくださってありがとうございました！\n        生成モデルと言えば、Transformerや拡散モデルが主流ですが、非常に長い系列長（音声など）を生成する場合には、まだまだVAEが使われいてるように見えます。\n    - paragraph [ref=s1e917]: VAEは一度に全てのデータを生成できるため、速度が必要なアプリケーションにも適していますし、圧縮器としての機能もあるため、大きなデータを潜在表現に圧縮し、その潜在表現をTransformerや拡散モデルによって再構成するといった使い方がよくなされており、古い概念ではありますが、まだまだVAEには価値があるなと感じております。\n    - paragraph [ref=s1e918]: わかった気になっていましたが、VAEに関して詳細に色々調べていると、まだまだ理解があやふやな部分もあり、記事を書きながら理解度を高めることができました！\n    - paragraph [ref=s1e919]: この記事が、何らかの形でみなさまの手助けになれば幸いです！\n    - paragraph [ref=s1e920]: ここまで読んでくださり、ありがとうございました！\n    - heading "参考文献" [level=1] [ref=s1e921]\n    - paragraph [ref=s1e922]:\n        - iframe [ref=s1e924]: <could not take iframe snapshot>\n        - text: VAEの論文です。\n    - paragraph [ref=s1e925]:\n        - link "ゼロから作るDeep Learning ❺ ―生成モデル編" [ref=s1e926]:\n            - /url: https://amzn.to/3Aw9vCt\n    - paragraph [ref=s1e927]: 圧倒的な名著シリーズである「ゼロから作るDeep Learning」シリーズの5作目である、生成モデル編です。\n        この本では、最終的に拡散モデルを理解することを目的に、VAEや混合ガウスモデル、正規分布まで遡り、本当にゼロから中身を理解しながら実装をすることができる、大変素晴らしい書籍になっています。\n    - paragraph [ref=s1e929]:\n        - link "妥協しないデータ分析のための 微積分+線形代数入門" [ref=s1e930]:\n            - /url: https://amzn.to/4hE7ftx\n    - paragraph [ref=s1e931]: 本書の後半にもVAEやELBOについての簡単な記載があり、本記事を書くうえで表現を参考にさせていただきました。\n        確率分布のどの部分が計算可能で、どの部分が計算不可能なのかが常に明示されながら進んでいくので、初学者には非常に分かりやすい書籍でした！\n    - paragraph [ref=s1e933]: 特に、VAEなどの生成AIをただ使うだけでなく、ちゃんと理論を理解したいと思った場合、論文や理論書を理解するために「線形代数」や「微積分学」の事前知識が必要になります。\n        本書は、初学者が一番最初に読んで必要な基礎知識が習得できる良本になります。\n    - paragraph [ref=s1e935]:\n        - link "生成 Deep Learning 第2版 ―絵を描き、物語や音楽を作り、ゲームをプレイする" [ref=s1e936]:\n            - /url: https://amzn.to/4g8hjJg\n    - paragraph [ref=s1e937]: 本書の序盤で、Auto\n        EncoderとVAEの違いを説明してくれています。しかもそれぞれの潜在空間の分布を可視化してくれているので、違いが分かりやすく、理解もしやすいです。\n        さらに、本書はVAEだけでなくGANや画像生成、音楽生成、テキスト生成など、ありとあらゆる生成AIの紹介と解説をしてくれており、必読の一冊となっております。\n    - paragraph [ref=s1e939]: （書籍のリンクはAmazonアフィリエイトリンクになっております）\n    - button "いいね" [ref=s1e942]:\n        - img [ref=s1e943]\n    - text: "104"\n    - button "ブックマーク" [ref=s1e959]:\n        - img [ref=s1e960]\n    - text: "43"\n    - button "その他の操作" [ref=s1e965]:\n        - img [ref=s1e966]\n    - tooltip "X(Twitter)にポスト" [ref=s1e969]:\n        - img [ref=s1e970]\n        - text: X(Twitter)にポスト\n    - tooltip "Facebookに投稿" [ref=s1e976]:\n        - img [ref=s1e977]\n        - text: Facebookに投稿\n    - tooltip "はてなブックマークに登録" [ref=s1e983]:\n        - img [ref=s1e984]\n        - text: はてなブックマークに登録\n    - complementary [ref=s1e987]:\n        - link "asap" [ref=s1e989]:\n            - /url: /asap\n            - img "asap" [ref=s1e990]\n        - link "asap" [ref=s1e992]:\n            - /url: /asap\n        - paragraph [ref=s1e994]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n        - button "フォロー" [ref=s1e998]\n        - tooltip "@personabb" [ref=s1e999]:\n            - img "GitHub" [ref=s1e1000]\n            - text: "@personabb"\n        - tooltip "@asap2650" [ref=s1e1002]:\n            - img "X(Twitter)" [ref=s1e1003]\n            - text: "@asap2650"\n        - tooltip "yard.tips" [ref=s1e1009]:\n            - img "リンク" [ref=s1e1010]\n            - text: yard.tips\n    - complementary [ref=s1e1013]:\n        - text: バッジを贈って著者を応援しよう\n        - paragraph [ref=s1e1017]: バッジを受け取った著者にはZennから現金やAmazonギフトカードが還元されます。\n        - button "バッジを贈る" [ref=s1e1019]\n        - img [ref=s1e1020]\n    - heading "Discussion" [level=3] [ref=s1e1029]\n    - img [ref=s1e1030]\n    - text: ログインするとコメントできます\n    - button "Login" [ref=s1e1034]\n    - complementary [ref=s1e1035]:\n        - link "asap" [ref=s1e1040]:\n            - /url: /asap\n            - img "asap" [ref=s1e1041]\n        - link "asap" [ref=s1e1043]:\n            - /url: /asap\n        - button "フォロー" [ref=s1e1045]\n        - link "@personabb" [ref=s1e1046]:\n            - /url: https://github.com/personabb\n            - tooltip "@personabb" [ref=s1e1047]:\n                - img "GitHub" [ref=s1e1048]\n                - text: "@personabb"\n        - link "@asap2650" [ref=s1e1050]:\n            - /url: https://twitter.com/asap2650\n            - tooltip "@asap2650" [ref=s1e1051]:\n                - img "X(Twitter)" [ref=s1e1052]\n                - text: "@asap2650"\n        - link "yard.tips" [ref=s1e1058]:\n            - /url: https://yard.tips/asap2650\n            - tooltip "yard.tips" [ref=s1e1059]:\n                - img "リンク" [ref=s1e1060]\n                - text: yard.tips\n        - paragraph [ref=s1e1063]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n        - button "バッジを贈る" [ref=s1e1066]\n        - link "バッジを贈るとは" [ref=s1e1068]:\n            - /url: /faq#badges\n            - text: バッジを贈るとは\n            - img [ref=s1e1069]\n        - text: 目次\n        - list [ref=s1e1078]:\n            - listitem [ref=s1e1079]:\n                - link "はじめに" [ref=s1e1080]:\n                    - /url: "#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB"\n            - listitem [ref=s1e1081]:\n                - link "参考文献" [ref=s1e1082]:\n                    - /url: "#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"\n            - listitem [ref=s1e1083]:\n                - link "関連記事" [ref=s1e1084]:\n                    - /url: "#%E9%96%A2%E9%80%A3%E8%A8%98%E4%BA%8B"\n            - listitem [ref=s1e1085]:\n                - link "Auto Encoder" [ref=s1e1086]:\n                    - /url: "#auto-encoder"\n                - list [ref=s1e1087]:\n                    - listitem [ref=s1e1088]:\n                        - link "Auto Encoderの紹介" [ref=s1e1089]:\n                            - /url: "#auto-encoder%E3%81%AE%E7%B4%B9%E4%BB%8B"\n                    - listitem [ref=s1e1090]:\n                        - link "Auto Encoderの本質" [ref=s1e1091]:\n                            - /url: "#auto-encoder%E3%81%AE%E6%9C%AC%E8%B3%AA"\n                    - listitem [ref=s1e1092]:\n                        - link "Auto Encoerの課題" [ref=s1e1093]:\n                            - /url: "#auto-encoer%E3%81%AE%E8%AA%B2%E9%A1%8C"\n            - listitem [ref=s1e1094]:\n                - link "VAE（Variational Auto Encoder）" [ref=s1e1095]:\n                    - /url: "#vae%EF%BC%88variational-auto-encoder%EF%BC%89"\n                - list [ref=s1e1096]:\n                    - listitem [ref=s1e1097]:\n                        - link "VAEと生成モデル" [ref=s1e1098]:\n                            - /url: "#vae%E3%81%A8%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB"\n                    - listitem [ref=s1e1099]:\n                        - link "VAEの挙動・振る舞い" [ref=s1e1100]:\n                            - /url: "#vae%E3%81%AE%E6%8C%99%E5%8B%95%E3%83%BB%E6%8C%AF%E3%82%8B%E8%88%9E%E3%8\\\n                                1%84"\n                    - listitem [ref=s1e1101]:\n                        - link "VAEを生成モデルとして利用する" [ref=s1e1102]:\n                            - /url: "#vae%E3%82%92%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB%E3%81%A8%E3%8\\\n                                1%97%E3%81%A6%E5%88%A9%E7%94%A8%E3%81%99%E3%82%\\\n                                8B"\n            - listitem [ref=s1e1103]:\n                - link "最後にVAEの課題" [ref=s1e1104]:\n                    - /url: "#%E6%9C%80%E5%BE%8C%E3%81%ABvae%E3%81%AE%E8%AA%B2%E9%A1%8C"\n                - list [ref=s1e1105]:\n                    - listitem [ref=s1e1106]:\n                        - link "Posterior Collapse" [ref=s1e1107]:\n                            - /url: "#posterior-collapse"\n                    - listitem [ref=s1e1108]:\n                        - link "画像がボケやすい" [ref=s1e1109]:\n                            - /url: "#%E7%94%BB%E5%83%8F%E3%81%8C%E3%83%9C%E3%82%B1%E3%82%84%E3%81%99%E3%81%8\\\n                                4"\n            - listitem [ref=s1e1110]:\n                - link "まとめ" [ref=s1e1111]:\n                    - /url: "#%E3%81%BE%E3%81%A8%E3%82%81"\n            - listitem [ref=s1e1112]:\n                - link "参考文献" [ref=s1e1113]:\n                    - /url: "#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE-1"\n        - paragraph [ref=s1e1115]: Zennからのお知らせ\n        - article [ref=s1e1118]:\n            - link "第２回 AI Agent Hackathon with Google Cloud 第２回 AI Agent Hackathon with Google Cloud 4/14登録受付開始！" [ref=s1e1119]:\n                - /url: /hackathons/google-cloud-japan-ai-hackathon-vol2\n                - img "第２回 AI Agent Hackathon with Google Cloud" [ref=s1e1121]\n                - paragraph [ref=s1e1122]: 第２回 AI Agent Hackathon with Google Cloud 4/14登録受付開始！\n        - article [ref=s1e1123]:\n            - link "Zenncafe日比谷#2 生成AIで変わる仕事 Zenncafe日比谷#2 生成AIで変わる仕事" [ref=s1e1124]:\n                - /url: https://zenn.connpass.com/event/351131/\n                - img "Zenncafe日比谷#2 生成AIで変わる仕事" [ref=s1e1126]\n                - paragraph [ref=s1e1127]: Zenncafe日比谷#2 生成AIで変わる仕事\n- contentinfo [ref=s1e1129]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1133]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1134]\n    - paragraph [ref=s1e1142]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e1144]:\n        - heading "About" [level=4] [ref=s1e1145]\n        - list [ref=s1e1146]:\n            - listitem [ref=s1e1147]:\n                - link "Zennについて" [ref=s1e1149]:\n                    - /url: /about\n            - listitem [ref=s1e1150]:\n                - link "運営会社" [ref=s1e1151]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e1152]:\n                - link "お知らせ・リリース" [ref=s1e1153]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e1154]:\n                - link "イベント" [ref=s1e1156]:\n                    - /url: /events\n    - navigation [ref=s1e1157]:\n        - heading "Guides" [level=4] [ref=s1e1158]\n        - list [ref=s1e1159]:\n            - listitem [ref=s1e1160]:\n                - link "使い方" [ref=s1e1162]:\n                    - /url: /manual\n            - listitem [ref=s1e1163]:\n                - link "法人向けメニュー" [ref=s1e1165]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e1167]:\n                - link "Publication / Pro" [ref=s1e1169]:\n                    - /url: /publications\n            - listitem [ref=s1e1170]:\n                - link "よくある質問" [ref=s1e1172]:\n                    - /url: /faq\n    - navigation [ref=s1e1173]:\n        - heading "Links" [level=4] [ref=s1e1174]\n        - list [ref=s1e1175]:\n            - listitem [ref=s1e1176]:\n                - link "X(Twitter)" [ref=s1e1177]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e1178]:\n                - link "GitHub" [ref=s1e1179]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e1180]:\n                - link "メディアキット" [ref=s1e1182]:\n                    - /url: /mediakit\n    - navigation [ref=s1e1183]:\n        - heading "Legal" [level=4] [ref=s1e1184]\n        - list [ref=s1e1185]:\n            - listitem [ref=s1e1186]:\n                - link "利用規約" [ref=s1e1188]:\n                    - /url: /terms\n            - listitem [ref=s1e1189]:\n                - link "プライバシーポリシー" [ref=s1e1191]:\n                    - /url: /privacy\n            - listitem [ref=s1e1192]:\n                - link "特商法表記" [ref=s1e1194]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e1197]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e1198]\n- alert [ref=s1e1217]\n```\n', name='browser_navigate', tool_call_id='45a87836-eb09-4499-bf44-a2bcdbd3ec2b'), AIMessage(content='「VAEって結局何者なの？」という記事は、生成モデルの一つであるVAE（Variational Autoencoder）について解説した記事です。\n\n記事の内容をまとめると、以下のようになります。\n\n*   **VAEとは:** VAEは、生成モデルとして重要な「対数尤度の最大化」問題を解くために必要な構造を用意したら、結果的にAuto Encoderのような形になったというモデルです。\n*   **Auto Encoderとの違い:** VAEを理解するためには、まずAuto Encoderを理解する必要があります。Auto Encoderは、入力データと出力を一致させるように学習させるネットワークですが、それ自体では生成モデルとして活用することが難しいという課題があります。\n*   **VAEの仕組み:** VAEは、Encoderで入力データを潜在表現と呼ばれる低次元のデータに圧縮し、Decoderでその潜在表現から元のデータを復元します。この際、Encoderは潜在表現の平均と分散を出力し、その分布からサンプリングされた値をDecoderに入力します。これにより、VAEは潜在空間に滑らかさを持ち、生成モデルとして機能することができます。\n*   **VAEの学習:** VAEは、ELBO（Evidence Lower Bound）と呼ばれる目的関数を最大化するように学習されます。ELBOは、再構成誤差と正則化項の和で表され、VAEはこのバランスをとりながら学習を進めます。\n*   **VAEの課題:** VAEには、Posterior Collapse（事後崩壊）や生成される画像がボケやすいといった課題があります。これらの課題は、潜在表現の次元数や正則化項の強さを調整することで、ある程度緩和することができます。\n*   **Stable DiffusionにおけるVAE:** Stable Diffusionでは、VAEが画像のエンコード・デコードを担当しています。ただし、Stable Diffusionでは、VAEの潜在表現の事前分布が標準正規分布から大きくずれており、VAEはAuto Encoderに近い挙動をしています。\n\nこの記事は、VAEの基本的な仕組みから、学習方法、課題、そしてStable DiffusionにおけるVAEの役割まで、幅広く解説しています。特に、VAEの潜在空間の性質や、Stable DiffusionにおけるVAEの挙動について詳しく解説している点が特徴です。', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-bf93ed43-18df-4eb1-bd5a-f1423de25a27-0', usage_metadata={'input_tokens': 70378, 'output_tokens': 467, 'total_tokens': 70845, 'input_token_details': {'cache_read': 0}}), HumanMessage(content=[{'type': 'text', 'text': '3番目のいいねが多い記事は？'}], additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-ed215635-eb93-42e2-8508-9905e26caa2a-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap'}, 'id': 'fb8928e6-1802-4cb0-a1b1-235173be5629', 'type': 'tool_call'}], usage_metadata={'input_tokens': 70853, 'output_tokens': 13, 'total_tokens': 70866, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap\n\n- Page URL: https://zenn.dev/asap\n- Page Title: asapさんの記事一覧 | Zenn\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- banner [ref=s1e32]:\n    - img "asap" [ref=s1e36]\n    - heading "asap" [level=1] [ref=s1e39]\n    - button "フォロー" [ref=s1e41]\n    - paragraph [ref=s1e43]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n    - button "2,214 Likes" [ref=s1e46]\n    - button "310 Followers" [ref=s1e48]\n    - link "@personabb" [ref=s1e51]:\n        - /url: https://github.com/personabb\n        - tooltip "@personabb" [ref=s1e52]:\n            - img "GitHub" [ref=s1e53]\n            - text: "@personabb"\n    - link "@asap2650" [ref=s1e55]:\n        - /url: https://twitter.com/asap2650\n        - tooltip "@asap2650" [ref=s1e56]:\n            - img "X(Twitter)" [ref=s1e57]\n            - text: "@asap2650"\n    - link "yard.tips" [ref=s1e63]:\n        - /url: https://yard.tips/asap2650\n        - tooltip "yard.tips" [ref=s1e64]:\n            - img [ref=s1e65]\n            - text: yard.tips\n    - link "RSS" [ref=s1e67]:\n        - /url: /asap/feed\n        - tooltip "RSS" [ref=s1e68]:\n            - img [ref=s1e69]\n            - text: RSS\n- link "Articles 72" [ref=s1e80]:\n    - /url: /asap\n- link "Scraps 5" [ref=s1e82]:\n    - /url: /asap?tab=scraps\n- link "Comments" [ref=s1e84]:\n    - /url: /asap?tab=comments\n- contentinfo [ref=s1e88]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e92]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e93]\n    - paragraph [ref=s1e101]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e103]:\n        - heading "About" [level=4] [ref=s1e104]\n        - list [ref=s1e105]:\n            - listitem [ref=s1e106]:\n                - link "Zennについて" [ref=s1e108]:\n                    - /url: /about\n            - listitem [ref=s1e109]:\n                - link "運営会社" [ref=s1e110]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e111]:\n                - link "お知らせ・リリース" [ref=s1e112]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e113]:\n                - link "イベント" [ref=s1e115]:\n                    - /url: /events\n    - navigation [ref=s1e116]:\n        - heading "Guides" [level=4] [ref=s1e117]\n        - list [ref=s1e118]:\n            - listitem [ref=s1e119]:\n                - link "使い方" [ref=s1e121]:\n                    - /url: /manual\n            - listitem [ref=s1e122]:\n                - link "法人向けメニュー" [ref=s1e124]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e126]:\n                - link "Publication / Pro" [ref=s1e128]:\n                    - /url: /publications\n            - listitem [ref=s1e129]:\n                - link "よくある質問" [ref=s1e131]:\n                    - /url: /faq\n    - navigation [ref=s1e132]:\n        - heading "Links" [level=4] [ref=s1e133]\n        - list [ref=s1e134]:\n            - listitem [ref=s1e135]:\n                - link "X(Twitter)" [ref=s1e136]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e137]:\n                - link "GitHub" [ref=s1e138]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e139]:\n                - link "メディアキット" [ref=s1e141]:\n                    - /url: /mediakit\n    - navigation [ref=s1e142]:\n        - heading "Legal" [level=4] [ref=s1e143]\n        - list [ref=s1e144]:\n            - listitem [ref=s1e145]:\n                - link "利用規約" [ref=s1e147]:\n                    - /url: /terms\n            - listitem [ref=s1e148]:\n                - link "プライバシーポリシー" [ref=s1e150]:\n                    - /url: /privacy\n            - listitem [ref=s1e151]:\n                - link "特商法表記" [ref=s1e153]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e156]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e157]\n- alert [ref=s1e176]\n```\n', name='browser_navigate', tool_call_id='fb8928e6-1802-4cb0-a1b1-235173be5629'), AIMessage(content='asapさんのZennの記事の中で3番目にいいねが多い記事は、「💯 拡散モデルの理論って面白いよね」で、いいね数は87です。', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-db79d227-62d3-4b10-ab28-7edd9f261c16-0', usage_metadata={'input_tokens': 72673, 'output_tokens': 35, 'total_tokens': 72708, 'input_token_details': {'cache_read': 0}})]}
=================================
asapさんのZennの記事の中で3番目にいいねが多い記事は、「💯 拡散モデルの理論って面白いよね」で、いいね数は87です。








入力してください: 4番目にいいねが多い記事は？中身も解説して
response:  {'messages': [HumanMessage(content=[{'type': 'text', 'text': '「https://zenn.dev/asap」この執筆者のもっともいいねがついている記事を調査して、その記事の中身を取得し、その記事がなぜ評価されているのかを分析してください。'}], additional_kwargs={}, response_metadata={}), AIMessage(content='zenn.devのasap氏の最もいいねが多い記事を調べ、その内容を取得し、評価されている理由を分析する、というご要望ですね。\n\nまず、zenn.dev/asapにアクセスして、最もいいねが多い記事のタイトルとURLを特定する必要があります。その後、そのURLにアクセスして記事の内容を取得します。最後に、記事の内容から評価されている理由を推測します。\n\nでは、始めましょう。\n\n1.  zenn.dev/asapにアクセスし、最もいいねが多い記事の情報を取得します。', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-989fe5ca-2637-4757-878f-db26e13526b8-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap'}, 'id': '58fad86e-0b01-4fc6-a8cc-57deb5c48b41', 'type': 'tool_call'}], usage_metadata={'input_tokens': 778, 'output_tokens': 128, 'total_tokens': 906, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap\n\n- Page URL: https://zenn.dev/asap\n- Page Title: asapさんの記事一覧 | Zenn\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- banner [ref=s1e32]:\n    - img "asap" [ref=s1e36]\n    - heading "asap" [level=1] [ref=s1e39]\n    - button "フォロー" [ref=s1e41]\n    - paragraph [ref=s1e43]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n    - button "2,214 Likes" [ref=s1e46]\n    - button "310 Followers" [ref=s1e48]\n    - link "@personabb" [ref=s1e51]:\n        - /url: https://github.com/personabb\n        - tooltip "@personabb" [ref=s1e52]:\n            - img "GitHub" [ref=s1e53]\n            - text: "@personabb"\n    - link "@asap2650" [ref=s1e55]:\n        - /url: https://twitter.com/asap2650\n        - tooltip "@asap2650" [ref=s1e56]:\n            - img "X(Twitter)" [ref=s1e57]\n            - text: "@asap2650"\n    - link "yard.tips" [ref=s1e63]:\n        - /url: https://yard.tips/asap2650\n        - tooltip "yard.tips" [ref=s1e64]:\n            - img [ref=s1e65]\n            - text: yard.tips\n    - link "RSS" [ref=s1e67]:\n        - /url: /asap/feed\n        - tooltip "RSS" [ref=s1e68]:\n            - img [ref=s1e69]\n            - text: RSS\n- link "Articles 72" [ref=s1e80]:\n    - /url: /asap\n- link "Scraps 5" [ref=s1e82]:\n    - /url: /asap?tab=scraps\n- link "Comments" [ref=s1e84]:\n    - /url: /asap?tab=comments\n- article [ref=s1e89]:\n    - link "tech" [ref=s1e90]:\n        - /url: /tech-or-idea\n    - link "❄️ LangChainでGemini-2.0 APIの画像編集を実行する方法 6日前 いいねされた数 18" [ref=s1e91]:\n        - /url: /asap/articles/1d5e6ec06c380f\n        - text: ❄️\n        - heading "LangChainでGemini-2.0 APIの画像編集を実行する方法" [level=3] [ref=s1e95]\n        - time [ref=s1e98]: 6日前\n        - img "いいねされた数" [ref=s1e100]\n        - text: "18"\n- article [ref=s1e102]:\n    - link "tech" [ref=s1e103]:\n        - /url: /tech-or-idea\n    - link "🙌 完全自律型AIエージェントOSS【LangManus】完全に理解した！ 13日前 いいねされた数 48" [ref=s1e104]:\n        - /url: /asap/articles/b27e832c2a49b7\n        - text: 🙌\n        - heading "完全自律型AIエージェントOSS【LangManus】完全に理解した！" [level=3] [ref=s1e108]\n        - time [ref=s1e111]: 13日前\n        - img "いいねされた数" [ref=s1e113]\n        - text: "48"\n- article [ref=s1e115]:\n    - link "tech" [ref=s1e116]:\n        - /url: /tech-or-idea\n    - link "💫 LangChainでOpenAI gpt-4o / o1にPDFを入力する方法 20日前 いいねされた数 9" [ref=s1e117]:\n        - /url: /asap/articles/74885fab863c9e\n        - text: 💫\n        - heading "LangChainでOpenAI gpt-4o / o1にPDFを入力する方法" [level=3]\n          [ref=s1e121]\n        - time [ref=s1e124]: 20日前\n        - img "いいねされた数" [ref=s1e126]\n        - text: "9"\n- article [ref=s1e128]:\n    - link "tech" [ref=s1e129]:\n        - /url: /tech-or-idea\n    - link "👣 Ollama + LangChainで画像を入力する方法【Gemma3 27B】 27日前 いいねされた数 14" [ref=s1e130]:\n        - /url: /asap/articles/18b110d551b811\n        - text: 👣\n        - heading "Ollama + LangChainで画像を入力する方法【Gemma3 27B】" [level=3]\n          [ref=s1e134]\n        - time [ref=s1e137]: 27日前\n        - img "いいねされた数" [ref=s1e139]\n        - text: "14"\n- article [ref=s1e141]:\n    - link "tech" [ref=s1e142]:\n        - /url: /tech-or-idea\n    - link "🦄 Google CloudのVector StoreでRAG Agentsを構築する【Big Querty編】 28日前 いいねされた数 12" [ref=s1e143]:\n        - /url: /asap/articles/3d258861c6c528\n        - text: 🦄\n        - heading "Google CloudのVector StoreでRAG Agentsを構築する【Big Querty編】"\n          [level=3] [ref=s1e147]\n        - time [ref=s1e150]: 28日前\n        - img "いいねされた数" [ref=s1e152]\n        - text: "12"\n- article [ref=s1e154]:\n    - link "tech" [ref=s1e155]:\n        - /url: /tech-or-idea\n    - link "🎶 最新音楽生成AI 【DiffRhythm】をGoogle Colabで動かせるようにしました！（簡易版） 1ヶ月前 いいねされた数 39" [ref=s1e156]:\n        - /url: /asap/articles/4edc4a71de8961\n        - text: 🎶\n        - heading "最新音楽生成AI 【DiffRhythm】をGoogle Colabで動かせるようにしました！（簡易版）"\n          [level=3] [ref=s1e160]\n        - time [ref=s1e163]: 1ヶ月前\n        - img "いいねされた数" [ref=s1e165]\n        - text: "39"\n- article [ref=s1e167]:\n    - link "tech" [ref=s1e168]:\n        - /url: /tech-or-idea\n    - link "🐤 LangGraphのTool Callingを利用して、RAG Agentsを構築する（後編） 1ヶ月前 いいねされた数 12" [ref=s1e169]:\n        - /url: /asap/articles/c58cfab9c1362a\n        - text: 🐤\n        - heading "LangGraphのTool Callingを利用して、RAG Agentsを構築する（後編）" [level=3]\n          [ref=s1e173]\n        - time [ref=s1e176]: 1ヶ月前\n        - img "いいねされた数" [ref=s1e178]\n        - text: "12"\n- article [ref=s1e180]:\n    - link "idea" [ref=s1e181]:\n        - /url: /tech-or-idea\n    - link "🏆 イベントに登壇してきた話【資料もこちら】 1ヶ月前 いいねされた数 20" [ref=s1e182]:\n        - /url: /asap/articles/89d2a13bdc75a3\n        - text: 🏆\n        - heading "イベントに登壇してきた話【資料もこちら】" [level=3] [ref=s1e186]\n        - time [ref=s1e189]: 1ヶ月前\n        - img "いいねされた数" [ref=s1e191]\n        - text: "20"\n- article [ref=s1e193]:\n    - link "tech" [ref=s1e194]:\n        - /url: /tech-or-idea\n    - link "👌 Grok API（Grok 2）で遊んでみた！【Elyza Tasks 100】 2ヶ月前 いいねされた数 11" [ref=s1e195]:\n        - /url: /asap/articles/6aa01bde32163d\n        - text: 👌\n        - heading "Grok API（Grok 2）で遊んでみた！【Elyza Tasks 100】" [level=3]\n          [ref=s1e199]\n        - time [ref=s1e202]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e204]\n        - text: "11"\n- article [ref=s1e206]:\n    - link "tech" [ref=s1e207]:\n        - /url: /tech-or-idea\n    - link "📑 Multi-head Latent Attention (MLA)の詳細【be absorbedとは？】 2ヶ月前 いいねされた数 22" [ref=s1e208]:\n        - /url: /asap/articles/be3d4b60d8ac92\n        - text: 📑\n        - heading "Multi-head Latent Attention (MLA)の詳細【be absorbedとは？】"\n          [level=3] [ref=s1e212]\n        - time [ref=s1e215]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e217]\n        - text: "22"\n- article [ref=s1e219]:\n    - link "tech" [ref=s1e220]:\n        - /url: /tech-or-idea\n    - link "⛳ LangGraphのTool Callingを利用して、RAG Agentsを構築する（前編） 2ヶ月前 いいねされた数 46" [ref=s1e221]:\n        - /url: /asap/articles/2ad7eb5e7560cf\n        - text: ⛳\n        - heading "LangGraphのTool Callingを利用して、RAG Agentsを構築する（前編）" [level=3]\n          [ref=s1e225]\n        - time [ref=s1e228]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e230]\n        - text: "46"\n- article [ref=s1e232]:\n    - link "tech" [ref=s1e233]:\n        - /url: /tech-or-idea\n    - link "🫠 【LangChain】で【Transformer】を利用する場合に【chat_template】を設定する方法【エラー対処記事】 2ヶ月前 いいねされた数 13" [ref=s1e234]:\n        - /url: /asap/articles/985f3580ea476d\n        - text: 🫠\n        - heading\n          "【LangChain】で【Transformer】を利用する場合に【chat_template】を設定する方法【エラー対処記事】"\n          [level=3] [ref=s1e238]\n        - time [ref=s1e241]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e243]\n        - text: "13"\n- article [ref=s1e245]:\n    - link "tech" [ref=s1e246]:\n        - /url: /tech-or-idea\n    - link "🌊 採点理由を出力しながらローカルLLMを評価する【Elyza Tasks 100】 2ヶ月前 いいねされた数 26" [ref=s1e247]:\n        - /url: /asap/articles/3a99c34fc1d4cb\n        - text: 🌊\n        - heading "採点理由を出力しながらローカルLLMを評価する【Elyza Tasks 100】" [level=3]\n          [ref=s1e251]\n        - time [ref=s1e254]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e256]\n        - text: "26"\n- article [ref=s1e258]:\n    - link "tech" [ref=s1e259]:\n        - /url: /tech-or-idea\n    - link "📑 DeepSeek-R1の論文読んだ？【勉強になるよ】 2ヶ月前 いいねされた数 921" [ref=s1e260]:\n        - /url: /asap/articles/34237ad87f8511\n        - text: 📑\n        - heading "DeepSeek-R1の論文読んだ？【勉強になるよ】" [level=3] [ref=s1e264]\n        - time [ref=s1e267]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e269]\n        - text: "921"\n- article [ref=s1e271]:\n    - link "tech" [ref=s1e272]:\n        - /url: /tech-or-idea\n    - link "📖 VAEって結局何者なの？ 3ヶ月前 いいねされた数 104" [ref=s1e273]:\n        - /url: /asap/articles/6caa9043276424\n        - text: 📖\n        - heading "VAEって結局何者なの？" [level=3] [ref=s1e277]\n        - time [ref=s1e280]: 3ヶ月前\n        - img "いいねされた数" [ref=s1e282]\n        - text: "104"\n- article [ref=s1e284]:\n    - link "tech" [ref=s1e285]:\n        - /url: /tech-or-idea\n    - link "🦔 初めてのSageMaker AIでSFTTrainerによるLLMのフルパラメータファインチューニングを試す 3ヶ月前 いいねされた数 9" [ref=s1e286]:\n        - /url: /asap/articles/3f31b77010624c\n        - text: 🦔\n        - heading "初めてのSageMaker AIでSFTTrainerによるLLMのフルパラメータファインチューニングを試す"\n          [level=3] [ref=s1e290]\n        - time [ref=s1e293]: 3ヶ月前\n        - img "いいねされた数" [ref=s1e295]\n        - text: "9"\n- article [ref=s1e297]:\n    - link "tech" [ref=s1e298]:\n        - /url: /tech-or-idea\n    - link "😺 【動画翻訳】OpenAIの研究者によるo1やLLMに対するディスカッションの翻訳 3ヶ月前 いいねされた数 12" [ref=s1e299]:\n        - /url: /asap/articles/2d2832bf3288aa\n        - text: 😺\n        - heading "【動画翻訳】OpenAIの研究者によるo1やLLMに対するディスカッションの翻訳" [level=3]\n          [ref=s1e303]\n        - time [ref=s1e306]: 3ヶ月前\n        - img "いいねされた数" [ref=s1e308]\n        - text: "12"\n- article [ref=s1e310]:\n    - link "tech" [ref=s1e311]:\n        - /url: /tech-or-idea\n    - link "🧚\u200d♀️ AIでオリジナル楽曲を作り、自作のかわいい声で歌ってもらう方法 3ヶ月前 いいねされた数 28" [ref=s1e312]:\n        - /url: /asap/articles/886af0fe48dda3\n        - text: 🧚\u200d♀️\n        - heading "AIでオリジナル楽曲を作り、自作のかわいい声で歌ってもらう方法" [level=3] [ref=s1e316]\n        - time [ref=s1e319]: 3ヶ月前\n        - img "いいねされた数" [ref=s1e321]\n        - text: "28"\n- article [ref=s1e323]:\n    - link "tech" [ref=s1e324]:\n        - /url: /tech-or-idea\n    - link "🎮 Google ColabでSeed-VCによる声色変換を試す 4ヶ月前 いいねされた数 27" [ref=s1e325]:\n        - /url: /asap/articles/9c54aef739a6ed\n        - text: 🎮\n        - heading "Google ColabでSeed-VCによる声色変換を試す" [level=3] [ref=s1e329]\n        - time [ref=s1e332]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e334]\n        - text: "27"\n- article [ref=s1e336]:\n    - link "tech" [ref=s1e337]:\n        - /url: /tech-or-idea\n    - link "🐡 LLMの性能における重要補題【Johnson–Lindenstrauss lemma】のお気持ち〜Attentionの数式理解を添えて〜 4ヶ月前 いいねされた数 21" [ref=s1e338]:\n        - /url: /asap/articles/6bdf89d8e964ec\n        - text: 🐡\n        - heading "LLMの性能における重要補題【Johnson–Lindenstrauss\n          lemma】のお気持ち〜Attentionの数式理解を添えて〜" [level=3] [ref=s1e342]\n        - time [ref=s1e345]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e347]\n        - text: "21"\n- article [ref=s1e349]:\n    - link "tech" [ref=s1e350]:\n        - /url: /tech-or-idea\n    - link "📏 【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】 4ヶ月前 いいねされた数 50" [ref=s1e351]:\n        - /url: /asap/articles/ea9f42edff6573\n        - text: 📏\n        - heading "【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】"\n          [level=3] [ref=s1e355]\n        - time [ref=s1e358]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e360]\n        - text: "50"\n- article [ref=s1e362]:\n    - link "tech" [ref=s1e363]:\n        - /url: /tech-or-idea\n    - link "🎙️ 高品質合成音声モデル【Style-Bert-VITS2】をGoogle Colabで学習し、【Aivis Speech】で利用する方法 4ヶ月前 いいねされた数 24" [ref=s1e364]:\n        - /url: /asap/articles/5c437ce2943ed7\n        - text: 🎙️\n        - heading "高品質合成音声モデル【Style-Bert-VITS2】をGoogle Colabで学習し、【Aivis\n          Speech】で利用する方法" [level=3] [ref=s1e368]\n        - time [ref=s1e371]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e373]\n        - text: "24"\n- article [ref=s1e375]:\n    - link "tech" [ref=s1e376]:\n        - /url: /tech-or-idea\n    - link "🎄 【LangChain】LCEL ステップバイステップ ハンズオン 〜なろう短編小説の自動生成＆評価・改善を行う「AIエージェント」を添えて〜 4ヶ月前 いいねされた数 19" [ref=s1e377]:\n        - /url: /asap/articles/aa587ad8d76105\n        - text: 🎄\n        - heading "【LangChain】LCEL ステップバイステップ ハンズオン\n          〜なろう短編小説の自動生成＆評価・改善を行う「AIエージェント」を添えて〜" [level=3] [ref=s1e381]\n        - time [ref=s1e384]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e386]\n        - text: "19"\n- article [ref=s1e388]:\n    - link "tech" [ref=s1e389]:\n        - /url: /tech-or-idea\n    - link "💻 LangGraphによる「AIエージェントWebアプリ」を作成する【Next.js】 4ヶ月前 いいねされた数 45" [ref=s1e390]:\n        - /url: /asap/articles/a350fe4ac00343\n        - text: 💻\n        - heading "LangGraphによる「AIエージェントWebアプリ」を作成する【Next.js】" [level=3]\n          [ref=s1e394]\n        - time [ref=s1e397]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e399]\n        - text: "45"\n- article [ref=s1e401]:\n    - link "tech" [ref=s1e402]:\n        - /url: /tech-or-idea\n    - link "🤔 LangGraphにて入力情報が不十分な場合に、ユーザへ追加質問をするAIエージェントを作る方法 5ヶ月前 いいねされた数 41" [ref=s1e403]:\n        - /url: /asap/articles/03548f844efe6f\n        - text: 🤔\n        - heading "LangGraphにて入力情報が不十分な場合に、ユーザへ追加質問をするAIエージェントを作る方法" [level=3]\n          [ref=s1e407]\n        - time [ref=s1e410]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e412]\n        - text: "41"\n- article [ref=s1e414]:\n    - link "tech" [ref=s1e415]:\n        - /url: /tech-or-idea\n    - link "🤖 高品質動画生成AI【Allegro】をローカルで実行する【VRAM12GB】 5ヶ月前 いいねされた数 10" [ref=s1e416]:\n        - /url: /asap/articles/dac57655eb0314\n        - text: 🤖\n        - heading "高品質動画生成AI【Allegro】をローカルで実行する【VRAM12GB】" [level=3] [ref=s1e420]\n        - time [ref=s1e423]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e425]\n        - text: "10"\n- article [ref=s1e427]:\n    - link "tech" [ref=s1e428]:\n        - /url: /tech-or-idea\n    - link "🌊 フレーム補間技術【RIFE】を利用してCogVideoXのFPSを改善しよう 5ヶ月前 いいねされた数 2" [ref=s1e429]:\n        - /url: /asap/articles/dbeef72b7bfdc3\n        - text: 🌊\n        - heading "フレーム補間技術【RIFE】を利用してCogVideoXのFPSを改善しよう" [level=3] [ref=s1e433]\n        - time [ref=s1e436]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e438]\n        - text: "2"\n- article [ref=s1e440]:\n    - link "tech" [ref=s1e441]:\n        - /url: /tech-or-idea\n    - link "🔥 LangGraphを使ってAIエージェントを実装する方法 5ヶ月前 いいねされた数 34" [ref=s1e442]:\n        - /url: /asap/articles/5da9cf01703a47\n        - text: 🔥\n        - heading "LangGraphを使ってAIエージェントを実装する方法" [level=3] [ref=s1e446]\n        - time [ref=s1e449]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e451]\n        - text: "34"\n- article [ref=s1e453]:\n    - link "tech" [ref=s1e454]:\n        - /url: /tech-or-idea\n    - link "🌈 動画生成AI【CogVideoX】をローカルで実行する 5ヶ月前 いいねされた数 9" [ref=s1e455]:\n        - /url: /asap/articles/a4077e8b9ddf1a\n        - text: 🌈\n        - heading "動画生成AI【CogVideoX】をローカルで実行する" [level=3] [ref=s1e459]\n        - time [ref=s1e462]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e464]\n        - text: "9"\n- article [ref=s1e466]:\n    - link "tech" [ref=s1e467]:\n        - /url: /tech-or-idea\n    - link "🛎️ ローカルでDIfyを使ってAIエージェントを実装する方法 5ヶ月前 いいねされた数 55" [ref=s1e468]:\n        - /url: /asap/articles/a35254020fbbae\n        - text: 🛎️\n        - heading "ローカルでDIfyを使ってAIエージェントを実装する方法" [level=3] [ref=s1e472]\n        - time [ref=s1e475]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e477]\n        - text: "55"\n- article [ref=s1e479]:\n    - link "tech" [ref=s1e480]:\n        - /url: /tech-or-idea\n    - link "💫 高品質動画生成AI【Pyramid Flow】をローカルで実行する 6ヶ月前 いいねされた数 12" [ref=s1e481]:\n        - /url: /asap/articles/e1880903625cb5\n        - text: 💫\n        - heading "高品質動画生成AI【Pyramid Flow】をローカルで実行する" [level=3] [ref=s1e485]\n        - time [ref=s1e488]: 6ヶ月前\n        - img "いいねされた数" [ref=s1e490]\n        - text: "12"\n- article [ref=s1e492]:\n    - link "tech" [ref=s1e493]:\n        - /url: /tech-or-idea\n    - link "📖 不偏分散で N-1 で割る理由 を自力で導出する 6ヶ月前 いいねされた数 5" [ref=s1e494]:\n        - /url: /asap/articles/f0cba34c2ac2ac\n        - text: 📖\n        - heading "不偏分散で N-1 で割る理由 を自力で導出する" [level=3] [ref=s1e498]\n        - time [ref=s1e501]: 6ヶ月前\n        - img "いいねされた数" [ref=s1e503]\n        - text: "5"\n- article [ref=s1e505]:\n    - link "tech" [ref=s1e506]:\n        - /url: /tech-or-idea\n    - link "✍️ 勾配降下法 ・ 誤差逆伝播法 を自力で導出する 6ヶ月前 いいねされた数 25" [ref=s1e507]:\n        - /url: /asap/articles/643f604d444813\n        - text: ✍️\n        - heading "勾配降下法 ・ 誤差逆伝播法 を自力で導出する" [level=3] [ref=s1e511]\n        - time [ref=s1e514]: 6ヶ月前\n        - img "いいねされた数" [ref=s1e516]\n        - text: "25"\n- article [ref=s1e518]:\n    - link "tech" [ref=s1e519]:\n        - /url: /tech-or-idea\n    - link "💰 Open AI Realtime APIで、会話履歴を削除して高額請求を回避したい！【Pythonサンプルコード】 2024/10/07 いいねされた数 25" [ref=s1e520]:\n        - /url: /asap/articles/af07fcedbbef61\n        - text: 💰\n        - heading "Open AI Realtime APIで、会話履歴を削除して高額請求を回避したい！【Pythonサンプルコード】"\n          [level=3] [ref=s1e524]\n        - time [ref=s1e527]: 2024/10/07\n        - img "いいねされた数" [ref=s1e529]\n        - text: "25"\n- article [ref=s1e531]:\n    - link "tech" [ref=s1e532]:\n        - /url: /tech-or-idea\n    - link "✈️ Open AI Realtime APIのPythonサンプルコードで、AI発話にカットイン（割り込み）する方法 2024/10/06 いいねされた数 24" [ref=s1e533]:\n        - /url: /asap/articles/563500af4649da\n        - text: ✈️\n        - heading "Open AI Realtime APIのPythonサンプルコードで、AI発話にカットイン（割り込み）する方法"\n          [level=3] [ref=s1e537]\n        - time [ref=s1e540]: 2024/10/06\n        - img "いいねされた数" [ref=s1e542]\n        - text: "24"\n- article [ref=s1e544]:\n    - link "tech" [ref=s1e545]:\n        - /url: /tech-or-idea\n    - link "🥰 Open AI Realtime APIのPythonサンプルコードを作成 2024/10/04 いいねされた数 79" [ref=s1e546]:\n        - /url: /asap/articles/4368fd306b592a\n        - text: 🥰\n        - heading "Open AI Realtime APIのPythonサンプルコードを作成" [level=3] [ref=s1e550]\n        - time [ref=s1e553]: 2024/10/04\n        - img "いいねされた数" [ref=s1e555]\n        - text: "79"\n- article [ref=s1e557]:\n    - link "tech" [ref=s1e558]:\n        - /url: /tech-or-idea\n    - link "📗 主成分分析を自力で導出する 2024/10/03 いいねされた数 6" [ref=s1e559]:\n        - /url: /asap/articles/ff8f34d19ca6a4\n        - text: 📗\n        - heading "主成分分析を自力で導出する" [level=3] [ref=s1e563]\n        - time [ref=s1e566]: 2024/10/03\n        - img "いいねされた数" [ref=s1e568]\n        - text: "6"\n- article [ref=s1e570]:\n    - link "tech" [ref=s1e571]:\n        - /url: /tech-or-idea\n    - link "😍 FLUX.1-dev で ControlNet-depth と LoRA【QTCanimatedFeeling-FLUX】 を試す 2024/10/02 いいねされた数 3" [ref=s1e572]:\n        - /url: /asap/articles/9192c2363dd96a\n        - text: 😍\n        - heading "FLUX.1-dev で ControlNet-depth と LoRA【QTCanimatedFeeling-FLUX】\n          を試す" [level=3] [ref=s1e576]\n        - time [ref=s1e579]: 2024/10/02\n        - img "いいねされた数" [ref=s1e581]\n        - text: "3"\n- article [ref=s1e583]:\n    - link "tech" [ref=s1e584]:\n        - /url: /tech-or-idea\n    - link "📗 拡散モデルの理論について（拡散過程の条件付き確率・逆拡散過程のスケジューラ） 2024/09/23 いいねされた数 8" [ref=s1e585]:\n        - /url: /asap/articles/8aaa896a02f168\n        - text: 📗\n        - heading "拡散モデルの理論について（拡散過程の条件付き確率・逆拡散過程のスケジューラ）" [level=3] [ref=s1e589]\n        - time [ref=s1e592]: 2024/09/23\n        - img "いいねされた数" [ref=s1e594]\n        - text: "8"\n- article [ref=s1e596]:\n    - link "tech" [ref=s1e597]:\n        - /url: /tech-or-idea\n    - link "📖 分散共分散行列の魅力 2024/09/20 いいねされた数 14" [ref=s1e598]:\n        - /url: /asap/articles/2269cd74c5ddc2\n        - text: 📖\n        - heading "分散共分散行列の魅力" [level=3] [ref=s1e602]\n        - time [ref=s1e605]: 2024/09/20\n        - img "いいねされた数" [ref=s1e607]\n        - text: "14"\n- article [ref=s1e609]:\n    - link "tech" [ref=s1e610]:\n        - /url: /tech-or-idea\n    - link "😍 超強力汎用ControlNet「AnyTest v4」を利用する方法【diffusers】 2024/09/17 いいねされた数 9" [ref=s1e611]:\n        - /url: /asap/articles/c08dffa41bec03\n        - text: 😍\n        - heading "超強力汎用ControlNet「AnyTest v4」を利用する方法【diffusers】" [level=3]\n          [ref=s1e615]\n        - time [ref=s1e618]: 2024/09/17\n        - img "いいねされた数" [ref=s1e620]\n        - text: "9"\n- article [ref=s1e622]:\n    - link "tech" [ref=s1e623]:\n        - /url: /tech-or-idea\n    - link "🌍 FLUX.1-devでControlNetを利用したら画像の質が下がったのでImg2Imgを使って改善してみた【Diffusers】 2024/09/13 いいねされた数 2" [ref=s1e624]:\n        - /url: /asap/articles/912d6de879d006\n        - text: 🌍\n        - heading\n          "FLUX.1-devでControlNetを利用したら画像の質が下がったのでImg2Imgを使って改善してみた【Diffusers】"\n          [level=3] [ref=s1e628]\n        - time [ref=s1e631]: 2024/09/13\n        - img "いいねされた数" [ref=s1e633]\n        - text: "2"\n- article [ref=s1e635]:\n    - link "tech" [ref=s1e636]:\n        - /url: /tech-or-idea\n    - link "🎮 【Next.js】ポートレートの表情を編集する自作UIを作ってみた【ComfyUI-AdvancedLivePortrait】 2024/09/11 いいねされた数 8" [ref=s1e637]:\n        - /url: /asap/articles/c49551ff14d5e9\n        - text: 🎮\n        - heading\n          "【Next.js】ポートレートの表情を編集する自作UIを作ってみた【ComfyUI-AdvancedLivePortrait】"\n          [level=3] [ref=s1e641]\n        - time [ref=s1e644]: 2024/09/11\n        - img "いいねされた数" [ref=s1e646]\n        - text: "8"\n- article [ref=s1e648]:\n    - link "tech" [ref=s1e649]:\n        - /url: /tech-or-idea\n    - link "📖 ComfyUI-AdvancedLivePortraitを使ってみる（No WebUI）（Google Colab） 2024/09/10 いいねされた数 7" [ref=s1e650]:\n        - /url: /asap/articles/c0a1b82a10f0e2\n        - text: 📖\n        - heading "ComfyUI-AdvancedLivePortraitを使ってみる（No WebUI）（Google Colab）"\n          [level=3] [ref=s1e654]\n        - time [ref=s1e657]: 2024/09/10\n        - img "いいねされた数" [ref=s1e659]\n        - text: "7"\n- article [ref=s1e661]:\n    - link "tech" [ref=s1e662]:\n        - /url: /tech-or-idea\n    - link "⛳ websocketsでリアルタイム音声処理 2024/09/06 いいねされた数 9" [ref=s1e663]:\n        - /url: /asap/articles/47f839550e5535\n        - text: ⛳\n        - heading "websocketsでリアルタイム音声処理" [level=3] [ref=s1e667]\n        - time [ref=s1e670]: 2024/09/06\n        - img "いいねされた数" [ref=s1e672]\n        - text: "9"\n- article [ref=s1e674]:\n    - link "tech" [ref=s1e675]:\n        - /url: /tech-or-idea\n    - link "😍 生成AIをGoogle Colabで簡単に 【FLUX.1-dev + ControlNet + LoRA】【Diffusers】 2024/08/31 いいねされた数 9" [ref=s1e676]:\n        - /url: /asap/articles/e4c199cecf1836\n        - text: 😍\n        - heading "生成AIをGoogle Colabで簡単に 【FLUX.1-dev + ControlNet +\n          LoRA】【Diffusers】" [level=3] [ref=s1e680]\n        - time [ref=s1e683]: 2024/08/31\n        - img "いいねされた数" [ref=s1e685]\n        - text: "9"\n- article [ref=s1e687]:\n    - link "tech" [ref=s1e688]:\n        - /url: /tech-or-idea\n    - link "👋 Style-Bert-VITS2のAPI化 2024/08/28 いいねされた数 6" [ref=s1e689]:\n        - /url: /asap/articles/12b0f31625a90f\n        - text: 👋\n        - heading "Style-Bert-VITS2のAPI化" [level=3] [ref=s1e693]\n        - time [ref=s1e696]: 2024/08/28\n        - img "いいねされた数" [ref=s1e698]\n        - text: "6"\n- article [ref=s1e700]:\n    - link "tech" [ref=s1e701]:\n        - /url: /tech-or-idea\n    - link "😊 【超簡単】1枚の画像と音声から口パク動画を作りたい！【EchoMimic】 2024/08/25 いいねされた数 6" [ref=s1e702]:\n        - /url: /asap/articles/db1ed88f291b98\n        - text: 😊\n        - heading "【超簡単】1枚の画像と音声から口パク動画を作りたい！【EchoMimic】" [level=3] [ref=s1e706]\n        - time [ref=s1e709]: 2024/08/25\n        - img "いいねされた数" [ref=s1e711]\n        - text: "6"\n- article [ref=s1e713]:\n    - link "tech" [ref=s1e714]:\n        - /url: /tech-or-idea\n    - link "✍️ 【Next.js】生成AIによる「お絵描きサポートWebアプリ」を作りたい！【全くわからない人】 2024/08/20 いいねされた数 8" [ref=s1e715]:\n        - /url: /asap/articles/8f51ffd4fc29b8\n        - text: ✍️\n        - heading "【Next.js】生成AIによる「お絵描きサポートWebアプリ」を作りたい！【全くわからない人】" [level=3]\n          [ref=s1e719]\n        - time [ref=s1e722]: 2024/08/20\n        - img "いいねされた数" [ref=s1e724]\n        - text: "8"\n- article [ref=s1e726]:\n    - link "tech" [ref=s1e727]:\n        - /url: /tech-or-idea\n    - link "🔥 GPT-SoVITS ver2を学習する【WebUI使わない】 2024/08/15 いいねされた数 8" [ref=s1e728]:\n        - /url: /asap/articles/e64c6fce28e0f0\n        - text: 🔥\n        - heading "GPT-SoVITS ver2を学習する【WebUI使わない】" [level=3] [ref=s1e732]\n        - time [ref=s1e735]: 2024/08/15\n        - img "いいねされた数" [ref=s1e737]\n        - text: "8"\n- article [ref=s1e739]:\n    - link "tech" [ref=s1e740]:\n        - /url: /tech-or-idea\n    - link "🔥 GPT-soVITS ver2をモジュールとして動かす 2024/08/08 いいねされた数 9" [ref=s1e741]:\n        - /url: /asap/articles/964a7d863d8896\n        - text: 🔥\n        - heading "GPT-soVITS ver2をモジュールとして動かす" [level=3] [ref=s1e745]\n        - time [ref=s1e748]: 2024/08/08\n        - img "いいねされた数" [ref=s1e750]\n        - text: "9"\n- article [ref=s1e752]:\n    - link "tech" [ref=s1e753]:\n        - /url: /tech-or-idea\n    - link "💯 拡散モデルの理論って面白いよね 2024/08/07 いいねされた数 87" [ref=s1e754]:\n        - /url: /asap/articles/4092ab60570b05\n        - text: 💯\n        - heading "拡散モデルの理論って面白いよね" [level=3] [ref=s1e758]\n        - time [ref=s1e761]: 2024/08/07\n        - img "いいねされた数" [ref=s1e763]\n        - text: "87"\n- article [ref=s1e765]:\n    - link "tech" [ref=s1e766]:\n        - /url: /tech-or-idea\n    - link "🙆\u200d♀️ 画像生成AIの生成途中経過を可視化したい！【Diffusers】 2024/08/06 いいねされた数 4" [ref=s1e767]:\n        - /url: /asap/articles/7940b17be86da7\n        - text: 🙆\u200d♀️\n        - heading "画像生成AIの生成途中経過を可視化したい！【Diffusers】" [level=3] [ref=s1e771]\n        - time [ref=s1e774]: 2024/08/06\n        - img "いいねされた数" [ref=s1e776]\n        - text: "4"\n- article [ref=s1e778]:\n    - link "tech" [ref=s1e779]:\n        - /url: /tech-or-idea\n    - link "🚀 生成AIをGoogle Colabで簡単に 【Part10 SDXL+ANIMAGINE +ControlNet+LoRA編（全部載せ）】 2024/08/03 いいねされた数 3" [ref=s1e780]:\n        - /url: /asap/articles/aa856819e2a722\n        - text: 🚀\n        - heading "生成AIをGoogle Colabで簡単に 【Part10 SDXL+ANIMAGINE\n          +ControlNet+LoRA編（全部載せ）】" [level=3] [ref=s1e784]\n        - time [ref=s1e787]: 2024/08/03\n        - img "いいねされた数" [ref=s1e789]\n        - text: "3"\n- article [ref=s1e791]:\n    - link "tech" [ref=s1e792]:\n        - /url: /tech-or-idea\n    - link "😸 生成AIをGoogle Colaboratoryで簡単に 【Part6.5 画像生成AI SDXL+ANIMAGINE XL 3.1編】 2024/07/31 いいねされた数 3" [ref=s1e793]:\n        - /url: /asap/articles/f9ac2e83c820ff\n        - text: 😸\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part6.5 画像生成AI SDXL+ANIMAGINE\n          XL 3.1編】" [level=3] [ref=s1e797]\n        - time [ref=s1e800]: 2024/07/31\n        - img "いいねされた数" [ref=s1e802]\n        - text: "3"\n- article [ref=s1e804]:\n    - link "tech" [ref=s1e805]:\n        - /url: /tech-or-idea\n    - link "🎃 生成AIをGoogle Colaboratoryで簡単に 【Part9 画像生成AI SD3+ControlNet編】 2024/07/29 いいねされた数 3" [ref=s1e806]:\n        - /url: /asap/articles/53560859d273e4\n        - text: 🎃\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part9 画像生成AI SD3+ControlNet編】"\n          [level=3] [ref=s1e810]\n        - time [ref=s1e813]: 2024/07/29\n        - img "いいねされた数" [ref=s1e815]\n        - text: "3"\n- article [ref=s1e817]:\n    - link "tech" [ref=s1e818]:\n        - /url: /tech-or-idea\n    - link "✨ 生成AIをGoogle Colaboratoryで簡単に 【Part8 画像生成AI Stable Diffusion 3 Medium編】 2024/07/25 いいねされた数 4" [ref=s1e819]:\n        - /url: /asap/articles/6f6ca9aa0513d8\n        - text: ✨\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part8 画像生成AI Stable Diffusion 3\n          Medium編】" [level=3] [ref=s1e823]\n        - time [ref=s1e826]: 2024/07/25\n        - img "いいねされた数" [ref=s1e828]\n        - text: "4"\n- article [ref=s1e830]:\n    - link "tech" [ref=s1e831]:\n        - /url: /tech-or-idea\n    - link "🌟 生成AIをGoogle Colaboratoryで簡単に 【Part7 画像生成AI SDXL Turbo編】 2024/07/23 いいねされた数 2" [ref=s1e832]:\n        - /url: /asap/articles/ed58a6899c1c26\n        - text: 🌟\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part7 画像生成AI SDXL Turbo編】"\n          [level=3] [ref=s1e836]\n        - time [ref=s1e839]: 2024/07/23\n        - img "いいねされた数" [ref=s1e841]\n        - text: "2"\n- article [ref=s1e843]:\n    - link "tech" [ref=s1e844]:\n        - /url: /tech-or-idea\n    - link "🌟 生成AIをGoogle Colaboratoryで簡単に 【Part6 画像生成AI Stable Diffusion XL編】 2024/07/19 いいねされた数 4" [ref=s1e845]:\n        - /url: /asap/articles/e4791be54f90ce\n        - text: 🌟\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part6 画像生成AI Stable Diffusion\n          XL編】" [level=3] [ref=s1e849]\n        - time [ref=s1e852]: 2024/07/19\n        - img "いいねされた数" [ref=s1e854]\n        - text: "4"\n- article [ref=s1e856]:\n    - link "tech" [ref=s1e857]:\n        - /url: /tech-or-idea\n    - link "🎤 生成AIをローカルで簡単に 【Part5.5 faster-whisper+マイク録音編】 2024/07/16 いいねされた数 7" [ref=s1e858]:\n        - /url: /asap/articles/2c0d421e68ef16\n        - text: 🎤\n        - heading "生成AIをローカルで簡単に 【Part5.5 faster-whisper+マイク録音編】" [level=3]\n          [ref=s1e862]\n        - time [ref=s1e865]: 2024/07/16\n        - img "いいねされた数" [ref=s1e867]\n        - text: "7"\n- article [ref=s1e869]:\n    - link "tech" [ref=s1e870]:\n        - /url: /tech-or-idea\n    - link "📖 生成AIをGoogle Colaboratoryで簡単に 【Part5 音声認識AI faster-whisper編】 2024/07/13 いいねされた数 2" [ref=s1e871]:\n        - /url: /asap/articles/ba8fcb1880165e\n        - text: 📖\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part5 音声認識AI faster-whisper編】"\n          [level=3] [ref=s1e875]\n        - time [ref=s1e878]: 2024/07/13\n        - img "いいねされた数" [ref=s1e880]\n        - text: "2"\n- article [ref=s1e882]:\n    - link "tech" [ref=s1e883]:\n        - /url: /tech-or-idea\n    - link "😸 生成AIをGoogle Colaboratoryで簡単に 【Part4 音声認識AI Whisper API編】 2024/07/11 いいねされた数 2" [ref=s1e884]:\n        - /url: /asap/articles/ecaa5631e42195\n        - text: 😸\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part4 音声認識AI Whisper API編】"\n          [level=3] [ref=s1e888]\n        - time [ref=s1e891]: 2024/07/11\n        - img "いいねされた数" [ref=s1e893]\n        - text: "2"\n- article [ref=s1e895]:\n    - link "tech" [ref=s1e896]:\n        - /url: /tech-or-idea\n    - link "😊 生成AIをGoogle Colaboratoryで簡単に 【Part3 テキスト生成 ローカルLLM編】 2024/07/08 いいねされた数 4" [ref=s1e897]:\n        - /url: /asap/articles/8c3d5164d14f6e\n        - text: 😊\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part3 テキスト生成 ローカルLLM編】"\n          [level=3] [ref=s1e901]\n        - time [ref=s1e904]: 2024/07/08\n        - img "いいねされた数" [ref=s1e906]\n        - text: "4"\n- article [ref=s1e908]:\n    - link "tech" [ref=s1e909]:\n        - /url: /tech-or-idea\n    - link "🐷 生成AIをGoogle Colaboratoryで簡単に 【Part2 テキスト生成ChatGPT API編】 2024/07/05 いいねされた数 2" [ref=s1e910]:\n        - /url: /asap/articles/f410dd5d407652\n        - text: 🐷\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part2 テキスト生成ChatGPT API編】"\n          [level=3] [ref=s1e914]\n        - time [ref=s1e917]: 2024/07/05\n        - img "いいねされた数" [ref=s1e919]\n        - text: "2"\n- article [ref=s1e921]:\n    - link "tech" [ref=s1e922]:\n        - /url: /tech-or-idea\n    - link "🗂 生成AIをGoogle Colaboratoryで簡単に 【Part1 合成音声Style-Bert-VITS2編】 2024/07/02 いいねされた数 3" [ref=s1e923]:\n        - /url: /asap/articles/7bdb20dcb5cc56\n        - text: 🗂\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part1 合成音声Style-Bert-VITS2編】"\n          [level=3] [ref=s1e927]\n        - time [ref=s1e930]: 2024/07/02\n        - img "いいねされた数" [ref=s1e932]\n        - text: "3"\n- article [ref=s1e934]:\n    - link "tech" [ref=s1e935]:\n        - /url: /tech-or-idea\n    - link "👏 Remdisの環境構築で沼った話（＋使い方の紹介） 2024/06/26 いいねされた数 14" [ref=s1e936]:\n        - /url: /asap/articles/101379d8951b08\n        - text: 👏\n        - heading "Remdisの環境構築で沼った話（＋使い方の紹介）" [level=3] [ref=s1e940]\n        - time [ref=s1e943]: 2024/06/26\n        - img "いいねされた数" [ref=s1e945]\n        - text: "14"\n- article [ref=s1e947]:\n    - link "tech" [ref=s1e948]:\n        - /url: /tech-or-idea\n    - link "🚀 Style-Bert-VITS2の使い方メモ 2024/06/17 いいねされた数 34" [ref=s1e949]:\n        - /url: /asap/articles/f8c0621cdd74cc\n        - text: 🚀\n        - heading "Style-Bert-VITS2の使い方メモ" [level=3] [ref=s1e953]\n        - time [ref=s1e956]: 2024/06/17\n        - img "いいねされた数" [ref=s1e958]\n        - text: "34"\n- article [ref=s1e960]:\n    - link "tech" [ref=s1e961]:\n        - /url: /tech-or-idea\n    - link "💭 Style-Bert-VITS2のVoice Conversion機能の復活に挑戦してみた（前編） 2024/06/09 いいねされた数 9" [ref=s1e962]:\n        - /url: /asap/articles/df2c610d1389e4\n        - text: 💭\n        - heading "Style-Bert-VITS2のVoice Conversion機能の復活に挑戦してみた（前編）" [level=3]\n          [ref=s1e966]\n        - time [ref=s1e969]: 2024/06/09\n        - img "いいねされた数" [ref=s1e971]\n        - text: "9"\n- article [ref=s1e973]:\n    - link "tech" [ref=s1e974]:\n        - /url: /tech-or-idea\n    - link "😸 OpenAI Assistants API v2を色々いじってみた（使い方）（File Search編） 2024/05/28 いいねされた数 10" [ref=s1e975]:\n        - /url: /asap/articles/5c2f8ffe2a46ce\n        - text: 😸\n        - heading "OpenAI Assistants API v2を色々いじってみた（使い方）（File Search編）"\n          [level=3] [ref=s1e979]\n        - time [ref=s1e982]: 2024/05/28\n        - img "いいねされた数" [ref=s1e984]\n        - text: "10"\n- article [ref=s1e986]:\n    - link "tech" [ref=s1e987]:\n        - /url: /tech-or-idea\n    - link "👻 OpenAI Assistants API v2を色々いじってみた（使い方）（code interpreter編） 2024/05/27 いいねされた数 5" [ref=s1e988]:\n        - /url: /asap/articles/b0ce16ea1832ac\n        - text: 👻\n        - heading "OpenAI Assistants API v2を色々いじってみた（使い方）（code interpreter編）"\n          [level=3] [ref=s1e992]\n        - time [ref=s1e995]: 2024/05/27\n        - img "いいねされた数" [ref=s1e997]\n        - text: "5"\n- article [ref=s1e999]:\n    - link "tech" [ref=s1e1000]:\n        - /url: /tech-or-idea\n    - link "😑 【自分用】「AI後輩ちゃん」の個人的つまづきポイントまとめ 2024/05/09 いいねされた数 3" [ref=s1e1001]:\n        - /url: /asap/articles/98b04c5ea82bd9\n        - text: 😑\n        - heading "【自分用】「AI後輩ちゃん」の個人的つまづきポイントまとめ" [level=3] [ref=s1e1005]\n        - time [ref=s1e1008]: 2024/05/09\n        - img "いいねされた数" [ref=s1e1010]\n        - text: "3"\n- article [ref=s1e1012]:\n    - link "tech" [ref=s1e1013]:\n        - /url: /tech-or-idea\n    - link "😍 音声対話システム「AI後輩ちゃん」を作ってみた 2024/05/04 いいねされた数 17" [ref=s1e1014]:\n        - /url: /asap/articles/5b1b7553fcaa76\n        - text: 😍\n        - heading "音声対話システム「AI後輩ちゃん」を作ってみた" [level=3] [ref=s1e1018]\n        - time [ref=s1e1021]: 2024/05/04\n        - img "いいねされた数" [ref=s1e1023]\n        - text: "17"\n- contentinfo [ref=s1e1026]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1030]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1031]\n    - paragraph [ref=s1e1039]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e1041]:\n        - heading "About" [level=4] [ref=s1e1042]\n        - list [ref=s1e1043]:\n            - listitem [ref=s1e1044]:\n                - link "Zennについて" [ref=s1e1046]:\n                    - /url: /about\n            - listitem [ref=s1e1047]:\n                - link "運営会社" [ref=s1e1048]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e1049]:\n                - link "お知らせ・リリース" [ref=s1e1050]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e1051]:\n                - link "イベント" [ref=s1e1053]:\n                    - /url: /events\n    - navigation [ref=s1e1054]:\n        - heading "Guides" [level=4] [ref=s1e1055]\n        - list [ref=s1e1056]:\n            - listitem [ref=s1e1057]:\n                - link "使い方" [ref=s1e1059]:\n                    - /url: /manual\n            - listitem [ref=s1e1060]:\n                - link "法人向けメニュー" [ref=s1e1062]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e1064]:\n                - link "Publication / Pro" [ref=s1e1066]:\n                    - /url: /publications\n            - listitem [ref=s1e1067]:\n                - link "よくある質問" [ref=s1e1069]:\n                    - /url: /faq\n    - navigation [ref=s1e1070]:\n        - heading "Links" [level=4] [ref=s1e1071]\n        - list [ref=s1e1072]:\n            - listitem [ref=s1e1073]:\n                - link "X(Twitter)" [ref=s1e1074]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e1075]:\n                - link "GitHub" [ref=s1e1076]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e1077]:\n                - link "メディアキット" [ref=s1e1079]:\n                    - /url: /mediakit\n    - navigation [ref=s1e1080]:\n        - heading "Legal" [level=4] [ref=s1e1081]\n        - list [ref=s1e1082]:\n            - listitem [ref=s1e1083]:\n                - link "利用規約" [ref=s1e1085]:\n                    - /url: /terms\n            - listitem [ref=s1e1086]:\n                - link "プライバシーポリシー" [ref=s1e1088]:\n                    - /url: /privacy\n            - listitem [ref=s1e1089]:\n                - link "特商法表記" [ref=s1e1091]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e1094]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e1095]\n- alert [ref=s1e1114]\n```\n', name='browser_navigate', tool_call_id='58fad86e-0b01-4fc6-a8cc-57deb5c48b41'), AIMessage(content='`https://zenn.dev/asap`にアクセスしました。\n\n執筆者の記事一覧から、いいねの数が多い記事を探します。\n\n- 「DeepSeek-R1の論文読んだ？【勉強になるよ】」 が921いいね\n- 「拡散モデルの理論って面白いよね」 が87いいね\n- 「Open AI Realtime APIのPythonサンプルコードを作成」 が79いいね\n- 「📏 【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】」 が50いいね\n- 「🙌 完全自律型AIエージェントOSS【LangManus】完全に理解した！」 が48いいね\n\n最もいいねが多い記事は「DeepSeek-R1の論文読んだ？【勉強になるよ】」です。\n記事の中身を取得します。', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap/articles/34237ad87f8511"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-27104099-cd7e-4de1-8c89-51060ed9c6f1-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap/articles/34237ad87f8511'}, 'id': 'd85f8505-9c2a-4960-b17f-9ef88d983d9d', 'type': 'tool_call'}], usage_metadata={'input_tokens': 17995, 'output_tokens': 216, 'total_tokens': 18211, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap/articles/34237ad87f8511\n\n- Page URL: https://zenn.dev/asap/articles/34237ad87f8511\n- Page Title: DeepSeek-R1の論文読んだ？【勉強になるよ】\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- article [ref=s1e32]:\n    - text: 📑\n    - heading "DeepSeek-R1の論文読んだ？【勉強になるよ】" [level=1] [ref=s1e38]\n    - text: 2025/01/27に公開\n    - tooltip "更新日" [ref=s1e45]:\n        - img [ref=s1e46]\n        - text: 2025/02/22 更新日\n    - link "12件" [ref=s1e51]:\n        - /url: "#discuss"\n        - img [ref=s1e52]\n        - text: 12件\n    - link "強化学習" [ref=s1e63]:\n        - /url: /topics/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92\n        - img [ref=s1e65]\n        - text: 強化学習\n    - link "蒸留" [ref=s1e67]:\n        - /url: /topics/%E8%92%B8%E7%95%99\n        - img [ref=s1e69]\n        - text: 蒸留\n    - link "LLM" [ref=s1e71]:\n        - /url: /topics/llm\n        - img [ref=s1e73]\n        - text: LLM\n    - link "生成 AI" [ref=s1e75]:\n        - /url: /topics/%E7%94%9F%E6%88%90ai\n        - img [ref=s1e77]\n        - text: 生成 AI\n    - link "DeepSeek" [ref=s1e79]:\n        - /url: /topics/deepseek\n        - img [ref=s1e81]\n        - text: DeepSeek\n    - link "tech" [ref=s1e83]:\n        - /url: /tech-or-idea\n        - img [ref=s1e85]\n        - text: tech\n    - heading "はじめに" [level=1] [ref=s1e89]\n    - paragraph [ref=s1e90]: タイトル通りDeepSeek-R1の論文を読んだので、少しでもわかりやすく説明できればと思います。\n        まだ論文を読んでいない方は、ぜひ原文も読んでみてください！\n    - paragraph [ref=s1e92]:\n        - text: 難しい数式や概念がほとんど使われていないため、論文の中では全然読みやすい方だと思います。読み物として読めるレベルです。\n        - iframe [ref=s1e94]:\n            - text: Loading...\n            - alert [ref=f1s1e8]\n    - paragraph [ref=s1e95]: とはいえ、本記事では細かい部分など間違っている部分もあるかもしれないので、ぜひ誤りがあったら、こそっと教えてください！\n    - complementary [ref=s1e96]:\n        - text: "!"\n        - paragraph [ref=s1e99]: 実は、本論文を解説する上で、著者らが公開しているもう一つの論文が大きく関連しています。 そちらの論文の内容も若干解説を入れております。\n        - paragraph [ref=s1e101]:\n            - text: 「\n            - \'link "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models" [ref=s1e102]\':\n                - /url: https://arxiv.org/html/2402.03300v3\n            - text: 」\n        - paragraph [ref=s1e103]: 本記事でも少し触れますが、上記の論文も非常に勉強になるため、ぜひ読んでみてください。\n    - complementary [ref=s1e104]:\n        - text: "!"\n        - paragraph [ref=s1e107]:\n            - text: 本記事は、\n            - link "DeepSeek-R1の論文" [ref=s1e108]:\n                - /url: https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\n            - text: と\n            - link "DeepSeekMathの論文" [ref=s1e109]:\n                - /url: https://arxiv.org/html/2402.03300v3\n            - text: を読んだ私の理解をもとに記載しています。\n        - paragraph [ref=s1e110]: 本論文で使われている技術に関しては、ある程度の知識を持っているので、大きくは外していないとは思いますが、私の主観も入っている部分もありますので、ご了承ください。\n    - paragraph [ref=s1e111]:\n        - text: また、DeepSeek-R1の論文が公開される前に、小型モデルに対して同様の実験（強化学習）をしていたグループがあるようです。\n            そちらのレポートは下記になります。\n        - iframe [ref=s1e114]:\n            - \'link "7B Model and 8K Examples: Emerging Reasoning with Reinforcement Learning is Both Effective and Effic A replication of DeepSeek-R1 training on small models with limited data hkust-nlp.notion.site favicon image hkust-nlp.notion.site hkust-nlp.notion.site thumbnail image" [ref=f2s1e6]\':\n                - /url: https://hkust-nlp.notion.site/simplerl-reason\n                - \'heading "7B Model and 8K Examples: Emerging Reasoning with\n                  Reinforcement Learning is Both Effective and Effic" [level=1]\n                  [ref=f2s1e8]\'\n                - text: A replication of DeepSeek-R1 training on small models with limited data\n                - img "hkust-nlp.notion.site favicon image" [ref=f2s1e11]\n                - text: hkust-nlp.notion.site\n                - img "hkust-nlp.notion.site thumbnail image" [ref=f2s1e13]\n            - alert [ref=f2s1e15]\n    - paragraph [ref=s1e115]: 意図せず、DeepSeek-R1-Zeroの再現実験のようなレポートになっていますが、レポートの著者はDeepSeek-R1論文の公開前から実験していると主張しています。\n        こちらも非常に興味深かったため紹介です。\n    - heading "本論文の興味深いところ" [level=1] [ref=s1e117]\n    - paragraph [ref=s1e118]: 本論文は、大きく分けて3つの構成でできています\n    - list [ref=s1e119]:\n        - listitem [ref=s1e120]:\n            - strong [ref=s1e121]: 強化学習による思考能力の強化\n            - list [ref=s1e122]:\n                - listitem [ref=s1e123]: LLM（DeepSeek-V3-Base）に対して「強化学習のみ」を適用させたところ、強力な思考能力を獲得した「DeepSeek-R1-Zero」が作成されました。\n                - listitem [ref=s1e124]: 本論文のメインストリームであり、非常に興味深い示唆が得られる部分です。\n        - listitem [ref=s1e125]:\n            - strong [ref=s1e126]: SFTによるさらなる強化\n            - list [ref=s1e127]:\n                - listitem [ref=s1e128]: DeepSeek-V3-Baseに対して、コールドスタート問題を解消するために、CoT（Chain of\n                    Thought）データセットによるSFT（教師付きファインチューニング）を実施\n                - listitem [ref=s1e129]: その後、強化学習を適用させることで、o1レベルの性能を獲得した「DeepSeek-R1」が作成されました。\n                - listitem [ref=s1e130]: 非常に強力なLLMを獲得するまでの反復学習などの試行錯誤などを学ぶことができます。\n        - listitem [ref=s1e131]:\n            - strong [ref=s1e132]: 蒸留による小型モデルの性能強化\n            - list [ref=s1e133]:\n                - listitem [ref=s1e134]: 上記で作成されたDeepSeek-R1により、他社の小型オープンソースLLMを蒸留しました。\n                - listitem [ref=s1e135]: その結果、元の性能を大幅に上回る、非常に高い性能の小型LLMが得られました。\n    - paragraph [ref=s1e136]: その中でも私が特に面白いなと思った要素は下記の要素になります\n    - list [ref=s1e137]:\n        - listitem [ref=s1e138]:\n            - strong [ref=s1e139]: 強化学習手法「GRPO」すごい！\n        - listitem [ref=s1e140]:\n            - strong [ref=s1e141]: ルールベース報酬で思考能力が向上するのがすごい！\n        - listitem [ref=s1e142]:\n            - strong [ref=s1e143]: aha moment（ハッとする瞬間）面白い！\n        - listitem [ref=s1e144]:\n            - strong [ref=s1e145]: 蒸留vs強化学習の比較実験から重要な示唆が得られた！\n    - heading "本論文の流れ" [level=1] [ref=s1e146]\n    - heading "前提のモデル構造" [level=3] [ref=s1e147]\n    - paragraph [ref=s1e148]: そもそも、大前提として、この論文は「DeepSeek-V3-Base」という強力なLLMの性能を改善する論文です。\n        これ単体でも、GPT-4oレベルの能力を持っているオープンソースLLMになります。\n    - paragraph [ref=s1e150]: 本論文のモデルアーキテクチャは「DeepSeek-V3-Base」と完全に同じものを利用します。\n        （つまりモデルアーキテクチャをかえずに、学習方法を工夫してLLMの性能を上げる挑戦になります）\n    - heading "DeepSeek-R1-Zeroを強化学習を用いて作る" [level=3] [ref=s1e152]\n    - paragraph [ref=s1e153]: その上で、まずは、「DeepSeek-V3-Base」に対して、大規模な強化学習のみを適応させることで、強化学習の可能性について探究する話が始まります。\n        結果として、「DeepSeek-R1-Zero」ができます。\n        「DeepSeek-R1-Zero」は、SFT（教師付きファインチューニング）をしなくても、強化学習のみで圧倒的な思考能力を取得できることを示す興味深いモデルです。\n    - heading "DeepSeek-R1をSFT+強化学習を用いて作る" [level=3] [ref=s1e156]\n    - paragraph [ref=s1e157]: その後、あまりにも「DeepSeek-R1-Zero」の性能が良かったことから、さらなる探究が進められました。\n    - paragraph [ref=s1e158]: この時点で、「DeepSeek-R1-Zero」には下記の課題がありました。\n    - list [ref=s1e159]:\n        - listitem [ref=s1e160]: 学習初期段階にて、強化学習の学習が不安定であり、収束に時間がかかる（コールドスタート問題）\n        - listitem [ref=s1e161]:\n            - text: 思考能力は高いが、言語が混在（日本語で話していたら急に中国語になるなど）し、人間が読みにくい\n            - list [ref=s1e162]:\n                - listitem [ref=s1e163]: 結果、高度な思考が必要なタスクには強いが、一般的タスク（ロールプレイやライティングなど）には使いにくいという評価\n    - paragraph [ref=s1e164]: そこで、「DeepSeek-V3-Base」に対して、少量のCoT（Chain of\n        Thought）データセットを用いて、SFT（教師付き微調整）を行います。このデータは、著者らが収集・構築したものになります。\n        その後、SFT後のモデルに対して、大規模強化学習を行います。\n        このとき、「DeepSeek-R1-Zero」を作成した際の報酬に加え、「言語一貫性報酬」を導入して、上述した言語の不一致の問題を緩和させます。\n    - paragraph [ref=s1e167]: その後、得られたモデル（チェックポイント）を利用して、さらなるSFT（教師付き微調整）学習データを収集し、追加SFTを行います。\n        さらに、その後、改めて２段階目の大規模強化学習を実施することで、最終的に「DeepSeek-R1」が得られます。\n    - heading "小型モデルへの蒸留を試す" [level=3] [ref=s1e169]\n    - paragraph [ref=s1e170]: 最後に、小型モデルに対する「DeepSeek-R1」による蒸留が試されています。\n        その中で、小型モデルに対して、「DeepSeek-R1」による蒸留を実施するのと、「DeepSeek-R1-Zero」と同様の強化学習手法で思考能力を強化するのと、どちらが性能向上に寄与するのかを調査してくれています。\n    - paragraph [ref=s1e172]: これが非常に興味深いです。\n    - heading "DeepSeek-R1-Zeroを作るまで" [level=1] [ref=s1e173]\n    - paragraph [ref=s1e174]: では、ここから論文の詳細な解説に入っていきます。\n    - paragraph [ref=s1e175]: まずは、「DeepSeek-R1」の前段階である「DeepSeek-R1-Zero」を作るまでの手法を解説します。\n    - paragraph [ref=s1e176]:\n        - strong [ref=s1e177]: 前段階と侮るなかれ、「DeepSeek-R1-Zero」を作るまでの手法と示唆にこそ、本論文の貴重な示唆が含まれています。\n    - heading "元々のアーキテクチャ" [level=2] [ref=s1e178]\n    - paragraph [ref=s1e179]: 今回作る「DeepSeek-R1-Zero」や「DeepSeek-R1」は、新しいモデル構造を採用して、性能を向上させたわけではありません。\n    - paragraph [ref=s1e180]: 元々のアーキテクチャは「DeepSeek-V3-Base」というアーキテクチャになります。\n        「DeepSeek-V3-Base」は、681Bのモデルパラメータを持つ大規模言語モデルで、性能はGPT-4oに匹敵する非常に高性能モデルありながら、モデル重みが完全に公開されているオープンソースモデルです。\n    - paragraph [ref=s1e182]: 「DeepSeek-V3-Base」に対して、Instructionチューニングを施したchatモデルに関しては、APIでも公開されています。\n        非常に安価に使えるにも関わらず、GPT-4oに匹敵する性能を誇ることから、人気のモデルになっております。\n    - complementary [ref=s1e184]:\n        - text: "!"\n        - paragraph [ref=s1e187]: 安価なモデルではありますが、APIの提供元は中国の企業ですので、データの取り扱いなどは十分注意してください。\n            基本的には、API利用の際には、収集されて困るような情報は入力しないようにするのが無難です。\n            （オープンソースモデルをローカルで動かす場合は、おそらく心配ありません）\n    - paragraph [ref=s1e190]: 本論文のすごいところは、モデル構造を変えずに、「DeepSeek-V3-Base」に対する学習方法を工夫しただけで、OpenAI\n        o1に匹敵する性能のLLMを開発していることです。\n        具体的には、大規模強化学習をLLMに対して適用することで、「DeepSeek-R1-Zero」を開発します。\n    - heading "すごいぞ！強化学習" [level=2] [ref=s1e192]\n    - heading "強化学習って何？" [level=3] [ref=s1e193]\n    - paragraph [ref=s1e194]: そもそも「強化学習って何？」という方もいらっしゃると思うので、簡単に説明します。\n        （以前、スイカゲームを強化学習でプレイしたこともあるので、いずれ詳しい解説記事を作る予定ですが、ここではざっくり説明します。）\n    - paragraph [ref=s1e196]: 強化学習とは、一言で言えば「一連の行動の流れ（シークエンス）によって得られる、累積報酬の期待値を最大化する」ことを目指す学習手法です。\n        ある行動をとった結果がすぐにはわからず、時間が経ってから影響が現れるような問題に適しています。\n        専門的に言うと、「マルコフ決定過程（MDP）」が成立する問題で活用されます。\n    - heading "テトリスで考える" [level=3] [ref=s1e199]\n    - paragraph [ref=s1e200]: 例えば、「テトリス」を考えてみましょう。\n    - paragraph [ref=s1e201]: テトリスは、上から「ミノ」と呼ばれる、さまざまな形のブロックを落として、横一列を綺麗に埋めてブロックを消去するゲームです。\n    - paragraph [ref=s1e202]: 横一列がすべて揃うと、その列が消えて、「ポイント」を獲得できます。\n        逆に、列を消せずに、ブロックが一定の高さまで積み上がると「ゲームオーバ」になります。\n    - paragraph [ref=s1e204]: テトリスを強化学習で解こうとすると、「どうすれば最も多くのポイントを得られるか」をAIに学習させることが重要になります。\n        単純に列を揃えて消すだけではなく、「次の手」や「その次の手」を見据えて、より良い結果につながるようにミノを配置することも必要です。\n        しかし、通常の学習方法(よくある入出力を近似させる学習方法)では、目の前の結果だけに注目してしまい、将来的な高得点につながる手を学習させることは難しいです。\n    - paragraph [ref=s1e207]:\n        - strong [ref=s1e208]: そこで役立つのが「強化学習」です。\n    - paragraph [ref=s1e209]: 強化学習では、ゲームの最初から最後までの流れを通して、「どう行動すれば最終的に得られる得点を最大化できるか」を学習できます。\n        このように、途中の行動の良し悪しがすぐにはわからなくても、最終的なゴール（得点の最大化）が設定できる問題に対して、強化学習は非常に効果的です。\n    - paragraph [ref=s1e211]:\n        - img [ref=s1e212]\n    - group [ref=s1e213]: 補足\n    - heading "強化学習の事例「AlphaGo Zero」" [level=3] [ref=s1e215]\n    - paragraph [ref=s1e216]: さらに、強化学習の威力をお伝えする事例を記載します。\n    - paragraph [ref=s1e217]: 2015年にアメリカのDeepMind社（Googleの傘下）が「AlphaGo」というモデルを開発しました。これはAIが囲碁をプレイするモデルで、当時の世界王者をハンディキャップなしで破るという、非常に衝撃的なモデルでした。\n        （当時としては、非常にセンセーショナルな出来事だったため、記憶にある方もいらっしゃるかと思います）\n    - paragraph [ref=s1e219]: この「AlphaGo」は、深層学習と強化学習により学習されています。\n        プロ棋士の棋譜データで深層学習で学習させた上で、強化学習による自己対局（AI同士の対局）を実施したモデルです。\n    - paragraph [ref=s1e221]:\n        - text: さて、ここまでも十分すごいですが、\n        - strong [ref=s1e223]: さらにすごいのはここからです。\n    - paragraph [ref=s1e224]: DeepMind社は後継モデルとして「AlphaGo Zero」というモデルを開発しました。\n        このモデルは、プロ棋士の棋譜データを「使わず」に、完全に「強化学習のみ」で学習されたモデルです。\n        つまり、囲碁のルールだけを与え、あとは最初から最後までAI同士の自己対局のみで学習されています。\n        （この場合、対局に勝利したら報酬、敗北したら罰を与える形で強化学習をし、対局に勝利した時に取っていた手（行動）の選択確率を上げるように学習しています）\n    - paragraph [ref=s1e228]:\n        - strong [ref=s1e229]: そして、その「AlphaGo Zero」は「AlphaGo」に対して、100戦全勝で圧勝しました。\n    - paragraph [ref=s1e230]: AIが囲碁の学習をする際に、人間が持つ定石の情報などは不要だったわけです。\n    - paragraph [ref=s1e231]: これが強化学習の威力です。\n        余談ですが、今回の論文で作成する「DeepSeek-R1-Zero」も（「DeepSeek-V3-Base」をベースにはしますが）強化学習のみで学習されます。\n        名前に入っている「Zero」は同じく強化学習のみで開発された「AlphaGo Zero」のオマージュかなと思います。\n    - heading "そもそもLLMで強化学習とは？" [level=2] [ref=s1e234]\n    - paragraph [ref=s1e235]: 「なんかゲームみたいなものに、強化学習を使うとうまくいきそう」というのは理解していただけたかなと思います。\n    - paragraph [ref=s1e236]:\n        - text: では、LLMに対してはどうか、というのを解説していきます。\n        - strong [ref=s1e238]: 実は、LLMと強化学習は非常に相性がいいです。\n    - paragraph [ref=s1e239]: なぜなら、LLMは一度にすべての文章を出力するAIではなく、1tokenずつ出力するモデルであり、その文章が良い文章か悪い文章かは、すべてのtokenを出力してみないとわからないからです。\n    - paragraph [ref=s1e240]:\n        - text: テトリスでは、ミノを一つずつ落とし、一列が消えたタイミングで、ポイントを取得するという形で評価されます。\n        - img [ref=s1e242]\n    - paragraph [ref=s1e243]:\n        - text: LLMでも、同様にtokenを1tokenずつ出力し、一文全部が出力されたタイミングで、その文章の良し悪しを評価することができれば、強化学習と同じ枠組みを使うことができるというわけです。\n        - img [ref=s1e245]\n    - heading "報酬モデルを作成し、累積報酬を最大化" [level=3] [ref=s1e246]\n    - paragraph [ref=s1e247]:\n        - text: テトリスでは、ゲームシステムが勝手に点数をつけてくれますが、LLMにおいて文章の良し悪しを勝手に判断してくれる便利な機能はありません。そこで我々が、その判断モデルを作ってあげる必要があります。\n            そこで、ニューラルネットワークを利用して、LLMに入力されたプロンプト\n        - math [ref=s1e252]: x\n        - text: に対して、LLMが生成した文章\n        - math [ref=s1e259]: "y"\n        - text: の点数\n        - math [ref=s1e266]: r\n        - text: を出力するようなネットワークを「報酬モデル\n        - math [ref=s1e273]: r θ ( r ∣ y , x )\n        - text: 」として作成することを考えます。\n    - paragraph [ref=s1e286]:\n        - text: まず初めに、データセットとして、「プロンプト\n        - math [ref=s1e290]: x\n        - text: 」「好ましい回答\n        - math [ref=s1e297]: y +\n        - text: 」「好ましくない回答\n        - math [ref=s1e306]: y −\n        - text: 」の3つが組となっているデータセットを用意します。 報酬モデル\n        - math [ref=s1e316]: r θ ( r ∣ y , x )\n        - text: は、「プロンプト\n        - math [ref=s1e332]: x\n        - text: 」と「出力\n        - math [ref=s1e339]: "y"\n        - text: 」の両方を入力することで、報酬（点数）を出力するモデルです。\n    - paragraph [ref=s1e343]:\n        - text: そこで、好ましい回答の報酬推定値\n        - math [ref=s1e347]: r θ ( r ∣ y + , x )\n        - text: と、好ましくない回答の報酬推定値\n        - math [ref=s1e365]: r θ ( r ∣ y − , x )\n        - text: の差を一定以上にするような損失関数を用いて学習を行います。 すると、報酬モデル\n        - math [ref=s1e384]: r θ ( r ∣ y , x )\n        - text: は適切に、好ましい回答\n        - math [ref=s1e400]: y +\n        - text: には高い点数を、好ましくない回答\n        - math [ref=s1e409]: y −\n        - text: には低い点数をつけるようになります。\n    - paragraph [ref=s1e415]:\n        - text: 報酬モデル\n        - math [ref=s1e419]: r θ ( r ∣ y , x )\n        - text: か完成したら、強化学習をするための要素は整います。\n    - paragraph [ref=s1e432]:\n        - text: 状態\n        - math [ref=s1e436]: x\n        - text: は、LLMに入力される「プロンプト」で、LLMの出力が\n        - math [ref=s1e443]: "y"\n        - text: とします。 報酬\n        - math [ref=s1e451]: r\n        - text: は、報酬モデル\n        - math [ref=s1e458]: r θ ( r ∣ y , x )\n        - text: の出力値です。 LLMが全文出力したら、プロンプト\n        - math [ref=s1e475]: x\n        - text: と、LLMの出力\n        - math [ref=s1e482]: "y"\n        - text: を報酬モデルに与えることで、取得します。 方策は、LLMが出力する各tokenごとの確率分布自体です。 このLLM自体を「方策モデル\n        - math [ref=s1e491]: π ϕ ( y t ∣ x , y < t )\n        - text: 」と呼びます。\n    - complementary [ref=s1e510]:\n        - text: "!"\n        - paragraph [ref=s1e513]:\n            - text: LLM（\n            - math [ref=s1e517]: π ϕ ( y t ∣ x , y < t )\n            - text: ）はユーザが入力したプロンプト\n            - math [ref=s1e539]: x\n            - text: と、それまでにLLMが出力した内容\n            - math [ref=s1e546]: y < t\n            - text: をもとに、\n            - math [ref=s1e557]: t\n            - text: ステップ目のtoken\n            - math [ref=s1e564]: y t\n            - text: を出力します。\n        - paragraph [ref=s1e570]: これが、LLMは、「文章の次に出てくる単語を一つずつ予測するモデル」と言われる所以です。\n    - paragraph [ref=s1e571]: これにより、累積報酬を最大化するように方策モデル（LLM）を学習することで、LLMは好ましい回答を出力するようにネットワークを更新できます。\n    - paragraph [ref=s1e572]: これが、LLMを強化学習で学習するということです。 このような手法をRLHF（Reinforcement\n        learning from human feedback）と総称します。\n    - group [ref=s1e574]: 細かい話\n    - heading "つよつよ手法「PPO（Proximal Policy Optimization）」" [level=3] [ref=s1e576]\n    - paragraph [ref=s1e577]: ここまでで、「LLMと強化学習は相性が良い」こと、「強化学習では累積報酬の期待値（状態価値）を最大化することが重要」であることを話しました。\n    - paragraph [ref=s1e578]: さて、実際にLLMに強化学習を組み込む場合、しばしばPPO（Proximal Policy\n        Optimization）という手法が利用されます。\n        （なんなら、LLM以外で強化学習を実用する上でも、最初に選択肢になるベースライン的に強力な手法でもあります）\n    - paragraph [ref=s1e580]: 今回はPPOが主題ではないので、簡単に説明します。\n    - heading "PPOの簡易的な説明" [level=4] [ref=s1e581]\n    - paragraph [ref=s1e582]:\n        - text: PPOでは、行動を確率的に出力する方策モデル（LLM）と、現在の状態価値を推定する状態価値モデルを用意します。 状態価値\n        - math [ref=s1e587]: V t\n        - text: とは、「累積報酬の期待値」として定義できます。 したがって、現時点から、方策に従って行動をした場合の、将来的な報酬総和の期待値が「状態価値\n        - math [ref=s1e597]: V t\n        - text: 」です。\n    - paragraph [ref=s1e603]:\n        - text: 強化学習の枠組みで累積報酬\n        - math [ref=s1e607]: G t\n        - text: が計算されるため、その期待値である状態価値\n        - math [ref=s1e616]: V t\n        - text: も学習が可能です。 一方で、方策モデルを学習する際に、Advantage\n        - math [ref=s1e626]: A t\n        - text: というものを利用します。\n    - paragraph [ref=s1e632]:\n        - text: Advantage\n        - math [ref=s1e636]: A t\n        - text: というのは下記の式で表されます。\n    - math [ref=s1e648]: A t = G t − V t ( v t ∣ y < t , x )\n    - paragraph [ref=s1e675]:\n        - text: 状態価値\n        - math [ref=s1e679]: V t ( v t ∣ y < t , x )\n        - text: は、累積報酬\n        - math [ref=s1e701]: G t\n        - text: の期待値（大雑把にいうと平均）になるように学習されているという前提があるため、Advantage\n        - math [ref=s1e710]: A t\n        - text: が正になるということは、今回LLMが出力したtoken\n        - math [ref=s1e719]: y t\n        - text: は、将来的に高い報酬を得ることができる「良い出力であった」と判断できます。\n    - paragraph [ref=s1e725]:\n        - text: したがって、方策モデル\n        - math [ref=s1e729]: π ϕ ( y t ∣ x , y < t )\n        - text: はAdvantage\n        - math [ref=s1e751]: A t\n        - text: を最大化するように学習します。\n    - paragraph [ref=s1e757]:\n        - text: Advantage\n        - math [ref=s1e761]: A t\n        - text: を考えずに、単純に累積報酬\n        - math [ref=s1e770]: G t\n        - text: を最大化するように考えることもできます。（むしろそれが初期の強化学習です）\n    - complementary [ref=s1e776]:\n        - text: "!"\n        - paragraph [ref=s1e779]: （2025年2月15日 追記） この辺りの表現は、わかりやすくするために若干誤解を招く表現になっています。\n            下に「細かい話」として補足説明を書きました。\n    - paragraph [ref=s1e782]: しかしその場合、そもそもどんな行動を取っても報酬が高くなる状態と、どんな行動を取っても報酬が低くなってしまう状態において、勾配のスケールがばらつくため、学習の安定性が低くなります。\n    - paragraph [ref=s1e783]:\n        - text: PPOでは、Advantage\n        - math [ref=s1e787]: A t\n        - text: を考えることにより、出力したtokenが平均よりも良いか悪いかだけで学習をすることができ、状態ごとの報酬の差をモデルが考慮する必要がなくなります。\n    - complementary [ref=s1e793]:\n        - text: "!"\n        - paragraph [ref=s1e796]: 本来、PPOの貢献は、あるパラメータ更新時において、更新前と更新後の確率分布が大きく変わらないような制約を、少ない計算量で加えることです。\n            ただ、この話は、DeepSeek-R1-Zeroにおける強化学習を理解するためには、必ずしも重要ではありません。\n            したがって、今回の記事では、必要な部分だけに絞り説明をしました。\n    - group [ref=s1e799]: 細かい話（追記）\n    - heading "著者発案「GRPO（Group Relative Policy Optimization）」" [level=3]\n      [ref=s1e801]\n    - paragraph [ref=s1e802]: さて、PPOは強化学習の手法において、高い性能を発揮しますが、LLMに適用することを考えると、状態価値モデルを別途学習しないといけないという点において、計算量的に大変な手法です。\n    - paragraph [ref=s1e803]: "一方で、PPOの状態価値モデルがやっていたことは、Advantage A_t(a_t|y_t,\n        y_{<t},x) を計算し、今回出力したtokenが、平均よりも良いのか悪いのかを判断することです。"\n    - paragraph [ref=s1e806]: "そこで、LLMにおいては、わざわざ状態価値モデルを用意して平均を計算しなくても、実際に複数文章を生成して、そ\\\n        れぞれの報酬を平均することで、Advantage A_t(a_t|y_t, y_{<t},x) を計算しようと考えました。\n        こうすることで、状態価値モデルなしで、Advantage A_t(a_t|y_t, y_{<t},x)\n        を計算できるため、計算量を大幅に削減できることができます。"\n    - paragraph [ref=s1e812]:\n        - text: この手法は「GRPO（Group Relative Policy Optimization）」と呼びます。\n            本論文の著者が、過去に公開した論文で提案した手法です。\n        - iframe [ref=s1e815]: <could not take iframe snapshot>\n    - paragraph [ref=s1e816]:\n        - text: 下記が、論文中で提示されている「PPO」と「GRPO」の違いを説明する図です。\n        - img [ref=s1e818]\n        - text: "このように状態価値モデル（Value model）なしに、Advantage A_t(a_t|y_t, y_{<t},x)\n            が計算できていることがわかります。"\n    - complementary [ref=s1e821]:\n        - text: "!"\n        - paragraph [ref=s1e824]: 本記事でのアルファベットの付け方と、上記の図のアルファベットの付け方が異なることに注意してください。\n        - paragraph [ref=s1e825]: 上記の図（GRPO）では、 プロンプトを q 、LLMの各出力文章を o_i （ i\n            はそれぞれの文章単位でのインデックス）、各文章ごとの報酬モデルの出力を r_i 、報酬 r_i\n            の平均から計算された各文章ごとのAdvantageを A_i としています。\n        - paragraph [ref=s1e839]: 本記事での添え字は、token単位での添え字を採用しています。 例えば A_t なら t token目でのAdvantageです。\n        - paragraph [ref=s1e845]: 上記の図では、文章単位での添え字です。 例えば、 A_i なら、 i 文章目でのAdvantageとなります。\n            従って A_i はベクトルとなっており、中にtokenごとのAdvantageの値も格納されているはずです。\n    - heading "報酬モデルを作らずルールベースで実施" [level=3] [ref=s1e854]\n    - paragraph [ref=s1e855]:\n        - text: 改めて、LLMに強化学習を適用するために必要なものをまとめます。 （下記の図のGRPOの手法を利用します）\n        - img [ref=s1e858]\n    - paragraph [ref=s1e859]: "まず、大前提として方策モデル \\\\pi_\\\\phi(y_t|x,y_{<t}) （LLM）は必要です。"\n    - paragraph [ref=s1e862]: "上の図中における「Reference Model」とは、「強化学習をする前」の方策モデル\n        \\\\pi_{ref}(y_t|x,y_{<t}) （つまり、DeepSeek-V3-Base）です。 また、「Policy\n        Model」というのは、「強化学習中」の現時点での方策モデル \\\\pi_\\\\phi(y_t|x,y_{<t}) を指しています。"\n    - paragraph [ref=s1e868]: そして、「プロンプト」「良い回答」「悪い回答」の3つを組とするデータセットで学習された、報酬モデル\n        r_\\theta(r|y,x) （Reward Model）が必要です。 報酬モデル r_\\theta(r|y,x) は「プロンプト x\n        」と「LLMの出力回答 y 」を入力することで、「報酬 r 」を出力します。\n    - paragraph [ref=s1e880]: この二つがあれば、著者が提案したGRPOという手法で、強化学習が可能になります。 （GRPOでは、状態価値モデルは不要です）\n    - paragraph [ref=s1e882]:\n        - text: しかしながら著者らは、さらに驚くべきことに、「DeepSeek-R1-Zero」を学習する際に、\n        - strong [ref=s1e884]: 報酬モデルを使わずに、ルールベースで報酬を計算しています。\n        - text: それにより、圧倒的な計算効率を達成しています。\n    - paragraph [ref=s1e886]: 著者らが設定しているルールは下記の二つです。\n    - list [ref=s1e887]:\n        - listitem [ref=s1e888]:\n            - strong [ref=s1e889]: 精度報酬\n            - list [ref=s1e890]:\n                - listitem [ref=s1e891]: 応答が正しいかどうかを評価する\n                - listitem [ref=s1e892]: 例えば、数学の問題であれば、最終的な回答が答えと合っているかどうかで報酬を渡す。\n        - listitem [ref=s1e893]:\n            - strong [ref=s1e894]: フォーマット報酬\n            - list [ref=s1e895]:\n                - listitem [ref=s1e896]:\n                    - text: 思考プロセスを\n                    - code [ref=s1e897]: <think>\n                    - text: と\n                    - code [ref=s1e898]: </think>\n                    - text: タグの間に置くことをモデルに強制し、合っていない場合はペナルティを課す。\n    - paragraph [ref=s1e899]:\n        - text: このたった2つの報酬のみをルールベースで設定しており、\n        - strong [ref=s1e901]: ニューラルネットワークベースの報酬モデルは利用していません\n    - paragraph [ref=s1e902]: ニューラルネットワークの報酬モデルを利用せずに、ルールベース報酬だけで強化学習をすることにより、報酬モデル分の計算量を削減しています。\n    - heading "この報酬設計で良い理由" [level=4] [ref=s1e903]\n    - paragraph [ref=s1e904]: フォーマット報酬は、モデルに出力形式を強制するものであるので、思考能力を向上させる直接的な報酬ではありません。\n        であれば、「DeepSeek-R1-Zero」の圧倒的な思考能力を支えているのは、「精度報酬」になります。\n    - paragraph [ref=s1e906]: 本論文での、強化学習の段階で利用しているデータセットは、おそらく数学データセットです。\n    - paragraph [ref=s1e907]:\n        - text: 数学の問題は、基本的に回答が一意に定まるため、ルールベースであっても出力が正しいかどうかは判断できます。\n        - strong [ref=s1e909]: そして、数学の問題に正答するためには、思考能力を強化する必要があります。\n    - complementary [ref=s1e910]:\n        - text: "!"\n        - paragraph [ref=s1e913]: ここが本論文の非常に面白い部分です。\n            DeepSeek-R1-Zeroを学習する際に、より長い長考をさせるようなデータセットによる学習や、思考部分に対して報酬を与えるような仕組みは「全く無く」、ただ、数学の問題の回答が合っているか間違っているかのゴール報酬のみが与えられています。\n        - paragraph [ref=s1e915]: それだけにも関わらず、自動的に数学の問題を正答するために、思考能力を強化するように学習が進んでおり、強化学習の威力を感じます。\n    - paragraph [ref=s1e916]: 強化学習の枠組みにおいて、最終的に報酬を得る（問題に正解する）ために、今、何のtokenを出力しないといけないのか、が最適化されます。\n        結果として、問題の正当に近づくようなtokenを出力する能力、すなわち問題をステップバイステップで解く能力が磨かれることになります。\n    - paragraph [ref=s1e918]: このように、数学という問題の答えを合わせるように報酬設計をすることで、勝手に思考能力が強化される、というのは非常に面白いなと思います。\n    - heading "（補足）データセットの作り方（の考察）" [level=4] [ref=s1e919]\n    - paragraph [ref=s1e920]:\n        - text: 本論文での、強化学習に利用するデータセットは、おそらく\n        - link "上述した論文" [ref=s1e921]:\n            - /url: https://arxiv.org/html/2402.03300v3\n        - text: の前半に記載されている数学データセットです。\n    - complementary [ref=s1e922]:\n        - text: "!"\n        - paragraph [ref=s1e925]: 強化学習に利用したデータセットに関しては、本論文で言及されていなかったと思うので憶測です。間違っていたら教えてください。\n    - paragraph [ref=s1e926]: 本論文の主要部分ではないですが、興味深いため紹介します。\n    - paragraph [ref=s1e927]:\n        - img [ref=s1e928]\n        - text: 引用：\n        - \'link "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models" [ref=s1e929]\':\n            - /url: https://arxiv.org/html/2402.03300v3\n    - paragraph [ref=s1e930]:\n        - strong [ref=s1e931]: 1. Train a FastText Model\n        - text: まず初めに、「OpenWebMath」という高品質の数学のウェブテキストのデータセットを初期のシードコーパスとして利用します。\n            このコーパスから50万のデータを正例、 Common\n            Crawlデータセット（大規模なWebページコーパス）から取得した50万のデータを負例として、fastTextモデルを学習します。\n            こうすることにより、fastTextモデルを利用して、Webページから、それが数学コンテンツか、非数学コンテンツかを分類することができます。\n    - paragraph [ref=s1e935]:\n        - strong [ref=s1e936]: 2. Recall Math-Related Webpages From Common Crawl\n    - paragraph [ref=s1e937]: 学習した、fastTextモデルを利用して、Common\n        Crawlデータセットから数学のWebページを取得します。 このとき、低品質な数学コンテンツを除外するために、 fastText\n        モデルによって予測されたスコアをもとにランク付けした、上位のスコアのページのみを取得します。\n    - paragraph [ref=s1e939]:\n        - strong [ref=s1e940]: 3. Discover Math-Related Domains\n    - paragraph [ref=s1e941]: この方法で取得されたWebページは、数学の問題ではありますが、偏りがあります。\n        なぜなら、初期の学習に利用された「OpenWebMath」データセットには、少数のデータしかなく、さらに上位のスコアのWebコーパス（類似度の高いWebコーパス）しか収集していないため、「OpenWebMath」データセットと似たような数学Webページしか、収集できていないからです。\n    - paragraph [ref=s1e943]: そこで、上記の方法で収集したWebページの親ドメインに着目します。\n        そして、親ドメイン内のWebページのうち、10%以上のWebページが、今回の方法で収集されている場合、親ドメイン自体が数学に関連するドメインであると、考えることができます。\n    - complementary [ref=s1e945]:\n        - text: "!"\n        - paragraph [ref=s1e948]: 例えば、「mathhoge.com」の配下のページの10%以上がfastText\n            モデルによって収集されていた場合は、「mathhoge.com」自体を数学コンテンツであると分類します。\n    - paragraph [ref=s1e949]: あとは、この親ドメイン内の、まだ収集されていないWebページから、高品質な数学Webページを収集することで、幅広い数学のコンテンツを収集できそうです。\n    - paragraph [ref=s1e950]:\n        - strong [ref=s1e951]: 4. Annotate Math-Related URL Path From Labelers\n    - paragraph [ref=s1e952]: 分類されたドメイン内の数学コンテンツに対して、「手動」でURLに注釈をつけ、注釈をつけたもののうち、またシードコーパスに収集されていないWebページを、シードコーパスに追加します。\n    - paragraph [ref=s1e953]: この方法を繰り返すことで、高品質かつ多様な数学データセットを収集することができます。\n        著者らはこのデータ収集を4回繰り返したのちに、3,550万の数学Webページ、合計1200億トークンのコーパスを完成させました。\n    - paragraph [ref=s1e955]: 4回の繰り返しの段階で、データのうち98%が3回目までの繰り返しですでに収集されていたことがわかったため、4回で繰り返し作業を終了したとのことです。\n    - heading "（補足）DPO（Direct Preference Optimization）との違い" [level=3]\n      [ref=s1e956]\n    - paragraph [ref=s1e957]: DPO（Direct Preference Optimization）という手法があります。\n        こちらは、強化学習を利用して、LLMに好ましい回答を学習させるRLHF（Reinforcement learning from human\n        feedback）と同様の効果を、強化学習を使わずに学習させることができる手法です。\n    - paragraph [ref=s1e959]: "強化学習をしないため、報酬モデルも状態価値モデルも必要なく、単純に「プロンプト x 」と「好ましい回答 y^+\n        」「好ましくない回答 y^- 」の3つが組となっているデータセット D を用意すれば、あとは \\\\pi_\\\\phi(y^+|x) 、\n        \\\\pi_\\\\phi(y^-|x) 、 \\\\pi_{ref}(y^+|x) 、 \\\\pi_{ref}(y^+|x)\n        を計算して、損失関数に落とし込むことで、学習が可能になります。"\n    - paragraph [ref=s1e976]: このように、「報酬モデルも状態価値モデルも必要ない」の部分が、本論文で提案している「GRPO（Group\n        Relative Policy Optimization）+ルールベース報酬」の手法の利点と近いです。\n    - paragraph [ref=s1e977]: ただし、DPOでは、上述した通り、「プロンプト x 」と「好ましい回答 y^+ 」「好ましくない回答 y^-\n        」の3つが組となっているデータセットが必要です。 逆に言えば、このデータセットの範囲内でしか学習ができません。\n    - paragraph [ref=s1e985]: 一方で、著者らが提案する「GRPO（Group Relative Policy\n        Optimization）+ルールベース報酬」の手法では、数学などの答えが一意に決まり、ルールベースで正解不正解を判断できるデータセット（つまり、数学データセット）であれば、「プロンプト\n        x 」と「回答 y 」の組だけで、学習が可能です。\n    - paragraph [ref=s1e990]:\n        - strong [ref=s1e991]: したがって、DPOよりもはるかに多くのデータを用いて学習ができます。\n        - text: これが、今回の提案手法の利点になりそうです。\n    - heading "著者らの執念" [level=3] [ref=s1e993]\n    - paragraph [ref=s1e994]: 著者らは、LLMを強化学習に適用する上で、計算量的に障害になる報酬モデルと状態価値モデルを利用しない斬新な手法を提案しました。\n        これにより、圧倒的な計算効率にて、強化学習を適用させました。\n    - paragraph [ref=s1e996]: また、似た効果を持つ手法としてDPOという手法があります。 ただし、DPOでは、利用できるデータセットが制限されるため、大規模な学習は困難です。\n    - paragraph [ref=s1e998]: しかしながら著者らの手法は、ただ数学のように一定の推論の結果、答えが一意に定まるような問題を解かせ続ければ、ルールベース報酬による強化学習で、思考機能を強化させることができてしまっています。\n    - paragraph [ref=s1e999]:\n        - strong [ref=s1e1000]: 絶対に、学習データの量を減らさずに、計算量を徹底的に削減してやろうという著者らの執念を感じるとともに、こういう工夫に積み重ねが、高性能と圧倒的に格安なAPI料金の両立を達成しているのだろうなと感じました。\n    - heading "ZeroのSystemプロンプト" [level=2] [ref=s1e1001]\n    - paragraph [ref=s1e1002]: 続いては、少し話が変わり、「DeepSeek-R1-Zero」に与えているsystemプロンプトについて紹介します。\n        このように、systemプロンプトが公開されているというのは、非常に勉強になりますね。とても嬉しいです\n    - paragraph [ref=s1e1004]: systemプロンプトは下記です。\n    - paragraph [ref=s1e1005]: "A conversation between User and Assistant. The user\n        asks a question, and the Assistant solves it. The assistant first thinks\n        about the reasoning process in the mind and then provides the user with\n        the answer. The reasoning process and answer are enclosed within <think>\n        </think> and <answer> </answer> tags, respectively, i.e., <think>\n        reasoning process here </think> <answer> answer here </answer>. User:\n        prompt. Assistant:"\n    - paragraph [ref=s1e1006]: 訳）\n        ユーザーとアシスタントの会話。ユーザーが質問し、アシスタントがそれを解決する。アシスタントは、まず頭の中で推論プロセスを考え、それからユーザに答えを提供します。推論過程と答えはそれぞれ<think>\n        </think>タグと<answer> </answer>タグで囲まれます。つまり、<think> 推論過程はここ</think>\n        <answer>答えはここ</answer>です。ユーザー：プロンプト アシスタント：\n    - paragraph [ref=s1e1008]: 上記部分の「prompt」の部分は、ユーザが入力したプロンプトの内容に書き換えられます。\n        このように、systemプロンプトにおいて<think> </think>タグを利用して、その中で思考をさせる形になっています。\n    - heading "Zeroの「Aha Moment」" [level=2] [ref=s1e1010]\n    - paragraph [ref=s1e1011]: 「Aha Moment」とは、辞書で調べると「〔問題の答えが突然ひらめいたときなどの〕なるほど！と思う瞬間」とのことです。\n    - paragraph [ref=s1e1012]: 実は、DeepSeek-R1-Zeroの出力において、それが発生したそうです。 数学の問題を解く途中で、下記のような出力が見られました。\n    - paragraph [ref=s1e1014]: "Wait, wait. Wait. That’s an aha moment I can flag\n        here. Let’s reevaluate this step-by-step to identify if the correct sum\n        can be · · · We started with the equation:"\n    - paragraph [ref=s1e1017]: 訳） 待て、待て。待ってくれ。これはハッとさせられる瞬間だ。 正しい和が - - - -\n        になるかどうかを確認するために、ステップ・バイ・ステップで再評価してみよう。\n    - paragraph [ref=s1e1020]: これは恐ろしいことです。\n        モデルは途中まで推論を実施しながら、途中で自身の出力結果を見直し、さらにもう一度ステップバイステップで検討をするという、高度な問題解決能力を獲得しています。\n        （自分の考えを見直して、さらに良い戦略を発見する。これができる人間がどれくらいいるのだろうか）\n    - paragraph [ref=s1e1023]: 何度も記載していますが、DeepSeek-R1-Zeroを学習する際に、問題の解き方を明示的にモデルに教えることや、思考部分を強化させるような報酬は与えていません。\n        ただ、数学の問題に正解することで得られる報酬を与えるだけで、モデルが自律的に高度な問題解決戦略を獲得しています。\n        （もちろん、推論の途中で、検算を行うなどのデータセットで学習もさせていません。もしかしたら、DeepSeek-V3-Baseを学習させるときのSFTデータセットには含まれているかもですが。）\n    - paragraph [ref=s1e1026]: これこそが、本論文で何度も述べている、「強化学習の可能性」です。\n        強化学習は（そのほかの学習手法とは異なり）（人間にとって）予想外で、かつ洗練された結果を導き、人工知能が新たなレベルの知性を手にいれるための核となる技術になりそうです。\n    - heading "DeepSeek-R1-Zeroのまとめ" [level=2] [ref=s1e1028]\n    - paragraph [ref=s1e1029]: DeepSeek-R1-ZeroはDeepSeek-V3-Baseに対して、「GRPO+ルールベース報酬」の強化学習「のみ」を適用し、圧倒的な思考能力を獲得したモデルになります。\n    - paragraph [ref=s1e1030]: DeepSeek-R1-Zeroを学習する際に、問題の解き方を明示的にモデルに教えることや、思考部分を強化させるような報酬は与えていません。\n        ただ、数学の問題に正解することで得られる報酬を与えるだけで、モデルが自律的に高度な問題解決戦略を獲得するという、とても興味深く、恐ろしい結果が示されています。\n    - heading "DeepSeek-R1への道" [level=1] [ref=s1e1032]\n    - paragraph [ref=s1e1033]: さて、ここまで、「DeepSeek-R1-Zero」について、たくさん語ってきましたが、とても魅力的でしたよね？\n        あくまで、「DeepSeek-R1-Zero」は「DeepSeek-V3-Base」に対して、強化学習のみを適用させて、強化学習の可能性を探ったモデルでした。\n    - paragraph [ref=s1e1035]:\n        - text: その結果、こんなにも魅力的なモデル（DeepSeek-R1-Zero）ができてしまったら、、、\n        - strong [ref=s1e1037]: SFTも入れて完全体のモデルを作りたくなりますよね？？？\n    - paragraph [ref=s1e1038]: というモチベーションで「DeepSeek-R1」は作られています。\n    - paragraph [ref=s1e1039]: もし、DeepSeek-R1-Zeroの性能がそんなに高くなかったら、今騒がれているDeepSeek-R1は作ろうという発想にすらならなかったと思います。\n    - heading "DeepSeek-R1-Zeroの課題" [level=2] [ref=s1e1040]\n    - paragraph [ref=s1e1041]: さて、非常に高性能で、示唆に富んだDeepSeek-R1-Zeroでしたが、課題はいくつかあります。 それは下記です。\n    - list [ref=s1e1043]:\n        - listitem [ref=s1e1044]: 学習初期は強化学習の学習が不安定で収束に時間がかかる（コールドスタート問題）\n        - listitem [ref=s1e1045]:\n            - text: 思考能力は高いが、言語が混在（日本語で話していたら急に中国語になるなど）し、人間が理解しにくい\n            - list [ref=s1e1046]:\n                - listitem [ref=s1e1047]: 結果、高度な思考タスクには強いが、（ロールプレイなどの）一般的なタスクには使えないという評価\n    - paragraph [ref=s1e1048]: そこで、上記の課題を解決する旅に出ます。\n    - heading "課題対処への道" [level=2] [ref=s1e1049]\n    - paragraph [ref=s1e1050]: DeepSeek-R1-Zeroでは、ベースモデルであるDeepSeek-V3-Baseに対して、そのまま上述した強化学習を用いて学習をしていました。\n    - paragraph [ref=s1e1051]:\n        - text: しかし、もともとDeepSeek-V3-Baseは長考モデルとして学習されたモデルではないため、いくらsystemプロンプトで長考をさせる制限（\n        - code [ref=s1e1052]: <think>\n        - text: タグ）を入れたとしても、強化学習の初期フェーズは不安定な学習になってしまいます。\n            （例えば、「そもそも適切な形式で思考をしない」、「思考形式は合っているが、意味のある思考をしていない」、その結果として「数学の問題になかなか正答できないまま、報酬が得られず、学習が不安定になる」が考えられます）\n    - paragraph [ref=s1e1054]: そこで、学習初期のコールドスタート問題を防ぐために、DeepSeek-V3-Baseに対して、SFT（Supervised\n        Fine-Tuning）を実施します。 そのために著者らは、少量のCoT（Chain-of-Thought）データセットを構築しています。\n    - heading "CoTデータセットの構築+SFT" [level=3] [ref=s1e1056]\n    - paragraph [ref=s1e1057]: データセットの構築方法は下記が一例として提示されております。\n    - list [ref=s1e1058]:\n        - listitem [ref=s1e1059]: いくつかの長いCoT解答を提示し、モデルに長いCoT形式の回答を生成させてデータセットに追加する\n        - listitem [ref=s1e1060]: モデルに対して、自己反省や検証を行いながら、詳細な回答生成を促すようなプロンプトを送信し、その出力をデータセットに追加する\n        - listitem [ref=s1e1061]: DeepSeek-R1-Zeroの出力を、読みやすい形式に整理してからデータセットに追加する\n        - listitem [ref=s1e1062]: モデルの出力結果を人間が精査して、より適切なCoTデータの形に整理してから、データセットに追加する\n    - paragraph [ref=s1e1063]: 上記の方法で、数千件のCoTデータを構築し、そのデータを用いて、DeepSeek-V3-BaseをSFTしています。\n    - heading "CoTデータセットを用いることの利点" [level=3] [ref=s1e1064]\n    - paragraph [ref=s1e1065]: 著者らは、下記の要素において、CoTデータセットを用いることに利点があると考えています。\n    - paragraph [ref=s1e1066]:\n        - strong [ref=s1e1067]: 読みやすさ\n        - text: DeepSeek-R1-Zeroの一番の課題は、その出力結果が人間が理解するのが難しいことです。\n            回答には、複数の言語が混在していたり、マークダウン形式などの読みやすい形式が適切に使われていないことで、重要な情報が何かわかりにくい状態でした。\n    - paragraph [ref=s1e1070]: そこで、今回のデータセットを作成する際には、各回答の末尾に「要約文」を入れるようにすることで、人間が読みやすい形に出力させるようにしています。\n        具体的には下記のような出力形式で、全てのCoTデータセットは作成されています。\n    - code [ref=s1e1075]: "|special_token|<reasoning_process>|special_token|<summary>"\n    - button "クリップボードにコピー" [ref=s1e1076]:\n        - img [ref=s1e1077]\n    - button "右端で折り返す" [ref=s1e1078]:\n        - img [ref=s1e1079]\n    - paragraph [ref=s1e1080]:\n        - code [ref=s1e1081]: <reasoning_process>\n        - text: の部分に、モデルの思考に該当する部分が、\n        - code [ref=s1e1082]: <summary>\n        - text: の部分に最終的な結論の要約が記載されます。\n    - paragraph [ref=s1e1083]: これにより、「DeepSeek-R1-Zero」にあった、重要な情報が何かわかりにくいという人間が理解するのが難しい問題を緩和します。\n        （言語の混在に関しての対処は後述します）\n    - paragraph [ref=s1e1085]:\n        - strong [ref=s1e1086]: ポテンシャル\n        - text: 今回、強化学習のみを実施するのではなく、人間の事前知識に基づいて作成したCoTデータセットでSFTを実施しています。\n            また、SFTと強化学習を複数回反復して、繰り返し学習をしています。\n            その結果、DeepSeek-R1はDeepSeek-R1-Zeroよりも良い結果を得ることができました。\n    - paragraph [ref=s1e1090]: 著者らは、このような反復的な学習こそが、長考モデル（reasoning models）を学習する上で良い方法であると主張しています。\n    - heading "推論思考の強化学習" [level=3] [ref=s1e1091]\n    - paragraph [ref=s1e1092]: CoTデータセットによりSFTしたDeepSeek-V3-Baseに対して、上述した（DeepSeek-R1-Zeroを学習した）強化学習手法を適用します。\n        これにより、モデルの思考能力が大幅に強化されます。\n    - paragraph [ref=s1e1094]: 一方で、「DeepSeek-R1-Zero」には、言語不一致の問題がありました。\n        そこで、DeepSeek-R1-Zeroのルールベース報酬に、「言語一貫性報酬」を追加で導入しています。\n    - paragraph [ref=s1e1096]: この報酬導入で、実は、モデルの思考能力が若干落ちてしまうことが実験的にわかっています。\n        しかしながら、この報酬があることで、ようやく人間が理解しやすい文章を生成してくれるようになります。\n    - heading "さらなる追加のSFT" [level=3] [ref=s1e1098]\n    - paragraph [ref=s1e1099]: 上記の強化学習が収束したら、さらにデータを追加してSFTをやり直します。\n        ここでは、初期段階の、CoTに重きを置いた、コールドスタートを解消するための学習データだけにとどまらず、ライティング、ロールプレイといった汎用タスクの性能を向上させるため、他ドメインからのデータも合わせて収集します。\n    - heading "Reasoning Data" [level=4] [ref=s1e1101]\n    - paragraph [ref=s1e1102]: まずは、前段階と同様に思考能力を向上させるために、CoT学習データ（Reasoning Data）を取得します。\n    - paragraph [ref=s1e1103]: 取得方法は以下です。\n    - list [ref=s1e1104]:\n        - listitem [ref=s1e1105]:\n            - text: 強化学習済みモデルの出力から、適切なものを収集して学習データに追加\n            - list [ref=s1e1106]:\n                - listitem [ref=s1e1107]: 前段の強化学習では、ルールベースの報酬モデルを利用していたので、それが使える問題（数学など）のみを学習データとしていたが、次の強化学習では報酬モデルを作成するため、もっと広い範囲でCoTができるデータも学習データに追加\n                - listitem [ref=s1e1108]: モデルの出力はまだ読みにくいところがあるため、さまざまフィルタリング\n    - paragraph [ref=s1e1109]: この結果、合計で600kのReasoning Data（学習データ）を収集しています。\n    - heading "Non-Reasoning Data" [level=4] [ref=s1e1110]\n    - paragraph [ref=s1e1111]: ライティングやQ＆A、翻訳などのNon-Reasoning\n        Dataについては、DeepSeek-V3のSFTデータセットの一部を再利用して、学習データを収集しています。 上記のReasoning\n        Data以外のタスク（Non-Reasoning\n        Data）に関しては、DeepSeek-V3によって簡易的なCoTを実施して、回答を生成し、学習データとします。\n        ただし、本当に簡単なタスク（あいさつとか）に関しては、CoTを学習データに追加しません。\n    - paragraph [ref=s1e1114]: こうすることで、合計200kのトレーニングサンプルを最終的に収集したそうです。\n    - paragraph [ref=s1e1115]: 上記の合計800kサンプルの学習データを利用して、DeepSeek-V3-Base（初回強化学習済み）を2epoch分SFTを実施しています。\n    - heading "最後の強化学習" [level=3] [ref=s1e1116]\n    - paragraph [ref=s1e1117]: 最後に、モデルの思考能力をさらに磨くと同時に、モデルの無害性などを向上させることを目的に、強化学習を実施します。\n    - paragraph [ref=s1e1118]: 思考タスクの強化学習に関しては、これまで説明したのと同様に、数学、コード、論理的推論の領域でルールベース報酬を利用して強化学習をします。\n        一方で、思考タスク以外の一般的なデータに対しては、ルールベースで回答の良し悪しを判断することはできないため、報酬モデルを別途DeepSeek-V3パイプラインをベースに作成しています。\n    - heading "DeepSeek-R1のまとめ" [level=2] [ref=s1e1120]\n    - paragraph [ref=s1e1121]: 以上より、DeepSeek-R1では、DeepSeek-V3-Baseに対して、SFTと強化学習を2回実施することにより構築されています。\n    - paragraph [ref=s1e1122]: SFTにより、強化学習のコールドスタート問題の解消と、モデル出力のユーザフレンドリー性の向上、一般的なタスクに対する性能向上に貢献しています。\n    - heading "結果" [level=1] [ref=s1e1123]\n    - paragraph [ref=s1e1124]: 結果に関しては、他の方がさまざま検証していたり、話題になっているので詳しくは書きません。\n        ただ、DeepSeek-R1は、現在のオープンソースモデルの中では、最も高い性能と言ってよく、また、Open\n        AIのo1モデルにも匹敵する性能だと言われています。\n    - heading "蒸留" [level=1] [ref=s1e1126]\n    - paragraph [ref=s1e1127]: 最後に、蒸留モデルについての解説します。（論文でも言及されています）\n        DeepSeek-R1は671Bものモデルパラメータを持つため、一般的な家庭用PCで動作させるのは難しいです。\n    - paragraph [ref=s1e1129]: そこで、より小さなモデルパラメータのモデルに、DeepSeek-R1のような思考能力を与えるために、蒸留を実施しています。\n        （公式が、個人の利用者のことも考えて、家庭用PCで動作する思考モデルを開発してくれるのは非常にありがたいですね）\n    - paragraph [ref=s1e1131]: 具体的には、Qwen2.5-Math-1.5B、Qwen2.5-Math-7B、Qwen2.5-14B、Qwen2.5-32B、Llama-3.1-8B、Llama-3.3-70B-Instructに対して、DeepSeek-R1から取得した800kのトレーニングサンプルを利用して、直接FineTuningする形で、蒸留を実施しています。\n    - paragraph [ref=s1e1132]: 蒸留モデルに対しては、SFTのみを実施し、後段の強化学習は実施しておらず、蒸留モデルに対する追加の強化学習の探究は、著者らが実施する予定はないそうです。\n        （著者らは、強化学習を追加で実施することにより性能が上がると思っているが、その探究は、別のコミュニティに任せると記述しています）\n    - heading "蒸留 vs 強化学習" [level=2] [ref=s1e1134]\n    - paragraph [ref=s1e1135]: 蒸留後の強化学習に関しては、著者らは実験していない、と記述していました。\n        では、小型モデルに対して、蒸留を実施する前に、強化学習を実施したらどうなるのでしょうか？\n        DeepSeek-V3-Baseのように、大幅に思考能力が強化されて、性能向上が期待できるのでしょうか？\n    - paragraph [ref=s1e1138]: それに関しては、著者らが実験してくれています。\n    - paragraph [ref=s1e1139]: ここでは、「Qwen-32B-Base」モデルを利用して、10kステップ以上の大規模強化学習を実施し、「DeepSeek-R1-Zero-Qwen-32B」を構築しました。\n        一方で、「Qwen-32B-Base」モデルに対して、蒸留を実施したモデルを、「DeepSeek-R1Distill-Qwen-32B」モデルとして、この二つのモデルを比較します。\n    - paragraph [ref=s1e1141]: その結果を比較すると、小型モデルに対する強化学習を適用した「DeepSeek-R1-Zero-Qwen-32B」よりも、小型モデルに蒸留を適用した「DeepSeek-R1Distill-Qwen-32B」モデルの方がはるかに高い性能でした。\n    - paragraph [ref=s1e1142]:\n        - strong [ref=s1e1143]: 以下は私の主観ですが、\n        - text: 以上の結果から、\n        - strong [ref=s1e1146]: 強化学習による思考能力の向上は、安易に小型モデルに試していい戦略でない\n        - text: ということが示唆されていると思います。\n    - paragraph [ref=s1e1148]: 強化学習による思考能力の大幅な強化は、おそらくですが「DeepSeek-V3」という元々大規模かつ性能の高いモデルであったからこそ、達成されたのだと思います。\n    - paragraph [ref=s1e1149]: DeepSeek-R1-Zeroの課題でも説明した通り、今回のルールベース報酬では、正しい数学の解答を生成できないと永遠に報酬が獲得できません。\n        小型モデルでは、このコールドスタートの問題が、より致命的なレベルで発生するのだと思います。\n    - paragraph [ref=s1e1151]: したがって、思考モデルを作りたいからといって、小型モデルに対して、安易にルールベース強化学習を試すのはおすすめしません。\n        しかしながら、コールドスタート問題を解決できる戦略があるのであれば、有効な手段になると思われます。\n        （例えば、DeepSeek-R1で蒸留済みの小型モデルを利用する、最初から数学に強い小型モデルで強化学習を試すなどです。重要なのは強化学習のコールドスタートを解消できるかどうかだと思われます。）\n    - paragraph [ref=s1e1154]:\n        - text: （追記） また、同様の実験を小型モデルに対して試したグループもいるようです。 そちらの実験レポートが下記になります。\n        - iframe [ref=s1e1158]: <could not take iframe snapshot>\n        - text: （DeepSeek-R1-Zeroと同様に、「Aha Moment」が見られることもわかっており、非常に興味深いです）\n    - paragraph [ref=s1e1159]: ただ、こちらのレポートにおいても、ベースモデルとしては数学に特化したモデルを利用しています。\n        このことからも、ある程度の数学能力は必要なのだなと思います。\n    - heading "今後の展望" [level=1] [ref=s1e1161]\n    - paragraph [ref=s1e1162]: DeepSeek-R1は、さまざまなタスクにおいて、OpenAI\n        o1に匹敵する性能を達成した、驚異的なオープンソースモデルですが、苦手としている部分もあります。 それが今後の展望に記載されています。\n        （今後の展望を見れば、DeepSeek-R1が苦手な部分を知ることができます）\n    - paragraph [ref=s1e1165]: それは以下の項目です。\n    - list [ref=s1e1166]:\n        - listitem [ref=s1e1167]:\n            - strong [ref=s1e1168]: 一般的な能力\n            - list [ref=s1e1169]:\n                - listitem [ref=s1e1170]: 関数呼び出しやマルチターン対話、複雑なロールプレイング。、json出力において、DeepSeek-V3に及ばない\n        - listitem [ref=s1e1171]:\n            - strong [ref=s1e1172]: 言語の混在\n            - list [ref=s1e1173]:\n                - listitem [ref=s1e1174]: 現在、DeepSeek-R1は英語と中国語に最適化されているため、そのほかの言語を利用した場合、言語の混在が発生する可能性がある\n        - listitem [ref=s1e1175]:\n            - strong [ref=s1e1176]: プロンプトエンジニアリング\n            - list [ref=s1e1177]:\n                - listitem [ref=s1e1178]: DeepSeek-R1はプロンプトに非常に敏感で、特にFewーshotプロンプトを利用すると、パフォーマンスが低下する。\n                - listitem [ref=s1e1179]: 基本的には、問題を直接シンプルに記述し、例を提示しない（Zero-shotプロンプト）を利用することを推奨している\n        - listitem [ref=s1e1180]:\n            - strong [ref=s1e1181]: ソフトウェアエンジニアリングタスク\n            - list [ref=s1e1182]:\n                - listitem [ref=s1e1183]: コード生成やコード保管などのソフトウェアエンジニアリングタスクは、評価に時間がかかるため、強化学習の効率が悪くなることから、現時点では強化学習において、このようなタスクは学習していない。\n                    大々的には学習しておらず、一部のテスト可能な範囲でしか学習できていない。\n                - listitem [ref=s1e1185]: 結果として、このタスクの性能は、DeepSeek-V3からほとんど改善していない。\n    - complementary [ref=s1e1186]:\n        - text: "!"\n        - paragraph [ref=s1e1189]: （追記） masaishiさんのコメントに基づき修正させていただきました。 私の読み間違いで誤解を与えてしまい申し訳ございません。\n        - blockquote [ref=s1e1192]:\n            - paragraph [ref=s1e1193]: 2.2.2. Reward Modeling ・・・ ・Accuracy\n                rewards:・・・Similarly, for LeetCode problems, a compiler can be\n                used to generate feedback based on predefined test cases.\n        - paragraph [ref=s1e1196]: と記載のあるように、全くコード生成などを学習していないわけではなく、一部の、簡単にテストができる問題のみ学習しているようです。\n            ただ、そういう簡易的なタスクだけしか学習できていないため、性能向上は小さめということだと思われます。\n    - paragraph [ref=s1e1198]: 上記の内容が、現時点での「DeepSeek-R1」の課題です。\n        ですが、今後のアップデートで改善していくとも記載されているため、現時点で不満があったとしても、上記に記載されている項目であれば、今後改善する可能性が高いです。\n    - heading "まとめ" [level=1] [ref=s1e1200]\n    - paragraph [ref=s1e1201]: ここまで読んでくださった方、本当にお疲れ様でした。\n        （これでも説明はだいぶ短くした方なんです・・・本当は、もっと強化学習の部分とかを数式をもとに説明したかったです）\n    - paragraph [ref=s1e1203]: DeepSeek-R1の論文いかがでしたか？\n        私にとっては、非常に勉強になるような示唆や知見に溢れており、こんな論文を公開してくれて、かつオープンソースのモデルとしても公開してくれる「DeepSeek-AI」には感謝しかありません。\n    - paragraph [ref=s1e1205]: 本記事が、皆様の理解の助けになり、一人でも多くの方が原論文に興味を持って、読んでくれましたら幸いです。\n        ここまで読んでくださってありがとうございました！\n    - heading "参考文献" [level=1] [ref=s1e1207]\n    - paragraph [ref=s1e1208]: 本論文は、論文の中では読みやすい方だとは思いますが、強化学習に関しては、一定程度の基礎知識は要求されます。\n        そこで、本論文を読む上で必要な知識を収集できるおすすめの書籍に関して記載します。 （少なくとも私はこの書籍で学習しました）\n    - paragraph [ref=s1e1211]: （書籍のリンクはamazonアフィリエイトリンクになります）\n    - heading "強化学習" [level=4] [ref=s1e1212]\n    - paragraph [ref=s1e1213]: 強化学習に関しておすすめの本は、下記の本たちです。\n    - paragraph [ref=s1e1214]:\n        - link "ゼロから作るDeep Learning ❹ ―強化学習編" [ref=s1e1215]:\n            - /url: https://amzn.to/40qFuNg\n        - text: まずは、定番の本です。\n            強化学習に関して、本当に最初の部分（マルコフ決定過程とは何か）から、非常に詳細かつわかりやすく解説されており、深層学習を利用して、どのように強化学習をすることできるのかまで、しっかり書かれているためおすすめです。\n            また、コードと理論のバランスもちょうどよく、理系大学生とかであれば問題なく読み進められると思います。\n            特に、強化学習の基礎の部分が、詳細かつ分かりやすく書かれているため、一冊目におすすめです。\n    - paragraph [ref=s1e1220]:\n        - link "機械学習スタートアップシリーズ Ｐｙｔｈｏｎで学ぶ強化学習 ［改訂第２版］ 入門から実践まで (ＫＳ情報科学専門書)" [ref=s1e1221]:\n            - /url: https://amzn.to/4joKbjg\n    - paragraph [ref=s1e1223]: 普段からpythonを使っており、数式よりも実装に興味がある方にとっては、こちらの書籍もおすすめです。\n        こちらの書籍は、強化学習の基礎よりは、少し応用というか、実用を意識した書籍になっており、実務でよく使われるモデルなどに関しても、詳しく説明されています。\n        強化学習の基礎はわかっている人や、もっと実装をベースに学びたいというかたにおすすめです。\n    - paragraph [ref=s1e1226]:\n        - link "強化学習 (機械学習プロフェッショナルシリーズ)" [ref=s1e1227]:\n            - /url: https://amzn.to/40NX5QM\n        - link "強化学習から信頼できる意思決定へ (AI/データサイエンスライブラリ“基礎から応用へ” 5)" [ref=s1e1229]:\n            - /url: https://amzn.to/4jtFq8d\n    - paragraph [ref=s1e1230]: 名著です。\n        どちらも、理論説明の部分が非常に面白く、正直難易度は高いですが、強化学習の理論をちゃんと理解したいという方におすすめの書籍です。\n        下の本に関しては、2024年の本ということもあり、近年の強化学習の実世界での応用と、理論的な定式化の架け橋になっているような本になっており、非常に面白いです。\n    - heading "AlphaGO Zero" [level=4] [ref=s1e1233]\n    - paragraph [ref=s1e1234]:\n        - link "AlphaZero 深層学習・強化学習・探索 人工知能プログラミング実践入門" [ref=s1e1235]:\n            - /url: https://amzn.to/3Cdnuy1\n    - paragraph [ref=s1e1236]: だいぶ昔に購入させていただいた本ですが、まさか2025年に活躍することがあろうとは。\n        という感じですが、こちらの書籍も参考にさせていただきました。 Alpha\n        Zeroを作る上で核となる技術を余すところなく解説されており、私は非常に勉強になりました。 Alpha\n        Zeroに興味がある方は、ぜひ一度ご一読ください。\n    - heading "LLM、RLHF, DPO" [level=4] [ref=s1e1240]\n    - paragraph [ref=s1e1241]:\n        - link "大規模言語モデル入門Ⅱ〜生成型LLMの実装と評価" [ref=s1e1242]:\n            - /url: https://amzn.to/3Cs3Uhn\n        - text: よく紹介させていただいておりますが、こちらの書籍は、LLMのファインチューニングから、RLHF、RAG、分散学習にかけて、本当に幅広く解説されており、いつも参考にさせていただいております。\n            今回のDPOやRLHFの解説部分でも、とても活躍してくれました。\n            LLMを取り扱っている方は、とりあえず買っておいても損はないと思います。\n            さまざまな章、ページで読めば読むほど新しい発見が得られる、スルメのような本だなといつも思っております。\n    - button "いいね" [ref=s1e1249]:\n        - img [ref=s1e1250]\n    - text: "921"\n    - button "ブックマーク" [ref=s1e1266]:\n        - img [ref=s1e1267]\n    - text: "344"\n    - button "その他の操作" [ref=s1e1272]:\n        - img [ref=s1e1273]\n    - tooltip "X(Twitter)にポスト" [ref=s1e1276]:\n        - img [ref=s1e1277]\n        - text: X(Twitter)にポスト\n    - tooltip "Facebookに投稿" [ref=s1e1283]:\n        - img [ref=s1e1284]\n        - text: Facebookに投稿\n    - tooltip "はてなブックマークに登録" [ref=s1e1290]:\n        - img [ref=s1e1291]\n        - text: はてなブックマークに登録\n    - complementary [ref=s1e1294]:\n        - link "asap" [ref=s1e1296]:\n            - /url: /asap\n            - img "asap" [ref=s1e1297]\n        - link "asap" [ref=s1e1299]:\n            - /url: /asap\n        - paragraph [ref=s1e1301]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n        - button "フォロー" [ref=s1e1305]\n        - tooltip "@personabb" [ref=s1e1306]:\n            - img "GitHub" [ref=s1e1307]\n            - text: "@personabb"\n        - tooltip "@asap2650" [ref=s1e1309]:\n            - img "X(Twitter)" [ref=s1e1310]\n            - text: "@asap2650"\n        - tooltip "yard.tips" [ref=s1e1316]:\n            - img "リンク" [ref=s1e1317]\n            - text: yard.tips\n    - complementary [ref=s1e1320]:\n        - text: バッジを贈って著者を応援しよう\n        - paragraph [ref=s1e1324]: バッジを受け取った著者にはZennから現金やAmazonギフトカードが還元されます。\n        - button "バッジを贈る" [ref=s1e1326]\n        - img [ref=s1e1327]\n    - text: この記事に贈られたバッジ\n    - img "Thank you" [ref=s1e1338]\n    - img [ref=s1e1340]\n    - img "参考になった" [ref=s1e1352]\n    - img [ref=s1e1354]\n    - img "もっと読みたい" [ref=s1e1363]\n    - img [ref=s1e1365]\n    - img "参考になった" [ref=s1e1375]\n    - img [ref=s1e1377]\n    - img "もっと読みたい" [ref=s1e1386]\n    - img [ref=s1e1388]\n    - heading "Discussion" [level=3] [ref=s1e1400]\n    - article [ref=s1e1403]:\n        - link "はまなすなぎさ" [ref=s1e1405]:\n            - /url: /nhamanasu\n            - img "はまなすなぎさ" [ref=s1e1406]\n        - link "はまなすなぎさ" [ref=s1e1407]:\n            - /url: /nhamanasu\n        - link "2025年1月28日 8時33分" [ref=s1e1408]:\n            - /url: "#comment-f230c9d295b3f5"\n            - tooltip "2025年1月28日 8時33分" [ref=s1e1409]: 2ヶ月前 2025年1月28日 8時33分\n        - button "メニューを開く" [ref=s1e1411]:\n            - img [ref=s1e1412]\n        - paragraph [ref=s1e1417]: 非常にためになる骨太な記事、ありがとうございます！とても勉強になります。\n        - paragraph [ref=s1e1418]: ちょっとした誤字かも？の箇所なのですが、冒頭の「強化学習手法「DRPO」すごい！」の部分は「GRPO」ではないかなと思いました。ご報告まで。\n        - button "いいね" [ref=s1e1421]:\n            - img [ref=s1e1422]\n        - text: "1"\n        - button "コメント 1" [ref=s1e1437]:\n            - img "コメント" [ref=s1e1438]\n            - text: "1"\n        - article [ref=s1e1442]:\n            - link "asap" [ref=s1e1444]:\n                - /url: /asap\n                - img "asap" [ref=s1e1445]\n            - link "asap" [ref=s1e1446]:\n                - /url: /asap\n            - link "2025年1月28日 10時23分" [ref=s1e1447]:\n                - /url: "#comment-41cdebd92d8d44"\n                - tooltip "2025年1月28日 10時23分" [ref=s1e1448]: 2ヶ月前 2025年1月28日 10時23分\n            - button "メニューを開く" [ref=s1e1450]:\n                - img [ref=s1e1451]\n            - paragraph [ref=s1e1456]: 読んでくださりありがとうございます！ おっしゃる通りです！修正させていただきました！\n            - button "いいね" [ref=s1e1460]:\n                - img [ref=s1e1461]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1477]\n    - article [ref=s1e1479]:\n        - link "bang" [ref=s1e1481]:\n            - /url: /zenmnm\n            - img "bang" [ref=s1e1482]\n        - link "bang" [ref=s1e1483]:\n            - /url: /zenmnm\n        - link "2025年1月28日 13時46分" [ref=s1e1484]:\n            - /url: "#comment-d0aac9375fbbf7"\n            - tooltip "2025年1月28日 13時46分" [ref=s1e1485]: 2ヶ月前 2025年1月28日 13時46分\n        - button "メニューを開く" [ref=s1e1487]:\n            - img [ref=s1e1488]\n        - paragraph [ref=s1e1493]: AIについてはまだ詳しくありませんが、興味はあります。同じように古いモデルを使って新しいモデルを作るのですが、蒸留方法の代わりにモデルを組み合わせることは可能でしょうか?sakana\n            aiのようにです\n        - button "いいね" [ref=s1e1496]:\n            - img [ref=s1e1497]\n        - text: "1"\n        - button "コメント 2" [ref=s1e1512]:\n            - img "コメント" [ref=s1e1513]\n            - text: "2"\n        - article [ref=s1e1517]:\n            - link "asap" [ref=s1e1519]:\n                - /url: /asap\n                - img "asap" [ref=s1e1520]\n            - link "asap" [ref=s1e1521]:\n                - /url: /asap\n            - link "2025年1月28日 15時33分" [ref=s1e1522]:\n                - /url: "#comment-27e79c6ee51f2a"\n                - tooltip "2025年1月28日 15時33分" [ref=s1e1523]: 2ヶ月前 2025年1月28日 15時33分\n            - button "メニューを開く" [ref=s1e1525]:\n                - img [ref=s1e1526]\n            - paragraph [ref=s1e1531]: モデルマージに関してはあまり詳しくないですが、基本的には同じアーキテクチャのモデルを利用することになると思います。したがってDeepSeek-R1によって蒸留されたLlamaモデルとかであれば、他のLlamaモデル（同じモデルアーキテクチャ）とモデルマージすることで、推論機能を付け加えたり、日本語機能を強化したりすることが期待できると思います。\n            - button "いいね" [ref=s1e1534]:\n                - img [ref=s1e1535]\n        - article [ref=s1e1549]:\n            - link "bang" [ref=s1e1551]:\n                - /url: /zenmnm\n                - img "bang" [ref=s1e1552]\n            - link "bang" [ref=s1e1553]:\n                - /url: /zenmnm\n            - link "2025年1月28日 16時42分" [ref=s1e1554]:\n                - /url: "#comment-0221309240798f"\n                - tooltip "2025年1月28日 16時42分" [ref=s1e1555]: 2ヶ月前 2025年1月28日 16時42分\n            - button "メニューを開く" [ref=s1e1557]:\n                - img [ref=s1e1558]\n            - paragraph [ref=s1e1563]: お返事ありがとうございます。私はまだ初心者なので、これは私の軽い考えです。今では研究のプロセスごとにいくつかの方法があり、それらを組み合わせたり微調整したりすれば、たくさんのモデルができるようになっているでしょう。\n            - button "いいね" [ref=s1e1566]:\n                - img [ref=s1e1567]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1583]\n    - article [ref=s1e1585]:\n        - link "HWSeungwoo" [ref=s1e1587]:\n            - /url: /syoyuu96\n            - img "HWSeungwoo" [ref=s1e1588]\n        - link "HWSeungwoo" [ref=s1e1589]:\n            - /url: /syoyuu96\n        - link "2025年1月28日 19時28分" [ref=s1e1590]:\n            - /url: "#comment-a07230ddbdaebe"\n            - tooltip "2025年1月28日 19時28分" [ref=s1e1591]: 2ヶ月前 2025年1月28日 19時28分\n        - button "メニューを開く" [ref=s1e1593]:\n            - img [ref=s1e1594]\n        - paragraph [ref=s1e1599]: Deep\n            Seekに関する記事やニュースはたくさん目にしますが、ここまで深掘りされた内容には驚きました。とても勉強になりました。ありがとうございます！\n        - button "いいね" [ref=s1e1602]:\n            - img [ref=s1e1603]\n        - text: "1"\n        - button "コメント 1" [ref=s1e1618]:\n            - img "コメント" [ref=s1e1619]\n            - text: "1"\n        - article [ref=s1e1623]:\n            - link "asap" [ref=s1e1625]:\n                - /url: /asap\n                - img "asap" [ref=s1e1626]\n            - link "asap" [ref=s1e1627]:\n                - /url: /asap\n            - link "2025年1月28日 19時32分" [ref=s1e1628]:\n                - /url: "#comment-1e1e3abc86df00"\n                - tooltip "2025年1月28日 19時32分" [ref=s1e1629]: 2ヶ月前 2025年1月28日 19時32分\n            - button "メニューを開く" [ref=s1e1631]:\n                - img [ref=s1e1632]\n            - paragraph [ref=s1e1637]: 読んでくださってありがとうございます！ コメント嬉しいです！ありがとうございます！\n            - button "いいね" [ref=s1e1641]:\n                - img [ref=s1e1642]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1658]\n    - article [ref=s1e1660]:\n        - link "masaishi" [ref=s1e1662]:\n            - /url: /masaishi\n            - img "masaishi" [ref=s1e1663]\n        - link "masaishi" [ref=s1e1664]:\n            - /url: /masaishi\n        - link "2025年1月29日 16時14分" [ref=s1e1665]:\n            - /url: "#comment-a0d75bd96cadfc"\n            - tooltip "2025年1月29日 16時14分" [ref=s1e1666]: 2ヶ月前 2025年1月29日 16時14分\n        - button "メニューを開く" [ref=s1e1668]:\n            - img [ref=s1e1669]\n        - paragraph [ref=s1e1674]: 素晴らしい記事をありがとうございます！\n            GRPOなどの強化学習手法に関する解説も大変勉強になりました。この記事をきっかけに、DeepSeek-R1の論文にも興味を持ち、読んでみようと思えました。\n        - paragraph [ref=s1e1676]: 一点だけ気になったことがあり、もし私の理解が誤っていたら申し訳ありません。\n            "コード生成やコード保管などのソフトウェアエンジニアリングタスク"の強化学習に用いられていないとありますが、論文の\n            2.2.2（Reward Modeling）の\n        - blockquote [ref=s1e1678]:\n            - paragraph [ref=s1e1679]: For LeetCode problems, a compiler can be used to\n                generate feedback based on predefined test cases.\n        - paragraph [ref=s1e1680]: という記述を見ると、少なくともアルゴリズム問題のコード生成には強化学習が用いられているのかなと思います。\n        - paragraph [ref=s1e1681]: ソフトウェアエンジニアリングタスクは強化学習していませんが、コード生成自体は行っているような気がするので、注釈などで少し補足していただけると、読者としては誤解が減って助かるかもしれません。しかし、かなり細かい点ですので、最終的には著者さまの判断にお任せいたします。\n        - button "いいね" [ref=s1e1684]:\n            - img [ref=s1e1685]\n        - text: "2"\n        - button "コメント 2" [ref=s1e1700]:\n            - img "コメント" [ref=s1e1701]\n            - text: "2"\n        - article [ref=s1e1705]:\n            - link "asap" [ref=s1e1707]:\n                - /url: /asap\n                - img "asap" [ref=s1e1708]\n            - link "asap" [ref=s1e1709]:\n                - /url: /asap\n            - link "2025年1月29日 16時37分" [ref=s1e1710]:\n                - /url: "#comment-afb254c15106b7"\n                - tooltip "2025年1月29日 16時37分" [ref=s1e1711]: 2ヶ月前 2025年1月29日 16時37分\n            - button "メニューを開く" [ref=s1e1713]:\n                - img [ref=s1e1714]\n            - paragraph [ref=s1e1719]: コメントありがとうございます！ おっしゃる通りでした！読み間違いです。ご指摘助かります。\n            - blockquote [ref=s1e1721]:\n                - paragraph [ref=s1e1722]: Due to the long evaluation times, which impact the\n                    efficiency of the RL process, large-scale RL has not been\n                    applied extensively in software engineering tasks.\n            - paragraph [ref=s1e1723]: この部分の文章をもとに記載していましたが、大々的には適用されていないだけで、一部は学習に入っているっぽいですね。\n            - paragraph [ref=s1e1724]: 助かります！修正もこれからさせていただきます\n            - button "いいね" [ref=s1e1727]:\n                - img [ref=s1e1728]\n            - text: "2"\n        - article [ref=s1e1743]:\n            - link "masaishi" [ref=s1e1745]:\n                - /url: /masaishi\n                - img "masaishi" [ref=s1e1746]\n            - link "masaishi" [ref=s1e1747]:\n                - /url: /masaishi\n            - link "2025年1月29日 19時5分" [ref=s1e1748]:\n                - /url: "#comment-debff7de1aad74"\n                - tooltip "2025年1月29日 19時5分" [ref=s1e1749]: 2ヶ月前 2025年1月29日 19時5分\n            - button "メニューを開く" [ref=s1e1751]:\n                - img [ref=s1e1752]\n            - paragraph [ref=s1e1757]: ご丁寧にありがとうございます！\n            - button "いいね" [ref=s1e1760]:\n                - img [ref=s1e1761]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1777]\n    - article [ref=s1e1779]:\n        - link "RyotoTanaka" [ref=s1e1781]:\n            - /url: /ryototanaka\n            - img "RyotoTanaka" [ref=s1e1782]\n        - link "RyotoTanaka" [ref=s1e1783]:\n            - /url: /ryototanaka\n        - link "2025年1月30日 8時0分" [ref=s1e1784]:\n            - /url: "#comment-4ae639fe8d1892"\n            - tooltip "2025年1月30日 8時0分" [ref=s1e1785]: 2ヶ月前 2025年1月30日 8時0分\n        - button "メニューを開く" [ref=s1e1787]:\n            - img [ref=s1e1788]\n        - paragraph [ref=s1e1793]: 読み応えがありました。素晴らしい記事をありがとうございます！ 些細な事なのですが、DPOは Direct\n            Preference Optimization ではないでしょうか。\n        - button "いいね" [ref=s1e1797]:\n            - img [ref=s1e1798]\n        - text: "1"\n        - button "コメント 1" [ref=s1e1813]:\n            - img "コメント" [ref=s1e1814]\n            - text: "1"\n        - article [ref=s1e1818]:\n            - link "asap" [ref=s1e1820]:\n                - /url: /asap\n                - img "asap" [ref=s1e1821]\n            - link "asap" [ref=s1e1822]:\n                - /url: /asap\n            - link "2025年1月30日 13時15分" [ref=s1e1823]:\n                - /url: "#comment-ec7e829404f7bc"\n                - tooltip "2025年1月30日 13時15分" [ref=s1e1824]: 2ヶ月前 2025年1月30日 13時15分\n            - button "メニューを開く" [ref=s1e1826]:\n                - img [ref=s1e1827]\n            - paragraph [ref=s1e1832]: ほんとですね！間違えて入力してました ありがとうございます！\n            - button "いいね" [ref=s1e1836]:\n                - img [ref=s1e1837]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1853]\n    - text: ログインするとコメントできます\n    - button "Login" [ref=s1e1857]\n    - complementary [ref=s1e1858]:\n        - link "asap" [ref=s1e1863]:\n            - /url: /asap\n            - img "asap" [ref=s1e1864]\n        - link "asap" [ref=s1e1866]:\n            - /url: /asap\n        - button "フォロー" [ref=s1e1868]\n        - link "@personabb" [ref=s1e1869]:\n            - /url: https://github.com/personabb\n            - tooltip "@personabb" [ref=s1e1870]:\n                - img "GitHub" [ref=s1e1871]\n                - text: "@personabb"\n        - link "@asap2650" [ref=s1e1873]:\n            - /url: https://twitter.com/asap2650\n            - tooltip "@asap2650" [ref=s1e1874]:\n                - img "X(Twitter)" [ref=s1e1875]\n                - text: "@asap2650"\n        - link "yard.tips" [ref=s1e1881]:\n            - /url: https://yard.tips/asap2650\n            - tooltip "yard.tips" [ref=s1e1882]:\n                - img "リンク" [ref=s1e1883]\n                - text: yard.tips\n        - paragraph [ref=s1e1886]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n        - button "バッジを贈る" [ref=s1e1889]\n        - link "バッジを贈るとは" [ref=s1e1891]:\n            - /url: /faq#badges\n            - text: バッジを贈るとは\n            - img [ref=s1e1892]\n        - text: 目次\n        - list [ref=s1e1901]:\n            - listitem [ref=s1e1902]:\n                - link "はじめに" [ref=s1e1903]:\n                    - /url: "#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB"\n            - listitem [ref=s1e1904]:\n                - link "本論文の興味深いところ" [ref=s1e1905]:\n                    - /url: "#%E6%9C%AC%E8%AB%96%E6%96%87%E3%81%AE%E8%88%88%E5%91%B3%E6%B7%B1%E3%81%8\\\n                        4%E3%81%A8%E3%81%93%E3%82%8D"\n            - listitem [ref=s1e1906]:\n                - link "本論文の流れ" [ref=s1e1907]:\n                    - /url: "#%E6%9C%AC%E8%AB%96%E6%96%87%E3%81%AE%E6%B5%81%E3%82%8C"\n                - list [ref=s1e1908]:\n                    - listitem [ref=s1e1909]:\n                        - link "前提のモデル構造" [ref=s1e1910]:\n                            - /url: "#%E5%89%8D%E6%8F%90%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E6%A7%8B%E9%80%A\\\n                                0"\n                    - listitem [ref=s1e1911]:\n                        - link "DeepSeek-R1-Zeroを強化学習を用いて作る" [ref=s1e1912]:\n                            - /url: "#deepseek-r1-zero%E3%82%92%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E3%82%92%\\\n                                E7%94%A8%E3%81%84%E3%81%A6%E4%BD%9C%E3%82%8B"\n                    - listitem [ref=s1e1913]:\n                        - link "DeepSeek-R1をSFT+強化学習を用いて作る" [ref=s1e1914]:\n                            - /url: "#deepseek-r1%E3%82%92sft%2B%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E3%82%92\\\n                                %E7%94%A8%E3%81%84%E3%81%A6%E4%BD%9C%E3%82%8B"\n                    - listitem [ref=s1e1915]:\n                        - link "小型モデルへの蒸留を試す" [ref=s1e1916]:\n                            - /url: "#%E5%B0%8F%E5%9E%8B%E3%83%A2%E3%83%87%E3%83%AB%E3%81%B8%E3%81%AE%E8%92%B\\\n                                8%E7%95%99%E3%82%92%E8%A9%A6%E3%81%99"\n            - listitem [ref=s1e1917]:\n                - link "DeepSeek-R1-Zeroを作るまで" [ref=s1e1918]:\n                    - /url: "#deepseek-r1-zero%E3%82%92%E4%BD%9C%E3%82%8B%E3%81%BE%E3%81%A7"\n                - list [ref=s1e1919]:\n                    - listitem [ref=s1e1920]:\n                        - link "元々のアーキテクチャ" [ref=s1e1921]:\n                            - /url: "#%E5%85%83%E3%80%85%E3%81%AE%E3%82%A2%E3%83%BC%E3%82%AD%E3%83%86%E3%82%A\\\n                                F%E3%83%81%E3%83%A3"\n                    - listitem [ref=s1e1922]:\n                        - link "すごいぞ！強化学習" [ref=s1e1923]:\n                            - /url: "#%E3%81%99%E3%81%94%E3%81%84%E3%81%9E%EF%BC%81%E5%BC%B7%E5%8C%96%E5%AD%A\\\n                                6%E7%BF%92"\n                    - listitem [ref=s1e1924]:\n                        - link "そもそもLLMで強化学習とは？" [ref=s1e1925]:\n                            - /url: "#%E3%81%9D%E3%82%82%E3%81%9D%E3%82%82llm%E3%81%A7%E5%BC%B7%E5%8C%96%E5%A\\\n                                D%A6%E7%BF%92%E3%81%A8%E3%81%AF%EF%BC%9F"\n                    - listitem [ref=s1e1926]:\n                        - link "ZeroのSystemプロンプト" [ref=s1e1927]:\n                            - /url: "#zero%E3%81%AEsystem%E3%83%97%E3%83%AD%E3%83%B3%E3%83%97%E3%83%88"\n                    - listitem [ref=s1e1928]:\n                        - link "Zeroの「Aha Moment」" [ref=s1e1929]:\n                            - /url: "#zero%E3%81%AE%E3%80%8Caha-moment%E3%80%8D"\n                    - listitem [ref=s1e1930]:\n                        - link "DeepSeek-R1-Zeroのまとめ" [ref=s1e1931]:\n                            - /url: "#deepseek-r1-zero%E3%81%AE%E3%81%BE%E3%81%A8%E3%82%81"\n            - listitem [ref=s1e1932]:\n                - link "DeepSeek-R1への道" [ref=s1e1933]:\n                    - /url: "#deepseek-r1%E3%81%B8%E3%81%AE%E9%81%93"\n                - list [ref=s1e1934]:\n                    - listitem [ref=s1e1935]:\n                        - link "DeepSeek-R1-Zeroの課題" [ref=s1e1936]:\n                            - /url: "#deepseek-r1-zero%E3%81%AE%E8%AA%B2%E9%A1%8C"\n                    - listitem [ref=s1e1937]:\n                        - link "課題対処への道" [ref=s1e1938]:\n                            - /url: "#%E8%AA%B2%E9%A1%8C%E5%AF%BE%E5%87%A6%E3%81%B8%E3%81%AE%E9%81%93"\n                    - listitem [ref=s1e1939]:\n                        - link "DeepSeek-R1のまとめ" [ref=s1e1940]:\n                            - /url: "#deepseek-r1%E3%81%AE%E3%81%BE%E3%81%A8%E3%82%81"\n            - listitem [ref=s1e1941]:\n                - link "結果" [ref=s1e1942]:\n                    - /url: "#%E7%B5%90%E6%9E%9C"\n            - listitem [ref=s1e1943]:\n                - link "蒸留" [ref=s1e1944]:\n                    - /url: "#%E8%92%B8%E7%95%99"\n                - list [ref=s1e1945]:\n                    - listitem [ref=s1e1946]:\n                        - link "蒸留 vs 強化学習" [ref=s1e1947]:\n                            - /url: "#%E8%92%B8%E7%95%99-vs-%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92"\n            - listitem [ref=s1e1948]:\n                - link "今後の展望" [ref=s1e1949]:\n                    - /url: "#%E4%BB%8A%E5%BE%8C%E3%81%AE%E5%B1%95%E6%9C%9B"\n            - listitem [ref=s1e1950]:\n                - link "まとめ" [ref=s1e1951]:\n                    - /url: "#%E3%81%BE%E3%81%A8%E3%82%81"\n            - listitem [ref=s1e1952]:\n                - link "参考文献" [ref=s1e1953]:\n                    - /url: "#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"\n        - paragraph [ref=s1e1955]: Zennからのお知らせ\n        - article [ref=s1e1958]:\n            - link "第２回 AI Agent Hackathon with Google Cloud 第２回 AI Agent Hackathon with Google Cloud 4/14登録受付開始！" [ref=s1e1959]:\n                - /url: /hackathons/google-cloud-japan-ai-hackathon-vol2\n                - img "第２回 AI Agent Hackathon with Google Cloud" [ref=s1e1961]\n                - paragraph [ref=s1e1962]: 第２回 AI Agent Hackathon with Google Cloud 4/14登録受付開始！\n        - article [ref=s1e1963]:\n            - link "Zenncafe日比谷#2 生成AIで変わる仕事 Zenncafe日比谷#2 生成AIで変わる仕事" [ref=s1e1964]:\n                - /url: https://zenn.connpass.com/event/351131/\n                - img "Zenncafe日比谷#2 生成AIで変わる仕事" [ref=s1e1966]\n                - paragraph [ref=s1e1967]: Zenncafe日比谷#2 生成AIで変わる仕事\n- contentinfo [ref=s1e1969]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1973]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1974]\n    - paragraph [ref=s1e1982]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e1984]:\n        - heading "About" [level=4] [ref=s1e1985]\n        - list [ref=s1e1986]:\n            - listitem [ref=s1e1987]:\n                - link "Zennについて" [ref=s1e1989]:\n                    - /url: /about\n            - listitem [ref=s1e1990]:\n                - link "運営会社" [ref=s1e1991]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e1992]:\n                - link "お知らせ・リリース" [ref=s1e1993]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e1994]:\n                - link "イベント" [ref=s1e1996]:\n                    - /url: /events\n    - navigation [ref=s1e1997]:\n        - heading "Guides" [level=4] [ref=s1e1998]\n        - list [ref=s1e1999]:\n            - listitem [ref=s1e2000]:\n                - link "使い方" [ref=s1e2002]:\n                    - /url: /manual\n            - listitem [ref=s1e2003]:\n                - link "法人向けメニュー" [ref=s1e2005]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e2007]:\n                - link "Publication / Pro" [ref=s1e2009]:\n                    - /url: /publications\n            - listitem [ref=s1e2010]:\n                - link "よくある質問" [ref=s1e2012]:\n                    - /url: /faq\n    - navigation [ref=s1e2013]:\n        - heading "Links" [level=4] [ref=s1e2014]\n        - list [ref=s1e2015]:\n            - listitem [ref=s1e2016]:\n                - link "X(Twitter)" [ref=s1e2017]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e2018]:\n                - link "GitHub" [ref=s1e2019]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e2020]:\n                - link "メディアキット" [ref=s1e2022]:\n                    - /url: /mediakit\n    - navigation [ref=s1e2023]:\n        - heading "Legal" [level=4] [ref=s1e2024]\n        - list [ref=s1e2025]:\n            - listitem [ref=s1e2026]:\n                - link "利用規約" [ref=s1e2028]:\n                    - /url: /terms\n            - listitem [ref=s1e2029]:\n                - link "プライバシーポリシー" [ref=s1e2031]:\n                    - /url: /privacy\n            - listitem [ref=s1e2032]:\n                - link "特商法表記" [ref=s1e2034]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e2037]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e2038]\n- alert [ref=s1e2057]\n```\n', name='browser_navigate', tool_call_id='d85f8505-9c2a-4960-b17f-9ef88d983d9d'), AIMessage(content='まず、指定されたZennの著者のページにアクセスし、最もいいねが多い記事を特定しました。その結果、「DeepSeek-R1の論文読んだ？【勉強になるよ】」という記事が最も評価されていることがわかりました。\n\n次に、その記事の内容を詳しく調べました。このブログ記事は、DeepSeek-R1という大規模言語モデルに関する論文を解説したものです。記事は、論文の内容をわかりやすく説明することに重点を置いており、特に強化学習の概念や、DeepSeek-R1を開発する上で用いられたGRPO（Group Relative Policy Optimization）という手法について詳しく解説しています。\n\n記事が評価されている理由として、以下の点が考えられます。\n\n1.  **技術的な深さと分かりやすさの両立:** 大規模言語モデルや強化学習といった複雑なトピックを扱いながらも、専門知識がない読者にも理解できるように、かみ砕いた説明がなされています。\n2.  **論文の内容の網羅性:** DeepSeek-R1の論文の重要なポイントを漏れなくカバーしており、論文を読むための良い入門となっています。\n3.  **著者自身の考察:** 論文の内容を解説するだけでなく、著者自身の考察や解釈が加えられており、読者に新たな視点を提供しています。\n4.  **読者への配慮:** 記事の冒頭で、誤りがあるかもしれないことを断り、コメントでの指摘を歓迎するなど、読者とのコミュニケーションを重視する姿勢が見られます。\n5.  **参考文献の紹介:** 記事の最後に、強化学習や関連技術を学ぶための参考文献が紹介されており、読者の学習をサポートする姿勢が評価されていると考えられます。\n\nこれらの要素が組み合わさることで、この記事はDeepSeek-R1に関心のあるエンジニアにとって非常に価値のある情報源となり、高い評価を得ていると考えられます。', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-d5ac6129-cca3-4759-ac6a-eafe68db505f-0', usage_metadata={'input_tokens': 47987, 'output_tokens': 387, 'total_tokens': 48374, 'input_token_details': {'cache_read': 0}}), HumanMessage(content=[{'type': 'text', 'text': 'LLMに強化学習ってどういうこと？GRPOって何？'}], additional_kwargs={}, response_metadata={}), AIMessage(content='LLM（大規模言語モデル）に対する強化学習と、GRPO（Group Relative Policy Optimization）について説明します。\n\n### LLMに対する強化学習\n\nLLMに対する強化学習は、LLMがより良いテキストを生成するように学習させるための手法です。通常のLLMの学習では、大量のテキストデータを用いて、与えられたテキストの続きを予測するように学習します。これに対して、強化学習では、LLMが生成したテキストに対して「報酬」を与え、その報酬を最大化するように学習します。\n\n**なぜ強化学習が必要なのか？**\n\nLLMは、大量のテキストデータから学習することで、自然な文章を生成する能力を獲得します。しかし、必ずしも人間が望むような文章を生成するとは限りません。例えば、以下のような課題があります。\n\n*   **目的の達成:** 特定のタスク（質問応答、要約など）において、必ずしも最適な回答を生成するとは限らない。\n*   **安全性:** 有害な情報や偏見のある情報を生成する可能性がある。\n*   **創造性:** 新しいアイデアや表現を生み出す能力が限られている。\n\n強化学習は、これらの課題を克服するために用いられます。\n\n**強化学習の基本的な流れ**\n\n1.  **LLM（エージェント）:** テキストを生成する主体。\n2.  **環境:** LLMがテキストを生成する対象となるタスクや状況。\n3.  **報酬:** LLMが生成したテキストの良し悪しを評価する指標。\n4.  **学習:** LLMが報酬を最大化するように、テキスト生成の方針（方策）を調整する。\n\n具体的には、以下のような手順で学習が進みます。\n\n1.  LLMがプロンプト（指示）を受け取り、テキストを生成します。\n2.  生成されたテキストに対して、報酬関数が報酬を与えます。\n    *   例えば、質問応答タスクであれば、正解との一致度合いが高いほど高い報酬を与えます。\n    *   安全性に関する報酬であれば、有害な情報を生成した場合に低い報酬を与えます。\n3.  LLMは、その報酬に基づいて、自身のテキスト生成の方針を調整します。\n    *   具体的には、報酬が高くなるような単語やフレーズを選択する確率を高めます。\n4.  このプロセスを繰り返すことで、LLMはより高い報酬を得られるようなテキストを生成するように学習していきます。\n\n### GRPO（Group Relative Policy Optimization）とは？\n\nGRPOは、LLMに対する強化学習の一種で、特に大規模なLLMを効率的に学習させるために開発された手法です。\n\n**PPO（Proximal Policy Optimization）との比較**\n\nLLMの強化学習でよく用いられる手法として、PPO（Proximal Policy Optimization）があります。PPOでは、報酬に加えて、LLMの出力が大きく変化しないように制約を加えながら学習を進めます。しかし、PPOでは、状態価値モデルという追加のモデルを学習する必要があり、計算コストが高くなるという課題がありました。\n\nGRPOは、この状態価値モデルを不要にし、より効率的に学習を進めることを目指します。\n\n**GRPOの仕組み**\n\nGRPOでは、LLMに複数のテキストを生成させ、それらのテキストに対する報酬を比較することで、相対的な評価を行います。具体的には、以下の手順で学習が進みます。\n\n1.  LLMが同じプロンプトに対して、複数のテキスト（例えば4つ）を生成します。\n2.  それぞれのテキストに対して報酬関数が報酬を与えます。\n3.  それぞれのテキストの報酬を比較し、最も高い報酬を得たテキストを「良いテキスト」、それ以外のテキストを「悪いテキスト」とします。\n4.  LLMは、「良いテキスト」を生成する確率を高め、「悪いテキスト」を生成する確率を下げるように、自身のテキスト生成の方針を調整します。\n\nこのように、GRPOでは、テキスト間の相対的な評価を用いることで、状態価値モデルを学習する必要がなくなり、計算コストを削減することができます。\n\n**GRPOの利点**\n\n*   **計算効率:** 状態価値モデルが不要なため、PPOよりも計算コストが低い。\n*   **大規模LLMへの適用:** 大規模なLLMでも効率的に学習できる。\n\n**まとめ**\n\nLLMに対する強化学習は、LLMがより良いテキストを生成するように学習させるための手法であり、GRPOはその効率的な学習を可能にする手法の一つです。GRPOは、テキスト間の相対的な評価を用いることで、状態価値モデルを不要にし、計算コストを削減することができます。', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-4d11a554-e7d5-4420-9ac9-b8efaccf8b11-0', usage_metadata={'input_tokens': 48388, 'output_tokens': 996, 'total_tokens': 49384, 'input_token_details': {'cache_read': 0}}), HumanMessage(content=[{'type': 'text', 'text': '2番目にいいねが多い記事は何？'}], additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-aae69546-e8ee-4891-9040-daee70739fa4-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap'}, 'id': '61c67f9f-6525-4601-af0d-36cd5f4631a2', 'type': 'tool_call'}], usage_metadata={'input_tokens': 49392, 'output_tokens': 13, 'total_tokens': 49405, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap\n\n- Page URL: https://zenn.dev/asap\n- Page Title: asapさんの記事一覧 | Zenn\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- banner [ref=s1e32]:\n    - img "asap" [ref=s1e36]\n    - heading "asap" [level=1] [ref=s1e39]\n    - button "フォロー" [ref=s1e41]\n    - paragraph [ref=s1e43]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n    - button "2,214 Likes" [ref=s1e46]\n    - button "310 Followers" [ref=s1e48]\n    - link "@personabb" [ref=s1e51]:\n        - /url: https://github.com/personabb\n        - tooltip "@personabb" [ref=s1e52]:\n            - img "GitHub" [ref=s1e53]\n            - text: "@personabb"\n    - link "@asap2650" [ref=s1e55]:\n        - /url: https://twitter.com/asap2650\n        - tooltip "@asap2650" [ref=s1e56]:\n            - img "X(Twitter)" [ref=s1e57]\n            - text: "@asap2650"\n    - link "yard.tips" [ref=s1e63]:\n        - /url: https://yard.tips/asap2650\n        - tooltip "yard.tips" [ref=s1e64]:\n            - img [ref=s1e65]\n            - text: yard.tips\n    - link "RSS" [ref=s1e67]:\n        - /url: /asap/feed\n        - tooltip "RSS" [ref=s1e68]:\n            - img [ref=s1e69]\n            - text: RSS\n- link "Articles 72" [ref=s1e80]:\n    - /url: /asap\n- link "Scraps 5" [ref=s1e82]:\n    - /url: /asap?tab=scraps\n- link "Comments" [ref=s1e84]:\n    - /url: /asap?tab=comments\n- contentinfo [ref=s1e88]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e92]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e93]\n    - paragraph [ref=s1e101]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e103]:\n        - heading "About" [level=4] [ref=s1e104]\n        - list [ref=s1e105]:\n            - listitem [ref=s1e106]:\n                - link "Zennについて" [ref=s1e108]:\n                    - /url: /about\n            - listitem [ref=s1e109]:\n                - link "運営会社" [ref=s1e110]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e111]:\n                - link "お知らせ・リリース" [ref=s1e112]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e113]:\n                - link "イベント" [ref=s1e115]:\n                    - /url: /events\n    - navigation [ref=s1e116]:\n        - heading "Guides" [level=4] [ref=s1e117]\n        - list [ref=s1e118]:\n            - listitem [ref=s1e119]:\n                - link "使い方" [ref=s1e121]:\n                    - /url: /manual\n            - listitem [ref=s1e122]:\n                - link "法人向けメニュー" [ref=s1e124]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e126]:\n                - link "Publication / Pro" [ref=s1e128]:\n                    - /url: /publications\n            - listitem [ref=s1e129]:\n                - link "よくある質問" [ref=s1e131]:\n                    - /url: /faq\n    - navigation [ref=s1e132]:\n        - heading "Links" [level=4] [ref=s1e133]\n        - list [ref=s1e134]:\n            - listitem [ref=s1e135]:\n                - link "X(Twitter)" [ref=s1e136]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e137]:\n                - link "GitHub" [ref=s1e138]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e139]:\n                - link "メディアキット" [ref=s1e141]:\n                    - /url: /mediakit\n    - navigation [ref=s1e142]:\n        - heading "Legal" [level=4] [ref=s1e143]\n        - list [ref=s1e144]:\n            - listitem [ref=s1e145]:\n                - link "利用規約" [ref=s1e147]:\n                    - /url: /terms\n            - listitem [ref=s1e148]:\n                - link "プライバシーポリシー" [ref=s1e150]:\n                    - /url: /privacy\n            - listitem [ref=s1e151]:\n                - link "特商法表記" [ref=s1e153]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e156]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e157]\n- alert [ref=s1e176]\n```\n', name='browser_navigate', tool_call_id='61c67f9f-6525-4601-af0d-36cd5f4631a2'), AIMessage(content='asapさんのZennの記事の中で2番目にいいねが多い記事は、「VAEって結局何者なの？」で、いいね数は104です。', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-7c7bf5a2-7650-41d4-9cc5-c1fb3e0de2f1-0', usage_metadata={'input_tokens': 51212, 'output_tokens': 32, 'total_tokens': 51244, 'input_token_details': {'cache_read': 0}}), HumanMessage(content=[{'type': 'text', 'text': 'どんな記事ですか？解説してください。'}], additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap/articles/6caa9043276424"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-093f5ac0-5315-41a0-aaf8-f4fef2243382-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap/articles/6caa9043276424'}, 'id': '45a87836-eb09-4499-bf44-a2bcdbd3ec2b', 'type': 'tool_call'}], usage_metadata={'input_tokens': 51251, 'output_tokens': 28, 'total_tokens': 51279, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap/articles/6caa9043276424\n\n- Page URL: https://zenn.dev/asap/articles/6caa9043276424\n- Page Title: VAEって結局何者なの？\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- article [ref=s1e32]:\n    - text: 📖\n    - heading "VAEって結局何者なの？" [level=1] [ref=s1e38]\n    - text: 2025/01/20に公開\n    - link "ディープラーニング" [ref=s1e52]:\n        - /url: /topics/deeplearning\n        - img [ref=s1e54]\n        - text: ディープラーニング\n    - link "VAE" [ref=s1e56]:\n        - /url: /topics/vae\n        - img [ref=s1e58]\n        - text: VAE\n    - link "Stable Diffusion" [ref=s1e60]:\n        - /url: /topics/stablediffusion\n        - img [ref=s1e62]\n        - text: Stable Diffusion\n    - link "生成 AI" [ref=s1e64]:\n        - /url: /topics/%E7%94%9F%E6%88%90ai\n        - img [ref=s1e66]\n        - text: 生成 AI\n    - link "elbo" [ref=s1e68]:\n        - /url: /topics/elbo\n        - img [ref=s1e70]\n        - text: elbo\n    - link "tech" [ref=s1e72]:\n        - /url: /tech-or-idea\n        - img [ref=s1e74]\n        - text: tech\n    - heading "はじめに" [level=1] [ref=s1e78]\n    - paragraph [ref=s1e79]: 今回は、満を持してVAE（Variational Auto Encoder）をちゃんと理解していこうと思います。\n    - paragraph [ref=s1e80]: VAEに関しては、だいたい知っていますが、MusicGenという音楽生成AIを理解しようと思った時に、関連してRVQ-GANが出てきたので、再勉強をしています。\n    - paragraph [ref=s1e81]: 今後、下記の流れで記事を書いていく予定です。\n    - paragraph [ref=s1e82]: VAE（今回） ↓ VQ-VAE ↓ RQ-VAE,RVQ-GAN ↓ MusicGen\n    - paragraph [ref=s1e89]: 今回は、流れの最初であり、現在でも非常に重要な概念であるVAEに関して記事を書きます。\n    - paragraph [ref=s1e90]: VAEは、Stable\n        Diffusionの中でも中核として使われていたりと、比較的古い概念でありながら、まだまだ活躍が期待できる汎用ネットワークです。\n        本記事が、みなさまの理解の手助けになれば、これほど嬉しいことはございません。\n    - heading "参考文献" [level=1] [ref=s1e92]\n    - paragraph [ref=s1e93]:\n        - iframe [ref=s1e95]:\n            - link "Auto-Encoding Variational Bayes How can we perform efficient inference and learning in directed probabilistic models, in the presenc arxiv.org favicon image arxiv.org arxiv.org thumbnail image" [ref=f1s1e6]:\n                - /url: https://arxiv.org/abs/1312.6114\n                - heading "Auto-Encoding Variational Bayes" [level=1]\n                  [ref=f1s1e8]\n                - text: How can we perform efficient inference and learning in directed\n                    probabilistic models, in the presenc\n                - img "arxiv.org favicon image" [ref=f1s1e11]\n                - text: arxiv.org\n                - img "arxiv.org thumbnail image" [ref=f1s1e13]\n            - alert [ref=f1s1e15]\n        - text: VAEの論文です。\n    - paragraph [ref=s1e96]:\n        - link "ゼロから作るDeep Learning ❺ ―生成モデル編" [ref=s1e97]:\n            - /url: https://amzn.to/3Aw9vCt\n    - paragraph [ref=s1e98]: 圧倒的な名著シリーズである「ゼロから作るDeep Learning」シリーズの5作目である、生成モデル編です。\n        この本では、最終的に拡散モデルを理解することを目的に、VAEや混合ガウスモデル、正規分布まで遡り、本当にゼロから中身を理解しながら実装をすることができる、大変素晴らしい書籍になっています。\n    - paragraph [ref=s1e100]:\n        - link "妥協しないデータ分析のための 微積分+線形代数入門" [ref=s1e101]:\n            - /url: https://amzn.to/4hE7ftx\n    - paragraph [ref=s1e102]: 本書の後半にもVAEやELBOについての簡単な記載があり、本記事を書くうえで表現を参考にさせていただきました。\n        確率分布のどの部分が計算可能で、どの部分が計算不可能なのかが常に明示されながら進んでいくので、初学者には非常に分かりやすい書籍でした！\n    - paragraph [ref=s1e104]: 特に、VAEなどの生成AIをただ使うだけでなく、ちゃんと理論を理解したいと思った場合、論文や理論書を理解するために「線形代数」や「微積分学」の事前知識が必要になります。\n        本書は、初学者が一番最初に読んで必要な基礎知識が習得できる良本になります。\n    - paragraph [ref=s1e106]:\n        - link "生成 Deep Learning 第2版 ―絵を描き、物語や音楽を作り、ゲームをプレイする" [ref=s1e107]:\n            - /url: https://amzn.to/4g8hjJg\n    - paragraph [ref=s1e108]: 本書の序盤で、Auto\n        EncoderとVAEの違いを説明してくれています。しかもそれぞれの潜在空間の分布を可視化してくれているので、違いが分かりやすく、理解もしやすいです。\n        さらに、本書はVAEだけでなくGANや画像生成、音楽生成、テキスト生成など、ありとあらゆる生成AIの紹介と解説をしてくれており、必読の一冊となっております。\n    - paragraph [ref=s1e110]: （書籍のリンクはAmazonアフィリエイトリンクになっております）\n    - heading "関連記事" [level=1] [ref=s1e111]\n    - paragraph [ref=s1e112]: VAEに関連した記事は過去のも書いております。 こちらの記事もご覧いただけると嬉しいです！\n    - paragraph [ref=s1e114]:\n        - iframe [ref=s1e116]:\n            - link "【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】 asapさんによる記事 zenn.dev favicon image zenn.dev zenn.dev thumbnail image" [ref=f2s1e6]:\n                - /url: https://zenn.dev/asap/articles/ea9f42edff6573\n                - heading\n                  "【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】"\n                  [level=1] [ref=f2s1e8]\n                - text: asapさんによる記事\n                - img "zenn.dev favicon image" [ref=f2s1e11]\n                - text: zenn.dev\n                - img "zenn.dev thumbnail image" [ref=f2s1e13]\n            - alert [ref=f2s1e15]\n    - heading "Auto Encoder" [level=1] [ref=s1e117]\n    - heading "Auto Encoderの紹介" [level=2] [ref=s1e118]\n    - paragraph [ref=s1e119]: VAEを語る上で欠かせないのは、Auto Encoderの存在です。 Auto\n        Encoderというのは、ニューラルネットワークにより構築されたネットワーク構造の一つで、入力と出力を一致させるように学習させるネットワークです。\n    - paragraph [ref=s1e121]:\n        - img [ref=s1e122]\n    - paragraph [ref=s1e123]: 最も単純なAuto Encoderは上記のような構造をしています。\n        前段は、入力データから、それより次元の小さな潜在表現を生成するEncoderです。\n        上の図では、入力データを重みデータによる線形変換（+活性化層における非線形変換）で成り立っています。\n        Encoderでは、次元の大きな入力データを、次元の小さい潜在表現に圧縮する処理を行います。\n    - paragraph [ref=s1e127]: 後段は、潜在表現から、それより次元の大きな出力データを生成するDecoderです。\n        出力データは入力と一致するように学習がなされています。\n        このとき、Decoderでは、次元の小さな潜在表現から、出力データ（入力データ）を復元する処理を行います。\n        この「圧縮(符号化)」と「復元(複合化)」の構造から、「Encoder」「Decoder」と名付けられており、この圧縮・復元をデータを与えれば自動で実施するニューラルネットワークなので、「Auto\n        Encoder」と呼ばれます。\n    - heading "Auto Encoderの本質" [level=2] [ref=s1e131]\n    - paragraph [ref=s1e132]: ここでの本質は、次元の大きい入力データを、潜在表現に圧縮し、その潜在表現から、入力データを再構成することです。\n    - heading "小さな潜在表現から元データを復元できる理由" [level=3] [ref=s1e133]\n    - paragraph [ref=s1e134]: これが達成される理由は2つあります。\n    - paragraph [ref=s1e135]: 1つ目は、世の中にあるデータは冗長であることです。\n        世の中に存在するデータを冗長なデータが多いです。例えば画像データであれば、生の01でデータを保持するよりも、pngなどの圧縮形式を利用してデータを保存した方が、データ量を抑えることができます。\n        これは、画像中の「隣接する画素は、近い値が多い」という事前知識を用いることで、データを効率的に圧縮することができるからです。\n    - complementary [ref=s1e138]:\n        - text: "!"\n        - paragraph [ref=s1e141]: 話が脱線しますが、pngは可逆圧縮と呼ばれる手法であるため、この手法を利用してデータ量を削減した場合であっても、元の画像データを「完全に」復元することができます。これはzipなども同様です。\n        - paragraph [ref=s1e142]: 一方、画像でよく使われている、jpgやHEIFなどの規格は、非可逆圧縮と呼ばれる手法です。\n            この手法でデータ量を削減した場合は、元の画像から劣化することになります。\n            ただし、その劣化具合はみなさんご存知の通り非常に小さく、かつデータ削減量も可逆圧縮手法と比較して大幅に削減できます。\n    - paragraph [ref=s1e145]: 2つ目は、Auto Encoderの重みデータに、入力データを復元するための情報を学習しているからです。\n    - paragraph [ref=s1e146]: 例えば画像であれば、Auto\n        Encoderを利用する際に、事前に大規模画像データセットを利用して学習が行われます。その学習を通して、Decoderは「どうやったら少ない潜在表現から画像データを品質高く復元できるのか」、Encoderは「大量の画像データのうち、どの情報を潜在表現に埋め込めば、Decoderが元の画像を復元できるのか」を学習しています。\n        その結果、ちょうどpngで説明した通り、「隣接する画素は、近い値が多い」のような自然画像全般に通ずる事前知識を獲得し、その情報を用いることで、少ないデータ量から、元の入力データを復元するようなネットワークを構築することができるというわけです。\n    - heading "構造" [level=3] [ref=s1e148]\n    - paragraph [ref=s1e149]: 先ほど記載した通り、Auto\n        Encoderの本質は、次元の大きい入力データを、潜在表現に圧縮し、小さな潜在表現から、入力データを再構成することです。\n        そして、これが達成できるのであれば、どんなEncoder,Decoderの構造をしていたとしても、それはAuto\n        Encoderの一種であると考えることができます。\n    - paragraph [ref=s1e151]: 例えば画像を処理する際には、しばしば畳み込みニューラルネットワークやTransformerなどが使われます。\n        当然、これらを利用したAuto Encoderも考えることができます。\n    - paragraph [ref=s1e153]: 潜在表現も単純な1次元データだけではなく、画像のような3次元（縦, 横,\n        チャネル）データを考えることができます。 （ResNetなどのGAP前の層の出力結果をEncoderの出力として利用することもできます）\n    - paragraph [ref=s1e155]: したがって、以降ではEncoder Decoder構造を記載する際には、下記のように記載することにします。\n    - paragraph [ref=s1e156]:\n        - img [ref=s1e157]\n    - heading "Auto Encoerの課題" [level=2] [ref=s1e158]\n    - paragraph [ref=s1e159]: Auto Encoerはそれ単体では、それなりに優れた情報圧縮器ではありますが、それ以外での活用可能性が難しいネットワークではありました。\n    - paragraph [ref=s1e160]: 入力データと形状は同じな、出力データを出力する際には、ネットワーク構造が使われることも多いです（例えばセマンティックセグメンテーションタスクでU-Netが利用されるなど）が、同じ入出力になるように学習することというのは、情報圧縮以外の文脈で利用するのは難しいです。\n    - paragraph [ref=s1e161]: そこで、生成モデルとしての活用可能性が考えられました、\n        例えば、潜在表現を何らかの形で得ることができれば、その潜在表現をDecoderに入力することで、対応した出力データ（画像など）を得ることができます。\n        これが達成されれば、AIに絵を描かせたり、音楽を生成させたりすることが可能です。\n    - paragraph [ref=s1e164]: ただし、Auto Encoderでは、その潜在表現をEncoderを利用すること以外で取得することは困難です。\n        ここで、Auto Encoderによる画像生成の夢は途絶えたかのように見えました。\n    - heading "VAE（Variational Auto Encoder）" [level=1] [ref=s1e166]\n    - heading "VAEと生成モデル" [level=2] [ref=s1e167]\n    - heading "生成モデルの観点から見たVAE" [level=3] [ref=s1e168]\n    - paragraph [ref=s1e169]:\n        - text: VAEというのは、生成モデルとして重要な「対数尤度の最大化」問題を解くために必要な構造を用意したら、\n        - strong [ref=s1e171]: 結果的にAuto Encoderのような形になったというモデルです。\n    - paragraph [ref=s1e172]: すなわち、Auto\n        Encoderの構造があって、それをこうしたら生成モデルとして使えるようになった。というよりは、生成モデルを作る上で重要な「対数尤度を最大化する」という方向性で、理論的に式変形を繰り返し、計算不可能な部分を取り除き、計算が難しい部分をニューラルネットで近似することで、結果としてAuto\n        Encoderの構造が現れたという解釈があっているような気がします。\n    - heading "対数尤度の最大化からモデル構造を考える" [level=3] [ref=s1e173]\n    - heading "対数尤度の最大化に関しての軽い復習" [level=4] [ref=s1e174]\n    - paragraph [ref=s1e175]:\n        - text: 「対数尤度の最大化」に関しては下記の記事で解説しておりますので、そちらをご覧ください。\n        - iframe [ref=s1e177]: <could not take iframe snapshot>\n    - paragraph [ref=s1e178]:\n        - text: 上記の記事では詳細に記載していますが、簡単に解説すると、生成モデルにおいて重要なのは、対数尤度\n        - math [ref=s1e182]: p θ ( x )\n        - text: を最大化することです。 ここで、 x は実際のデータで、 \\theta は生成モデルのパラメータです。 したがって、 p_\\theta(x)\n            は対数尤度の確率分布になります。\n    - paragraph [ref=s1e199]:\n        - text: 生成モデルのパラメータ自体にはランダム性がないため、様々なバリエーションのデータを生成するためには、入力にランダムにサンプリングされたデータや定数を入力し、生成モデルがデータを再構成します。\n            イメージとしては下記のイメージです。\n        - img [ref=s1e202]\n    - complementary [ref=s1e203]:\n        - text: "!"\n        - paragraph [ref=s1e206]:\n            - text: ネットワークが確率分布を生成するというのは違和感があるかと思います。 その辺りの詳細は、下記の記事でも解説しているので参考にしてみてください。\n            - link "https://zenn.dev/asap/articles/4092ab60570b05" [ref=s1e209]:\n                - /url: https://zenn.dev/asap/articles/4092ab60570b05\n        - paragraph [ref=s1e210]: "ここでは、ネットワークが出力したデータ y を「平均」、単位行列 I を「分散」とする多変数正規分布\n            \\\\mathcal{N}(y, I) を考えることで、Decoderの出力を確率分布として計算しています。\n            実用上は、ネットワークが出力した平均 y の値を、そのまま生成したデータ x\'=y として扱います。"\n    - paragraph [ref=s1e222]: "しかし、上記で計算しているのは、尤度 p_{\\\\theta}(x|z) です。\n        （なぜなら、Decoderはパラメータ \\\\theta と潜在表現 z によって条件付けられたネットワークであるためです。）"\n    - paragraph [ref=s1e230]: これを用いて対数尤度 p_\\theta(x) を計算するには、下記の式を最適化する必要があります。\n    - text: "\\\\log p_{\\\\theta}(x) = \\\\mathbb{E}_{q_{\\\\psi}(z|x)}[\\\\log\n        p_{\\\\theta}(x|z)] - \\\\mathrm{KL}\\\\left(q_{\\\\psi}(z|x) \\\\parallel\n        p(z)\\\\right) + \\\\mathrm{KL}\\\\left(q_{\\\\psi}(z|x) \\\\parallel\n        p_{\\\\theta}(z|x)\\\\right)"\n    - paragraph [ref=s1e236]: "この中で、事後分布 p_{\\\\theta}(z|x) は計算不可能なため、近似事後分布\n        q_{\\\\psi}(z|x) を用意して、KLダイバージェンスに押し込みます。\n        そうすることで、第三項のKLダイバージェンスは非負であるため、この項を除いた部分(ELBO)を最大化することで、間接的に対数尤度を最大化するこ\\\n        とができます。"\n    - paragraph [ref=s1e242]: "このとき必要な、近似事後分布 q_{\\\\psi}(z|x) を計算するために、パラメータ \\\\psi\n        を持ち、データ x を入力して、潜在表現 z の確率分布を出力するようなネットワークとして、Encoderが必要になります。"\n    - paragraph [ref=s1e251]:\n        - text: 詳しくは\n        - link "上述した記事" [ref=s1e252]:\n            - /url: https://zenn.dev/asap/articles/ea9f42edff6573\n        - text: をご覧ください。\n    - heading "Encoderが平均と分散を出力する理由" [level=4] [ref=s1e253]\n    - paragraph [ref=s1e254]:\n        - text: ここまでの議論を考慮すると、下記のようなモデル構造になります。\n        - img [ref=s1e256]\n    - paragraph [ref=s1e257]: "上述した通り、Decoderの場合は、Decoderが出力するデータ y を「平均」、単位行列 I\n        を「分散」とする多変数正規分布 \\\\mathcal{N}(y, I) を確率分布として持ちますが、実用上は、Decoderが出力した平均 y\n        の値を、そのまま生成データ x\'=y と考えることができます。"\n    - paragraph [ref=s1e268]: 一方で、Encoderは確率分布を生成する必要があります。\n        さらにいうと、ELBOの式の中で、Encoderが出力する確率分布を用いて下記の式を計算する必要があります。\n    - text: "\\\\mathrm{KL}\\\\left(q_{\\\\psi}(z|x) \\\\parallel p(z)\\\\right)"\n    - paragraph [ref=s1e273]: ここで、 p(z) は潜在表現 z の事前分布なので、ベイズ推定等と同様に、こちらが設定することができます。\n        ネットワークは、こちらが設定した事前分布が成立するように学習します。\n        今回の場合、真の事前分布がどんな形状になるのかの事前情報を持っていないので、あまりバイアス情報が入らないような分布を設定することが望ましいです。\n    - paragraph [ref=s1e280]: "問題は、 q_{\\\\psi}(z|x) です。"\n    - paragraph [ref=s1e283]: この分布は特殊な分布になってはいけません。 なぜなら、KLダイバージェンスを計算できる分布の種類はあまり多くないからです。\n    - complementary [ref=s1e285]:\n        - text: "!"\n        - paragraph [ref=s1e288]: 一般的な連続確率分布同士のKLダイバージェンスを計算するのは、積分計算が入るため困難です。\n            しかしながら、指数型分布のような確率分布であれば計算できる余地があります。\n        - paragraph [ref=s1e290]: 例えば、正規分布同士のKLダイバージェンスであれば、比較的簡単に計算をすることができます。\n    - paragraph [ref=s1e291]: したがって、Encoderが生成する確率分布に、何らかの制限を加える必要があります。 今回は、確率分布が正規分布になるように制限をかけます。\n    - paragraph [ref=s1e293]: "その場合、Encoderは、平均 \\\\mu と分散 \\\\sigma^2\n        のパラメータを出力するような構造を考えることで、Encoderが正規分布を表現することができます。 そして、Encoderが出力した、平均\n        \\\\mu ・分散 \\\\sigma^2 パラメータから得られる正規分布 \\\\mathcal{N}(\\\\mu, \\\\sigma)\n        からサンプリングされた値を潜在表現 z とすることで、ELBOを計算することができます。 この場合、計算効率化のため、事前分布 p(z)\n        には平均 0 、分散 I の標準正規分布 \\\\mathcal{N}(0, I) を設定します。 標準正規分布 \\\\mathcal{N}(0,\n        I) は、正規分布とのKLダイバージェンスの計算が簡単であることと、平均 0 、分散 I\n        の条件下におけるエントロピーが最大になる分布（つまり、そのほかの事前情報を含まない分布）であるため、採用されています。"\n    - paragraph [ref=s1e323]: これを達成するためのモデル構造は下記になります。\n    - paragraph [ref=s1e324]:\n        - img [ref=s1e325]\n    - paragraph [ref=s1e326]:\n        - text: これでVAEを構築することができました。 上の図をよく見ると、\n        - strong [ref=s1e329]: まるでAuto Encoderのような構造になっていると思います。\n    - heading "VAEの挙動・振る舞い" [level=2] [ref=s1e330]\n    - heading "VAEの目的関数" [level=3] [ref=s1e331]\n    - paragraph [ref=s1e332]: 改めてVAEの目的関数は下記になります。\n    - complementary [ref=s1e333]:\n        - text: "!"\n        - paragraph [ref=s1e336]: ELBO\n        - text: "\\\\mathrm{ELBO} = \\\\mathbb{E}_{q_{\\\\psi}(z|x)}[\\\\log p_{\\\\theta}(x|z)] -\n            \\\\mathrm{KL}\\\\left(q_{\\\\psi}(z|x) \\\\parallel p(z)\\\\right)"\n    - paragraph [ref=s1e340]: このELBOを最大化するようにVAEのネットワークパラメータ( \\theta 、 \\psi )が最適化されます。\n    - paragraph [ref=s1e345]:\n        - text: ここで、\n        - link "先日書いた記事" [ref=s1e346]:\n            - /url: https://zenn.dev/asap/articles/ea9f42edff6573#%E7%AC%AC%E4%B8%80%E9%A0%85%E3%82%92%E8%80%83%E3%81%88%E3%82%8B\n        - text: にも記載しましたが、第1項は「再構成誤差」、第2項は「正則化項」になります。\n    - paragraph [ref=s1e347]: 式変形を行うと、第1項はDecoderの出力 y と、正解データ x との2乗誤差の負数になります。\n        したがってELBOを最大化する際には、2乗誤差を最小化する方向にVAEのネットワークパラメータ( \\theta 、 \\psi\n        )が最適化されます。\n    - complementary [ref=s1e357]:\n        - text: "!"\n        - paragraph [ref=s1e360]: なお、ここで再構成誤差が2乗誤差となるのは、VAEの出力の確率分布を正規分布と仮定したためです。\n            もし、出力の分布を「ベルヌーイ分布」と仮定した場合、再構成誤差項は下記のようにクロスエントロピー損失と等価になります。\n        - paragraph [ref=s1e362]: ベルヌーイ分布は、データ内のあるサンプル i を考えると、下記で表せます。\n        - text: "p_{\\\\theta}(x_i|z_i) = y_i^{x_i} (1 - y_i)^{1 - x_i}"\n        - paragraph [ref=s1e368]: "この時、ELBOの第1項は、期待値計算において、テンプルサイズ1で近似すると、 \\\\log\n            p_{\\\\theta}(x|z) に近似できるため、これを計算すると、"\n        - text: "\\\\log p_{\\\\theta}(x_i|z_i) = x_i \\\\log y_i + (1 - x_i) \\\\log (1 - y_i)"\n        - paragraph [ref=s1e374]: となるため、まさしくクロスエントロピー項となります。\n        - paragraph [ref=s1e375]: ただし、ベルヌーイ分布と仮定するためには、生成するデータ x が[0,1]である必要があります。\n    - paragraph [ref=s1e378]: "第2項は、Encoderが出力する q_{\\\\psi}(z|x) と平均 0 、分散 I の標準正規分布\n        \\\\mathcal{N}(0, I) とのKLダイバージェンスの負数になります。\n        したがってELBOを最大化する際には、KLダイバージェンス自体を最小化する方向にVAEのEncoderパラメータ( \\\\psi\n        )が最適化されます。 KLダイバージェンスは、2つの分布が一致しているときに最小値 0 をとります。したがって、Encoderが出力する確率分布\n        q_{\\\\psi}(z|x) が標準正規分布 \\\\mathcal{N}(0, I) に近づくように最適化されます。"\n    - heading "ELBOの第2項" [level=3] [ref=s1e397]\n    - paragraph [ref=s1e398]: 第1項は再構成誤差のため、分かりやすいと思いますが、第2項がどう言う意味を含むのかが分かりにくいと思います。\n    - complementary [ref=s1e399]:\n        - text: "!"\n        - paragraph [ref=s1e402]: ELBOの第2項\n        - text: "- \\\\mathrm{KL}\\\\left(q_{\\\\psi}(z|x) \\\\parallel p(z)\\\\right)"\n    - paragraph [ref=s1e406]: "ここでは、潜在表現 z の事前分布が標準正規分布 \\\\mathcal{N}(0, I) に従うと仮定していました。"\n    - paragraph [ref=s1e411]:\n        - text: "また、実は、Encoderが出力する近似事後分布 q_{\\\\psi}(z|x) は、ELBOに関係なく、実際の計算不能な真の事後分布\n            p_{\\\\theta}(z|x) に近づくように学習されています。 この詳細は、"\n        - link "以前書いた記事" [ref=s1e417]:\n            - /url: https://zenn.dev/asap/articles/ea9f42edff6573#elbo%E3%81%8C%E5%AF%BE%E6%95%B0%E5%B0%A4%E5%BA%A6%E3%81%AB%E8%BF%91%E3%81%A5%E3%81%8F%E6%9D%A1%E4%BB%B6\n        - text: "をご覧ください。 これは主に、Encoderが出力する近似事後分布 q_{\\\\psi}(z|x) が、真の事後分布\n            p_{\\\\theta}(z|x) に近づくことで、Decoderの再構成誤差が小さくなることに由来します。"\n    - paragraph [ref=s1e423]: "その上で、ELBOの第2項では、Encoderが出力する近似事後分布 q_{\\\\psi}(z|x)\n        は、標準正規分布 \\\\mathcal{N}(0, I) に近づくようにも学習されることを示しています。"\n    - paragraph [ref=s1e428]: "したがって、上記の議論をまとめると、Encoderが出力する近似事後分布 q_{\\\\psi}(z|x)\n        は、標準正規分布 \\\\mathcal{N}(0, I)\n        に近づくようにも学習されますが、その上で、Decoderの再構成誤差とバランスをとりながら、真の事後分布 p_{\\\\theta}(z|x)\n        にも近づくように学習されます。"\n    - paragraph [ref=s1e435]: "さらに言うと、真の事後分布 p_{\\\\theta}(z|x) は、Decoderパラメータ \\\\theta\n        依存して変化します。（計算できないですが、必ず存在し、 \\\\theta と x に依存します） この場合、真の事後分布\n        p_{\\\\theta}(z|x) もEncoderが出力する近似事後分布 q_{\\\\psi}(z|x)\n        に近づくような力学が多少なりとも働くことになります。"\n    - paragraph [ref=s1e449]: したがってDecoder側は、過度に複雑な事後分布をEncoder側に近似することを要求せず、ある程度、真の事後分布が標準正規分布に近づくように、（すなわち標準正規分布からデータを再構成できるように）学習が協力して進んでいく挙動をします。\n    - paragraph [ref=s1e450]: "以上を考慮すると、Decoderは、Encoderがないときには、事前分布として仮定した、標準正規分布\n        \\\\mathcal{N}(0, I) からサンプリングされた潜在表現 z を利用することで、そこそこ意味のあるデータを生成することができます。\n        この特性から、VAEは「生成モデル」として利用できることがわかります。"\n    - complementary [ref=s1e456]:\n        - text: "!"\n        - paragraph [ref=s1e459]: ただし、この関係は非常に難しいバランスの上で成立しています。\n        - paragraph [ref=s1e460]: 潜在表現の次元が増えたり、目的関数の項ごとの比率が変わるなどして、再構成誤差項と正則化項のバランスが崩れた際には、上記が成立しなくなります。\n            それについては後述します。\n        - paragraph [ref=s1e462]: しかし、その場合であっても、上記のような力学はある程度働きながら、学習が進むことになります。\n    - heading "Decoderが学習する潜在表現のランダム性" [level=3] [ref=s1e463]\n    - paragraph [ref=s1e464]: VAEの全体の構造を改めて下記に提示します。\n    - paragraph [ref=s1e465]:\n        - img [ref=s1e466]\n    - paragraph [ref=s1e467]: "こちらを見ると、Decoderに入力される潜在表現 z というのは、平均 \\\\mu ・分散 \\\\sigma^2\n        パラメータから得られる正規分布 \\\\mathcal{N}(\\\\mu, \\\\sigma) からサンプリングされた値を用いていることがわかります。\n        すなわち、学習時にDecoderが見る潜在表現 z には多少のランダム性がございます。\n        これは、Encoderに入力されるデータが同じデータであっても、decoderはランダムに異なる潜在表現 z から入力データ x\n        を再構成することを示します。"\n    - paragraph [ref=s1e484]: "そしてこのサンプリングは、平均ベクトル \\\\mu を中心として、正規分布 \\\\mathcal{N}(\\\\mu,\n        \\\\sigma) としてサンプリングされるため、中心から若干だけズレた値がサンプリングされることになります。"\n    - paragraph [ref=s1e489]: このことを考えると、Decoderは毎回若干異なる潜在表現 z から、入力データ x\n        を再構成することになります。 したがって、Decoderが入力データ x を再構成する際の潜在表現 z の許容範囲が広くなるように学習されます。\n        これを大量の入力 x に対して学習を実施すると、似たようなデータ同士では、似たような潜在表現 z\n        をEncoderがサンプリングするようになり、大きく異なるデータ同士では、異なる潜在表現 z をEncoderがサンプリングするようになります。\n    - paragraph [ref=s1e506]: これが達成されると、潜在表現 z が少し変わると、再構成データ y も少しだけ変わるという滑らかさをDecoderは得ることができます。\n    - paragraph [ref=s1e511]: この特性のおかげで、VAEは、2種類のデータ（例えば、男性の顔写真と女性の顔写真）を構成する潜在表現 z\n        の間を動かすことで、出力データ y は男性の顔から女性の顔に滑らかに変化するような挙動をするようになります。\n        これはよくVAEの解説サイトなどで見ることのできる挙動だと思います。\n    - heading "VAEはどうやって学習しているのか" [level=3] [ref=s1e517]\n    - paragraph [ref=s1e518]: もう一度VAEの構成図を表示します。\n    - paragraph [ref=s1e519]:\n        - img [ref=s1e520]\n    - paragraph [ref=s1e521]: ここまで読んで、勘のいい読者は、下記のように考えるかもしれません。\n        「確率分布からのSampling処理は微分不可能なため、学習できないのでないか」と\n    - paragraph [ref=s1e523]: その疑問はおっしゃる通りです。 ではここから、VAEの構造を損なわない形で、上記の図中からSampling構造を取り除くことを考えます。\n    - heading "Decoder出力部分" [level=4] [ref=s1e525]\n    - paragraph [ref=s1e526]: "図を見ると、Decoderの出力部分にてSampling構造があります。\n        生成モデルは、「対数尤度を最大化する」という目的のため、確率分布を出力することが求められますが、Decoderの出力はベクトルです。したがって\\\n        、出力ベクトル y を平均、単位行列 I を分散とする正規分布 \\\\mathcal{N}(y, I)\n        を確率分布として考え、そこからサンプリングされた値 x\' を生成データだと考えます。"\n    - paragraph [ref=s1e536]: この形で、目的関数ELBOを計算すると、出力ベクトル y と、入力データ x\n        の2乗誤差を最小化することが最終的な目的関数になることがわかります。\n        したがって、Sampling処理をなくすため、Decoderが出力するベクトル y 自体を、生成したデータ x\'=y と見なすことが可能です。\n    - complementary [ref=s1e546]:\n        - text: "!"\n        - paragraph [ref=s1e549]: 可能と言っているのは、 x\'=y と解釈したところで、目的関数は変わらず、パラメータを更新する微分値が変わらないことを指しています。\n    - paragraph [ref=s1e552]:\n        - text: したがって、これを考慮すると下記のような図になります。\n        - img [ref=s1e554]\n    - heading "Encoder出力部分" [level=4] [ref=s1e555]\n    - paragraph [ref=s1e556]: "続いて、Samplingが残っている箇所は、Encoderの出力する平均 \\\\mu ・分散 \\\\sigma^2\n        パラメータから得られる正規分布 \\\\mathcal{N}(\\\\mu, \\\\sigma) からサンプリングして、潜在表現 z\n        を得る部分になります。"\n    - paragraph [ref=s1e565]: この部分では、微分可能な計算グラフにするために、「Reparameterization\n        Trick」という手法を利用しています。 統計学に詳しい方には、「標準化の逆」という説明がしっくりくるかもしれません。\n    - complementary [ref=s1e567]:\n        - text: "!"\n        - paragraph [ref=s1e570]: 標準化とは、平均 \\mu 、分散 \\sigma^2 の複数データ z に対して、下記の処理を施した後のデータ\n            \\epsilon は、平均 0 、分散 1 となる変換です。\n        - text: "\\\\epsilon = \\\\dfrac{z - \\\\mu}{\\\\sigma}"\n    - paragraph [ref=s1e586]: "VAEでは、Encoderが平均 \\\\mu ・分散 \\\\sigma^2 を出力し、そこから得られる正規分布\n        \\\\mathcal{N}(\\\\mu, \\\\sigma) から潜在表現 z をサンプリングしています。"\n    - paragraph [ref=s1e595]: "このようなサンプリングの操作を、計算グラフの外に出すことで計算可能にする手法をReparameterizet\\\n        ion Trickと呼びます。 具体的には、計算グラフの外であらかじめ、平均 0 ・分散 1 の標準正規分布 \\\\mathcal{N}(0,\n        1) から乱数 \\\\epsilon を取得しておきます。"\n    - paragraph [ref=s1e605]: "その後、潜在表現 z は下記の計算を行うことで、まるで「標準化の逆」を行ったように、平均 \\\\mu ・分散\n        \\\\sigma^2 の正規分布 \\\\mathcal{N}(\\\\mu, \\\\sigma) からサンプリングされたような値に変換できます。"\n    - text: z = \\mu + \\sigma \\epsilon\n    - paragraph [ref=s1e617]: この処理を反映させると下記のような図になります。\n    - paragraph [ref=s1e618]:\n        - img [ref=s1e619]\n    - paragraph [ref=s1e620]: この処理により、DeocderとEncoderを繋ぐ部分にはSampling処理が現れなくなったため、Decoder側からEncoder側に勾配が正しく伝播するようになるため、学習が可能なネットワークになりました。\n    - heading "VAEを生成モデルとして利用する" [level=2] [ref=s1e621]\n    - heading "VAEの特徴" [level=3] [ref=s1e622]\n    - paragraph [ref=s1e623]: さて、ここまでの議論からVAEには下記の特徴を持つことが言えます。\n    - list [ref=s1e624]:\n        - listitem [ref=s1e625]:\n            - text: VAEの潜在表現 z には滑らかさが存在し、 z を少し変化させると生成されるデータ x\' が少しだけ変化する\n            - list [ref=s1e632]:\n                - listitem [ref=s1e633]:\n                    - text: この特徴より、潜在空間内において、2地点を行き来することで、滑らかに画像を変化させることができる\n                    - list [ref=s1e634]:\n                        - listitem [ref=s1e635]: 例えば、男性の顔写真から女性の顔写真へと滑らかに生成データを変化させることができる\n        - listitem [ref=s1e636]:\n            - text: "VAEの潜在表現の事前分布 p(z) は標準正規分布 \\\\mathcal{N}(0, 1) を仮定している"\n            - list [ref=s1e641]:\n                - listitem [ref=s1e642]: "標準正規分布 \\\\mathcal{N}(0, 1) から潜在表現 z\n                    をサンプリングすることができ、それを用いて「ある程度意味のある」データ x\' を生成できる"\n    - paragraph [ref=s1e649]: したがって、新しいデータを生成できるという観点で、生成モデルと呼ぶことができます。\n    - heading "落とし穴" [level=3] [ref=s1e650]\n    - paragraph [ref=s1e651]: と、私も昔は思っておりました。 いや、完全に間違っている訳ではないのですが、時には理論通りに進まないものです。\n    - paragraph [ref=s1e653]: 正確には、VAEの特徴は場合によっては正しくて、場合によっては正しくないです。\n        「目的関数の再構成誤差と正則化項のバランス」が崩れた場合に、VAEの特徴は機能しなくなります。\n        （正確には、ある程度その特徴は持っているが、だんだん機能しなくなってくるというイメージ）\n    - paragraph [ref=s1e656]: これから、説明させていただきます。 ここで、VAEの目的関数であるELBOを再提示します。\n    - complementary [ref=s1e658]: "! \\\\mathrm{ELBO} =\n        \\\\mathbb{E}_{q_{\\\\psi}(z|x)}[\\\\log p_{\\\\theta}(x|z)] -\n        \\\\mathrm{KL}\\\\left(q_{\\\\psi}(z|x) \\\\parallel p(z)\\\\right)"\n    - paragraph [ref=s1e664]: この式は、第1項の再構成誤差項と、第2項の正則化項の「バランス」をとって学習が進むことがわかっています。\n        このバランスは、例えば、第2項に対してハイパーパラメータを導入し、その寄与割合を変化させることによっても変わります。 （これを\n        β-VAE（ベータVAE）と呼びます）\n    - paragraph [ref=s1e667]:\n        - strong [ref=s1e668]: そしてもう一つ、潜在表現の次元数によっても変わります。\n    - paragraph [ref=s1e669]: よく見るVAEの解説記事では、MNISTなどの簡単な画像に対して、潜在表現の次元を2次元に設定して検証している記事が多いと思います。\n        これは、VAEの潜在表現の分布と、手描き文字の数字を対応付けて可視化するために非常に優れた例示の仕方だと思っております。\n        一方で、「VAEであれば、必ず、潜在表現の事前分布が正規分布に落ち着く」というようなミスリードを読者に引き起こす危険性もはらんでいると思っております。\n    - heading "潜在表現の次元が小さい場合" [level=3] [ref=s1e672]\n    - paragraph [ref=s1e673]: 他記事のように、VAEの潜在表現 z が2次元の場合、潜在表現 z はたった二つの値しか持たないことになります。\n        2次元の潜在表現 z では、画像の情報を保持することは困難です。\n    - paragraph [ref=s1e681]: この場合、VAEは、潜在空間 z に情報を埋め込むのは諦めます。 そして、潜在空間 z\n        はあくまで「乱数のサンプリング+おおよそのラベル情報」と割り切り、画像の再構成情報をDecoderに埋め込むように学習します。\n    - paragraph [ref=s1e687]: "すると、潜在表現 z は画像情報などの追加情報をほとんど持たないため、事前分布\n        p(z)=\\\\mathcal{N}(0, 1) に従って分布されるようになります。"\n    - paragraph [ref=s1e692]: この場合は、上記の「VAEの特徴」で記載した生成モデルとしての特徴を強く持つことになります。\n    - paragraph [ref=s1e693]: これは、ELBOの正則化項の寄与率を大きくしても発生します。\n    - heading "潜在変数の次元が多い場合" [level=3] [ref=s1e694]\n    - paragraph [ref=s1e695]: 一方で、潜在変数の次元数が多い場合はどうか。\n        潜在変数の次元数が多い場合は、あまり解説に向かないため、解説記事は少ないです。\n        しかし、潜在変数の次元数が多いVAEが使われており、比較的解析しやすい技術はわかります。\n    - paragraph [ref=s1e698]:\n        - strong [ref=s1e699]: みなさんご存知、Stable Diffusionです。\n    - heading "Stable Diffusionのモデル構造" [level=4] [ref=s1e700]\n    - paragraph [ref=s1e701]:\n        - img [ref=s1e702]\n        - text: （引用：\n        - link "https://github.com/CompVis/latent-diffusion/blob/main/assets/modelfigure.png）" [ref=s1e703]:\n            - /url: https://github.com/CompVis/latent-diffusion/blob/main/assets/modelfigure.png%EF%BC%89\n    - paragraph [ref=s1e704]: Stable Diffusionのモデル構造は上記のようになっております。\n    - paragraph [ref=s1e705]:\n        - strong [ref=s1e706]: Conditioningブロック\n    - paragraph [ref=s1e707]: 一番右の「Conditioning」ブロックは、画像の条件付けに使われる部分です。 例えば Stable\n        Diffusionにおける、Text to Imageモデルの場合、プロンプトと呼ばれる呪文で、生成される画像のテーマを制御することができます。\n    - paragraph [ref=s1e709]: なぜかというと、まず、「Conditioning」ブロックで使われいてる、CLIPやT5\n        Encoderと呼ばれるtext Encoderにより、プロンプトを特徴量に変換しています。\n        このとき、CLIPは、「画像とテキストの特徴量が一致」するように学習されているため、プロンプトからCLIPを通じて得られた特徴量は、画像の特徴量と同様に取り扱うことができます。\n        そのため、プロンプトの特徴量を、画像特徴量として、後述する「クロスアテンション」にて、計算することができるようになるからです。\n    - paragraph [ref=s1e712]:\n        - strong [ref=s1e713]: Latent Spaceブロック\n        - text: 真ん中の「Latent Space」ブロックでは、潜在表現 z を生成するブロックになります。\n            ここでは、「拡散モデル」というモデルを利用して、潜在表現 z を生成します。 （拡散モデルに関しては\n        - link "こちらの記事も" [ref=s1e721]:\n            - /url: https://zenn.dev/asap/articles/4092ab60570b05\n        - text: ご覧ください）\n    - paragraph [ref=s1e722]: また、拡散モデルでは、TransformerやU-netというモデルが利用されていますが、そのどちらにおいても、CLIPから得られたプロンプト特徴量を「クロスアテンション」により注入しています。\n    - paragraph [ref=s1e723]:\n        - text: クロスアテンションとは、処理中の画像特徴量と、プロンプト特徴量の類似度を計算し、類似度が高い画像特徴量を増幅させるような処理が行われており、この処理により、プロンプトに適した画像になるように潜在表現\n            z が生成されていきます。 （アテンションに関しては、\n        - link "こちらの記事" [ref=s1e727]:\n            - /url: https://zenn.dev/asap/articles/6bdf89d8e964ec#self-attension%E3%81%A8%E3%81%AF\n        - text: でも解説しております。）\n    - paragraph [ref=s1e728]:\n        - strong [ref=s1e729]: Pixel Spaceブロック\n        - text: 最後に、一番右側の「Pixel Space」ブロックが、VAEが担当する部分です。 上の段の x は生成したい入力データで、 \\varepsilon\n            と書かれている部分がEncoderです。\n    - paragraph [ref=s1e736]: "そして、下の段の \\\\tilde{x} が生成データで、 D と書かれているのが、Decoderになります。"\n    - paragraph [ref=s1e741]: "すなわち、拡散モデルが生成した潜在表現 z から、VAE decoderが出力画像 \\\\tilde{x} を生成していることになります。"\n    - complementary [ref=s1e746]:\n        - text: "!"\n        - paragraph [ref=s1e749]: ここでは、VAEは完全畳み込み層になっており、入力が3次元の画像（縦x横xチャネル）データの場合、潜在表現 z\n            は(縦/8 x 横/8 x 4)の形になっています。\n    - paragraph [ref=s1e752]: "さて、ここで重要なのは、Stable\n        Diffusionでは、VAEが使われてるはずですが、画像を生成する際に標準正規分布 \\\\mathcal{N}(0, 1)\n        からサンプリングした潜在表現 z を利用していないことです。"\n    - paragraph [ref=s1e757]: ちなみに、(詳しい方向けに補足しておくと)Stable Diffusion\n        XLで使われているのは、VQ-VAEではなく、VAEです。\n        （これはDiffusersライブラリでSDXLの部分を見ればわかります。Diffusersライブラリにて実装されているのは、通常のVAEです。）\n    - group [ref=s1e759]: 該当コード\n    - paragraph [ref=s1e761]: "Stable Diffusionでは何をやっているのかというと、標準正規分布 \\\\mathcal{N}(0,\n        1) から得られた潜在表現を、拡散モデルにより分布を変換し、変換後の潜在表現 z\n        をVAEのdecoderに入力することで、画像を生成しています。"\n    - paragraph [ref=s1e766]: "すなわち、上述した「VAEの特徴：標準正規分布 \\\\mathcal{N}(0, 1) から潜在表現 z\n        をサンプリングすることができ、それを用いて「ある程度意味のある」データ x\' を生成できる」が崩壊していることがわかります。"\n    - heading "Stable Diffusion内部のVAEの特徴" [level=4] [ref=s1e773]\n    - paragraph [ref=s1e774]:\n        - text: "実際、単純な標準正規分布 \\\\mathcal{N}(0, 1) から得られた潜在表現を、そのままVAE\n            decoderに入力すると、ノイズ画像が生成されます。 そのあたりは下記の記事などもご参照ください。"\n        - iframe [ref=s1e779]: <could not take iframe snapshot>\n    - paragraph [ref=s1e780]: 上記の記事中のgifを見ると、最初の方は砂嵐のようなノイズ画像が表示されており、だんだん拡散モデルにより潜在表現\n        z の分布が変化していくと、decoderが画像として出力できるようになっているのがわかると思います。\n    - heading "そのときVAEに何が起きているのか！？" [level=4] [ref=s1e783]\n    - paragraph [ref=s1e784]: "これは何が起きているのかというと、第一に、VAEの潜在表現 z の事前分布 p(z) が標準正規分布\n        \\\\mathcal{N}(0, 1) から大きくずれてしまっています。"\n    - paragraph [ref=s1e791]: これは、学習時に、ELBO第2項の正則化項が入っているのにも関わらず・・・です。\n    - paragraph [ref=s1e792]:\n        - text: 正則化項は、下記のコードの\n        - code [ref=s1e793]: DiagonalGaussianDistribution.kl\n        - text: メソッドにて計算がされています。\n        - iframe [ref=s1e795]: <could not take iframe snapshot>\n    - paragraph [ref=s1e796]: "では、なぜVAEの潜在表現 z の事前分布 p(z) が標準正規分布 \\\\mathcal{N}(0, 1)\n        から大きくずれるのかというと、VAEの潜在表現 z の次元数の多さが原因です。"\n    - paragraph [ref=s1e805]: 上述した通り、Stable Diffusionの潜在表現 z のサイズは「縦/8 x 横/8 x\n        4」になっています。 Stable Diffusion XLでは、学習される画像のサイズは「1024x1024」画素が一般的です。\n        このとき、潜在表現 z のサイズは、「128 x 128 x 4」となり、潜在表現 z の要素数は65536となります。\n        65536という数字は、MNISTの実験で使っていた2次元の潜在表現の次元数と比較して、非常に大きな次元数であることがわかります。\n    - paragraph [ref=s1e815]: このレベルで潜在表現 z の次元が大きくなると、潜在表現 z 自体にも画像の情報を埋め込めるようになります。\n        2次元しか潜在表現 z\n        がない場合、潜在表現に情報を埋め込むのは困難なため、画像の再構成情報はDecoderに全て埋め込まれるような挙動になると、記載しました。\n        しかし、今回の場合は、潜在表現 z とDecoderの協業で、画像を再構成することになります。\n    - paragraph [ref=s1e826]: これは、すなわち正則化項よりも、再構成損失項の方が強いバランスになります。\n        潜在表現に画像情報を埋め込むことによって、正則化項の損失が大きくなったとしても、その分大幅に再構成損失項の損失を減少させることができるからです。\n    - paragraph [ref=s1e828]: "その結果、潜在表現 z の事前分布 p(z) が、標準正規分布 \\\\mathcal{N}(0, 1)\n        から大きく離れるようになります。 その代わり、Stable Diffusionでは、より高品質な画像が生成できるようになっているわけです。"\n    - heading "だから拡散モデルがいる" [level=4] [ref=s1e836]\n    - paragraph [ref=s1e837]: ただし、VAEが正則化項を無視し始めると、画質が良くなる代わりに、潜在表現 z の事前分布 p(z)\n        は意味がわからない分布になります。 この意味のわからない分布を、人間は把握することはできず、その分布からサンプリングした潜在表現 z\n        をEncoderなしでは取得できません。\n    - paragraph [ref=s1e845]: そこで、その潜在表現の事前分布 p(z) を、拡散モデルに学習してもらいます\n        そして、その学習ずみ拡散モデルが、VAEの潜在表現 z\n        を生成することで、高品質なVAEの生成画像を維持しながら、生成モデルとして0から新しいデータを生み出しつつ、さらにプロンプトによる条件付けまで達成しています。\n        （だから、これだけ騒がれて、めちゃくちゃ使われているわけですね）\n    - complementary [ref=s1e852]:\n        - text: "!"\n        - paragraph [ref=s1e855]: この場合、VAEの特徴はかなり少なくなっており、どちらかというと Auto Encoderに近い挙動をします。\n            しかし、正則化項が完全になくなったわけではないため、ある程度、潜在表現の滑らかさなどは残っており、結果として、Auto\n            Encoderの潜在表現 z よりも、VAEの潜在表現 z の方が、拡散モデルが学習しやすくなっていると思われます。\n    - heading "最後にVAEの課題" [level=1] [ref=s1e861]\n    - paragraph [ref=s1e862]: ここまでVAEのいいところを説明してきましたが、VAEを単体で利用する場合は、まだまだ課題があります。\n    - heading "Posterior Collapse" [level=2] [ref=s1e863]\n    - paragraph [ref=s1e864]: "一つは「Posterior Collapse（事後崩壊）」の問題です。\n        これは、VAEの目的関数ELBOの正則化項が強すぎる場合、Encoderが出力する近似事後分布 q_{\\\\psi}(z|x) と、事前分布（\n        p(z)=\\\\mathcal{N}(0, 1) )が完全に一致してしまうことで発生します。\n        この場合、画像の再構成情報が完全にDecoderに入ってしまい、潜在表現 z\n        を無視した再構成が行われてしまいます。なまじDecoderの性能が高いと発生しやすい問題です。"\n    - paragraph [ref=s1e873]: posterior collapseの対策として、学習初期は正則化項の寄与率を下げて学習させ、だんだん寄与率を大きくしていくことが挙げられます。\n    - heading "画像がボケやすい" [level=2] [ref=s1e874]\n    - paragraph [ref=s1e875]: もう一つは、生成される画像がボケやすい点です。 Stable\n        Diffusionに搭載されているVAEのように、事前分布 p(z) の形状を完全に気にしないように学習する（つまり潜在表現 z\n        の次元数が非常に大きい）場合は起こりにくい問題ではありますが、ある程度正則化項による正則化を入れていきたい場合（つまり、事前分布 p(z)\n        に正規分布を仮定したい状況）では、この問題が発生しやすいです。\n    - paragraph [ref=s1e883]: これは、上述した通り、Decoderに入力される潜在表現 z\n        は、乱数のサンプリング後の値であるため、例え全く同じ画像を学習していたとしても、潜在表現 z の値は学習のたびに異なります。\n    - paragraph [ref=s1e888]: すると、似たような画像 y を生成する潜在表現 z は、潜在表現空間において、近づきやすくなります。\n        一方で、decoder側は、潜在表現 z が密集している空間の場合、ある潜在表現 z が、どの生成画像 x\n        に対応しているのか、そのランダム性のために判断することができなくなってしまうため、なるべく最終的な損失を小さくするために、密集している潜在表現\n        z から生成されうる画像 x の平均のような画像を学習してしまいます。 これは、潜在表現 z\n        の次元が小さい方が、潜在表現空間における密集度が高まるため、発生しやすくなります。（一般に低次元空間の方が2点間の距離は小さく、高次元空間の方が2点間の距離が広くなる）\n    - paragraph [ref=s1e907]: "これの対処法は、VAEの潜在表現の次元を増やすことか、第2項の寄与率を下げるくらいしかありませんでした。\n        しかし、これを行うと、どうしても生成モデルとしての特徴である、「潜在表現zの事前分布 p(z) が標準正規分布 \\\\mathcal{N}(0,\n        1) である」という仮定が消失しやすくなります。"\n    - paragraph [ref=s1e913]: したがって、バランス調整が難しい問題で、VAEが持つ悪い特徴の一つでしたが、後日記事を書くVQ-VAEでは、この問題を解消しています。\n    - heading "まとめ" [level=1] [ref=s1e914]\n    - paragraph [ref=s1e915]: ここまで読んでくださってありがとうございました！\n        生成モデルと言えば、Transformerや拡散モデルが主流ですが、非常に長い系列長（音声など）を生成する場合には、まだまだVAEが使われいてるように見えます。\n    - paragraph [ref=s1e917]: VAEは一度に全てのデータを生成できるため、速度が必要なアプリケーションにも適していますし、圧縮器としての機能もあるため、大きなデータを潜在表現に圧縮し、その潜在表現をTransformerや拡散モデルによって再構成するといった使い方がよくなされており、古い概念ではありますが、まだまだVAEには価値があるなと感じております。\n    - paragraph [ref=s1e918]: わかった気になっていましたが、VAEに関して詳細に色々調べていると、まだまだ理解があやふやな部分もあり、記事を書きながら理解度を高めることができました！\n    - paragraph [ref=s1e919]: この記事が、何らかの形でみなさまの手助けになれば幸いです！\n    - paragraph [ref=s1e920]: ここまで読んでくださり、ありがとうございました！\n    - heading "参考文献" [level=1] [ref=s1e921]\n    - paragraph [ref=s1e922]:\n        - iframe [ref=s1e924]: <could not take iframe snapshot>\n        - text: VAEの論文です。\n    - paragraph [ref=s1e925]:\n        - link "ゼロから作るDeep Learning ❺ ―生成モデル編" [ref=s1e926]:\n            - /url: https://amzn.to/3Aw9vCt\n    - paragraph [ref=s1e927]: 圧倒的な名著シリーズである「ゼロから作るDeep Learning」シリーズの5作目である、生成モデル編です。\n        この本では、最終的に拡散モデルを理解することを目的に、VAEや混合ガウスモデル、正規分布まで遡り、本当にゼロから中身を理解しながら実装をすることができる、大変素晴らしい書籍になっています。\n    - paragraph [ref=s1e929]:\n        - link "妥協しないデータ分析のための 微積分+線形代数入門" [ref=s1e930]:\n            - /url: https://amzn.to/4hE7ftx\n    - paragraph [ref=s1e931]: 本書の後半にもVAEやELBOについての簡単な記載があり、本記事を書くうえで表現を参考にさせていただきました。\n        確率分布のどの部分が計算可能で、どの部分が計算不可能なのかが常に明示されながら進んでいくので、初学者には非常に分かりやすい書籍でした！\n    - paragraph [ref=s1e933]: 特に、VAEなどの生成AIをただ使うだけでなく、ちゃんと理論を理解したいと思った場合、論文や理論書を理解するために「線形代数」や「微積分学」の事前知識が必要になります。\n        本書は、初学者が一番最初に読んで必要な基礎知識が習得できる良本になります。\n    - paragraph [ref=s1e935]:\n        - link "生成 Deep Learning 第2版 ―絵を描き、物語や音楽を作り、ゲームをプレイする" [ref=s1e936]:\n            - /url: https://amzn.to/4g8hjJg\n    - paragraph [ref=s1e937]: 本書の序盤で、Auto\n        EncoderとVAEの違いを説明してくれています。しかもそれぞれの潜在空間の分布を可視化してくれているので、違いが分かりやすく、理解もしやすいです。\n        さらに、本書はVAEだけでなくGANや画像生成、音楽生成、テキスト生成など、ありとあらゆる生成AIの紹介と解説をしてくれており、必読の一冊となっております。\n    - paragraph [ref=s1e939]: （書籍のリンクはAmazonアフィリエイトリンクになっております）\n    - button "いいね" [ref=s1e942]:\n        - img [ref=s1e943]\n    - text: "104"\n    - button "ブックマーク" [ref=s1e959]:\n        - img [ref=s1e960]\n    - text: "43"\n    - button "その他の操作" [ref=s1e965]:\n        - img [ref=s1e966]\n    - tooltip "X(Twitter)にポスト" [ref=s1e969]:\n        - img [ref=s1e970]\n        - text: X(Twitter)にポスト\n    - tooltip "Facebookに投稿" [ref=s1e976]:\n        - img [ref=s1e977]\n        - text: Facebookに投稿\n    - tooltip "はてなブックマークに登録" [ref=s1e983]:\n        - img [ref=s1e984]\n        - text: はてなブックマークに登録\n    - complementary [ref=s1e987]:\n        - link "asap" [ref=s1e989]:\n            - /url: /asap\n            - img "asap" [ref=s1e990]\n        - link "asap" [ref=s1e992]:\n            - /url: /asap\n        - paragraph [ref=s1e994]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n        - button "フォロー" [ref=s1e998]\n        - tooltip "@personabb" [ref=s1e999]:\n            - img "GitHub" [ref=s1e1000]\n            - text: "@personabb"\n        - tooltip "@asap2650" [ref=s1e1002]:\n            - img "X(Twitter)" [ref=s1e1003]\n            - text: "@asap2650"\n        - tooltip "yard.tips" [ref=s1e1009]:\n            - img "リンク" [ref=s1e1010]\n            - text: yard.tips\n    - complementary [ref=s1e1013]:\n        - text: バッジを贈って著者を応援しよう\n        - paragraph [ref=s1e1017]: バッジを受け取った著者にはZennから現金やAmazonギフトカードが還元されます。\n        - button "バッジを贈る" [ref=s1e1019]\n        - img [ref=s1e1020]\n    - heading "Discussion" [level=3] [ref=s1e1029]\n    - img [ref=s1e1030]\n    - text: ログインするとコメントできます\n    - button "Login" [ref=s1e1034]\n    - complementary [ref=s1e1035]:\n        - link "asap" [ref=s1e1040]:\n            - /url: /asap\n            - img "asap" [ref=s1e1041]\n        - link "asap" [ref=s1e1043]:\n            - /url: /asap\n        - button "フォロー" [ref=s1e1045]\n        - link "@personabb" [ref=s1e1046]:\n            - /url: https://github.com/personabb\n            - tooltip "@personabb" [ref=s1e1047]:\n                - img "GitHub" [ref=s1e1048]\n                - text: "@personabb"\n        - link "@asap2650" [ref=s1e1050]:\n            - /url: https://twitter.com/asap2650\n            - tooltip "@asap2650" [ref=s1e1051]:\n                - img "X(Twitter)" [ref=s1e1052]\n                - text: "@asap2650"\n        - link "yard.tips" [ref=s1e1058]:\n            - /url: https://yard.tips/asap2650\n            - tooltip "yard.tips" [ref=s1e1059]:\n                - img "リンク" [ref=s1e1060]\n                - text: yard.tips\n        - paragraph [ref=s1e1063]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n        - button "バッジを贈る" [ref=s1e1066]\n        - link "バッジを贈るとは" [ref=s1e1068]:\n            - /url: /faq#badges\n            - text: バッジを贈るとは\n            - img [ref=s1e1069]\n        - text: 目次\n        - list [ref=s1e1078]:\n            - listitem [ref=s1e1079]:\n                - link "はじめに" [ref=s1e1080]:\n                    - /url: "#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB"\n            - listitem [ref=s1e1081]:\n                - link "参考文献" [ref=s1e1082]:\n                    - /url: "#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"\n            - listitem [ref=s1e1083]:\n                - link "関連記事" [ref=s1e1084]:\n                    - /url: "#%E9%96%A2%E9%80%A3%E8%A8%98%E4%BA%8B"\n            - listitem [ref=s1e1085]:\n                - link "Auto Encoder" [ref=s1e1086]:\n                    - /url: "#auto-encoder"\n                - list [ref=s1e1087]:\n                    - listitem [ref=s1e1088]:\n                        - link "Auto Encoderの紹介" [ref=s1e1089]:\n                            - /url: "#auto-encoder%E3%81%AE%E7%B4%B9%E4%BB%8B"\n                    - listitem [ref=s1e1090]:\n                        - link "Auto Encoderの本質" [ref=s1e1091]:\n                            - /url: "#auto-encoder%E3%81%AE%E6%9C%AC%E8%B3%AA"\n                    - listitem [ref=s1e1092]:\n                        - link "Auto Encoerの課題" [ref=s1e1093]:\n                            - /url: "#auto-encoer%E3%81%AE%E8%AA%B2%E9%A1%8C"\n            - listitem [ref=s1e1094]:\n                - link "VAE（Variational Auto Encoder）" [ref=s1e1095]:\n                    - /url: "#vae%EF%BC%88variational-auto-encoder%EF%BC%89"\n                - list [ref=s1e1096]:\n                    - listitem [ref=s1e1097]:\n                        - link "VAEと生成モデル" [ref=s1e1098]:\n                            - /url: "#vae%E3%81%A8%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB"\n                    - listitem [ref=s1e1099]:\n                        - link "VAEの挙動・振る舞い" [ref=s1e1100]:\n                            - /url: "#vae%E3%81%AE%E6%8C%99%E5%8B%95%E3%83%BB%E6%8C%AF%E3%82%8B%E8%88%9E%E3%8\\\n                                1%84"\n                    - listitem [ref=s1e1101]:\n                        - link "VAEを生成モデルとして利用する" [ref=s1e1102]:\n                            - /url: "#vae%E3%82%92%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB%E3%81%A8%E3%8\\\n                                1%97%E3%81%A6%E5%88%A9%E7%94%A8%E3%81%99%E3%82%\\\n                                8B"\n            - listitem [ref=s1e1103]:\n                - link "最後にVAEの課題" [ref=s1e1104]:\n                    - /url: "#%E6%9C%80%E5%BE%8C%E3%81%ABvae%E3%81%AE%E8%AA%B2%E9%A1%8C"\n                - list [ref=s1e1105]:\n                    - listitem [ref=s1e1106]:\n                        - link "Posterior Collapse" [ref=s1e1107]:\n                            - /url: "#posterior-collapse"\n                    - listitem [ref=s1e1108]:\n                        - link "画像がボケやすい" [ref=s1e1109]:\n                            - /url: "#%E7%94%BB%E5%83%8F%E3%81%8C%E3%83%9C%E3%82%B1%E3%82%84%E3%81%99%E3%81%8\\\n                                4"\n            - listitem [ref=s1e1110]:\n                - link "まとめ" [ref=s1e1111]:\n                    - /url: "#%E3%81%BE%E3%81%A8%E3%82%81"\n            - listitem [ref=s1e1112]:\n                - link "参考文献" [ref=s1e1113]:\n                    - /url: "#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE-1"\n        - paragraph [ref=s1e1115]: Zennからのお知らせ\n        - article [ref=s1e1118]:\n            - link "第２回 AI Agent Hackathon with Google Cloud 第２回 AI Agent Hackathon with Google Cloud 4/14登録受付開始！" [ref=s1e1119]:\n                - /url: /hackathons/google-cloud-japan-ai-hackathon-vol2\n                - img "第２回 AI Agent Hackathon with Google Cloud" [ref=s1e1121]\n                - paragraph [ref=s1e1122]: 第２回 AI Agent Hackathon with Google Cloud 4/14登録受付開始！\n        - article [ref=s1e1123]:\n            - link "Zenncafe日比谷#2 生成AIで変わる仕事 Zenncafe日比谷#2 生成AIで変わる仕事" [ref=s1e1124]:\n                - /url: https://zenn.connpass.com/event/351131/\n                - img "Zenncafe日比谷#2 生成AIで変わる仕事" [ref=s1e1126]\n                - paragraph [ref=s1e1127]: Zenncafe日比谷#2 生成AIで変わる仕事\n- contentinfo [ref=s1e1129]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1133]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1134]\n    - paragraph [ref=s1e1142]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e1144]:\n        - heading "About" [level=4] [ref=s1e1145]\n        - list [ref=s1e1146]:\n            - listitem [ref=s1e1147]:\n                - link "Zennについて" [ref=s1e1149]:\n                    - /url: /about\n            - listitem [ref=s1e1150]:\n                - link "運営会社" [ref=s1e1151]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e1152]:\n                - link "お知らせ・リリース" [ref=s1e1153]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e1154]:\n                - link "イベント" [ref=s1e1156]:\n                    - /url: /events\n    - navigation [ref=s1e1157]:\n        - heading "Guides" [level=4] [ref=s1e1158]\n        - list [ref=s1e1159]:\n            - listitem [ref=s1e1160]:\n                - link "使い方" [ref=s1e1162]:\n                    - /url: /manual\n            - listitem [ref=s1e1163]:\n                - link "法人向けメニュー" [ref=s1e1165]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e1167]:\n                - link "Publication / Pro" [ref=s1e1169]:\n                    - /url: /publications\n            - listitem [ref=s1e1170]:\n                - link "よくある質問" [ref=s1e1172]:\n                    - /url: /faq\n    - navigation [ref=s1e1173]:\n        - heading "Links" [level=4] [ref=s1e1174]\n        - list [ref=s1e1175]:\n            - listitem [ref=s1e1176]:\n                - link "X(Twitter)" [ref=s1e1177]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e1178]:\n                - link "GitHub" [ref=s1e1179]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e1180]:\n                - link "メディアキット" [ref=s1e1182]:\n                    - /url: /mediakit\n    - navigation [ref=s1e1183]:\n        - heading "Legal" [level=4] [ref=s1e1184]\n        - list [ref=s1e1185]:\n            - listitem [ref=s1e1186]:\n                - link "利用規約" [ref=s1e1188]:\n                    - /url: /terms\n            - listitem [ref=s1e1189]:\n                - link "プライバシーポリシー" [ref=s1e1191]:\n                    - /url: /privacy\n            - listitem [ref=s1e1192]:\n                - link "特商法表記" [ref=s1e1194]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e1197]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e1198]\n- alert [ref=s1e1217]\n```\n', name='browser_navigate', tool_call_id='45a87836-eb09-4499-bf44-a2bcdbd3ec2b'), AIMessage(content='「VAEって結局何者なの？」という記事は、生成モデルの一つであるVAE（Variational Autoencoder）について解説した記事です。\n\n記事の内容をまとめると、以下のようになります。\n\n*   **VAEとは:** VAEは、生成モデルとして重要な「対数尤度の最大化」問題を解くために必要な構造を用意したら、結果的にAuto Encoderのような形になったというモデルです。\n*   **Auto Encoderとの違い:** VAEを理解するためには、まずAuto Encoderを理解する必要があります。Auto Encoderは、入力データと出力を一致させるように学習させるネットワークですが、それ自体では生成モデルとして活用することが難しいという課題があります。\n*   **VAEの仕組み:** VAEは、Encoderで入力データを潜在表現と呼ばれる低次元のデータに圧縮し、Decoderでその潜在表現から元のデータを復元します。この際、Encoderは潜在表現の平均と分散を出力し、その分布からサンプリングされた値をDecoderに入力します。これにより、VAEは潜在空間に滑らかさを持ち、生成モデルとして機能することができます。\n*   **VAEの学習:** VAEは、ELBO（Evidence Lower Bound）と呼ばれる目的関数を最大化するように学習されます。ELBOは、再構成誤差と正則化項の和で表され、VAEはこのバランスをとりながら学習を進めます。\n*   **VAEの課題:** VAEには、Posterior Collapse（事後崩壊）や生成される画像がボケやすいといった課題があります。これらの課題は、潜在表現の次元数や正則化項の強さを調整することで、ある程度緩和することができます。\n*   **Stable DiffusionにおけるVAE:** Stable Diffusionでは、VAEが画像のエンコード・デコードを担当しています。ただし、Stable Diffusionでは、VAEの潜在表現の事前分布が標準正規分布から大きくずれており、VAEはAuto Encoderに近い挙動をしています。\n\nこの記事は、VAEの基本的な仕組みから、学習方法、課題、そしてStable DiffusionにおけるVAEの役割まで、幅広く解説しています。特に、VAEの潜在空間の性質や、Stable DiffusionにおけるVAEの挙動について詳しく解説している点が特徴です。', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-bf93ed43-18df-4eb1-bd5a-f1423de25a27-0', usage_metadata={'input_tokens': 70378, 'output_tokens': 467, 'total_tokens': 70845, 'input_token_details': {'cache_read': 0}}), HumanMessage(content=[{'type': 'text', 'text': '3番目のいいねが多い記事は？'}], additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-ed215635-eb93-42e2-8508-9905e26caa2a-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap'}, 'id': 'fb8928e6-1802-4cb0-a1b1-235173be5629', 'type': 'tool_call'}], usage_metadata={'input_tokens': 70853, 'output_tokens': 13, 'total_tokens': 70866, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap\n\n- Page URL: https://zenn.dev/asap\n- Page Title: asapさんの記事一覧 | Zenn\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- banner [ref=s1e32]:\n    - img "asap" [ref=s1e36]\n    - heading "asap" [level=1] [ref=s1e39]\n    - button "フォロー" [ref=s1e41]\n    - paragraph [ref=s1e43]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n    - button "2,214 Likes" [ref=s1e46]\n    - button "310 Followers" [ref=s1e48]\n    - link "@personabb" [ref=s1e51]:\n        - /url: https://github.com/personabb\n        - tooltip "@personabb" [ref=s1e52]:\n            - img "GitHub" [ref=s1e53]\n            - text: "@personabb"\n    - link "@asap2650" [ref=s1e55]:\n        - /url: https://twitter.com/asap2650\n        - tooltip "@asap2650" [ref=s1e56]:\n            - img "X(Twitter)" [ref=s1e57]\n            - text: "@asap2650"\n    - link "yard.tips" [ref=s1e63]:\n        - /url: https://yard.tips/asap2650\n        - tooltip "yard.tips" [ref=s1e64]:\n            - img [ref=s1e65]\n            - text: yard.tips\n    - link "RSS" [ref=s1e67]:\n        - /url: /asap/feed\n        - tooltip "RSS" [ref=s1e68]:\n            - img [ref=s1e69]\n            - text: RSS\n- link "Articles 72" [ref=s1e80]:\n    - /url: /asap\n- link "Scraps 5" [ref=s1e82]:\n    - /url: /asap?tab=scraps\n- link "Comments" [ref=s1e84]:\n    - /url: /asap?tab=comments\n- contentinfo [ref=s1e88]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e92]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e93]\n    - paragraph [ref=s1e101]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e103]:\n        - heading "About" [level=4] [ref=s1e104]\n        - list [ref=s1e105]:\n            - listitem [ref=s1e106]:\n                - link "Zennについて" [ref=s1e108]:\n                    - /url: /about\n            - listitem [ref=s1e109]:\n                - link "運営会社" [ref=s1e110]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e111]:\n                - link "お知らせ・リリース" [ref=s1e112]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e113]:\n                - link "イベント" [ref=s1e115]:\n                    - /url: /events\n    - navigation [ref=s1e116]:\n        - heading "Guides" [level=4] [ref=s1e117]\n        - list [ref=s1e118]:\n            - listitem [ref=s1e119]:\n                - link "使い方" [ref=s1e121]:\n                    - /url: /manual\n            - listitem [ref=s1e122]:\n                - link "法人向けメニュー" [ref=s1e124]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e126]:\n                - link "Publication / Pro" [ref=s1e128]:\n                    - /url: /publications\n            - listitem [ref=s1e129]:\n                - link "よくある質問" [ref=s1e131]:\n                    - /url: /faq\n    - navigation [ref=s1e132]:\n        - heading "Links" [level=4] [ref=s1e133]\n        - list [ref=s1e134]:\n            - listitem [ref=s1e135]:\n                - link "X(Twitter)" [ref=s1e136]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e137]:\n                - link "GitHub" [ref=s1e138]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e139]:\n                - link "メディアキット" [ref=s1e141]:\n                    - /url: /mediakit\n    - navigation [ref=s1e142]:\n        - heading "Legal" [level=4] [ref=s1e143]\n        - list [ref=s1e144]:\n            - listitem [ref=s1e145]:\n                - link "利用規約" [ref=s1e147]:\n                    - /url: /terms\n            - listitem [ref=s1e148]:\n                - link "プライバシーポリシー" [ref=s1e150]:\n                    - /url: /privacy\n            - listitem [ref=s1e151]:\n                - link "特商法表記" [ref=s1e153]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e156]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e157]\n- alert [ref=s1e176]\n```\n', name='browser_navigate', tool_call_id='fb8928e6-1802-4cb0-a1b1-235173be5629'), AIMessage(content='asapさんのZennの記事の中で3番目にいいねが多い記事は、「💯 拡散モデルの理論って面白いよね」で、いいね数は87です。', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-db79d227-62d3-4b10-ab28-7edd9f261c16-0', usage_metadata={'input_tokens': 72673, 'output_tokens': 35, 'total_tokens': 72708, 'input_token_details': {'cache_read': 0}}), HumanMessage(content=[{'type': 'text', 'text': '4番目にいいねが多い記事は？中身も解説して'}], additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-7d87d69a-e022-497f-be64-80b1380f371b-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap'}, 'id': 'd37ce32f-80cf-457d-a5c2-214147053825', 'type': 'tool_call'}], usage_metadata={'input_tokens': 72719, 'output_tokens': 13, 'total_tokens': 72732, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap\n\n- Page URL: https://zenn.dev/asap\n- Page Title: asapさんの記事一覧 | Zenn\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- banner [ref=s1e32]:\n    - img "asap" [ref=s1e36]\n    - heading "asap" [level=1] [ref=s1e39]\n    - button "フォロー" [ref=s1e41]\n    - paragraph [ref=s1e43]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n    - button "2,214 Likes" [ref=s1e46]\n    - button "310 Followers" [ref=s1e48]\n    - link "@personabb" [ref=s1e51]:\n        - /url: https://github.com/personabb\n        - tooltip "@personabb" [ref=s1e52]:\n            - img "GitHub" [ref=s1e53]\n            - text: "@personabb"\n    - link "@asap2650" [ref=s1e55]:\n        - /url: https://twitter.com/asap2650\n        - tooltip "@asap2650" [ref=s1e56]:\n            - img "X(Twitter)" [ref=s1e57]\n            - text: "@asap2650"\n    - link "yard.tips" [ref=s1e63]:\n        - /url: https://yard.tips/asap2650\n        - tooltip "yard.tips" [ref=s1e64]:\n            - img [ref=s1e65]\n            - text: yard.tips\n    - link "RSS" [ref=s1e67]:\n        - /url: /asap/feed\n        - tooltip "RSS" [ref=s1e68]:\n            - img [ref=s1e69]\n            - text: RSS\n- link "Articles 72" [ref=s1e80]:\n    - /url: /asap\n- link "Scraps 5" [ref=s1e82]:\n    - /url: /asap?tab=scraps\n- link "Comments" [ref=s1e84]:\n    - /url: /asap?tab=comments\n- contentinfo [ref=s1e88]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e92]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e93]\n    - paragraph [ref=s1e101]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e103]:\n        - heading "About" [level=4] [ref=s1e104]\n        - list [ref=s1e105]:\n            - listitem [ref=s1e106]:\n                - link "Zennについて" [ref=s1e108]:\n                    - /url: /about\n            - listitem [ref=s1e109]:\n                - link "運営会社" [ref=s1e110]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e111]:\n                - link "お知らせ・リリース" [ref=s1e112]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e113]:\n                - link "イベント" [ref=s1e115]:\n                    - /url: /events\n    - navigation [ref=s1e116]:\n        - heading "Guides" [level=4] [ref=s1e117]\n        - list [ref=s1e118]:\n            - listitem [ref=s1e119]:\n                - link "使い方" [ref=s1e121]:\n                    - /url: /manual\n            - listitem [ref=s1e122]:\n                - link "法人向けメニュー" [ref=s1e124]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e126]:\n                - link "Publication / Pro" [ref=s1e128]:\n                    - /url: /publications\n            - listitem [ref=s1e129]:\n                - link "よくある質問" [ref=s1e131]:\n                    - /url: /faq\n    - navigation [ref=s1e132]:\n        - heading "Links" [level=4] [ref=s1e133]\n        - list [ref=s1e134]:\n            - listitem [ref=s1e135]:\n                - link "X(Twitter)" [ref=s1e136]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e137]:\n                - link "GitHub" [ref=s1e138]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e139]:\n                - link "メディアキット" [ref=s1e141]:\n                    - /url: /mediakit\n    - navigation [ref=s1e142]:\n        - heading "Legal" [level=4] [ref=s1e143]\n        - list [ref=s1e144]:\n            - listitem [ref=s1e145]:\n                - link "利用規約" [ref=s1e147]:\n                    - /url: /terms\n            - listitem [ref=s1e148]:\n                - link "プライバシーポリシー" [ref=s1e150]:\n                    - /url: /privacy\n            - listitem [ref=s1e151]:\n                - link "特商法表記" [ref=s1e153]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e156]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e157]\n- alert [ref=s1e176]\n```\n', name='browser_navigate', tool_call_id='d37ce32f-80cf-457d-a5c2-214147053825'), AIMessage(content='asapさんのZennの記事の中で4番目にいいねが多い記事は、「🥰 Open AI Realtime APIのPythonサンプルコードを作成」で、いいね数は79です。\n記事の中身を解説します。', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap/articles/4368fd306b592a"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-db44b5c9-f50b-4af3-8ec1-e436036de4f1-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap/articles/4368fd306b592a'}, 'id': '7cb428c3-7d50-4f34-819d-e06432672aa4', 'type': 'tool_call'}], usage_metadata={'input_tokens': 74539, 'output_tokens': 72, 'total_tokens': 74611, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap/articles/4368fd306b592a\n\n- Page URL: https://zenn.dev/asap/articles/4368fd306b592a\n- Page Title: Open AI Realtime APIのPythonサンプルコードを作成\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- article [ref=s1e32]:\n    - text: 🥰\n    - heading "Open AI Realtime APIのPythonサンプルコードを作成" [level=1] [ref=s1e38]\n    - text: 2024/10/04に公開\n    - tooltip "更新日" [ref=s1e45]:\n        - img [ref=s1e46]\n        - text: 2024/12/23 更新日\n    - link "OpenAI" [ref=s1e59]:\n        - /url: /topics/openai\n        - img [ref=s1e61]\n        - text: OpenAI\n    - link "ChatGPT" [ref=s1e63]:\n        - /url: /topics/chatgpt\n        - img [ref=s1e65]\n        - text: ChatGPT\n    - link "生成 AI" [ref=s1e67]:\n        - /url: /topics/%E7%94%9F%E6%88%90ai\n        - img [ref=s1e69]\n        - text: 生成 AI\n    - link "realtimeapi" [ref=s1e71]:\n        - /url: /topics/realtimeapi\n        - img [ref=s1e73]\n        - text: realtimeapi\n    - link "記事投稿コンテスト「今年の最も大きなチャレンジ」" [ref=s1e75]:\n        - /url: /topics/contest2024\n        - img [ref=s1e77]\n        - text: 記事投稿コンテスト「今年の最も大きなチャレンジ」\n    - link "tech" [ref=s1e79]:\n        - /url: /tech-or-idea\n        - img [ref=s1e81]\n        - text: tech\n    - heading "はじめに" [level=1] [ref=s1e85]\n    - complementary [ref=s1e86]:\n        - text: "!"\n        - paragraph [ref=s1e89]: （2024年12月22日 追記） 3点追記させてください。\n        - paragraph [ref=s1e91]:\n            - strong [ref=s1e92]: (1)\n            - text: 12月に新しいモデルが出ているようです。\n            - code [ref=s1e95]: gpt-4o-realtime-preview-2024-12-17\n        - paragraph [ref=s1e96]: こちらを利用すると、10月に出たモデルの60%OFFの料金で利用できるそうです。\n            本日、記事の中身のモデルを上記の最新のものに変更しました。\n        - paragraph [ref=s1e98]: 以前、本記事をご覧いただいた方は、モデル指定の箇所を変更いただけますと幸いです\n            （残念ながら2024年12月22日現在では、Azure版では公開されていないようです。今後に期待です）\n        - code [ref=s1e103]: WS_URL =\n            "wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-12-17"\n        - button "クリップボードにコピー" [ref=s1e106]:\n            - img [ref=s1e107]\n        - button "右端で折り返す" [ref=s1e108]:\n            - img [ref=s1e109]\n        - paragraph [ref=s1e110]:\n            - strong [ref=s1e111]: (2)\n            - text: websocketにおいて、新しいバージョンは引数の名前が変更されたようです。 これまでは、ヘッダー情報を与える際に、\n            - code [ref=s1e114]: extra_headers\n            - text: の引数を利用していましたが、新しいバーションでは\n            - code [ref=s1e115]: additional_headers\n            - text: になるそうです。\n            - link "https://stackoverflow.com/questions/65230690/send-headers-in-websockets-connection-request-from-python-client" [ref=s1e117]:\n                - /url: https://stackoverflow.com/questions/65230690/send-headers-in-websockets-connection-request-from-python-client\n        - paragraph [ref=s1e118]: 記事は最新版に対応できるように変更しました。\n        - paragraph [ref=s1e119]:\n            - text: 今回の件を反省して、バージョン指定込みの\n            - code [ref=s1e120]: requirements.txt\n            - text: も用意しました。\n        - text: requirements.txt\n        - code [ref=s1e126]: cffi==1.17.1 numpy==2.2.1 PyAudio==0.2.14 pycparser==2.22\n            soundfile==0.12.1 websockets==14.1\n        - button "クリップボードにコピー" [ref=s1e143]:\n            - img [ref=s1e144]\n        - button "右端で折り返す" [ref=s1e145]:\n            - img [ref=s1e146]\n        - paragraph [ref=s1e147]:\n            - strong [ref=s1e148]: (3)\n            - text: gpt-4o-miniを利用したRealtime APIが新しくリリースされています。 こちら、WebSocket\n                URLのモデルをgpt-4o-miniのものに変更するだけで、そのまま利用できることがわかりました。\n        - paragraph [ref=s1e151]: 下記のように設定してみてください。（その他箇所は変更不要です）\n        - code [ref=s1e155]: WS_URL =\n            "wss://api.openai.com/v1/realtime?model=gpt-4o-mini-realtime-preview-2024-12-17"\n        - button "クリップボードにコピー" [ref=s1e158]:\n            - img [ref=s1e159]\n        - button "右端で折り返す" [ref=s1e160]:\n            - img [ref=s1e161]\n        - paragraph [ref=s1e162]: （残念ながら2024年12月22日現在では、Azure版では公開されていないようです。今後に期待です）\n        - paragraph [ref=s1e163]: 長々と失礼いたしました。では以下本文です。\n    - paragraph [ref=s1e164]: タイトルの通りです。\n        AzureのAPIは先行で公開しておりましたが、ついにOpenAIからも、APIが解放されたので、取り急ぎpythonでサンプルコードを作成してみました。\n        （公式からはPythonのサンプルコードが公開されていないので、java\n        scriptのコードを参考にしてpythonに書き換える形で作成しました）\n    - paragraph [ref=s1e167]: サンプルコードなので、できるだけ1ファイルかつ、最小限の機能で実装しました。 なのでわかりやすいとは思います。\n        （その代わり、システムプロンプトなどはベタ書きしてます）\n    - paragraph [ref=s1e170]: リアルタイム音声対話ができるRealtime\n        APIをpythonで動かしたいけど、pythonのサンプルコードがなくて困っているという人向けに参考になれば幸いです。\n    - paragraph [ref=s1e171]: （追記） サンプルコードを少し変更することで、Azure版にも接続できることを確認しました。 サンプルコードの章にて変更箇所を紹介します。\n    - paragraph [ref=s1e174]: （2024年10月6日 AM11:52 追記）\n        AI発話中にこちらが発話しても、発話を止めてくれなかった問題ですが、解決方法を見つけましたので、改めて記事にする予定です。（今日は時間がなく・・・申し訳ございません）\n        （ちょっとサンプルコードからは離れると思ったので、別記事の予定です） おそらく10月7日になると思います。\n        記事書きましたら、こちらにもリンクを貼ります。\n    - paragraph [ref=s1e180]:\n        - text: 下記にて取り急ぎコードだけ共有します。コードの解説は後日更新します。 続編の記事は下記になります。\n        - iframe [ref=s1e184]:\n            - link "Open AI Realtime APIのPythonサンプルコードで、AI発話にカットイン（割り込み）する方法 asapさんによる記事 zenn.dev favicon image zenn.dev zenn.dev thumbnail image" [ref=f1s1e6]:\n                - /url: https://zenn.dev/asap/articles/563500af4649da\n                - heading "Open AI Realtime\n                  APIのPythonサンプルコードで、AI発話にカットイン（割り込み）する方法" [level=1]\n                  [ref=f1s1e8]\n                - text: asapさんによる記事\n                - img "zenn.dev favicon image" [ref=f1s1e11]\n                - text: zenn.dev\n                - img "zenn.dev thumbnail image" [ref=f1s1e13]\n            - alert [ref=f1s1e15]\n        - iframe [ref=s1e186]:\n            - link "Open AI Realtime APIで、会話履歴を削除して高額請求を回避したい！【Pythonサンプルコード】 asapさんによる記事 zenn.dev favicon image zenn.dev zenn.dev thumbnail image" [ref=f2s1e6]:\n                - /url: https://zenn.dev/asap/articles/af07fcedbbef61\n                - heading "Open AI Realtime\n                  APIで、会話履歴を削除して高額請求を回避したい！【Pythonサンプルコード】" [level=1]\n                  [ref=f2s1e8]\n                - text: asapさんによる記事\n                - img "zenn.dev favicon image" [ref=f2s1e11]\n                - text: zenn.dev\n                - img "zenn.dev thumbnail image" [ref=f2s1e13]\n            - alert [ref=f2s1e15]\n    - paragraph [ref=s1e187]: なお、実践上は上記で紹介するコードを利用するのをお勧めしますが、Realtime\n        APIをpythonで利用するにあたっての仕様などの理解は、本記事で紹介するサンプルコードを利用する方が理解しやすいと思います。\n    - heading "参考" [level=1] [ref=s1e188]\n    - paragraph [ref=s1e189]: 下記の公式ドキュメントを参考にしながら組みました。（というかそれ以外に情報がない・・・）\n    - paragraph [ref=s1e190]:\n        - iframe [ref=s1e192]:\n            - link "OpenAI Platform Explore developer resources, tutorials, API docs, and dynamic examples to get the most out of OpenAI platform.openai.com favicon image platform.openai.com platform.openai.com thumbnail image" [ref=f3s1e6]:\n                - /url: https://platform.openai.com/docs/guides/realtime?text-generation-quickstart-example=stream\n                - heading "OpenAI Platform" [level=1] [ref=f3s1e8]\n                - text: Explore developer resources, tutorials, API docs, and dynamic examples to\n                    get the most out of OpenAI\n                - img "platform.openai.com favicon image" [ref=f3s1e11]\n                - text: platform.openai.com\n                - img "platform.openai.com thumbnail image" [ref=f3s1e13]\n            - alert [ref=f3s1e15]\n        - iframe [ref=s1e194]: <could not take iframe snapshot>\n    - heading "サンプルコード" [level=1] [ref=s1e195]\n    - paragraph [ref=s1e196]:\n        - text: 取り急ぎ下記がサンプルコードです。 下記コマンドで実行できるとおもいます。 （\n        - code [ref=s1e199]: ~/.zshrc\n        - text: や\n        - code [ref=s1e200]: ~/.bashrc\n        - text: にOPENAI_API_KEYが記録されていることを前提にしていますが、わからなければ直接コード内にAPIKEYを書き下してもよいです。推奨はしません。）\n    - code [ref=s1e204]: pip install websockets soundfile numpy pyaudio\n    - button "クリップボードにコピー" [ref=s1e205]:\n        - img [ref=s1e206]\n    - button "右端で折り返す" [ref=s1e207]:\n        - img [ref=s1e208]\n    - complementary [ref=s1e209]:\n        - text: "!"\n        - paragraph [ref=s1e212]:\n            - text: バーション指定込みの\n            - code [ref=s1e213]: requirements.txt\n            - text: を用意したので、もしコードが動かない場合は、下記を試してみてください。\n        - text: requirements.txt\n        - code [ref=s1e219]: cffi==1.17.1 numpy==2.2.1 PyAudio==0.2.14 pycparser==2.22\n            soundfile==0.12.1 websockets==14.1\n        - button "クリップボードにコピー" [ref=s1e236]:\n            - img [ref=s1e237]\n        - button "右端で折り返す" [ref=s1e238]:\n            - img [ref=s1e239]\n        - code [ref=s1e243]: pip install -r requirements.txt\n        - button "クリップボードにコピー" [ref=s1e244]:\n            - img [ref=s1e245]\n        - button "右端で折り返す" [ref=s1e246]:\n            - img [ref=s1e247]\n    - code [ref=s1e251]: python main.py\n    - button "クリップボードにコピー" [ref=s1e252]:\n        - img [ref=s1e253]\n    - button "右端で折り返す" [ref=s1e254]:\n        - img [ref=s1e255]\n    - complementary [ref=s1e256]:\n        - text: "!"\n        - paragraph [ref=s1e259]:\n            - text: （追記） Macで実行する場合は、\n            - code [ref=s1e261]: pyaudio\n            - text: のインストールでエラーが出てしまうようです。 その場合は先に下記を実行してから\n            - code [ref=s1e263]: pip install\n            - text: をお試しください\n        - code [ref=s1e267]: brew install portaudio\n        - button "クリップボードにコピー" [ref=s1e268]:\n            - img [ref=s1e269]\n        - button "右端で折り返す" [ref=s1e270]:\n            - img [ref=s1e271]\n    - paragraph [ref=s1e272]: 実行したら、AIが発話するので、その後こちらはマイクに向かって発話すれば、会話を始めることができます。\n    - text: main.py\n    - code [ref=s1e278]: "import asyncio import websockets import pyaudio import numpy\n        as np import base64 import json import wave import io import os API_KEY\n        = os.environ.get(\'OPENAI_API_KEY\') #わからない人は、上の行をコメントアウトして、下記のように直接API\n        KEYを書き下してもよい #API_KEY = \\"sk-xxxxx\\" # WebSocket URLとヘッダー情報 WS_URL =\n        \\"wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-1\\\n        2-17\\" HEADERS = { \\"Authorization\\": \\"Bearer \\"+ API_KEY,\n        \\"OpenAI-Beta\\": \\"realtime=v1\\" } # PCM16形式に変換する関数 def\n        base64_to_pcm16(base64_audio): audio_data =\n        base64.b64decode(base64_audio) return audio_data # 音声を送信する非同期関数 async\n        def send_audio(websocket, stream, CHUNK): def read_audio_block():\n        \\"\\"\\"同期的に音声データを読み取る関数\\"\\"\\" try: return stream.read(CHUNK,\n        exception_on_overflow=False) except Exception as e: print(f\\"音声読み取りエラー:\n        {e}\\") return None print(\\"マイクから音声を取得して送信中...\\") while True: #\n        マイクから音声を取得 audio_data = await\n        asyncio.get_event_loop().run_in_executor(None, read_audio_block) if\n        audio_data is None: continue # 読み取りに失敗した場合はスキップ # PCM16データをBase64にエンコード\n        base64_audio = base64.b64encode(audio_data).decode(\\"utf-8\\")\n        audio_event = { \\"type\\": \\"input_audio_buffer.append\\", \\"audio\\":\n        base64_audio } # WebSocketで音声データを送信 await\n        websocket.send(json.dumps(audio_event)) await asyncio.sleep(0) #\n        サーバーから音声を受信して再生する非同期関数 async def receive_audio(websocket,\n        output_stream): print(\\"assistant: \\", end = \\"\\", flush = True) loop =\n        asyncio.get_event_loop() while True: # サーバーからの応答を受信 response = await\n        websocket.recv() response_data = json.loads(response) #\n        サーバーからの応答をリアルタイム（ストリーム）で表示 if \\"type\\" in response_data and\n        response_data[\\"type\\"] == \\"response.audio_transcript.delta\\":\n        print(response_data[\\"delta\\"], end = \\"\\", flush = True) #\n        サーバからの応答が完了したことを取得 elif \\"type\\" in response_data and\n        response_data[\\"type\\"] == \\"response.audio_transcript.done\\":\n        print(\\"\\\\nassistant: \\", end = \\"\\", flush = True) #\n        サーバーからの応答に音声データが含まれているか確認 if \\"delta\\" in response_data: if\n        response_data[\\"type\\"] == \\"response.audio.delta\\":\n        base64_audio_response = response_data[\\"delta\\"] if\n        base64_audio_response: pcm16_audio =\n        base64_to_pcm16(base64_audio_response) #音声データがある場合は、出力ストリームから再生 await\n        loop.run_in_executor(None, output_stream.write, pcm16_audio) #\n        マイクからの音声を取得し、WebSocketで送信しながらサーバーからの音声応答を再生する非同期関数 async def\n        stream_audio_and_receive_response(): # WebSocketに接続 async with\n        websockets.connect(WS_URL, additional_headers=HEADERS) as websocket:\n        print(\\"WebSocketに接続しました。\\") # 初期リクエスト (モダリティ設定) init_request = {\n        \\"type\\": \\"response.create\\", \\"response\\": { \\"modalities\\":\n        [\\"audio\\", \\"text\\"], \\"instructions\\": \\"ユーザーをサポートしてください。\\",\n        \\"voice\\": \\"echo\\" #\\"alloy\\", \\"echo\\", \\"shimmer\\", \\"ash\\",\n        \\"ballad\\",\\"coral\\",\\"sage\\",\\"verse\\" } } await\n        websocket.send(json.dumps(init_request)) print(\\"初期リクエストを送信しました。\\") #\n        PyAudioの設定 CHUNK = 2048 # マイクからの入力データのチャンクサイズ FORMAT = pyaudio.paInt16 #\n        PCM16形式 CHANNELS = 1 # モノラル RATE = 24000 # サンプリングレート（24kHz） #\n        PyAudioインスタンス p = pyaudio.PyAudio() # マイクストリームの初期化 stream =\n        p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True,\n        frames_per_buffer=CHUNK) # サーバーからの応答音声を再生するためのストリームを初期化 output_stream =\n        p.open(format=FORMAT, channels=CHANNELS, rate=RATE, output=True,\n        frames_per_buffer=CHUNK) print(\\"マイク入力およびサーバーからの音声再生を開始...\\") try: #\n        音声送信タスクと音声受信タスクを非同期で並行実行 send_task =\n        asyncio.create_task(send_audio(websocket, stream, CHUNK)) receive_task =\n        asyncio.create_task(receive_audio(websocket, output_stream)) #\n        タスクが終了するまで待機 await asyncio.gather(send_task, receive_task) except\n        KeyboardInterrupt: # キーボードの割り込みで終了 print(\\"終了します...\\") finally: #\n        ストリームを閉じる if stream.is_active(): stream.stop_stream() stream.close()\n        output_stream.stop_stream() output_stream.close() p.terminate() if\n        __name__ == \\"__main__\\":\n        asyncio.get_event_loop().run_until_complete(stream_audio_and_receive_re\\\n        sponse())"\n    - button "クリップボードにコピー" [ref=s1e707]:\n        - img [ref=s1e708]\n    - button "右端で折り返す" [ref=s1e709]:\n        - img [ref=s1e710]\n    - group [ref=s1e711]: Azure版に接続する\n    - heading "実行結果" [level=1] [ref=s1e713]\n    - paragraph [ref=s1e714]: 実行結果を表示します。（音量小さめなので、静かな場所でのご視聴をお勧めします）\n    - paragraph [ref=s1e715]:\n        - iframe [ref=s1e717]: <could not take iframe snapshot>\n    - paragraph [ref=s1e718]: 相手の発話開始までの応答時間はめちゃくちゃ早かったです。1.5秒かかってないんじゃないかな\n        こんなすごいのが出てくると、今まで、音声認識やらLLMやら合成音声を繋げて頑張ってた自分の自信が完全になくなりますね。\n    - paragraph [ref=s1e720]: ちなみに1ドルちょっと使ったら、すぐにRate Limitになりました。\n        tier1だからというのもあると思いますが・・・ →一瞬Rate\n        Limitになりましたが、記事書き終わった頃には解消されていました。結構早かったです。\n    - heading "詰まった点" [level=1] [ref=s1e723]\n    - paragraph [ref=s1e724]:\n        - text: 基本的には下記の記事をよく読めばわかる内容ですが、私はつまづいたので忘備録も兼ねて\n        - iframe [ref=s1e726]: <could not take iframe snapshot>\n    - paragraph [ref=s1e727]: とにかく重要なのは下記の部分\n    - code [ref=s1e731]: "audio_event = { \\"type\\": \\"input_audio_buffer.append\\",\n        \\"audio\\": base64_audio }"\n    - button "クリップボードにコピー" [ref=s1e741]:\n        - img [ref=s1e742]\n    - button "右端で折り返す" [ref=s1e743]:\n        - img [ref=s1e744]\n    - paragraph [ref=s1e745]:\n        - text: 今回のように、入出力をストリーミングで処理する場合は、\n        - code [ref=s1e746]: "\\"type\\": \\"input_audio_buffer.append"\n        - text: を指定する必要があります。\n    - paragraph [ref=s1e747]: この部分を下記のように書いていると一生うまくいきません（1敗）\n    - code [ref=s1e751]: "event = { type: \'conversation.item.create\', item: { type:\n        \'message\', role: \'user\', content: [ { type: \'input_audio\', audio:\n        base64AudioData } ] }"\n    - button "クリップボードにコピー" [ref=s1e778]:\n        - img [ref=s1e779]\n    - button "右端で折り返す" [ref=s1e780]:\n        - img [ref=s1e781]\n    - paragraph [ref=s1e782]: あとは単純にpythonの実装の話ですが、pyaudioを利用する場合は、入出力で同じスレッドを利用するとぶつぶつした音声になってしまうので、ちゃんとスレッドを分けつつ、時間がかかる処理は非同期で実施することをお勧めします（1敗）\n    - heading "コードの解説" [level=1] [ref=s1e783]\n    - paragraph [ref=s1e784]: 簡単にコードの解説をしていきます。\n        上記のコードは、OpenAIのモデルにWebsocketで接続する部分と、PyAudioによる入出力音声の制御の主に二つの処理によって構成されていますが、今回は、WebsocketでOpenAIのサーバと通信する部分を主に説明します。\n    - paragraph [ref=s1e786]: （解説のために順番などはバラバラになっています。実際の処理は実際のコードをご覧ください）\n    - heading "Websocketへの接続" [level=2] [ref=s1e787]\n    - paragraph [ref=s1e788]: 最初の接続は下記のようなコードで達成されます。\n    - code [ref=s1e792]: "API_KEY = os.environ.get(\'OPENAI_API_KEY\') # WebSocket\n        URLとヘッダー情報 WS_URL =\n        \\"wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-1\\\n        0-01\\" HEADERS = { \\"Authorization\\": \\"Bearer \\" + API_KEY,\n        \\"OpenAI-Beta\\": \\"realtime=v1\\" } # WebSocketに接続 async with\n        websockets.connect(WS_URL, additional_headers=HEADERS) as websocket:\n        print(\\"WebSocketに接続しました。\\")"\n    - button "クリップボードにコピー" [ref=s1e827]:\n        - img [ref=s1e828]\n    - button "右端で折り返す" [ref=s1e829]:\n        - img [ref=s1e830]\n    - paragraph [ref=s1e831]: API_KEYを環境変数から取得し、\n        OpenAIが指定するサーバURLと認証情報を付与したHEADERSを利用して、Websocket通信で接続しています。\n    - heading "最初のリクエスト" [level=2] [ref=s1e832]\n    - paragraph [ref=s1e833]: 接続後、最初のリクエストは下記のように実施します。\n    - code [ref=s1e837]: "async with websockets.connect(WS_URL,\n        additional_headers=HEADERS) as websocket: print(\\"WebSocketに接続しました。\\") #\n        初期リクエスト (モダリティ設定) init_request = { \\"type\\": \\"response.create\\",\n        \\"response\\": { \\"modalities\\": [\\"audio\\", \\"text\\"], \\"instructions\\":\n        \\"関西弁で回答してください。\\", \\"voice\\": \\"echo\\" #\\"alloy\\", \\"echo\\",\n        \\"shimmer\\", \\"ash\\", \\"ballad\\",\\"coral\\",\\"sage\\",\\"verse\\" } } await\n        websocket.send(json.dumps(init_request))"\n    - button "クリップボードにコピー" [ref=s1e886]:\n        - img [ref=s1e887]\n    - button "右端で折り返す" [ref=s1e888]:\n        - img [ref=s1e889]\n    - paragraph [ref=s1e890]: 最初の2行は前節と共通で、Websocket通信が接続している範囲内で処理をしてくださいという意味です。\n    - paragraph [ref=s1e891]:\n        - text: その上で、最初は\n        - code [ref=s1e892]: "\\"type\\": \\"response.create\\""\n        - text: のtypeを持つJSONを作成します。 詳細か下記をご覧ください。\n        - iframe [ref=s1e895]: <could not take iframe snapshot>\n    - paragraph [ref=s1e896]: 公式のドキュメントにも記載されていますが、下記の要素を指定できます・\n    - code [ref=s1e900]: "{ \\"event_id\\": \\"event_234\\", \\"type\\":\n        \\"response.create\\", \\"response\\": { \\"modalities\\": [\\"text\\",\n        \\"audio\\"], \\"instructions\\": \\"Please assist the user.\\", \\"voice\\":\n        \\"alloy\\", \\"output_audio_format\\": \\"pcm16\\", \\"tools\\": [ { \\"type\\":\n        \\"function\\", \\"name\\": \\"calculate_sum\\", \\"description\\": \\"Calculates\n        the sum of two numbers.\\", \\"parameters\\": { \\"type\\": \\"object\\",\n        \\"properties\\": { \\"a\\": { \\"type\\": \\"number\\" }, \\"b\\": { \\"type\\":\n        \\"number\\" } }, \\"required\\": [\\"a\\", \\"b\\"] } } ], \\"tool_choice\\":\n        \\"auto\\", \\"temperature\\": 0.7, \\"max_output_tokens\\": 150 } }"\n    - button "クリップボードにコピー" [ref=s1e1000]:\n        - img [ref=s1e1001]\n    - button "右端で折り返す" [ref=s1e1002]:\n        - img [ref=s1e1003]\n    - paragraph [ref=s1e1004]:\n        - text: 今回はFunction Callingなどは利用しないため、最低限の指定に収めていますが、\n        - code [ref=s1e1005]: temperature\n        - text: や\n        - code [ref=s1e1006]: max_output_tokens\n        - text: などのよく見る要素も、最初に指定できます。\n    - paragraph [ref=s1e1007]:\n        - text: 本記事では\n        - code [ref=s1e1008]: modalities\n        - text: と\n        - code [ref=s1e1009]: instructions\n        - text: と\n        - code [ref=s1e1010]: voice\n        - text: を主に設定しており、そのほかはDefaultの値（つまり上に記載した全文の情報）を利用しています。\n    - paragraph [ref=s1e1011]:\n        - code [ref=s1e1012]: modalities\n        - text: は、何の種類のモダリティを利用するかを指定しており、今回はSystem\n            PromptにはTextデータを、会話にはAudioデータを利用するため、両方指定しています。\n    - paragraph [ref=s1e1013]:\n        - code [ref=s1e1014]: instructions\n        - text: は俗にいうSystem Promptです。ここでAIにどのように振る舞ってほしいかを指定することができます。\n            今回はサンプルなので「関西弁で回答してください。」の一行だけですが、普通に長いPromptを投入することも可能です。\n    - paragraph [ref=s1e1016]:\n        - code [ref=s1e1017]: voice\n        - text: は対話でモデルが出す声のキャラクターです。具体的に現在どのモデルが利用できるのかが調べてもわからなかったため、PlayGroundで選択できるキャラクター（3名）のうち一人を選んでいます。\n            PlayGroundでは名前の先頭が大文字で書かれていますが、JSONでは小文字で書く必要があることに注意してください。\n    - complementary [ref=s1e1019]:\n        - text: "!"\n        - paragraph [ref=s1e1022]: （追記） 2024年12月22日現在、利用可能な声の種類はどんどん増えています。 現時点で確認できたのは、\n        - code [ref=s1e1028]: "#\\"alloy\\", \\"echo\\", \\"shimmer\\", \\"ash\\",\n            \\"ballad\\",\\"coral\\",\\"sage\\",\\"verse\\""\n        - button "クリップボードにコピー" [ref=s1e1030]:\n            - img [ref=s1e1031]\n        - button "右端で折り返す" [ref=s1e1032]:\n            - img [ref=s1e1033]\n        - paragraph [ref=s1e1034]: の8名です\n    - group [ref=s1e1035]: 最初に発話させたくなかったら\n    - heading "自分が発話した音声を送信する" [level=2] [ref=s1e1037]\n    - paragraph [ref=s1e1038]: 発話した音声を送信する方法はいくつかありますが、音声対話を行う場合は、ストリーミング（音声を細かいCHUNK単位で区切って、リアルタイムに送信する）で音声を送信する必要があります。\n        公式のドキュメントではあまり触れられていないので、私も沼りましたが、結論、下記のように送信すれば良いです。\n    - code [ref=s1e1043]: "# PCM16データをBase64にエンコード base64_audio =\n        base64.b64encode(audio_data).decode(\\"utf-8\\") audio_event = { \\"type\\":\n        \\"input_audio_buffer.append\\", \\"audio\\": base64_audio } #\n        WebSocketで音声データを送信 await websocket.send(json.dumps(audio_event))"\n    - button "クリップボードにコピー" [ref=s1e1070]:\n        - img [ref=s1e1071]\n    - button "右端で折り返す" [ref=s1e1072]:\n        - img [ref=s1e1073]\n    - paragraph [ref=s1e1074]: 当然、上記もWebsocketが接続している中で実行してください。\n    - paragraph [ref=s1e1075]:\n        - text: まず、前提として\n        - code [ref=s1e1076]: audio_data\n        - text: はPCM16というフォーマットの音声データのチャンクで、サンプリングレートは24,000Hzであることとします。\n            （そのほかの設定でも上手くいくのかもしれないですが、私はよく知らないです）\n            今回、私は音声のチャンクサイズを2048としています。おおよそ0.085秒程度の音声データです。\n    - paragraph [ref=s1e1079]: 今回は音声データをそのままWebsocketで送信できるわけではなく、一度、Base64形式にエンコードしたのちにJSONの要素として送る必要があるため、上記のような実装コードになっています。\n    - paragraph [ref=s1e1080]:\n        - text: また、Websocketで送るJSONの\n        - code [ref=s1e1081]: type\n        - text: も\n        - code [ref=s1e1082]: "\\"type\\": \\"input_audio_buffer.append\\""\n        - text: を利用する必要があります。 こちらは下記の公式ドキュメントをご覧ください。\n        - iframe [ref=s1e1085]: <could not take iframe snapshot>\n    - paragraph [ref=s1e1086]:\n        - text: ここで、\n        - code [ref=s1e1087]: audio\n        - text: には、エンコードされた音声のチャンクが入ります。 これをループして、マイク入力音声を少しずつ、全てWebsocketで送信していることになります。\n    - heading "AIが発話した音声を受信する" [level=2] [ref=s1e1089]\n    - paragraph [ref=s1e1090]: 受信に関しては下記のように取得できます。\n    - code [ref=s1e1094]: response = await websocket.recv() response_data = json.loads(response)\n    - button "クリップボードにコピー" [ref=s1e1104]:\n        - img [ref=s1e1105]\n    - button "右端で折り返す" [ref=s1e1106]:\n        - img [ref=s1e1107]\n    - paragraph [ref=s1e1108]: もちろん、Websocketが接続されている状態で実行してください。\n        OpenAIからはJSON形式でデータが返ってくるので、それを辞書型としてPythonで処理できるようにロードしています。\n    - paragraph [ref=s1e1110]: 取得後のデータの中身の処理は、実装するアプリケーションに依存しますが、音声対話では、最低限取得した音声を再生できることが求められます。\n    - paragraph [ref=s1e1111]: それは、下記で実行されます。\n    - code [ref=s1e1115]: "if \\"delta\\" in response_data: if response_data[\\"type\\"]\n        == \\"response.audio.delta\\": base64_audio_response =\n        response_data[\\"delta\\"] if base64_audio_response: pcm16_audio =\n        base64.b64decode(base64_audio_response) #音声データがある場合は、出力ストリームから再生 await\n        loop.run_in_executor(None, output_stream.write, pcm16_audio)"\n    - button "クリップボードにコピー" [ref=s1e1146]:\n        - img [ref=s1e1147]\n    - button "右端で折り返す" [ref=s1e1148]:\n        - img [ref=s1e1149]\n    - paragraph [ref=s1e1150]:\n        - text: 音声自体も、一度に全ての発話音声データが送られてくるわけではなく、一定のCHUNKに分割して返却されます。 返却されるJSONの\n        - code [ref=s1e1152]: type\n        - text: は\n        - code [ref=s1e1153]: "\\"response.audio.delta\\""\n        - text: となります。 公式のドキュメントは下記になります。\n        - iframe [ref=s1e1156]: <could not take iframe snapshot>\n    - paragraph [ref=s1e1157]:\n        - text: このJSONの\n        - code [ref=s1e1158]: delta\n        - text: にBase64でEncodeされた音声データが格納されています。\n            したがって、デコードすれば音声データになるため、デコード後、PyAudioで非同期に再生しています。\n    - heading "AIが発話した音声の文字起こしをストリーミングで取得する" [level=2] [ref=s1e1160]\n    - paragraph [ref=s1e1161]: 音声対話システムを実装するなら、AIが発話した内容の音声だけではなく、テキストでも表示がしたいです。 テキストを取得するには下記を実行します。\n    - code [ref=s1e1166]: "print(\\"assistant: \\", end = \\"\\", flush = True) ・・・ while\n        True: ・・・ # サーバーからの応答をリアルタイムに表示 if \\"type\\" in response_data and\n        response_data[\\"type\\"] == \\"response.audio_transcript.delta\\":\n        print(response_data[\\"delta\\"], end = \\"\\", flush = True) #\n        サーバからの応答が完了したことを取得 elif \\"type\\" in response_data and\n        response_data[\\"type\\"] == \\"response.audio_transcript.done\\":\n        print(\\"\\\\nassistant: \\", end = \\"\\", flush = True)"\n    - button "クリップボードにコピー" [ref=s1e1224]:\n        - img [ref=s1e1225]\n    - button "右端で折り返す" [ref=s1e1226]:\n        - img [ref=s1e1227]\n    - paragraph [ref=s1e1228]:\n        - text: ストリーミングでテキストを取得するには、\n        - code [ref=s1e1229]: response.audio_transcript.delta\n        - text: の\n        - code [ref=s1e1230]: type\n        - text: のJSONの中身を取得して、それをコンソールに表示します。 また、ストリーミングで取得するテキストが一段落ついたら\n        - code [ref=s1e1232]: response.audio_transcript.done\n        - text: の\n        - code [ref=s1e1233]: type\n        - text: のJSONが発行されるため、それを取得したら、次のテキストを表示する準備をしています。\n    - heading "実装できなかった部分" [level=1] [ref=s1e1234]\n    - paragraph [ref=s1e1235]: AIが話しているときに、こちらが話し始めても、うまく音声を止めてくれませんでした。\n        (止めてくれることもあれば、そのまま話し続けることもある） 上記のサンプルコードだけでは、その部分が弱いみたいです。\n        上手な実装を知っている方がいたら教えてください。\n    - paragraph [ref=s1e1240]:\n        - text: 上記に関しても実装できました！ 下記にて取り急ぎコードだけ共有します。コードの解説は後日更新します。\n        - iframe [ref=s1e1243]: <could not take iframe snapshot>\n    - paragraph [ref=s1e1244]: なお、実践上は上記で紹介するコードを利用するのをお勧めしますが、Realtime\n        APIをpythonで利用するにあたっての仕様などの理解は、本記事で紹介するサンプルコードを利用する方が理解しやすいと思います。\n    - paragraph [ref=s1e1245]: あと、キャラクターの音声を変える部分は記載してないですので、これもRate Limitが解除されたら記載します。 こちらは実装して、更新しました！\n    - code [ref=s1e1251]: "init_request = { \\"type\\": \\"response.create\\",\n        \\"response\\": { \\"modalities\\": [\\"audio\\", \\"text\\"], \\"instructions\\":\n        \\"ユーザーをサポートしてください。\\", \\"voice\\": \\"echo\\" #\\"alloy\\", \\"echo\\",\n        \\"shimmer\\", \\"ash\\", \\"ballad\\",\\"coral\\",\\"sage\\",\\"verse\\" } }"\n    - button "クリップボードにコピー" [ref=s1e1279]:\n        - img [ref=s1e1280]\n    - button "右端で折り返す" [ref=s1e1281]:\n        - img [ref=s1e1282]\n    - paragraph [ref=s1e1283]:\n        - text: 上記の\n        - code [ref=s1e1284]: instructions\n        - text: の部分が、最初にAIに与えるプロンプトになります。system promptのようなものです。 また\n        - code [ref=s1e1286]: voice\n        - text: のところを変更すると、声を変えることができます。 誰の声が使えるのかよくわからなかったですが、\n        - code [ref=s1e1288]: PlayGround\n        - text: にて利用できる名前は全て動作確認できました。 もっと増えるといいですね。\n    - heading "まとめ" [level=1] [ref=s1e1290]\n    - paragraph [ref=s1e1291]: 取り急ぎ、pythonで動かしたいけどサンプルコードがなくて困っているという方向けに、実装したものを公開します。\n        Rate Limitが解除されたら、もっといろいろ試して機能追加できたらいいなと思っています。\n    - paragraph [ref=s1e1293]: 今回の記事の本題とは異なるため紹介しませんでしたが、実はユーザが発話した音声の文字起こしを取得する機能もあります。したがってRAGのQueryに利用すれば、Realtime\n        APIとRAGを組み合わせることもできそうです。（試していない＋文字起こしの処理速度は遅いので、応答時間が長くなると思うが）\n    - paragraph [ref=s1e1294]:\n        - text: 下記に置いているコードは、ユーザが発話した音声の文字起こしも取得しています。（取得しただけ）\n        - iframe [ref=s1e1296]: <could not take iframe snapshot>\n    - paragraph [ref=s1e1297]:\n        - text: 続編として、下記二つの記事も書いています。 両方ともRealtime APIを利用する上で意識しないといけないことが記載されています。\n            お役に立てれば幸いです。\n        - iframe [ref=s1e1301]: <could not take iframe snapshot>\n        - iframe [ref=s1e1303]: <could not take iframe snapshot>\n    - paragraph [ref=s1e1304]: ありがとうございました！\n    - button "いいね" [ref=s1e1307]:\n        - img [ref=s1e1308]\n    - text: "79"\n    - button "ブックマーク" [ref=s1e1324]:\n        - img [ref=s1e1325]\n    - button "その他の操作" [ref=s1e1329]:\n        - img [ref=s1e1330]\n    - tooltip "X(Twitter)にポスト" [ref=s1e1333]:\n        - img [ref=s1e1334]\n        - text: X(Twitter)にポスト\n    - tooltip "Facebookに投稿" [ref=s1e1340]:\n        - img [ref=s1e1341]\n        - text: Facebookに投稿\n    - tooltip "はてなブックマークに登録" [ref=s1e1347]:\n        - img [ref=s1e1348]\n        - text: はてなブックマークに登録\n    - complementary [ref=s1e1351]:\n        - link "asap" [ref=s1e1353]:\n            - /url: /asap\n            - img "asap" [ref=s1e1354]\n        - link "asap" [ref=s1e1356]:\n            - /url: /asap\n        - paragraph [ref=s1e1358]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n        - button "フォロー" [ref=s1e1362]\n        - tooltip "@personabb" [ref=s1e1363]:\n            - img "GitHub" [ref=s1e1364]\n            - text: "@personabb"\n        - tooltip "@asap2650" [ref=s1e1366]:\n            - img "X(Twitter)" [ref=s1e1367]\n            - text: "@asap2650"\n        - tooltip "yard.tips" [ref=s1e1373]:\n            - img "リンク" [ref=s1e1374]\n            - text: yard.tips\n    - complementary [ref=s1e1377]:\n        - text: バッジを贈って著者を応援しよう\n        - paragraph [ref=s1e1381]: バッジを受け取った著者にはZennから現金やAmazonギフトカードが還元されます。\n        - button "バッジを贈る" [ref=s1e1383]\n        - img [ref=s1e1384]\n    - heading "Discussion" [level=3] [ref=s1e1393]\n    - img [ref=s1e1394]\n    - text: ログインするとコメントできます\n    - button "Login" [ref=s1e1398]\n    - complementary [ref=s1e1399]:\n        - link "asap" [ref=s1e1404]:\n            - /url: /asap\n            - img "asap" [ref=s1e1405]\n        - link "asap" [ref=s1e1407]:\n            - /url: /asap\n        - button "フォロー" [ref=s1e1409]\n        - link "@personabb" [ref=s1e1410]:\n            - /url: https://github.com/personabb\n            - tooltip "@personabb" [ref=s1e1411]:\n                - img "GitHub" [ref=s1e1412]\n                - text: "@personabb"\n        - link "@asap2650" [ref=s1e1414]:\n            - /url: https://twitter.com/asap2650\n            - tooltip "@asap2650" [ref=s1e1415]:\n                - img "X(Twitter)" [ref=s1e1416]\n                - text: "@asap2650"\n        - link "yard.tips" [ref=s1e1422]:\n            - /url: https://yard.tips/asap2650\n            - tooltip "yard.tips" [ref=s1e1423]:\n                - img "リンク" [ref=s1e1424]\n                - text: yard.tips\n        - paragraph [ref=s1e1427]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n        - button "バッジを贈る" [ref=s1e1430]\n        - link "バッジを贈るとは" [ref=s1e1432]:\n            - /url: /faq#badges\n            - text: バッジを贈るとは\n            - img [ref=s1e1433]\n        - text: 目次\n        - list [ref=s1e1442]:\n            - listitem [ref=s1e1443]:\n                - link "はじめに" [ref=s1e1444]:\n                    - /url: "#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB"\n            - listitem [ref=s1e1445]:\n                - link "参考" [ref=s1e1446]:\n                    - /url: "#%E5%8F%82%E8%80%83"\n            - listitem [ref=s1e1447]:\n                - link "サンプルコード" [ref=s1e1448]:\n                    - /url: "#%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%82%B3%E3%83%BC%E3%83%89"\n            - listitem [ref=s1e1449]:\n                - link "実行結果" [ref=s1e1450]:\n                    - /url: "#%E5%AE%9F%E8%A1%8C%E7%B5%90%E6%9E%9C"\n            - listitem [ref=s1e1451]:\n                - link "詰まった点" [ref=s1e1452]:\n                    - /url: "#%E8%A9%B0%E3%81%BE%E3%81%A3%E3%81%9F%E7%82%B9"\n            - listitem [ref=s1e1453]:\n                - link "コードの解説" [ref=s1e1454]:\n                    - /url: "#%E3%82%B3%E3%83%BC%E3%83%89%E3%81%AE%E8%A7%A3%E8%AA%AC"\n                - list [ref=s1e1455]:\n                    - listitem [ref=s1e1456]:\n                        - link "Websocketへの接続" [ref=s1e1457]:\n                            - /url: "#websocket%E3%81%B8%E3%81%AE%E6%8E%A5%E7%B6%9A"\n                    - listitem [ref=s1e1458]:\n                        - link "最初のリクエスト" [ref=s1e1459]:\n                            - /url: "#%E6%9C%80%E5%88%9D%E3%81%AE%E3%83%AA%E3%82%AF%E3%82%A8%E3%82%B9%E3%83%8\\\n                                8"\n                    - listitem [ref=s1e1460]:\n                        - link "自分が発話した音声を送信する" [ref=s1e1461]:\n                            - /url: "#%E8%87%AA%E5%88%86%E3%81%8C%E7%99%BA%E8%A9%B1%E3%81%97%E3%81%9F%E9%9F%B\\\n                                3%E5%A3%B0%E3%82%92%E9%80%81%E4%BF%A1%E3%81%99%\\\n                                E3%82%8B"\n                    - listitem [ref=s1e1462]:\n                        - link "AIが発話した音声を受信する" [ref=s1e1463]:\n                            - /url: "#ai%E3%81%8C%E7%99%BA%E8%A9%B1%E3%81%97%E3%81%9F%E9%9F%B3%E5%A3%B0%E3%82\\\n                                %92%E5%8F%97%E4%BF%A1%E3%81%99%E3%82%8B"\n                    - listitem [ref=s1e1464]:\n                        - link "AIが発話した音声の文字起こしをストリーミングで取得する" [ref=s1e1465]:\n                            - /url: "#ai%E3%81%8C%E7%99%BA%E8%A9%B1%E3%81%97%E3%81%9F%E9%9F%B3%E5%A3%B0%E3%81\\\n                                %AE%E6%96%87%E5%AD%97%E8%B5%B7%E3%81%93%E3%81%9\\\n                                7%E3%82%92%E3%82%B9%E3%83%88%E3%83%AA%E3%83%BC%\\\n                                E3%83%9F%E3%83%B3%E3%82%B0%E3%81%A7%E5%8F%96%E5\\\n                                %BE%97%E3%81%99%E3%82%8B"\n            - listitem [ref=s1e1466]:\n                - link "実装できなかった部分" [ref=s1e1467]:\n                    - /url: "#%E5%AE%9F%E8%A3%85%E3%81%A7%E3%81%8D%E3%81%AA%E3%81%8B%E3%81%A3%E3%81%9\\\n                        F%E9%83%A8%E5%88%86"\n            - listitem [ref=s1e1468]:\n                - link "まとめ" [ref=s1e1469]:\n                    - /url: "#%E3%81%BE%E3%81%A8%E3%82%81"\n        - paragraph [ref=s1e1471]: Zennからのお知らせ\n        - article [ref=s1e1474]:\n            - link "第２回 AI Agent Hackathon with Google Cloud 第２回 AI Agent Hackathon with Google Cloud 4/14登録受付開始！" [ref=s1e1475]:\n                - /url: /hackathons/google-cloud-japan-ai-hackathon-vol2\n                - img "第２回 AI Agent Hackathon with Google Cloud" [ref=s1e1477]\n                - paragraph [ref=s1e1478]: 第２回 AI Agent Hackathon with Google Cloud 4/14登録受付開始！\n        - article [ref=s1e1479]:\n            - link "Zenncafe日比谷#2 生成AIで変わる仕事 Zenncafe日比谷#2 生成AIで変わる仕事" [ref=s1e1480]:\n                - /url: https://zenn.connpass.com/event/351131/\n                - img "Zenncafe日比谷#2 生成AIで変わる仕事" [ref=s1e1482]\n                - paragraph [ref=s1e1483]: Zenncafe日比谷#2 生成AIで変わる仕事\n- contentinfo [ref=s1e1485]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1489]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1490]\n    - paragraph [ref=s1e1498]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e1500]:\n        - heading "About" [level=4] [ref=s1e1501]\n        - list [ref=s1e1502]:\n            - listitem [ref=s1e1503]:\n                - link "Zennについて" [ref=s1e1505]:\n                    - /url: /about\n            - listitem [ref=s1e1506]:\n                - link "運営会社" [ref=s1e1507]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e1508]:\n                - link "お知らせ・リリース" [ref=s1e1509]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e1510]:\n                - link "イベント" [ref=s1e1512]:\n                    - /url: /events\n    - navigation [ref=s1e1513]:\n        - heading "Guides" [level=4] [ref=s1e1514]\n        - list [ref=s1e1515]:\n            - listitem [ref=s1e1516]:\n                - link "使い方" [ref=s1e1518]:\n                    - /url: /manual\n            - listitem [ref=s1e1519]:\n                - link "法人向けメニュー" [ref=s1e1521]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e1523]:\n                - link "Publication / Pro" [ref=s1e1525]:\n                    - /url: /publications\n            - listitem [ref=s1e1526]:\n                - link "よくある質問" [ref=s1e1528]:\n                    - /url: /faq\n    - navigation [ref=s1e1529]:\n        - heading "Links" [level=4] [ref=s1e1530]\n        - list [ref=s1e1531]:\n            - listitem [ref=s1e1532]:\n                - link "X(Twitter)" [ref=s1e1533]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e1534]:\n                - link "GitHub" [ref=s1e1535]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e1536]:\n                - link "メディアキット" [ref=s1e1538]:\n                    - /url: /mediakit\n    - navigation [ref=s1e1539]:\n        - heading "Legal" [level=4] [ref=s1e1540]\n        - list [ref=s1e1541]:\n            - listitem [ref=s1e1542]:\n                - link "利用規約" [ref=s1e1544]:\n                    - /url: /terms\n            - listitem [ref=s1e1545]:\n                - link "プライバシーポリシー" [ref=s1e1547]:\n                    - /url: /privacy\n            - listitem [ref=s1e1548]:\n                - link "特商法表記" [ref=s1e1550]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e1553]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e1554]\n- alert [ref=s1e1573]\n```\n', name='browser_navigate', tool_call_id='7cb428c3-7d50-4f34-819d-e06432672aa4'), AIMessage(content='asapさんのZennの記事の中で4番目にいいねが多い記事は、「Open AI Realtime APIのPythonサンプルコードを作成」で、いいね数は79です。\n\nこの記事は、OpenAIが提供するRealtime APIをPythonで利用するためのサンプルコードを紹介したものです。\n\n**記事の内容**\n\n*   **はじめに:** OpenAIからRealtime APIが公開されたことを受け、Pythonでのサンプルコードを作成した経緯を説明しています。\n*   **参考:** OpenAIの公式ドキュメントを参考にコードを作成したことを述べています。\n*   **サンプルコード:** 実際に動作するPythonのサンプルコードが掲載されています。コードは、WebsocketでOpenAIのサーバーに接続し、音声データを送受信することでリアルタイムな音声対話を実現するものです。\n*   **実行結果:** 実際にコードを実行した結果のデモ動画が掲載されています。\n*   **詰まった点:** コードを作成する上で著者が遭遇した問題点や、解決策が紹介されています。\n*   **コードの解説:** サンプルコードの各部分について、詳細な解説が加えられています。Websocketへの接続方法、音声データの送受信方法、AIが発話した音声の文字起こしを取得する方法などが説明されています。\n*   **実装できなかった部分:** AIが話しているときに、ユーザーが話し始めても、AIの音声をうまく止められないという問題が残っていることが述べられています。\n\n**記事の特徴**\n\n*   **実践的な内容:** OpenAIのRealtime APIを実際にPythonで利用するための具体的な手順が、サンプルコードとともに解説されています。\n*   **初心者向け:** コードの解説が丁寧で、Realtime APIやPythonのWebsocket通信に詳しくない読者でも理解しやすいように書かれています。\n*   **問題点と解決策の提示:** 著者がコードを作成する上で遭遇した問題点や、解決策が共有されており、読者が同様の問題に遭遇した場合に役立つ情報が提供されています。\n\nこの記事は、OpenAIのRealtime APIをPythonで手軽に試してみたいエンジニアにとって、非常に役立つ情報源となっています。', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-2804f27a-3af5-491b-8aaa-b16c04f1559c-0', usage_metadata={'input_tokens': 89632, 'output_tokens': 432, 'total_tokens': 90064, 'input_token_details': {'cache_read': 0}})]}
=================================
asapさんのZennの記事の中で4番目にいいねが多い記事は、「Open AI Realtime APIのPythonサンプルコードを作成」で、いいね数は79です。

この記事は、OpenAIが提供するRealtime APIをPythonで利用するためのサンプルコードを紹介したものです。

**記事の内容**

*   **はじめに:** OpenAIからRealtime APIが公開されたことを受け、Pythonでのサンプルコードを作成した経緯を説明しています。
*   **参考:** OpenAIの公式ドキュメントを参考にコードを作成したことを述べています。
*   **サンプルコード:** 実際に動作するPythonのサンプルコードが掲載されています。コードは、WebsocketでOpenAIのサーバーに接続し、音声データを送受信することでリアルタイムな音声対話を実現するものです。
*   **実行結果:** 実際にコードを実行した結果のデモ動画が掲載されています。
*   **詰まった点:** コードを作成する上で著者が遭遇した問題点や、解決策が紹介されています。
*   **コードの解説:** サンプルコードの各部分について、詳細な解説が加えられています。Websocketへの接続方法、音声データの送受信方法、AIが発話した音声の文字起こしを取得する方法などが説明されています。
*   **実装できなかった部分:** AIが話しているときに、ユーザーが話し始めても、AIの音声をうまく止められないという問題が残っていることが述べられています。

**記事の特徴**

*   **実践的な内容:** OpenAIのRealtime APIを実際にPythonで利用するための具体的な手順が、サンプルコードとともに解説されています。
*   **初心者向け:** コードの解説が丁寧で、Realtime APIやPythonのWebsocket通信に詳しくない読者でも理解しやすいように書かれています。
*   **問題点と解決策の提示:** 著者がコードを作成する上で遭遇した問題点や、解決策が共有されており、読者が同様の問題に遭遇した場合に役立つ情報が提供されています。

この記事は、OpenAIのRealtime APIをPythonで手軽に試してみたいエンジニアにとって、非常に役立つ情報源となっています。











入力してください: GRPOを導入するにあたり、著者はどういう目的があって導入しましたか？著者の意図を教えてください。
response:  {'messages': [HumanMessage(content=[{'type': 'text', 'text': '「https://zenn.dev/asap」この執筆者のもっともいいねがついている記事を調査して、その記事の中身を取得し、その記事がなぜ評価されているのかを分析してください。'}], additional_kwargs={}, response_metadata={}), AIMessage(content='zenn.devのasap氏の最もいいねが多い記事を調べ、その内容を取得し、評価されている理由を分析する、というご要望ですね。\n\nまず、zenn.dev/asapにアクセスして、最もいいねが多い記事のタイトルとURLを特定する必要があります。その後、そのURLにアクセスして記事の内容を取得します。最後に、記事の内容から評価されている理由を推測します。\n\nでは、始めましょう。\n\n1.  zenn.dev/asapにアクセスし、最もいいねが多い記事の情報を取得します。', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-989fe5ca-2637-4757-878f-db26e13526b8-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap'}, 'id': '58fad86e-0b01-4fc6-a8cc-57deb5c48b41', 'type': 'tool_call'}], usage_metadata={'input_tokens': 778, 'output_tokens': 128, 'total_tokens': 906, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap\n\n- Page URL: https://zenn.dev/asap\n- Page Title: asapさんの記事一覧 | Zenn\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- banner [ref=s1e32]:\n    - img "asap" [ref=s1e36]\n    - heading "asap" [level=1] [ref=s1e39]\n    - button "フォロー" [ref=s1e41]\n    - paragraph [ref=s1e43]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n    - button "2,214 Likes" [ref=s1e46]\n    - button "310 Followers" [ref=s1e48]\n    - link "@personabb" [ref=s1e51]:\n        - /url: https://github.com/personabb\n        - tooltip "@personabb" [ref=s1e52]:\n            - img "GitHub" [ref=s1e53]\n            - text: "@personabb"\n    - link "@asap2650" [ref=s1e55]:\n        - /url: https://twitter.com/asap2650\n        - tooltip "@asap2650" [ref=s1e56]:\n            - img "X(Twitter)" [ref=s1e57]\n            - text: "@asap2650"\n    - link "yard.tips" [ref=s1e63]:\n        - /url: https://yard.tips/asap2650\n        - tooltip "yard.tips" [ref=s1e64]:\n            - img [ref=s1e65]\n            - text: yard.tips\n    - link "RSS" [ref=s1e67]:\n        - /url: /asap/feed\n        - tooltip "RSS" [ref=s1e68]:\n            - img [ref=s1e69]\n            - text: RSS\n- link "Articles 72" [ref=s1e80]:\n    - /url: /asap\n- link "Scraps 5" [ref=s1e82]:\n    - /url: /asap?tab=scraps\n- link "Comments" [ref=s1e84]:\n    - /url: /asap?tab=comments\n- article [ref=s1e89]:\n    - link "tech" [ref=s1e90]:\n        - /url: /tech-or-idea\n    - link "❄️ LangChainでGemini-2.0 APIの画像編集を実行する方法 6日前 いいねされた数 18" [ref=s1e91]:\n        - /url: /asap/articles/1d5e6ec06c380f\n        - text: ❄️\n        - heading "LangChainでGemini-2.0 APIの画像編集を実行する方法" [level=3] [ref=s1e95]\n        - time [ref=s1e98]: 6日前\n        - img "いいねされた数" [ref=s1e100]\n        - text: "18"\n- article [ref=s1e102]:\n    - link "tech" [ref=s1e103]:\n        - /url: /tech-or-idea\n    - link "🙌 完全自律型AIエージェントOSS【LangManus】完全に理解した！ 13日前 いいねされた数 48" [ref=s1e104]:\n        - /url: /asap/articles/b27e832c2a49b7\n        - text: 🙌\n        - heading "完全自律型AIエージェントOSS【LangManus】完全に理解した！" [level=3] [ref=s1e108]\n        - time [ref=s1e111]: 13日前\n        - img "いいねされた数" [ref=s1e113]\n        - text: "48"\n- article [ref=s1e115]:\n    - link "tech" [ref=s1e116]:\n        - /url: /tech-or-idea\n    - link "💫 LangChainでOpenAI gpt-4o / o1にPDFを入力する方法 20日前 いいねされた数 9" [ref=s1e117]:\n        - /url: /asap/articles/74885fab863c9e\n        - text: 💫\n        - heading "LangChainでOpenAI gpt-4o / o1にPDFを入力する方法" [level=3]\n          [ref=s1e121]\n        - time [ref=s1e124]: 20日前\n        - img "いいねされた数" [ref=s1e126]\n        - text: "9"\n- article [ref=s1e128]:\n    - link "tech" [ref=s1e129]:\n        - /url: /tech-or-idea\n    - link "👣 Ollama + LangChainで画像を入力する方法【Gemma3 27B】 27日前 いいねされた数 14" [ref=s1e130]:\n        - /url: /asap/articles/18b110d551b811\n        - text: 👣\n        - heading "Ollama + LangChainで画像を入力する方法【Gemma3 27B】" [level=3]\n          [ref=s1e134]\n        - time [ref=s1e137]: 27日前\n        - img "いいねされた数" [ref=s1e139]\n        - text: "14"\n- article [ref=s1e141]:\n    - link "tech" [ref=s1e142]:\n        - /url: /tech-or-idea\n    - link "🦄 Google CloudのVector StoreでRAG Agentsを構築する【Big Querty編】 28日前 いいねされた数 12" [ref=s1e143]:\n        - /url: /asap/articles/3d258861c6c528\n        - text: 🦄\n        - heading "Google CloudのVector StoreでRAG Agentsを構築する【Big Querty編】"\n          [level=3] [ref=s1e147]\n        - time [ref=s1e150]: 28日前\n        - img "いいねされた数" [ref=s1e152]\n        - text: "12"\n- article [ref=s1e154]:\n    - link "tech" [ref=s1e155]:\n        - /url: /tech-or-idea\n    - link "🎶 最新音楽生成AI 【DiffRhythm】をGoogle Colabで動かせるようにしました！（簡易版） 1ヶ月前 いいねされた数 39" [ref=s1e156]:\n        - /url: /asap/articles/4edc4a71de8961\n        - text: 🎶\n        - heading "最新音楽生成AI 【DiffRhythm】をGoogle Colabで動かせるようにしました！（簡易版）"\n          [level=3] [ref=s1e160]\n        - time [ref=s1e163]: 1ヶ月前\n        - img "いいねされた数" [ref=s1e165]\n        - text: "39"\n- article [ref=s1e167]:\n    - link "tech" [ref=s1e168]:\n        - /url: /tech-or-idea\n    - link "🐤 LangGraphのTool Callingを利用して、RAG Agentsを構築する（後編） 1ヶ月前 いいねされた数 12" [ref=s1e169]:\n        - /url: /asap/articles/c58cfab9c1362a\n        - text: 🐤\n        - heading "LangGraphのTool Callingを利用して、RAG Agentsを構築する（後編）" [level=3]\n          [ref=s1e173]\n        - time [ref=s1e176]: 1ヶ月前\n        - img "いいねされた数" [ref=s1e178]\n        - text: "12"\n- article [ref=s1e180]:\n    - link "idea" [ref=s1e181]:\n        - /url: /tech-or-idea\n    - link "🏆 イベントに登壇してきた話【資料もこちら】 1ヶ月前 いいねされた数 20" [ref=s1e182]:\n        - /url: /asap/articles/89d2a13bdc75a3\n        - text: 🏆\n        - heading "イベントに登壇してきた話【資料もこちら】" [level=3] [ref=s1e186]\n        - time [ref=s1e189]: 1ヶ月前\n        - img "いいねされた数" [ref=s1e191]\n        - text: "20"\n- article [ref=s1e193]:\n    - link "tech" [ref=s1e194]:\n        - /url: /tech-or-idea\n    - link "👌 Grok API（Grok 2）で遊んでみた！【Elyza Tasks 100】 2ヶ月前 いいねされた数 11" [ref=s1e195]:\n        - /url: /asap/articles/6aa01bde32163d\n        - text: 👌\n        - heading "Grok API（Grok 2）で遊んでみた！【Elyza Tasks 100】" [level=3]\n          [ref=s1e199]\n        - time [ref=s1e202]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e204]\n        - text: "11"\n- article [ref=s1e206]:\n    - link "tech" [ref=s1e207]:\n        - /url: /tech-or-idea\n    - link "📑 Multi-head Latent Attention (MLA)の詳細【be absorbedとは？】 2ヶ月前 いいねされた数 22" [ref=s1e208]:\n        - /url: /asap/articles/be3d4b60d8ac92\n        - text: 📑\n        - heading "Multi-head Latent Attention (MLA)の詳細【be absorbedとは？】"\n          [level=3] [ref=s1e212]\n        - time [ref=s1e215]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e217]\n        - text: "22"\n- article [ref=s1e219]:\n    - link "tech" [ref=s1e220]:\n        - /url: /tech-or-idea\n    - link "⛳ LangGraphのTool Callingを利用して、RAG Agentsを構築する（前編） 2ヶ月前 いいねされた数 46" [ref=s1e221]:\n        - /url: /asap/articles/2ad7eb5e7560cf\n        - text: ⛳\n        - heading "LangGraphのTool Callingを利用して、RAG Agentsを構築する（前編）" [level=3]\n          [ref=s1e225]\n        - time [ref=s1e228]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e230]\n        - text: "46"\n- article [ref=s1e232]:\n    - link "tech" [ref=s1e233]:\n        - /url: /tech-or-idea\n    - link "🫠 【LangChain】で【Transformer】を利用する場合に【chat_template】を設定する方法【エラー対処記事】 2ヶ月前 いいねされた数 13" [ref=s1e234]:\n        - /url: /asap/articles/985f3580ea476d\n        - text: 🫠\n        - heading\n          "【LangChain】で【Transformer】を利用する場合に【chat_template】を設定する方法【エラー対処記事】"\n          [level=3] [ref=s1e238]\n        - time [ref=s1e241]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e243]\n        - text: "13"\n- article [ref=s1e245]:\n    - link "tech" [ref=s1e246]:\n        - /url: /tech-or-idea\n    - link "🌊 採点理由を出力しながらローカルLLMを評価する【Elyza Tasks 100】 2ヶ月前 いいねされた数 26" [ref=s1e247]:\n        - /url: /asap/articles/3a99c34fc1d4cb\n        - text: 🌊\n        - heading "採点理由を出力しながらローカルLLMを評価する【Elyza Tasks 100】" [level=3]\n          [ref=s1e251]\n        - time [ref=s1e254]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e256]\n        - text: "26"\n- article [ref=s1e258]:\n    - link "tech" [ref=s1e259]:\n        - /url: /tech-or-idea\n    - link "📑 DeepSeek-R1の論文読んだ？【勉強になるよ】 2ヶ月前 いいねされた数 921" [ref=s1e260]:\n        - /url: /asap/articles/34237ad87f8511\n        - text: 📑\n        - heading "DeepSeek-R1の論文読んだ？【勉強になるよ】" [level=3] [ref=s1e264]\n        - time [ref=s1e267]: 2ヶ月前\n        - img "いいねされた数" [ref=s1e269]\n        - text: "921"\n- article [ref=s1e271]:\n    - link "tech" [ref=s1e272]:\n        - /url: /tech-or-idea\n    - link "📖 VAEって結局何者なの？ 3ヶ月前 いいねされた数 104" [ref=s1e273]:\n        - /url: /asap/articles/6caa9043276424\n        - text: 📖\n        - heading "VAEって結局何者なの？" [level=3] [ref=s1e277]\n        - time [ref=s1e280]: 3ヶ月前\n        - img "いいねされた数" [ref=s1e282]\n        - text: "104"\n- article [ref=s1e284]:\n    - link "tech" [ref=s1e285]:\n        - /url: /tech-or-idea\n    - link "🦔 初めてのSageMaker AIでSFTTrainerによるLLMのフルパラメータファインチューニングを試す 3ヶ月前 いいねされた数 9" [ref=s1e286]:\n        - /url: /asap/articles/3f31b77010624c\n        - text: 🦔\n        - heading "初めてのSageMaker AIでSFTTrainerによるLLMのフルパラメータファインチューニングを試す"\n          [level=3] [ref=s1e290]\n        - time [ref=s1e293]: 3ヶ月前\n        - img "いいねされた数" [ref=s1e295]\n        - text: "9"\n- article [ref=s1e297]:\n    - link "tech" [ref=s1e298]:\n        - /url: /tech-or-idea\n    - link "😺 【動画翻訳】OpenAIの研究者によるo1やLLMに対するディスカッションの翻訳 3ヶ月前 いいねされた数 12" [ref=s1e299]:\n        - /url: /asap/articles/2d2832bf3288aa\n        - text: 😺\n        - heading "【動画翻訳】OpenAIの研究者によるo1やLLMに対するディスカッションの翻訳" [level=3]\n          [ref=s1e303]\n        - time [ref=s1e306]: 3ヶ月前\n        - img "いいねされた数" [ref=s1e308]\n        - text: "12"\n- article [ref=s1e310]:\n    - link "tech" [ref=s1e311]:\n        - /url: /tech-or-idea\n    - link "🧚\u200d♀️ AIでオリジナル楽曲を作り、自作のかわいい声で歌ってもらう方法 3ヶ月前 いいねされた数 28" [ref=s1e312]:\n        - /url: /asap/articles/886af0fe48dda3\n        - text: 🧚\u200d♀️\n        - heading "AIでオリジナル楽曲を作り、自作のかわいい声で歌ってもらう方法" [level=3] [ref=s1e316]\n        - time [ref=s1e319]: 3ヶ月前\n        - img "いいねされた数" [ref=s1e321]\n        - text: "28"\n- article [ref=s1e323]:\n    - link "tech" [ref=s1e324]:\n        - /url: /tech-or-idea\n    - link "🎮 Google ColabでSeed-VCによる声色変換を試す 4ヶ月前 いいねされた数 27" [ref=s1e325]:\n        - /url: /asap/articles/9c54aef739a6ed\n        - text: 🎮\n        - heading "Google ColabでSeed-VCによる声色変換を試す" [level=3] [ref=s1e329]\n        - time [ref=s1e332]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e334]\n        - text: "27"\n- article [ref=s1e336]:\n    - link "tech" [ref=s1e337]:\n        - /url: /tech-or-idea\n    - link "🐡 LLMの性能における重要補題【Johnson–Lindenstrauss lemma】のお気持ち〜Attentionの数式理解を添えて〜 4ヶ月前 いいねされた数 21" [ref=s1e338]:\n        - /url: /asap/articles/6bdf89d8e964ec\n        - text: 🐡\n        - heading "LLMの性能における重要補題【Johnson–Lindenstrauss\n          lemma】のお気持ち〜Attentionの数式理解を添えて〜" [level=3] [ref=s1e342]\n        - time [ref=s1e345]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e347]\n        - text: "21"\n- article [ref=s1e349]:\n    - link "tech" [ref=s1e350]:\n        - /url: /tech-or-idea\n    - link "📏 【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】 4ヶ月前 いいねされた数 50" [ref=s1e351]:\n        - /url: /asap/articles/ea9f42edff6573\n        - text: 📏\n        - heading "【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】"\n          [level=3] [ref=s1e355]\n        - time [ref=s1e358]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e360]\n        - text: "50"\n- article [ref=s1e362]:\n    - link "tech" [ref=s1e363]:\n        - /url: /tech-or-idea\n    - link "🎙️ 高品質合成音声モデル【Style-Bert-VITS2】をGoogle Colabで学習し、【Aivis Speech】で利用する方法 4ヶ月前 いいねされた数 24" [ref=s1e364]:\n        - /url: /asap/articles/5c437ce2943ed7\n        - text: 🎙️\n        - heading "高品質合成音声モデル【Style-Bert-VITS2】をGoogle Colabで学習し、【Aivis\n          Speech】で利用する方法" [level=3] [ref=s1e368]\n        - time [ref=s1e371]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e373]\n        - text: "24"\n- article [ref=s1e375]:\n    - link "tech" [ref=s1e376]:\n        - /url: /tech-or-idea\n    - link "🎄 【LangChain】LCEL ステップバイステップ ハンズオン 〜なろう短編小説の自動生成＆評価・改善を行う「AIエージェント」を添えて〜 4ヶ月前 いいねされた数 19" [ref=s1e377]:\n        - /url: /asap/articles/aa587ad8d76105\n        - text: 🎄\n        - heading "【LangChain】LCEL ステップバイステップ ハンズオン\n          〜なろう短編小説の自動生成＆評価・改善を行う「AIエージェント」を添えて〜" [level=3] [ref=s1e381]\n        - time [ref=s1e384]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e386]\n        - text: "19"\n- article [ref=s1e388]:\n    - link "tech" [ref=s1e389]:\n        - /url: /tech-or-idea\n    - link "💻 LangGraphによる「AIエージェントWebアプリ」を作成する【Next.js】 4ヶ月前 いいねされた数 45" [ref=s1e390]:\n        - /url: /asap/articles/a350fe4ac00343\n        - text: 💻\n        - heading "LangGraphによる「AIエージェントWebアプリ」を作成する【Next.js】" [level=3]\n          [ref=s1e394]\n        - time [ref=s1e397]: 4ヶ月前\n        - img "いいねされた数" [ref=s1e399]\n        - text: "45"\n- article [ref=s1e401]:\n    - link "tech" [ref=s1e402]:\n        - /url: /tech-or-idea\n    - link "🤔 LangGraphにて入力情報が不十分な場合に、ユーザへ追加質問をするAIエージェントを作る方法 5ヶ月前 いいねされた数 41" [ref=s1e403]:\n        - /url: /asap/articles/03548f844efe6f\n        - text: 🤔\n        - heading "LangGraphにて入力情報が不十分な場合に、ユーザへ追加質問をするAIエージェントを作る方法" [level=3]\n          [ref=s1e407]\n        - time [ref=s1e410]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e412]\n        - text: "41"\n- article [ref=s1e414]:\n    - link "tech" [ref=s1e415]:\n        - /url: /tech-or-idea\n    - link "🤖 高品質動画生成AI【Allegro】をローカルで実行する【VRAM12GB】 5ヶ月前 いいねされた数 10" [ref=s1e416]:\n        - /url: /asap/articles/dac57655eb0314\n        - text: 🤖\n        - heading "高品質動画生成AI【Allegro】をローカルで実行する【VRAM12GB】" [level=3] [ref=s1e420]\n        - time [ref=s1e423]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e425]\n        - text: "10"\n- article [ref=s1e427]:\n    - link "tech" [ref=s1e428]:\n        - /url: /tech-or-idea\n    - link "🌊 フレーム補間技術【RIFE】を利用してCogVideoXのFPSを改善しよう 5ヶ月前 いいねされた数 2" [ref=s1e429]:\n        - /url: /asap/articles/dbeef72b7bfdc3\n        - text: 🌊\n        - heading "フレーム補間技術【RIFE】を利用してCogVideoXのFPSを改善しよう" [level=3] [ref=s1e433]\n        - time [ref=s1e436]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e438]\n        - text: "2"\n- article [ref=s1e440]:\n    - link "tech" [ref=s1e441]:\n        - /url: /tech-or-idea\n    - link "🔥 LangGraphを使ってAIエージェントを実装する方法 5ヶ月前 いいねされた数 34" [ref=s1e442]:\n        - /url: /asap/articles/5da9cf01703a47\n        - text: 🔥\n        - heading "LangGraphを使ってAIエージェントを実装する方法" [level=3] [ref=s1e446]\n        - time [ref=s1e449]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e451]\n        - text: "34"\n- article [ref=s1e453]:\n    - link "tech" [ref=s1e454]:\n        - /url: /tech-or-idea\n    - link "🌈 動画生成AI【CogVideoX】をローカルで実行する 5ヶ月前 いいねされた数 9" [ref=s1e455]:\n        - /url: /asap/articles/a4077e8b9ddf1a\n        - text: 🌈\n        - heading "動画生成AI【CogVideoX】をローカルで実行する" [level=3] [ref=s1e459]\n        - time [ref=s1e462]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e464]\n        - text: "9"\n- article [ref=s1e466]:\n    - link "tech" [ref=s1e467]:\n        - /url: /tech-or-idea\n    - link "🛎️ ローカルでDIfyを使ってAIエージェントを実装する方法 5ヶ月前 いいねされた数 55" [ref=s1e468]:\n        - /url: /asap/articles/a35254020fbbae\n        - text: 🛎️\n        - heading "ローカルでDIfyを使ってAIエージェントを実装する方法" [level=3] [ref=s1e472]\n        - time [ref=s1e475]: 5ヶ月前\n        - img "いいねされた数" [ref=s1e477]\n        - text: "55"\n- article [ref=s1e479]:\n    - link "tech" [ref=s1e480]:\n        - /url: /tech-or-idea\n    - link "💫 高品質動画生成AI【Pyramid Flow】をローカルで実行する 6ヶ月前 いいねされた数 12" [ref=s1e481]:\n        - /url: /asap/articles/e1880903625cb5\n        - text: 💫\n        - heading "高品質動画生成AI【Pyramid Flow】をローカルで実行する" [level=3] [ref=s1e485]\n        - time [ref=s1e488]: 6ヶ月前\n        - img "いいねされた数" [ref=s1e490]\n        - text: "12"\n- article [ref=s1e492]:\n    - link "tech" [ref=s1e493]:\n        - /url: /tech-or-idea\n    - link "📖 不偏分散で N-1 で割る理由 を自力で導出する 6ヶ月前 いいねされた数 5" [ref=s1e494]:\n        - /url: /asap/articles/f0cba34c2ac2ac\n        - text: 📖\n        - heading "不偏分散で N-1 で割る理由 を自力で導出する" [level=3] [ref=s1e498]\n        - time [ref=s1e501]: 6ヶ月前\n        - img "いいねされた数" [ref=s1e503]\n        - text: "5"\n- article [ref=s1e505]:\n    - link "tech" [ref=s1e506]:\n        - /url: /tech-or-idea\n    - link "✍️ 勾配降下法 ・ 誤差逆伝播法 を自力で導出する 6ヶ月前 いいねされた数 25" [ref=s1e507]:\n        - /url: /asap/articles/643f604d444813\n        - text: ✍️\n        - heading "勾配降下法 ・ 誤差逆伝播法 を自力で導出する" [level=3] [ref=s1e511]\n        - time [ref=s1e514]: 6ヶ月前\n        - img "いいねされた数" [ref=s1e516]\n        - text: "25"\n- article [ref=s1e518]:\n    - link "tech" [ref=s1e519]:\n        - /url: /tech-or-idea\n    - link "💰 Open AI Realtime APIで、会話履歴を削除して高額請求を回避したい！【Pythonサンプルコード】 2024/10/07 いいねされた数 25" [ref=s1e520]:\n        - /url: /asap/articles/af07fcedbbef61\n        - text: 💰\n        - heading "Open AI Realtime APIで、会話履歴を削除して高額請求を回避したい！【Pythonサンプルコード】"\n          [level=3] [ref=s1e524]\n        - time [ref=s1e527]: 2024/10/07\n        - img "いいねされた数" [ref=s1e529]\n        - text: "25"\n- article [ref=s1e531]:\n    - link "tech" [ref=s1e532]:\n        - /url: /tech-or-idea\n    - link "✈️ Open AI Realtime APIのPythonサンプルコードで、AI発話にカットイン（割り込み）する方法 2024/10/06 いいねされた数 24" [ref=s1e533]:\n        - /url: /asap/articles/563500af4649da\n        - text: ✈️\n        - heading "Open AI Realtime APIのPythonサンプルコードで、AI発話にカットイン（割り込み）する方法"\n          [level=3] [ref=s1e537]\n        - time [ref=s1e540]: 2024/10/06\n        - img "いいねされた数" [ref=s1e542]\n        - text: "24"\n- article [ref=s1e544]:\n    - link "tech" [ref=s1e545]:\n        - /url: /tech-or-idea\n    - link "🥰 Open AI Realtime APIのPythonサンプルコードを作成 2024/10/04 いいねされた数 79" [ref=s1e546]:\n        - /url: /asap/articles/4368fd306b592a\n        - text: 🥰\n        - heading "Open AI Realtime APIのPythonサンプルコードを作成" [level=3] [ref=s1e550]\n        - time [ref=s1e553]: 2024/10/04\n        - img "いいねされた数" [ref=s1e555]\n        - text: "79"\n- article [ref=s1e557]:\n    - link "tech" [ref=s1e558]:\n        - /url: /tech-or-idea\n    - link "📗 主成分分析を自力で導出する 2024/10/03 いいねされた数 6" [ref=s1e559]:\n        - /url: /asap/articles/ff8f34d19ca6a4\n        - text: 📗\n        - heading "主成分分析を自力で導出する" [level=3] [ref=s1e563]\n        - time [ref=s1e566]: 2024/10/03\n        - img "いいねされた数" [ref=s1e568]\n        - text: "6"\n- article [ref=s1e570]:\n    - link "tech" [ref=s1e571]:\n        - /url: /tech-or-idea\n    - link "😍 FLUX.1-dev で ControlNet-depth と LoRA【QTCanimatedFeeling-FLUX】 を試す 2024/10/02 いいねされた数 3" [ref=s1e572]:\n        - /url: /asap/articles/9192c2363dd96a\n        - text: 😍\n        - heading "FLUX.1-dev で ControlNet-depth と LoRA【QTCanimatedFeeling-FLUX】\n          を試す" [level=3] [ref=s1e576]\n        - time [ref=s1e579]: 2024/10/02\n        - img "いいねされた数" [ref=s1e581]\n        - text: "3"\n- article [ref=s1e583]:\n    - link "tech" [ref=s1e584]:\n        - /url: /tech-or-idea\n    - link "📗 拡散モデルの理論について（拡散過程の条件付き確率・逆拡散過程のスケジューラ） 2024/09/23 いいねされた数 8" [ref=s1e585]:\n        - /url: /asap/articles/8aaa896a02f168\n        - text: 📗\n        - heading "拡散モデルの理論について（拡散過程の条件付き確率・逆拡散過程のスケジューラ）" [level=3] [ref=s1e589]\n        - time [ref=s1e592]: 2024/09/23\n        - img "いいねされた数" [ref=s1e594]\n        - text: "8"\n- article [ref=s1e596]:\n    - link "tech" [ref=s1e597]:\n        - /url: /tech-or-idea\n    - link "📖 分散共分散行列の魅力 2024/09/20 いいねされた数 14" [ref=s1e598]:\n        - /url: /asap/articles/2269cd74c5ddc2\n        - text: 📖\n        - heading "分散共分散行列の魅力" [level=3] [ref=s1e602]\n        - time [ref=s1e605]: 2024/09/20\n        - img "いいねされた数" [ref=s1e607]\n        - text: "14"\n- article [ref=s1e609]:\n    - link "tech" [ref=s1e610]:\n        - /url: /tech-or-idea\n    - link "😍 超強力汎用ControlNet「AnyTest v4」を利用する方法【diffusers】 2024/09/17 いいねされた数 9" [ref=s1e611]:\n        - /url: /asap/articles/c08dffa41bec03\n        - text: 😍\n        - heading "超強力汎用ControlNet「AnyTest v4」を利用する方法【diffusers】" [level=3]\n          [ref=s1e615]\n        - time [ref=s1e618]: 2024/09/17\n        - img "いいねされた数" [ref=s1e620]\n        - text: "9"\n- article [ref=s1e622]:\n    - link "tech" [ref=s1e623]:\n        - /url: /tech-or-idea\n    - link "🌍 FLUX.1-devでControlNetを利用したら画像の質が下がったのでImg2Imgを使って改善してみた【Diffusers】 2024/09/13 いいねされた数 2" [ref=s1e624]:\n        - /url: /asap/articles/912d6de879d006\n        - text: 🌍\n        - heading\n          "FLUX.1-devでControlNetを利用したら画像の質が下がったのでImg2Imgを使って改善してみた【Diffusers】"\n          [level=3] [ref=s1e628]\n        - time [ref=s1e631]: 2024/09/13\n        - img "いいねされた数" [ref=s1e633]\n        - text: "2"\n- article [ref=s1e635]:\n    - link "tech" [ref=s1e636]:\n        - /url: /tech-or-idea\n    - link "🎮 【Next.js】ポートレートの表情を編集する自作UIを作ってみた【ComfyUI-AdvancedLivePortrait】 2024/09/11 いいねされた数 8" [ref=s1e637]:\n        - /url: /asap/articles/c49551ff14d5e9\n        - text: 🎮\n        - heading\n          "【Next.js】ポートレートの表情を編集する自作UIを作ってみた【ComfyUI-AdvancedLivePortrait】"\n          [level=3] [ref=s1e641]\n        - time [ref=s1e644]: 2024/09/11\n        - img "いいねされた数" [ref=s1e646]\n        - text: "8"\n- article [ref=s1e648]:\n    - link "tech" [ref=s1e649]:\n        - /url: /tech-or-idea\n    - link "📖 ComfyUI-AdvancedLivePortraitを使ってみる（No WebUI）（Google Colab） 2024/09/10 いいねされた数 7" [ref=s1e650]:\n        - /url: /asap/articles/c0a1b82a10f0e2\n        - text: 📖\n        - heading "ComfyUI-AdvancedLivePortraitを使ってみる（No WebUI）（Google Colab）"\n          [level=3] [ref=s1e654]\n        - time [ref=s1e657]: 2024/09/10\n        - img "いいねされた数" [ref=s1e659]\n        - text: "7"\n- article [ref=s1e661]:\n    - link "tech" [ref=s1e662]:\n        - /url: /tech-or-idea\n    - link "⛳ websocketsでリアルタイム音声処理 2024/09/06 いいねされた数 9" [ref=s1e663]:\n        - /url: /asap/articles/47f839550e5535\n        - text: ⛳\n        - heading "websocketsでリアルタイム音声処理" [level=3] [ref=s1e667]\n        - time [ref=s1e670]: 2024/09/06\n        - img "いいねされた数" [ref=s1e672]\n        - text: "9"\n- article [ref=s1e674]:\n    - link "tech" [ref=s1e675]:\n        - /url: /tech-or-idea\n    - link "😍 生成AIをGoogle Colabで簡単に 【FLUX.1-dev + ControlNet + LoRA】【Diffusers】 2024/08/31 いいねされた数 9" [ref=s1e676]:\n        - /url: /asap/articles/e4c199cecf1836\n        - text: 😍\n        - heading "生成AIをGoogle Colabで簡単に 【FLUX.1-dev + ControlNet +\n          LoRA】【Diffusers】" [level=3] [ref=s1e680]\n        - time [ref=s1e683]: 2024/08/31\n        - img "いいねされた数" [ref=s1e685]\n        - text: "9"\n- article [ref=s1e687]:\n    - link "tech" [ref=s1e688]:\n        - /url: /tech-or-idea\n    - link "👋 Style-Bert-VITS2のAPI化 2024/08/28 いいねされた数 6" [ref=s1e689]:\n        - /url: /asap/articles/12b0f31625a90f\n        - text: 👋\n        - heading "Style-Bert-VITS2のAPI化" [level=3] [ref=s1e693]\n        - time [ref=s1e696]: 2024/08/28\n        - img "いいねされた数" [ref=s1e698]\n        - text: "6"\n- article [ref=s1e700]:\n    - link "tech" [ref=s1e701]:\n        - /url: /tech-or-idea\n    - link "😊 【超簡単】1枚の画像と音声から口パク動画を作りたい！【EchoMimic】 2024/08/25 いいねされた数 6" [ref=s1e702]:\n        - /url: /asap/articles/db1ed88f291b98\n        - text: 😊\n        - heading "【超簡単】1枚の画像と音声から口パク動画を作りたい！【EchoMimic】" [level=3] [ref=s1e706]\n        - time [ref=s1e709]: 2024/08/25\n        - img "いいねされた数" [ref=s1e711]\n        - text: "6"\n- article [ref=s1e713]:\n    - link "tech" [ref=s1e714]:\n        - /url: /tech-or-idea\n    - link "✍️ 【Next.js】生成AIによる「お絵描きサポートWebアプリ」を作りたい！【全くわからない人】 2024/08/20 いいねされた数 8" [ref=s1e715]:\n        - /url: /asap/articles/8f51ffd4fc29b8\n        - text: ✍️\n        - heading "【Next.js】生成AIによる「お絵描きサポートWebアプリ」を作りたい！【全くわからない人】" [level=3]\n          [ref=s1e719]\n        - time [ref=s1e722]: 2024/08/20\n        - img "いいねされた数" [ref=s1e724]\n        - text: "8"\n- article [ref=s1e726]:\n    - link "tech" [ref=s1e727]:\n        - /url: /tech-or-idea\n    - link "🔥 GPT-SoVITS ver2を学習する【WebUI使わない】 2024/08/15 いいねされた数 8" [ref=s1e728]:\n        - /url: /asap/articles/e64c6fce28e0f0\n        - text: 🔥\n        - heading "GPT-SoVITS ver2を学習する【WebUI使わない】" [level=3] [ref=s1e732]\n        - time [ref=s1e735]: 2024/08/15\n        - img "いいねされた数" [ref=s1e737]\n        - text: "8"\n- article [ref=s1e739]:\n    - link "tech" [ref=s1e740]:\n        - /url: /tech-or-idea\n    - link "🔥 GPT-soVITS ver2をモジュールとして動かす 2024/08/08 いいねされた数 9" [ref=s1e741]:\n        - /url: /asap/articles/964a7d863d8896\n        - text: 🔥\n        - heading "GPT-soVITS ver2をモジュールとして動かす" [level=3] [ref=s1e745]\n        - time [ref=s1e748]: 2024/08/08\n        - img "いいねされた数" [ref=s1e750]\n        - text: "9"\n- article [ref=s1e752]:\n    - link "tech" [ref=s1e753]:\n        - /url: /tech-or-idea\n    - link "💯 拡散モデルの理論って面白いよね 2024/08/07 いいねされた数 87" [ref=s1e754]:\n        - /url: /asap/articles/4092ab60570b05\n        - text: 💯\n        - heading "拡散モデルの理論って面白いよね" [level=3] [ref=s1e758]\n        - time [ref=s1e761]: 2024/08/07\n        - img "いいねされた数" [ref=s1e763]\n        - text: "87"\n- article [ref=s1e765]:\n    - link "tech" [ref=s1e766]:\n        - /url: /tech-or-idea\n    - link "🙆\u200d♀️ 画像生成AIの生成途中経過を可視化したい！【Diffusers】 2024/08/06 いいねされた数 4" [ref=s1e767]:\n        - /url: /asap/articles/7940b17be86da7\n        - text: 🙆\u200d♀️\n        - heading "画像生成AIの生成途中経過を可視化したい！【Diffusers】" [level=3] [ref=s1e771]\n        - time [ref=s1e774]: 2024/08/06\n        - img "いいねされた数" [ref=s1e776]\n        - text: "4"\n- article [ref=s1e778]:\n    - link "tech" [ref=s1e779]:\n        - /url: /tech-or-idea\n    - link "🚀 生成AIをGoogle Colabで簡単に 【Part10 SDXL+ANIMAGINE +ControlNet+LoRA編（全部載せ）】 2024/08/03 いいねされた数 3" [ref=s1e780]:\n        - /url: /asap/articles/aa856819e2a722\n        - text: 🚀\n        - heading "生成AIをGoogle Colabで簡単に 【Part10 SDXL+ANIMAGINE\n          +ControlNet+LoRA編（全部載せ）】" [level=3] [ref=s1e784]\n        - time [ref=s1e787]: 2024/08/03\n        - img "いいねされた数" [ref=s1e789]\n        - text: "3"\n- article [ref=s1e791]:\n    - link "tech" [ref=s1e792]:\n        - /url: /tech-or-idea\n    - link "😸 生成AIをGoogle Colaboratoryで簡単に 【Part6.5 画像生成AI SDXL+ANIMAGINE XL 3.1編】 2024/07/31 いいねされた数 3" [ref=s1e793]:\n        - /url: /asap/articles/f9ac2e83c820ff\n        - text: 😸\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part6.5 画像生成AI SDXL+ANIMAGINE\n          XL 3.1編】" [level=3] [ref=s1e797]\n        - time [ref=s1e800]: 2024/07/31\n        - img "いいねされた数" [ref=s1e802]\n        - text: "3"\n- article [ref=s1e804]:\n    - link "tech" [ref=s1e805]:\n        - /url: /tech-or-idea\n    - link "🎃 生成AIをGoogle Colaboratoryで簡単に 【Part9 画像生成AI SD3+ControlNet編】 2024/07/29 いいねされた数 3" [ref=s1e806]:\n        - /url: /asap/articles/53560859d273e4\n        - text: 🎃\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part9 画像生成AI SD3+ControlNet編】"\n          [level=3] [ref=s1e810]\n        - time [ref=s1e813]: 2024/07/29\n        - img "いいねされた数" [ref=s1e815]\n        - text: "3"\n- article [ref=s1e817]:\n    - link "tech" [ref=s1e818]:\n        - /url: /tech-or-idea\n    - link "✨ 生成AIをGoogle Colaboratoryで簡単に 【Part8 画像生成AI Stable Diffusion 3 Medium編】 2024/07/25 いいねされた数 4" [ref=s1e819]:\n        - /url: /asap/articles/6f6ca9aa0513d8\n        - text: ✨\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part8 画像生成AI Stable Diffusion 3\n          Medium編】" [level=3] [ref=s1e823]\n        - time [ref=s1e826]: 2024/07/25\n        - img "いいねされた数" [ref=s1e828]\n        - text: "4"\n- article [ref=s1e830]:\n    - link "tech" [ref=s1e831]:\n        - /url: /tech-or-idea\n    - link "🌟 生成AIをGoogle Colaboratoryで簡単に 【Part7 画像生成AI SDXL Turbo編】 2024/07/23 いいねされた数 2" [ref=s1e832]:\n        - /url: /asap/articles/ed58a6899c1c26\n        - text: 🌟\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part7 画像生成AI SDXL Turbo編】"\n          [level=3] [ref=s1e836]\n        - time [ref=s1e839]: 2024/07/23\n        - img "いいねされた数" [ref=s1e841]\n        - text: "2"\n- article [ref=s1e843]:\n    - link "tech" [ref=s1e844]:\n        - /url: /tech-or-idea\n    - link "🌟 生成AIをGoogle Colaboratoryで簡単に 【Part6 画像生成AI Stable Diffusion XL編】 2024/07/19 いいねされた数 4" [ref=s1e845]:\n        - /url: /asap/articles/e4791be54f90ce\n        - text: 🌟\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part6 画像生成AI Stable Diffusion\n          XL編】" [level=3] [ref=s1e849]\n        - time [ref=s1e852]: 2024/07/19\n        - img "いいねされた数" [ref=s1e854]\n        - text: "4"\n- article [ref=s1e856]:\n    - link "tech" [ref=s1e857]:\n        - /url: /tech-or-idea\n    - link "🎤 生成AIをローカルで簡単に 【Part5.5 faster-whisper+マイク録音編】 2024/07/16 いいねされた数 7" [ref=s1e858]:\n        - /url: /asap/articles/2c0d421e68ef16\n        - text: 🎤\n        - heading "生成AIをローカルで簡単に 【Part5.5 faster-whisper+マイク録音編】" [level=3]\n          [ref=s1e862]\n        - time [ref=s1e865]: 2024/07/16\n        - img "いいねされた数" [ref=s1e867]\n        - text: "7"\n- article [ref=s1e869]:\n    - link "tech" [ref=s1e870]:\n        - /url: /tech-or-idea\n    - link "📖 生成AIをGoogle Colaboratoryで簡単に 【Part5 音声認識AI faster-whisper編】 2024/07/13 いいねされた数 2" [ref=s1e871]:\n        - /url: /asap/articles/ba8fcb1880165e\n        - text: 📖\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part5 音声認識AI faster-whisper編】"\n          [level=3] [ref=s1e875]\n        - time [ref=s1e878]: 2024/07/13\n        - img "いいねされた数" [ref=s1e880]\n        - text: "2"\n- article [ref=s1e882]:\n    - link "tech" [ref=s1e883]:\n        - /url: /tech-or-idea\n    - link "😸 生成AIをGoogle Colaboratoryで簡単に 【Part4 音声認識AI Whisper API編】 2024/07/11 いいねされた数 2" [ref=s1e884]:\n        - /url: /asap/articles/ecaa5631e42195\n        - text: 😸\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part4 音声認識AI Whisper API編】"\n          [level=3] [ref=s1e888]\n        - time [ref=s1e891]: 2024/07/11\n        - img "いいねされた数" [ref=s1e893]\n        - text: "2"\n- article [ref=s1e895]:\n    - link "tech" [ref=s1e896]:\n        - /url: /tech-or-idea\n    - link "😊 生成AIをGoogle Colaboratoryで簡単に 【Part3 テキスト生成 ローカルLLM編】 2024/07/08 いいねされた数 4" [ref=s1e897]:\n        - /url: /asap/articles/8c3d5164d14f6e\n        - text: 😊\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part3 テキスト生成 ローカルLLM編】"\n          [level=3] [ref=s1e901]\n        - time [ref=s1e904]: 2024/07/08\n        - img "いいねされた数" [ref=s1e906]\n        - text: "4"\n- article [ref=s1e908]:\n    - link "tech" [ref=s1e909]:\n        - /url: /tech-or-idea\n    - link "🐷 生成AIをGoogle Colaboratoryで簡単に 【Part2 テキスト生成ChatGPT API編】 2024/07/05 いいねされた数 2" [ref=s1e910]:\n        - /url: /asap/articles/f410dd5d407652\n        - text: 🐷\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part2 テキスト生成ChatGPT API編】"\n          [level=3] [ref=s1e914]\n        - time [ref=s1e917]: 2024/07/05\n        - img "いいねされた数" [ref=s1e919]\n        - text: "2"\n- article [ref=s1e921]:\n    - link "tech" [ref=s1e922]:\n        - /url: /tech-or-idea\n    - link "🗂 生成AIをGoogle Colaboratoryで簡単に 【Part1 合成音声Style-Bert-VITS2編】 2024/07/02 いいねされた数 3" [ref=s1e923]:\n        - /url: /asap/articles/7bdb20dcb5cc56\n        - text: 🗂\n        - heading "生成AIをGoogle Colaboratoryで簡単に 【Part1 合成音声Style-Bert-VITS2編】"\n          [level=3] [ref=s1e927]\n        - time [ref=s1e930]: 2024/07/02\n        - img "いいねされた数" [ref=s1e932]\n        - text: "3"\n- article [ref=s1e934]:\n    - link "tech" [ref=s1e935]:\n        - /url: /tech-or-idea\n    - link "👏 Remdisの環境構築で沼った話（＋使い方の紹介） 2024/06/26 いいねされた数 14" [ref=s1e936]:\n        - /url: /asap/articles/101379d8951b08\n        - text: 👏\n        - heading "Remdisの環境構築で沼った話（＋使い方の紹介）" [level=3] [ref=s1e940]\n        - time [ref=s1e943]: 2024/06/26\n        - img "いいねされた数" [ref=s1e945]\n        - text: "14"\n- article [ref=s1e947]:\n    - link "tech" [ref=s1e948]:\n        - /url: /tech-or-idea\n    - link "🚀 Style-Bert-VITS2の使い方メモ 2024/06/17 いいねされた数 34" [ref=s1e949]:\n        - /url: /asap/articles/f8c0621cdd74cc\n        - text: 🚀\n        - heading "Style-Bert-VITS2の使い方メモ" [level=3] [ref=s1e953]\n        - time [ref=s1e956]: 2024/06/17\n        - img "いいねされた数" [ref=s1e958]\n        - text: "34"\n- article [ref=s1e960]:\n    - link "tech" [ref=s1e961]:\n        - /url: /tech-or-idea\n    - link "💭 Style-Bert-VITS2のVoice Conversion機能の復活に挑戦してみた（前編） 2024/06/09 いいねされた数 9" [ref=s1e962]:\n        - /url: /asap/articles/df2c610d1389e4\n        - text: 💭\n        - heading "Style-Bert-VITS2のVoice Conversion機能の復活に挑戦してみた（前編）" [level=3]\n          [ref=s1e966]\n        - time [ref=s1e969]: 2024/06/09\n        - img "いいねされた数" [ref=s1e971]\n        - text: "9"\n- article [ref=s1e973]:\n    - link "tech" [ref=s1e974]:\n        - /url: /tech-or-idea\n    - link "😸 OpenAI Assistants API v2を色々いじってみた（使い方）（File Search編） 2024/05/28 いいねされた数 10" [ref=s1e975]:\n        - /url: /asap/articles/5c2f8ffe2a46ce\n        - text: 😸\n        - heading "OpenAI Assistants API v2を色々いじってみた（使い方）（File Search編）"\n          [level=3] [ref=s1e979]\n        - time [ref=s1e982]: 2024/05/28\n        - img "いいねされた数" [ref=s1e984]\n        - text: "10"\n- article [ref=s1e986]:\n    - link "tech" [ref=s1e987]:\n        - /url: /tech-or-idea\n    - link "👻 OpenAI Assistants API v2を色々いじってみた（使い方）（code interpreter編） 2024/05/27 いいねされた数 5" [ref=s1e988]:\n        - /url: /asap/articles/b0ce16ea1832ac\n        - text: 👻\n        - heading "OpenAI Assistants API v2を色々いじってみた（使い方）（code interpreter編）"\n          [level=3] [ref=s1e992]\n        - time [ref=s1e995]: 2024/05/27\n        - img "いいねされた数" [ref=s1e997]\n        - text: "5"\n- article [ref=s1e999]:\n    - link "tech" [ref=s1e1000]:\n        - /url: /tech-or-idea\n    - link "😑 【自分用】「AI後輩ちゃん」の個人的つまづきポイントまとめ 2024/05/09 いいねされた数 3" [ref=s1e1001]:\n        - /url: /asap/articles/98b04c5ea82bd9\n        - text: 😑\n        - heading "【自分用】「AI後輩ちゃん」の個人的つまづきポイントまとめ" [level=3] [ref=s1e1005]\n        - time [ref=s1e1008]: 2024/05/09\n        - img "いいねされた数" [ref=s1e1010]\n        - text: "3"\n- article [ref=s1e1012]:\n    - link "tech" [ref=s1e1013]:\n        - /url: /tech-or-idea\n    - link "😍 音声対話システム「AI後輩ちゃん」を作ってみた 2024/05/04 いいねされた数 17" [ref=s1e1014]:\n        - /url: /asap/articles/5b1b7553fcaa76\n        - text: 😍\n        - heading "音声対話システム「AI後輩ちゃん」を作ってみた" [level=3] [ref=s1e1018]\n        - time [ref=s1e1021]: 2024/05/04\n        - img "いいねされた数" [ref=s1e1023]\n        - text: "17"\n- contentinfo [ref=s1e1026]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1030]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1031]\n    - paragraph [ref=s1e1039]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e1041]:\n        - heading "About" [level=4] [ref=s1e1042]\n        - list [ref=s1e1043]:\n            - listitem [ref=s1e1044]:\n                - link "Zennについて" [ref=s1e1046]:\n                    - /url: /about\n            - listitem [ref=s1e1047]:\n                - link "運営会社" [ref=s1e1048]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e1049]:\n                - link "お知らせ・リリース" [ref=s1e1050]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e1051]:\n                - link "イベント" [ref=s1e1053]:\n                    - /url: /events\n    - navigation [ref=s1e1054]:\n        - heading "Guides" [level=4] [ref=s1e1055]\n        - list [ref=s1e1056]:\n            - listitem [ref=s1e1057]:\n                - link "使い方" [ref=s1e1059]:\n                    - /url: /manual\n            - listitem [ref=s1e1060]:\n                - link "法人向けメニュー" [ref=s1e1062]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e1064]:\n                - link "Publication / Pro" [ref=s1e1066]:\n                    - /url: /publications\n            - listitem [ref=s1e1067]:\n                - link "よくある質問" [ref=s1e1069]:\n                    - /url: /faq\n    - navigation [ref=s1e1070]:\n        - heading "Links" [level=4] [ref=s1e1071]\n        - list [ref=s1e1072]:\n            - listitem [ref=s1e1073]:\n                - link "X(Twitter)" [ref=s1e1074]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e1075]:\n                - link "GitHub" [ref=s1e1076]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e1077]:\n                - link "メディアキット" [ref=s1e1079]:\n                    - /url: /mediakit\n    - navigation [ref=s1e1080]:\n        - heading "Legal" [level=4] [ref=s1e1081]\n        - list [ref=s1e1082]:\n            - listitem [ref=s1e1083]:\n                - link "利用規約" [ref=s1e1085]:\n                    - /url: /terms\n            - listitem [ref=s1e1086]:\n                - link "プライバシーポリシー" [ref=s1e1088]:\n                    - /url: /privacy\n            - listitem [ref=s1e1089]:\n                - link "特商法表記" [ref=s1e1091]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e1094]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e1095]\n- alert [ref=s1e1114]\n```\n', name='browser_navigate', tool_call_id='58fad86e-0b01-4fc6-a8cc-57deb5c48b41'), AIMessage(content='`https://zenn.dev/asap`にアクセスしました。\n\n執筆者の記事一覧から、いいねの数が多い記事を探します。\n\n- 「DeepSeek-R1の論文読んだ？【勉強になるよ】」 が921いいね\n- 「拡散モデルの理論って面白いよね」 が87いいね\n- 「Open AI Realtime APIのPythonサンプルコードを作成」 が79いいね\n- 「📏 【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】」 が50いいね\n- 「🙌 完全自律型AIエージェントOSS【LangManus】完全に理解した！」 が48いいね\n\n最もいいねが多い記事は「DeepSeek-R1の論文読んだ？【勉強になるよ】」です。\n記事の中身を取得します。', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap/articles/34237ad87f8511"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-27104099-cd7e-4de1-8c89-51060ed9c6f1-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap/articles/34237ad87f8511'}, 'id': 'd85f8505-9c2a-4960-b17f-9ef88d983d9d', 'type': 'tool_call'}], usage_metadata={'input_tokens': 17995, 'output_tokens': 216, 'total_tokens': 18211, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap/articles/34237ad87f8511\n\n- Page URL: https://zenn.dev/asap/articles/34237ad87f8511\n- Page Title: DeepSeek-R1の論文読んだ？【勉強になるよ】\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- article [ref=s1e32]:\n    - text: 📑\n    - heading "DeepSeek-R1の論文読んだ？【勉強になるよ】" [level=1] [ref=s1e38]\n    - text: 2025/01/27に公開\n    - tooltip "更新日" [ref=s1e45]:\n        - img [ref=s1e46]\n        - text: 2025/02/22 更新日\n    - link "12件" [ref=s1e51]:\n        - /url: "#discuss"\n        - img [ref=s1e52]\n        - text: 12件\n    - link "強化学習" [ref=s1e63]:\n        - /url: /topics/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92\n        - img [ref=s1e65]\n        - text: 強化学習\n    - link "蒸留" [ref=s1e67]:\n        - /url: /topics/%E8%92%B8%E7%95%99\n        - img [ref=s1e69]\n        - text: 蒸留\n    - link "LLM" [ref=s1e71]:\n        - /url: /topics/llm\n        - img [ref=s1e73]\n        - text: LLM\n    - link "生成 AI" [ref=s1e75]:\n        - /url: /topics/%E7%94%9F%E6%88%90ai\n        - img [ref=s1e77]\n        - text: 生成 AI\n    - link "DeepSeek" [ref=s1e79]:\n        - /url: /topics/deepseek\n        - img [ref=s1e81]\n        - text: DeepSeek\n    - link "tech" [ref=s1e83]:\n        - /url: /tech-or-idea\n        - img [ref=s1e85]\n        - text: tech\n    - heading "はじめに" [level=1] [ref=s1e89]\n    - paragraph [ref=s1e90]: タイトル通りDeepSeek-R1の論文を読んだので、少しでもわかりやすく説明できればと思います。\n        まだ論文を読んでいない方は、ぜひ原文も読んでみてください！\n    - paragraph [ref=s1e92]:\n        - text: 難しい数式や概念がほとんど使われていないため、論文の中では全然読みやすい方だと思います。読み物として読めるレベルです。\n        - iframe [ref=s1e94]:\n            - text: Loading...\n            - alert [ref=f1s1e8]\n    - paragraph [ref=s1e95]: とはいえ、本記事では細かい部分など間違っている部分もあるかもしれないので、ぜひ誤りがあったら、こそっと教えてください！\n    - complementary [ref=s1e96]:\n        - text: "!"\n        - paragraph [ref=s1e99]: 実は、本論文を解説する上で、著者らが公開しているもう一つの論文が大きく関連しています。 そちらの論文の内容も若干解説を入れております。\n        - paragraph [ref=s1e101]:\n            - text: 「\n            - \'link "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models" [ref=s1e102]\':\n                - /url: https://arxiv.org/html/2402.03300v3\n            - text: 」\n        - paragraph [ref=s1e103]: 本記事でも少し触れますが、上記の論文も非常に勉強になるため、ぜひ読んでみてください。\n    - complementary [ref=s1e104]:\n        - text: "!"\n        - paragraph [ref=s1e107]:\n            - text: 本記事は、\n            - link "DeepSeek-R1の論文" [ref=s1e108]:\n                - /url: https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\n            - text: と\n            - link "DeepSeekMathの論文" [ref=s1e109]:\n                - /url: https://arxiv.org/html/2402.03300v3\n            - text: を読んだ私の理解をもとに記載しています。\n        - paragraph [ref=s1e110]: 本論文で使われている技術に関しては、ある程度の知識を持っているので、大きくは外していないとは思いますが、私の主観も入っている部分もありますので、ご了承ください。\n    - paragraph [ref=s1e111]:\n        - text: また、DeepSeek-R1の論文が公開される前に、小型モデルに対して同様の実験（強化学習）をしていたグループがあるようです。\n            そちらのレポートは下記になります。\n        - iframe [ref=s1e114]:\n            - \'link "7B Model and 8K Examples: Emerging Reasoning with Reinforcement Learning is Both Effective and Effic A replication of DeepSeek-R1 training on small models with limited data hkust-nlp.notion.site favicon image hkust-nlp.notion.site hkust-nlp.notion.site thumbnail image" [ref=f2s1e6]\':\n                - /url: https://hkust-nlp.notion.site/simplerl-reason\n                - \'heading "7B Model and 8K Examples: Emerging Reasoning with\n                  Reinforcement Learning is Both Effective and Effic" [level=1]\n                  [ref=f2s1e8]\'\n                - text: A replication of DeepSeek-R1 training on small models with limited data\n                - img "hkust-nlp.notion.site favicon image" [ref=f2s1e11]\n                - text: hkust-nlp.notion.site\n                - img "hkust-nlp.notion.site thumbnail image" [ref=f2s1e13]\n            - alert [ref=f2s1e15]\n    - paragraph [ref=s1e115]: 意図せず、DeepSeek-R1-Zeroの再現実験のようなレポートになっていますが、レポートの著者はDeepSeek-R1論文の公開前から実験していると主張しています。\n        こちらも非常に興味深かったため紹介です。\n    - heading "本論文の興味深いところ" [level=1] [ref=s1e117]\n    - paragraph [ref=s1e118]: 本論文は、大きく分けて3つの構成でできています\n    - list [ref=s1e119]:\n        - listitem [ref=s1e120]:\n            - strong [ref=s1e121]: 強化学習による思考能力の強化\n            - list [ref=s1e122]:\n                - listitem [ref=s1e123]: LLM（DeepSeek-V3-Base）に対して「強化学習のみ」を適用させたところ、強力な思考能力を獲得した「DeepSeek-R1-Zero」が作成されました。\n                - listitem [ref=s1e124]: 本論文のメインストリームであり、非常に興味深い示唆が得られる部分です。\n        - listitem [ref=s1e125]:\n            - strong [ref=s1e126]: SFTによるさらなる強化\n            - list [ref=s1e127]:\n                - listitem [ref=s1e128]: DeepSeek-V3-Baseに対して、コールドスタート問題を解消するために、CoT（Chain of\n                    Thought）データセットによるSFT（教師付きファインチューニング）を実施\n                - listitem [ref=s1e129]: その後、強化学習を適用させることで、o1レベルの性能を獲得した「DeepSeek-R1」が作成されました。\n                - listitem [ref=s1e130]: 非常に強力なLLMを獲得するまでの反復学習などの試行錯誤などを学ぶことができます。\n        - listitem [ref=s1e131]:\n            - strong [ref=s1e132]: 蒸留による小型モデルの性能強化\n            - list [ref=s1e133]:\n                - listitem [ref=s1e134]: 上記で作成されたDeepSeek-R1により、他社の小型オープンソースLLMを蒸留しました。\n                - listitem [ref=s1e135]: その結果、元の性能を大幅に上回る、非常に高い性能の小型LLMが得られました。\n    - paragraph [ref=s1e136]: その中でも私が特に面白いなと思った要素は下記の要素になります\n    - list [ref=s1e137]:\n        - listitem [ref=s1e138]:\n            - strong [ref=s1e139]: 強化学習手法「GRPO」すごい！\n        - listitem [ref=s1e140]:\n            - strong [ref=s1e141]: ルールベース報酬で思考能力が向上するのがすごい！\n        - listitem [ref=s1e142]:\n            - strong [ref=s1e143]: aha moment（ハッとする瞬間）面白い！\n        - listitem [ref=s1e144]:\n            - strong [ref=s1e145]: 蒸留vs強化学習の比較実験から重要な示唆が得られた！\n    - heading "本論文の流れ" [level=1] [ref=s1e146]\n    - heading "前提のモデル構造" [level=3] [ref=s1e147]\n    - paragraph [ref=s1e148]: そもそも、大前提として、この論文は「DeepSeek-V3-Base」という強力なLLMの性能を改善する論文です。\n        これ単体でも、GPT-4oレベルの能力を持っているオープンソースLLMになります。\n    - paragraph [ref=s1e150]: 本論文のモデルアーキテクチャは「DeepSeek-V3-Base」と完全に同じものを利用します。\n        （つまりモデルアーキテクチャをかえずに、学習方法を工夫してLLMの性能を上げる挑戦になります）\n    - heading "DeepSeek-R1-Zeroを強化学習を用いて作る" [level=3] [ref=s1e152]\n    - paragraph [ref=s1e153]: その上で、まずは、「DeepSeek-V3-Base」に対して、大規模な強化学習のみを適応させることで、強化学習の可能性について探究する話が始まります。\n        結果として、「DeepSeek-R1-Zero」ができます。\n        「DeepSeek-R1-Zero」は、SFT（教師付きファインチューニング）をしなくても、強化学習のみで圧倒的な思考能力を取得できることを示す興味深いモデルです。\n    - heading "DeepSeek-R1をSFT+強化学習を用いて作る" [level=3] [ref=s1e156]\n    - paragraph [ref=s1e157]: その後、あまりにも「DeepSeek-R1-Zero」の性能が良かったことから、さらなる探究が進められました。\n    - paragraph [ref=s1e158]: この時点で、「DeepSeek-R1-Zero」には下記の課題がありました。\n    - list [ref=s1e159]:\n        - listitem [ref=s1e160]: 学習初期段階にて、強化学習の学習が不安定であり、収束に時間がかかる（コールドスタート問題）\n        - listitem [ref=s1e161]:\n            - text: 思考能力は高いが、言語が混在（日本語で話していたら急に中国語になるなど）し、人間が読みにくい\n            - list [ref=s1e162]:\n                - listitem [ref=s1e163]: 結果、高度な思考が必要なタスクには強いが、一般的タスク（ロールプレイやライティングなど）には使いにくいという評価\n    - paragraph [ref=s1e164]: そこで、「DeepSeek-V3-Base」に対して、少量のCoT（Chain of\n        Thought）データセットを用いて、SFT（教師付き微調整）を行います。このデータは、著者らが収集・構築したものになります。\n        その後、SFT後のモデルに対して、大規模強化学習を行います。\n        このとき、「DeepSeek-R1-Zero」を作成した際の報酬に加え、「言語一貫性報酬」を導入して、上述した言語の不一致の問題を緩和させます。\n    - paragraph [ref=s1e167]: その後、得られたモデル（チェックポイント）を利用して、さらなるSFT（教師付き微調整）学習データを収集し、追加SFTを行います。\n        さらに、その後、改めて２段階目の大規模強化学習を実施することで、最終的に「DeepSeek-R1」が得られます。\n    - heading "小型モデルへの蒸留を試す" [level=3] [ref=s1e169]\n    - paragraph [ref=s1e170]: 最後に、小型モデルに対する「DeepSeek-R1」による蒸留が試されています。\n        その中で、小型モデルに対して、「DeepSeek-R1」による蒸留を実施するのと、「DeepSeek-R1-Zero」と同様の強化学習手法で思考能力を強化するのと、どちらが性能向上に寄与するのかを調査してくれています。\n    - paragraph [ref=s1e172]: これが非常に興味深いです。\n    - heading "DeepSeek-R1-Zeroを作るまで" [level=1] [ref=s1e173]\n    - paragraph [ref=s1e174]: では、ここから論文の詳細な解説に入っていきます。\n    - paragraph [ref=s1e175]: まずは、「DeepSeek-R1」の前段階である「DeepSeek-R1-Zero」を作るまでの手法を解説します。\n    - paragraph [ref=s1e176]:\n        - strong [ref=s1e177]: 前段階と侮るなかれ、「DeepSeek-R1-Zero」を作るまでの手法と示唆にこそ、本論文の貴重な示唆が含まれています。\n    - heading "元々のアーキテクチャ" [level=2] [ref=s1e178]\n    - paragraph [ref=s1e179]: 今回作る「DeepSeek-R1-Zero」や「DeepSeek-R1」は、新しいモデル構造を採用して、性能を向上させたわけではありません。\n    - paragraph [ref=s1e180]: 元々のアーキテクチャは「DeepSeek-V3-Base」というアーキテクチャになります。\n        「DeepSeek-V3-Base」は、681Bのモデルパラメータを持つ大規模言語モデルで、性能はGPT-4oに匹敵する非常に高性能モデルありながら、モデル重みが完全に公開されているオープンソースモデルです。\n    - paragraph [ref=s1e182]: 「DeepSeek-V3-Base」に対して、Instructionチューニングを施したchatモデルに関しては、APIでも公開されています。\n        非常に安価に使えるにも関わらず、GPT-4oに匹敵する性能を誇ることから、人気のモデルになっております。\n    - complementary [ref=s1e184]:\n        - text: "!"\n        - paragraph [ref=s1e187]: 安価なモデルではありますが、APIの提供元は中国の企業ですので、データの取り扱いなどは十分注意してください。\n            基本的には、API利用の際には、収集されて困るような情報は入力しないようにするのが無難です。\n            （オープンソースモデルをローカルで動かす場合は、おそらく心配ありません）\n    - paragraph [ref=s1e190]: 本論文のすごいところは、モデル構造を変えずに、「DeepSeek-V3-Base」に対する学習方法を工夫しただけで、OpenAI\n        o1に匹敵する性能のLLMを開発していることです。\n        具体的には、大規模強化学習をLLMに対して適用することで、「DeepSeek-R1-Zero」を開発します。\n    - heading "すごいぞ！強化学習" [level=2] [ref=s1e192]\n    - heading "強化学習って何？" [level=3] [ref=s1e193]\n    - paragraph [ref=s1e194]: そもそも「強化学習って何？」という方もいらっしゃると思うので、簡単に説明します。\n        （以前、スイカゲームを強化学習でプレイしたこともあるので、いずれ詳しい解説記事を作る予定ですが、ここではざっくり説明します。）\n    - paragraph [ref=s1e196]: 強化学習とは、一言で言えば「一連の行動の流れ（シークエンス）によって得られる、累積報酬の期待値を最大化する」ことを目指す学習手法です。\n        ある行動をとった結果がすぐにはわからず、時間が経ってから影響が現れるような問題に適しています。\n        専門的に言うと、「マルコフ決定過程（MDP）」が成立する問題で活用されます。\n    - heading "テトリスで考える" [level=3] [ref=s1e199]\n    - paragraph [ref=s1e200]: 例えば、「テトリス」を考えてみましょう。\n    - paragraph [ref=s1e201]: テトリスは、上から「ミノ」と呼ばれる、さまざまな形のブロックを落として、横一列を綺麗に埋めてブロックを消去するゲームです。\n    - paragraph [ref=s1e202]: 横一列がすべて揃うと、その列が消えて、「ポイント」を獲得できます。\n        逆に、列を消せずに、ブロックが一定の高さまで積み上がると「ゲームオーバ」になります。\n    - paragraph [ref=s1e204]: テトリスを強化学習で解こうとすると、「どうすれば最も多くのポイントを得られるか」をAIに学習させることが重要になります。\n        単純に列を揃えて消すだけではなく、「次の手」や「その次の手」を見据えて、より良い結果につながるようにミノを配置することも必要です。\n        しかし、通常の学習方法(よくある入出力を近似させる学習方法)では、目の前の結果だけに注目してしまい、将来的な高得点につながる手を学習させることは難しいです。\n    - paragraph [ref=s1e207]:\n        - strong [ref=s1e208]: そこで役立つのが「強化学習」です。\n    - paragraph [ref=s1e209]: 強化学習では、ゲームの最初から最後までの流れを通して、「どう行動すれば最終的に得られる得点を最大化できるか」を学習できます。\n        このように、途中の行動の良し悪しがすぐにはわからなくても、最終的なゴール（得点の最大化）が設定できる問題に対して、強化学習は非常に効果的です。\n    - paragraph [ref=s1e211]:\n        - img [ref=s1e212]\n    - group [ref=s1e213]: 補足\n    - heading "強化学習の事例「AlphaGo Zero」" [level=3] [ref=s1e215]\n    - paragraph [ref=s1e216]: さらに、強化学習の威力をお伝えする事例を記載します。\n    - paragraph [ref=s1e217]: 2015年にアメリカのDeepMind社（Googleの傘下）が「AlphaGo」というモデルを開発しました。これはAIが囲碁をプレイするモデルで、当時の世界王者をハンディキャップなしで破るという、非常に衝撃的なモデルでした。\n        （当時としては、非常にセンセーショナルな出来事だったため、記憶にある方もいらっしゃるかと思います）\n    - paragraph [ref=s1e219]: この「AlphaGo」は、深層学習と強化学習により学習されています。\n        プロ棋士の棋譜データで深層学習で学習させた上で、強化学習による自己対局（AI同士の対局）を実施したモデルです。\n    - paragraph [ref=s1e221]:\n        - text: さて、ここまでも十分すごいですが、\n        - strong [ref=s1e223]: さらにすごいのはここからです。\n    - paragraph [ref=s1e224]: DeepMind社は後継モデルとして「AlphaGo Zero」というモデルを開発しました。\n        このモデルは、プロ棋士の棋譜データを「使わず」に、完全に「強化学習のみ」で学習されたモデルです。\n        つまり、囲碁のルールだけを与え、あとは最初から最後までAI同士の自己対局のみで学習されています。\n        （この場合、対局に勝利したら報酬、敗北したら罰を与える形で強化学習をし、対局に勝利した時に取っていた手（行動）の選択確率を上げるように学習しています）\n    - paragraph [ref=s1e228]:\n        - strong [ref=s1e229]: そして、その「AlphaGo Zero」は「AlphaGo」に対して、100戦全勝で圧勝しました。\n    - paragraph [ref=s1e230]: AIが囲碁の学習をする際に、人間が持つ定石の情報などは不要だったわけです。\n    - paragraph [ref=s1e231]: これが強化学習の威力です。\n        余談ですが、今回の論文で作成する「DeepSeek-R1-Zero」も（「DeepSeek-V3-Base」をベースにはしますが）強化学習のみで学習されます。\n        名前に入っている「Zero」は同じく強化学習のみで開発された「AlphaGo Zero」のオマージュかなと思います。\n    - heading "そもそもLLMで強化学習とは？" [level=2] [ref=s1e234]\n    - paragraph [ref=s1e235]: 「なんかゲームみたいなものに、強化学習を使うとうまくいきそう」というのは理解していただけたかなと思います。\n    - paragraph [ref=s1e236]:\n        - text: では、LLMに対してはどうか、というのを解説していきます。\n        - strong [ref=s1e238]: 実は、LLMと強化学習は非常に相性がいいです。\n    - paragraph [ref=s1e239]: なぜなら、LLMは一度にすべての文章を出力するAIではなく、1tokenずつ出力するモデルであり、その文章が良い文章か悪い文章かは、すべてのtokenを出力してみないとわからないからです。\n    - paragraph [ref=s1e240]:\n        - text: テトリスでは、ミノを一つずつ落とし、一列が消えたタイミングで、ポイントを取得するという形で評価されます。\n        - img [ref=s1e242]\n    - paragraph [ref=s1e243]:\n        - text: LLMでも、同様にtokenを1tokenずつ出力し、一文全部が出力されたタイミングで、その文章の良し悪しを評価することができれば、強化学習と同じ枠組みを使うことができるというわけです。\n        - img [ref=s1e245]\n    - heading "報酬モデルを作成し、累積報酬を最大化" [level=3] [ref=s1e246]\n    - paragraph [ref=s1e247]:\n        - text: テトリスでは、ゲームシステムが勝手に点数をつけてくれますが、LLMにおいて文章の良し悪しを勝手に判断してくれる便利な機能はありません。そこで我々が、その判断モデルを作ってあげる必要があります。\n            そこで、ニューラルネットワークを利用して、LLMに入力されたプロンプト\n        - math [ref=s1e252]: x\n        - text: に対して、LLMが生成した文章\n        - math [ref=s1e259]: "y"\n        - text: の点数\n        - math [ref=s1e266]: r\n        - text: を出力するようなネットワークを「報酬モデル\n        - math [ref=s1e273]: r θ ( r ∣ y , x )\n        - text: 」として作成することを考えます。\n    - paragraph [ref=s1e286]:\n        - text: まず初めに、データセットとして、「プロンプト\n        - math [ref=s1e290]: x\n        - text: 」「好ましい回答\n        - math [ref=s1e297]: y +\n        - text: 」「好ましくない回答\n        - math [ref=s1e306]: y −\n        - text: 」の3つが組となっているデータセットを用意します。 報酬モデル\n        - math [ref=s1e316]: r θ ( r ∣ y , x )\n        - text: は、「プロンプト\n        - math [ref=s1e332]: x\n        - text: 」と「出力\n        - math [ref=s1e339]: "y"\n        - text: 」の両方を入力することで、報酬（点数）を出力するモデルです。\n    - paragraph [ref=s1e343]:\n        - text: そこで、好ましい回答の報酬推定値\n        - math [ref=s1e347]: r θ ( r ∣ y + , x )\n        - text: と、好ましくない回答の報酬推定値\n        - math [ref=s1e365]: r θ ( r ∣ y − , x )\n        - text: の差を一定以上にするような損失関数を用いて学習を行います。 すると、報酬モデル\n        - math [ref=s1e384]: r θ ( r ∣ y , x )\n        - text: は適切に、好ましい回答\n        - math [ref=s1e400]: y +\n        - text: には高い点数を、好ましくない回答\n        - math [ref=s1e409]: y −\n        - text: には低い点数をつけるようになります。\n    - paragraph [ref=s1e415]:\n        - text: 報酬モデル\n        - math [ref=s1e419]: r θ ( r ∣ y , x )\n        - text: か完成したら、強化学習をするための要素は整います。\n    - paragraph [ref=s1e432]:\n        - text: 状態\n        - math [ref=s1e436]: x\n        - text: は、LLMに入力される「プロンプト」で、LLMの出力が\n        - math [ref=s1e443]: "y"\n        - text: とします。 報酬\n        - math [ref=s1e451]: r\n        - text: は、報酬モデル\n        - math [ref=s1e458]: r θ ( r ∣ y , x )\n        - text: の出力値です。 LLMが全文出力したら、プロンプト\n        - math [ref=s1e475]: x\n        - text: と、LLMの出力\n        - math [ref=s1e482]: "y"\n        - text: を報酬モデルに与えることで、取得します。 方策は、LLMが出力する各tokenごとの確率分布自体です。 このLLM自体を「方策モデル\n        - math [ref=s1e491]: π ϕ ( y t ∣ x , y < t )\n        - text: 」と呼びます。\n    - complementary [ref=s1e510]:\n        - text: "!"\n        - paragraph [ref=s1e513]:\n            - text: LLM（\n            - math [ref=s1e517]: π ϕ ( y t ∣ x , y < t )\n            - text: ）はユーザが入力したプロンプト\n            - math [ref=s1e539]: x\n            - text: と、それまでにLLMが出力した内容\n            - math [ref=s1e546]: y < t\n            - text: をもとに、\n            - math [ref=s1e557]: t\n            - text: ステップ目のtoken\n            - math [ref=s1e564]: y t\n            - text: を出力します。\n        - paragraph [ref=s1e570]: これが、LLMは、「文章の次に出てくる単語を一つずつ予測するモデル」と言われる所以です。\n    - paragraph [ref=s1e571]: これにより、累積報酬を最大化するように方策モデル（LLM）を学習することで、LLMは好ましい回答を出力するようにネットワークを更新できます。\n    - paragraph [ref=s1e572]: これが、LLMを強化学習で学習するということです。 このような手法をRLHF（Reinforcement\n        learning from human feedback）と総称します。\n    - group [ref=s1e574]: 細かい話\n    - heading "つよつよ手法「PPO（Proximal Policy Optimization）」" [level=3] [ref=s1e576]\n    - paragraph [ref=s1e577]: ここまでで、「LLMと強化学習は相性が良い」こと、「強化学習では累積報酬の期待値（状態価値）を最大化することが重要」であることを話しました。\n    - paragraph [ref=s1e578]: さて、実際にLLMに強化学習を組み込む場合、しばしばPPO（Proximal Policy\n        Optimization）という手法が利用されます。\n        （なんなら、LLM以外で強化学習を実用する上でも、最初に選択肢になるベースライン的に強力な手法でもあります）\n    - paragraph [ref=s1e580]: 今回はPPOが主題ではないので、簡単に説明します。\n    - heading "PPOの簡易的な説明" [level=4] [ref=s1e581]\n    - paragraph [ref=s1e582]:\n        - text: PPOでは、行動を確率的に出力する方策モデル（LLM）と、現在の状態価値を推定する状態価値モデルを用意します。 状態価値\n        - math [ref=s1e587]: V t\n        - text: とは、「累積報酬の期待値」として定義できます。 したがって、現時点から、方策に従って行動をした場合の、将来的な報酬総和の期待値が「状態価値\n        - math [ref=s1e597]: V t\n        - text: 」です。\n    - paragraph [ref=s1e603]:\n        - text: 強化学習の枠組みで累積報酬\n        - math [ref=s1e607]: G t\n        - text: が計算されるため、その期待値である状態価値\n        - math [ref=s1e616]: V t\n        - text: も学習が可能です。 一方で、方策モデルを学習する際に、Advantage\n        - math [ref=s1e626]: A t\n        - text: というものを利用します。\n    - paragraph [ref=s1e632]:\n        - text: Advantage\n        - math [ref=s1e636]: A t\n        - text: というのは下記の式で表されます。\n    - math [ref=s1e648]: A t = G t − V t ( v t ∣ y < t , x )\n    - paragraph [ref=s1e675]:\n        - text: 状態価値\n        - math [ref=s1e679]: V t ( v t ∣ y < t , x )\n        - text: は、累積報酬\n        - math [ref=s1e701]: G t\n        - text: の期待値（大雑把にいうと平均）になるように学習されているという前提があるため、Advantage\n        - math [ref=s1e710]: A t\n        - text: が正になるということは、今回LLMが出力したtoken\n        - math [ref=s1e719]: y t\n        - text: は、将来的に高い報酬を得ることができる「良い出力であった」と判断できます。\n    - paragraph [ref=s1e725]:\n        - text: したがって、方策モデル\n        - math [ref=s1e729]: π ϕ ( y t ∣ x , y < t )\n        - text: はAdvantage\n        - math [ref=s1e751]: A t\n        - text: を最大化するように学習します。\n    - paragraph [ref=s1e757]:\n        - text: Advantage\n        - math [ref=s1e761]: A t\n        - text: を考えずに、単純に累積報酬\n        - math [ref=s1e770]: G t\n        - text: を最大化するように考えることもできます。（むしろそれが初期の強化学習です）\n    - complementary [ref=s1e776]:\n        - text: "!"\n        - paragraph [ref=s1e779]: （2025年2月15日 追記） この辺りの表現は、わかりやすくするために若干誤解を招く表現になっています。\n            下に「細かい話」として補足説明を書きました。\n    - paragraph [ref=s1e782]: しかしその場合、そもそもどんな行動を取っても報酬が高くなる状態と、どんな行動を取っても報酬が低くなってしまう状態において、勾配のスケールがばらつくため、学習の安定性が低くなります。\n    - paragraph [ref=s1e783]:\n        - text: PPOでは、Advantage\n        - math [ref=s1e787]: A t\n        - text: を考えることにより、出力したtokenが平均よりも良いか悪いかだけで学習をすることができ、状態ごとの報酬の差をモデルが考慮する必要がなくなります。\n    - complementary [ref=s1e793]:\n        - text: "!"\n        - paragraph [ref=s1e796]: 本来、PPOの貢献は、あるパラメータ更新時において、更新前と更新後の確率分布が大きく変わらないような制約を、少ない計算量で加えることです。\n            ただ、この話は、DeepSeek-R1-Zeroにおける強化学習を理解するためには、必ずしも重要ではありません。\n            したがって、今回の記事では、必要な部分だけに絞り説明をしました。\n    - group [ref=s1e799]: 細かい話（追記）\n    - heading "著者発案「GRPO（Group Relative Policy Optimization）」" [level=3]\n      [ref=s1e801]\n    - paragraph [ref=s1e802]: さて、PPOは強化学習の手法において、高い性能を発揮しますが、LLMに適用することを考えると、状態価値モデルを別途学習しないといけないという点において、計算量的に大変な手法です。\n    - paragraph [ref=s1e803]: "一方で、PPOの状態価値モデルがやっていたことは、Advantage A_t(a_t|y_t,\n        y_{<t},x) を計算し、今回出力したtokenが、平均よりも良いのか悪いのかを判断することです。"\n    - paragraph [ref=s1e806]: "そこで、LLMにおいては、わざわざ状態価値モデルを用意して平均を計算しなくても、実際に複数文章を生成して、そ\\\n        れぞれの報酬を平均することで、Advantage A_t(a_t|y_t, y_{<t},x) を計算しようと考えました。\n        こうすることで、状態価値モデルなしで、Advantage A_t(a_t|y_t, y_{<t},x)\n        を計算できるため、計算量を大幅に削減できることができます。"\n    - paragraph [ref=s1e812]:\n        - text: この手法は「GRPO（Group Relative Policy Optimization）」と呼びます。\n            本論文の著者が、過去に公開した論文で提案した手法です。\n        - iframe [ref=s1e815]: <could not take iframe snapshot>\n    - paragraph [ref=s1e816]:\n        - text: 下記が、論文中で提示されている「PPO」と「GRPO」の違いを説明する図です。\n        - img [ref=s1e818]\n        - text: "このように状態価値モデル（Value model）なしに、Advantage A_t(a_t|y_t, y_{<t},x)\n            が計算できていることがわかります。"\n    - complementary [ref=s1e821]:\n        - text: "!"\n        - paragraph [ref=s1e824]: 本記事でのアルファベットの付け方と、上記の図のアルファベットの付け方が異なることに注意してください。\n        - paragraph [ref=s1e825]: 上記の図（GRPO）では、 プロンプトを q 、LLMの各出力文章を o_i （ i\n            はそれぞれの文章単位でのインデックス）、各文章ごとの報酬モデルの出力を r_i 、報酬 r_i\n            の平均から計算された各文章ごとのAdvantageを A_i としています。\n        - paragraph [ref=s1e839]: 本記事での添え字は、token単位での添え字を採用しています。 例えば A_t なら t token目でのAdvantageです。\n        - paragraph [ref=s1e845]: 上記の図では、文章単位での添え字です。 例えば、 A_i なら、 i 文章目でのAdvantageとなります。\n            従って A_i はベクトルとなっており、中にtokenごとのAdvantageの値も格納されているはずです。\n    - heading "報酬モデルを作らずルールベースで実施" [level=3] [ref=s1e854]\n    - paragraph [ref=s1e855]:\n        - text: 改めて、LLMに強化学習を適用するために必要なものをまとめます。 （下記の図のGRPOの手法を利用します）\n        - img [ref=s1e858]\n    - paragraph [ref=s1e859]: "まず、大前提として方策モデル \\\\pi_\\\\phi(y_t|x,y_{<t}) （LLM）は必要です。"\n    - paragraph [ref=s1e862]: "上の図中における「Reference Model」とは、「強化学習をする前」の方策モデル\n        \\\\pi_{ref}(y_t|x,y_{<t}) （つまり、DeepSeek-V3-Base）です。 また、「Policy\n        Model」というのは、「強化学習中」の現時点での方策モデル \\\\pi_\\\\phi(y_t|x,y_{<t}) を指しています。"\n    - paragraph [ref=s1e868]: そして、「プロンプト」「良い回答」「悪い回答」の3つを組とするデータセットで学習された、報酬モデル\n        r_\\theta(r|y,x) （Reward Model）が必要です。 報酬モデル r_\\theta(r|y,x) は「プロンプト x\n        」と「LLMの出力回答 y 」を入力することで、「報酬 r 」を出力します。\n    - paragraph [ref=s1e880]: この二つがあれば、著者が提案したGRPOという手法で、強化学習が可能になります。 （GRPOでは、状態価値モデルは不要です）\n    - paragraph [ref=s1e882]:\n        - text: しかしながら著者らは、さらに驚くべきことに、「DeepSeek-R1-Zero」を学習する際に、\n        - strong [ref=s1e884]: 報酬モデルを使わずに、ルールベースで報酬を計算しています。\n        - text: それにより、圧倒的な計算効率を達成しています。\n    - paragraph [ref=s1e886]: 著者らが設定しているルールは下記の二つです。\n    - list [ref=s1e887]:\n        - listitem [ref=s1e888]:\n            - strong [ref=s1e889]: 精度報酬\n            - list [ref=s1e890]:\n                - listitem [ref=s1e891]: 応答が正しいかどうかを評価する\n                - listitem [ref=s1e892]: 例えば、数学の問題であれば、最終的な回答が答えと合っているかどうかで報酬を渡す。\n        - listitem [ref=s1e893]:\n            - strong [ref=s1e894]: フォーマット報酬\n            - list [ref=s1e895]:\n                - listitem [ref=s1e896]:\n                    - text: 思考プロセスを\n                    - code [ref=s1e897]: <think>\n                    - text: と\n                    - code [ref=s1e898]: </think>\n                    - text: タグの間に置くことをモデルに強制し、合っていない場合はペナルティを課す。\n    - paragraph [ref=s1e899]:\n        - text: このたった2つの報酬のみをルールベースで設定しており、\n        - strong [ref=s1e901]: ニューラルネットワークベースの報酬モデルは利用していません\n    - paragraph [ref=s1e902]: ニューラルネットワークの報酬モデルを利用せずに、ルールベース報酬だけで強化学習をすることにより、報酬モデル分の計算量を削減しています。\n    - heading "この報酬設計で良い理由" [level=4] [ref=s1e903]\n    - paragraph [ref=s1e904]: フォーマット報酬は、モデルに出力形式を強制するものであるので、思考能力を向上させる直接的な報酬ではありません。\n        であれば、「DeepSeek-R1-Zero」の圧倒的な思考能力を支えているのは、「精度報酬」になります。\n    - paragraph [ref=s1e906]: 本論文での、強化学習の段階で利用しているデータセットは、おそらく数学データセットです。\n    - paragraph [ref=s1e907]:\n        - text: 数学の問題は、基本的に回答が一意に定まるため、ルールベースであっても出力が正しいかどうかは判断できます。\n        - strong [ref=s1e909]: そして、数学の問題に正答するためには、思考能力を強化する必要があります。\n    - complementary [ref=s1e910]:\n        - text: "!"\n        - paragraph [ref=s1e913]: ここが本論文の非常に面白い部分です。\n            DeepSeek-R1-Zeroを学習する際に、より長い長考をさせるようなデータセットによる学習や、思考部分に対して報酬を与えるような仕組みは「全く無く」、ただ、数学の問題の回答が合っているか間違っているかのゴール報酬のみが与えられています。\n        - paragraph [ref=s1e915]: それだけにも関わらず、自動的に数学の問題を正答するために、思考能力を強化するように学習が進んでおり、強化学習の威力を感じます。\n    - paragraph [ref=s1e916]: 強化学習の枠組みにおいて、最終的に報酬を得る（問題に正解する）ために、今、何のtokenを出力しないといけないのか、が最適化されます。\n        結果として、問題の正当に近づくようなtokenを出力する能力、すなわち問題をステップバイステップで解く能力が磨かれることになります。\n    - paragraph [ref=s1e918]: このように、数学という問題の答えを合わせるように報酬設計をすることで、勝手に思考能力が強化される、というのは非常に面白いなと思います。\n    - heading "（補足）データセットの作り方（の考察）" [level=4] [ref=s1e919]\n    - paragraph [ref=s1e920]:\n        - text: 本論文での、強化学習に利用するデータセットは、おそらく\n        - link "上述した論文" [ref=s1e921]:\n            - /url: https://arxiv.org/html/2402.03300v3\n        - text: の前半に記載されている数学データセットです。\n    - complementary [ref=s1e922]:\n        - text: "!"\n        - paragraph [ref=s1e925]: 強化学習に利用したデータセットに関しては、本論文で言及されていなかったと思うので憶測です。間違っていたら教えてください。\n    - paragraph [ref=s1e926]: 本論文の主要部分ではないですが、興味深いため紹介します。\n    - paragraph [ref=s1e927]:\n        - img [ref=s1e928]\n        - text: 引用：\n        - \'link "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models" [ref=s1e929]\':\n            - /url: https://arxiv.org/html/2402.03300v3\n    - paragraph [ref=s1e930]:\n        - strong [ref=s1e931]: 1. Train a FastText Model\n        - text: まず初めに、「OpenWebMath」という高品質の数学のウェブテキストのデータセットを初期のシードコーパスとして利用します。\n            このコーパスから50万のデータを正例、 Common\n            Crawlデータセット（大規模なWebページコーパス）から取得した50万のデータを負例として、fastTextモデルを学習します。\n            こうすることにより、fastTextモデルを利用して、Webページから、それが数学コンテンツか、非数学コンテンツかを分類することができます。\n    - paragraph [ref=s1e935]:\n        - strong [ref=s1e936]: 2. Recall Math-Related Webpages From Common Crawl\n    - paragraph [ref=s1e937]: 学習した、fastTextモデルを利用して、Common\n        Crawlデータセットから数学のWebページを取得します。 このとき、低品質な数学コンテンツを除外するために、 fastText\n        モデルによって予測されたスコアをもとにランク付けした、上位のスコアのページのみを取得します。\n    - paragraph [ref=s1e939]:\n        - strong [ref=s1e940]: 3. Discover Math-Related Domains\n    - paragraph [ref=s1e941]: この方法で取得されたWebページは、数学の問題ではありますが、偏りがあります。\n        なぜなら、初期の学習に利用された「OpenWebMath」データセットには、少数のデータしかなく、さらに上位のスコアのWebコーパス（類似度の高いWebコーパス）しか収集していないため、「OpenWebMath」データセットと似たような数学Webページしか、収集できていないからです。\n    - paragraph [ref=s1e943]: そこで、上記の方法で収集したWebページの親ドメインに着目します。\n        そして、親ドメイン内のWebページのうち、10%以上のWebページが、今回の方法で収集されている場合、親ドメイン自体が数学に関連するドメインであると、考えることができます。\n    - complementary [ref=s1e945]:\n        - text: "!"\n        - paragraph [ref=s1e948]: 例えば、「mathhoge.com」の配下のページの10%以上がfastText\n            モデルによって収集されていた場合は、「mathhoge.com」自体を数学コンテンツであると分類します。\n    - paragraph [ref=s1e949]: あとは、この親ドメイン内の、まだ収集されていないWebページから、高品質な数学Webページを収集することで、幅広い数学のコンテンツを収集できそうです。\n    - paragraph [ref=s1e950]:\n        - strong [ref=s1e951]: 4. Annotate Math-Related URL Path From Labelers\n    - paragraph [ref=s1e952]: 分類されたドメイン内の数学コンテンツに対して、「手動」でURLに注釈をつけ、注釈をつけたもののうち、またシードコーパスに収集されていないWebページを、シードコーパスに追加します。\n    - paragraph [ref=s1e953]: この方法を繰り返すことで、高品質かつ多様な数学データセットを収集することができます。\n        著者らはこのデータ収集を4回繰り返したのちに、3,550万の数学Webページ、合計1200億トークンのコーパスを完成させました。\n    - paragraph [ref=s1e955]: 4回の繰り返しの段階で、データのうち98%が3回目までの繰り返しですでに収集されていたことがわかったため、4回で繰り返し作業を終了したとのことです。\n    - heading "（補足）DPO（Direct Preference Optimization）との違い" [level=3]\n      [ref=s1e956]\n    - paragraph [ref=s1e957]: DPO（Direct Preference Optimization）という手法があります。\n        こちらは、強化学習を利用して、LLMに好ましい回答を学習させるRLHF（Reinforcement learning from human\n        feedback）と同様の効果を、強化学習を使わずに学習させることができる手法です。\n    - paragraph [ref=s1e959]: "強化学習をしないため、報酬モデルも状態価値モデルも必要なく、単純に「プロンプト x 」と「好ましい回答 y^+\n        」「好ましくない回答 y^- 」の3つが組となっているデータセット D を用意すれば、あとは \\\\pi_\\\\phi(y^+|x) 、\n        \\\\pi_\\\\phi(y^-|x) 、 \\\\pi_{ref}(y^+|x) 、 \\\\pi_{ref}(y^+|x)\n        を計算して、損失関数に落とし込むことで、学習が可能になります。"\n    - paragraph [ref=s1e976]: このように、「報酬モデルも状態価値モデルも必要ない」の部分が、本論文で提案している「GRPO（Group\n        Relative Policy Optimization）+ルールベース報酬」の手法の利点と近いです。\n    - paragraph [ref=s1e977]: ただし、DPOでは、上述した通り、「プロンプト x 」と「好ましい回答 y^+ 」「好ましくない回答 y^-\n        」の3つが組となっているデータセットが必要です。 逆に言えば、このデータセットの範囲内でしか学習ができません。\n    - paragraph [ref=s1e985]: 一方で、著者らが提案する「GRPO（Group Relative Policy\n        Optimization）+ルールベース報酬」の手法では、数学などの答えが一意に決まり、ルールベースで正解不正解を判断できるデータセット（つまり、数学データセット）であれば、「プロンプト\n        x 」と「回答 y 」の組だけで、学習が可能です。\n    - paragraph [ref=s1e990]:\n        - strong [ref=s1e991]: したがって、DPOよりもはるかに多くのデータを用いて学習ができます。\n        - text: これが、今回の提案手法の利点になりそうです。\n    - heading "著者らの執念" [level=3] [ref=s1e993]\n    - paragraph [ref=s1e994]: 著者らは、LLMを強化学習に適用する上で、計算量的に障害になる報酬モデルと状態価値モデルを利用しない斬新な手法を提案しました。\n        これにより、圧倒的な計算効率にて、強化学習を適用させました。\n    - paragraph [ref=s1e996]: また、似た効果を持つ手法としてDPOという手法があります。 ただし、DPOでは、利用できるデータセットが制限されるため、大規模な学習は困難です。\n    - paragraph [ref=s1e998]: しかしながら著者らの手法は、ただ数学のように一定の推論の結果、答えが一意に定まるような問題を解かせ続ければ、ルールベース報酬による強化学習で、思考機能を強化させることができてしまっています。\n    - paragraph [ref=s1e999]:\n        - strong [ref=s1e1000]: 絶対に、学習データの量を減らさずに、計算量を徹底的に削減してやろうという著者らの執念を感じるとともに、こういう工夫に積み重ねが、高性能と圧倒的に格安なAPI料金の両立を達成しているのだろうなと感じました。\n    - heading "ZeroのSystemプロンプト" [level=2] [ref=s1e1001]\n    - paragraph [ref=s1e1002]: 続いては、少し話が変わり、「DeepSeek-R1-Zero」に与えているsystemプロンプトについて紹介します。\n        このように、systemプロンプトが公開されているというのは、非常に勉強になりますね。とても嬉しいです\n    - paragraph [ref=s1e1004]: systemプロンプトは下記です。\n    - paragraph [ref=s1e1005]: "A conversation between User and Assistant. The user\n        asks a question, and the Assistant solves it. The assistant first thinks\n        about the reasoning process in the mind and then provides the user with\n        the answer. The reasoning process and answer are enclosed within <think>\n        </think> and <answer> </answer> tags, respectively, i.e., <think>\n        reasoning process here </think> <answer> answer here </answer>. User:\n        prompt. Assistant:"\n    - paragraph [ref=s1e1006]: 訳）\n        ユーザーとアシスタントの会話。ユーザーが質問し、アシスタントがそれを解決する。アシスタントは、まず頭の中で推論プロセスを考え、それからユーザに答えを提供します。推論過程と答えはそれぞれ<think>\n        </think>タグと<answer> </answer>タグで囲まれます。つまり、<think> 推論過程はここ</think>\n        <answer>答えはここ</answer>です。ユーザー：プロンプト アシスタント：\n    - paragraph [ref=s1e1008]: 上記部分の「prompt」の部分は、ユーザが入力したプロンプトの内容に書き換えられます。\n        このように、systemプロンプトにおいて<think> </think>タグを利用して、その中で思考をさせる形になっています。\n    - heading "Zeroの「Aha Moment」" [level=2] [ref=s1e1010]\n    - paragraph [ref=s1e1011]: 「Aha Moment」とは、辞書で調べると「〔問題の答えが突然ひらめいたときなどの〕なるほど！と思う瞬間」とのことです。\n    - paragraph [ref=s1e1012]: 実は、DeepSeek-R1-Zeroの出力において、それが発生したそうです。 数学の問題を解く途中で、下記のような出力が見られました。\n    - paragraph [ref=s1e1014]: "Wait, wait. Wait. That’s an aha moment I can flag\n        here. Let’s reevaluate this step-by-step to identify if the correct sum\n        can be · · · We started with the equation:"\n    - paragraph [ref=s1e1017]: 訳） 待て、待て。待ってくれ。これはハッとさせられる瞬間だ。 正しい和が - - - -\n        になるかどうかを確認するために、ステップ・バイ・ステップで再評価してみよう。\n    - paragraph [ref=s1e1020]: これは恐ろしいことです。\n        モデルは途中まで推論を実施しながら、途中で自身の出力結果を見直し、さらにもう一度ステップバイステップで検討をするという、高度な問題解決能力を獲得しています。\n        （自分の考えを見直して、さらに良い戦略を発見する。これができる人間がどれくらいいるのだろうか）\n    - paragraph [ref=s1e1023]: 何度も記載していますが、DeepSeek-R1-Zeroを学習する際に、問題の解き方を明示的にモデルに教えることや、思考部分を強化させるような報酬は与えていません。\n        ただ、数学の問題に正解することで得られる報酬を与えるだけで、モデルが自律的に高度な問題解決戦略を獲得しています。\n        （もちろん、推論の途中で、検算を行うなどのデータセットで学習もさせていません。もしかしたら、DeepSeek-V3-Baseを学習させるときのSFTデータセットには含まれているかもですが。）\n    - paragraph [ref=s1e1026]: これこそが、本論文で何度も述べている、「強化学習の可能性」です。\n        強化学習は（そのほかの学習手法とは異なり）（人間にとって）予想外で、かつ洗練された結果を導き、人工知能が新たなレベルの知性を手にいれるための核となる技術になりそうです。\n    - heading "DeepSeek-R1-Zeroのまとめ" [level=2] [ref=s1e1028]\n    - paragraph [ref=s1e1029]: DeepSeek-R1-ZeroはDeepSeek-V3-Baseに対して、「GRPO+ルールベース報酬」の強化学習「のみ」を適用し、圧倒的な思考能力を獲得したモデルになります。\n    - paragraph [ref=s1e1030]: DeepSeek-R1-Zeroを学習する際に、問題の解き方を明示的にモデルに教えることや、思考部分を強化させるような報酬は与えていません。\n        ただ、数学の問題に正解することで得られる報酬を与えるだけで、モデルが自律的に高度な問題解決戦略を獲得するという、とても興味深く、恐ろしい結果が示されています。\n    - heading "DeepSeek-R1への道" [level=1] [ref=s1e1032]\n    - paragraph [ref=s1e1033]: さて、ここまで、「DeepSeek-R1-Zero」について、たくさん語ってきましたが、とても魅力的でしたよね？\n        あくまで、「DeepSeek-R1-Zero」は「DeepSeek-V3-Base」に対して、強化学習のみを適用させて、強化学習の可能性を探ったモデルでした。\n    - paragraph [ref=s1e1035]:\n        - text: その結果、こんなにも魅力的なモデル（DeepSeek-R1-Zero）ができてしまったら、、、\n        - strong [ref=s1e1037]: SFTも入れて完全体のモデルを作りたくなりますよね？？？\n    - paragraph [ref=s1e1038]: というモチベーションで「DeepSeek-R1」は作られています。\n    - paragraph [ref=s1e1039]: もし、DeepSeek-R1-Zeroの性能がそんなに高くなかったら、今騒がれているDeepSeek-R1は作ろうという発想にすらならなかったと思います。\n    - heading "DeepSeek-R1-Zeroの課題" [level=2] [ref=s1e1040]\n    - paragraph [ref=s1e1041]: さて、非常に高性能で、示唆に富んだDeepSeek-R1-Zeroでしたが、課題はいくつかあります。 それは下記です。\n    - list [ref=s1e1043]:\n        - listitem [ref=s1e1044]: 学習初期は強化学習の学習が不安定で収束に時間がかかる（コールドスタート問題）\n        - listitem [ref=s1e1045]:\n            - text: 思考能力は高いが、言語が混在（日本語で話していたら急に中国語になるなど）し、人間が理解しにくい\n            - list [ref=s1e1046]:\n                - listitem [ref=s1e1047]: 結果、高度な思考タスクには強いが、（ロールプレイなどの）一般的なタスクには使えないという評価\n    - paragraph [ref=s1e1048]: そこで、上記の課題を解決する旅に出ます。\n    - heading "課題対処への道" [level=2] [ref=s1e1049]\n    - paragraph [ref=s1e1050]: DeepSeek-R1-Zeroでは、ベースモデルであるDeepSeek-V3-Baseに対して、そのまま上述した強化学習を用いて学習をしていました。\n    - paragraph [ref=s1e1051]:\n        - text: しかし、もともとDeepSeek-V3-Baseは長考モデルとして学習されたモデルではないため、いくらsystemプロンプトで長考をさせる制限（\n        - code [ref=s1e1052]: <think>\n        - text: タグ）を入れたとしても、強化学習の初期フェーズは不安定な学習になってしまいます。\n            （例えば、「そもそも適切な形式で思考をしない」、「思考形式は合っているが、意味のある思考をしていない」、その結果として「数学の問題になかなか正答できないまま、報酬が得られず、学習が不安定になる」が考えられます）\n    - paragraph [ref=s1e1054]: そこで、学習初期のコールドスタート問題を防ぐために、DeepSeek-V3-Baseに対して、SFT（Supervised\n        Fine-Tuning）を実施します。 そのために著者らは、少量のCoT（Chain-of-Thought）データセットを構築しています。\n    - heading "CoTデータセットの構築+SFT" [level=3] [ref=s1e1056]\n    - paragraph [ref=s1e1057]: データセットの構築方法は下記が一例として提示されております。\n    - list [ref=s1e1058]:\n        - listitem [ref=s1e1059]: いくつかの長いCoT解答を提示し、モデルに長いCoT形式の回答を生成させてデータセットに追加する\n        - listitem [ref=s1e1060]: モデルに対して、自己反省や検証を行いながら、詳細な回答生成を促すようなプロンプトを送信し、その出力をデータセットに追加する\n        - listitem [ref=s1e1061]: DeepSeek-R1-Zeroの出力を、読みやすい形式に整理してからデータセットに追加する\n        - listitem [ref=s1e1062]: モデルの出力結果を人間が精査して、より適切なCoTデータの形に整理してから、データセットに追加する\n    - paragraph [ref=s1e1063]: 上記の方法で、数千件のCoTデータを構築し、そのデータを用いて、DeepSeek-V3-BaseをSFTしています。\n    - heading "CoTデータセットを用いることの利点" [level=3] [ref=s1e1064]\n    - paragraph [ref=s1e1065]: 著者らは、下記の要素において、CoTデータセットを用いることに利点があると考えています。\n    - paragraph [ref=s1e1066]:\n        - strong [ref=s1e1067]: 読みやすさ\n        - text: DeepSeek-R1-Zeroの一番の課題は、その出力結果が人間が理解するのが難しいことです。\n            回答には、複数の言語が混在していたり、マークダウン形式などの読みやすい形式が適切に使われていないことで、重要な情報が何かわかりにくい状態でした。\n    - paragraph [ref=s1e1070]: そこで、今回のデータセットを作成する際には、各回答の末尾に「要約文」を入れるようにすることで、人間が読みやすい形に出力させるようにしています。\n        具体的には下記のような出力形式で、全てのCoTデータセットは作成されています。\n    - code [ref=s1e1075]: "|special_token|<reasoning_process>|special_token|<summary>"\n    - button "クリップボードにコピー" [ref=s1e1076]:\n        - img [ref=s1e1077]\n    - button "右端で折り返す" [ref=s1e1078]:\n        - img [ref=s1e1079]\n    - paragraph [ref=s1e1080]:\n        - code [ref=s1e1081]: <reasoning_process>\n        - text: の部分に、モデルの思考に該当する部分が、\n        - code [ref=s1e1082]: <summary>\n        - text: の部分に最終的な結論の要約が記載されます。\n    - paragraph [ref=s1e1083]: これにより、「DeepSeek-R1-Zero」にあった、重要な情報が何かわかりにくいという人間が理解するのが難しい問題を緩和します。\n        （言語の混在に関しての対処は後述します）\n    - paragraph [ref=s1e1085]:\n        - strong [ref=s1e1086]: ポテンシャル\n        - text: 今回、強化学習のみを実施するのではなく、人間の事前知識に基づいて作成したCoTデータセットでSFTを実施しています。\n            また、SFTと強化学習を複数回反復して、繰り返し学習をしています。\n            その結果、DeepSeek-R1はDeepSeek-R1-Zeroよりも良い結果を得ることができました。\n    - paragraph [ref=s1e1090]: 著者らは、このような反復的な学習こそが、長考モデル（reasoning models）を学習する上で良い方法であると主張しています。\n    - heading "推論思考の強化学習" [level=3] [ref=s1e1091]\n    - paragraph [ref=s1e1092]: CoTデータセットによりSFTしたDeepSeek-V3-Baseに対して、上述した（DeepSeek-R1-Zeroを学習した）強化学習手法を適用します。\n        これにより、モデルの思考能力が大幅に強化されます。\n    - paragraph [ref=s1e1094]: 一方で、「DeepSeek-R1-Zero」には、言語不一致の問題がありました。\n        そこで、DeepSeek-R1-Zeroのルールベース報酬に、「言語一貫性報酬」を追加で導入しています。\n    - paragraph [ref=s1e1096]: この報酬導入で、実は、モデルの思考能力が若干落ちてしまうことが実験的にわかっています。\n        しかしながら、この報酬があることで、ようやく人間が理解しやすい文章を生成してくれるようになります。\n    - heading "さらなる追加のSFT" [level=3] [ref=s1e1098]\n    - paragraph [ref=s1e1099]: 上記の強化学習が収束したら、さらにデータを追加してSFTをやり直します。\n        ここでは、初期段階の、CoTに重きを置いた、コールドスタートを解消するための学習データだけにとどまらず、ライティング、ロールプレイといった汎用タスクの性能を向上させるため、他ドメインからのデータも合わせて収集します。\n    - heading "Reasoning Data" [level=4] [ref=s1e1101]\n    - paragraph [ref=s1e1102]: まずは、前段階と同様に思考能力を向上させるために、CoT学習データ（Reasoning Data）を取得します。\n    - paragraph [ref=s1e1103]: 取得方法は以下です。\n    - list [ref=s1e1104]:\n        - listitem [ref=s1e1105]:\n            - text: 強化学習済みモデルの出力から、適切なものを収集して学習データに追加\n            - list [ref=s1e1106]:\n                - listitem [ref=s1e1107]: 前段の強化学習では、ルールベースの報酬モデルを利用していたので、それが使える問題（数学など）のみを学習データとしていたが、次の強化学習では報酬モデルを作成するため、もっと広い範囲でCoTができるデータも学習データに追加\n                - listitem [ref=s1e1108]: モデルの出力はまだ読みにくいところがあるため、さまざまフィルタリング\n    - paragraph [ref=s1e1109]: この結果、合計で600kのReasoning Data（学習データ）を収集しています。\n    - heading "Non-Reasoning Data" [level=4] [ref=s1e1110]\n    - paragraph [ref=s1e1111]: ライティングやQ＆A、翻訳などのNon-Reasoning\n        Dataについては、DeepSeek-V3のSFTデータセットの一部を再利用して、学習データを収集しています。 上記のReasoning\n        Data以外のタスク（Non-Reasoning\n        Data）に関しては、DeepSeek-V3によって簡易的なCoTを実施して、回答を生成し、学習データとします。\n        ただし、本当に簡単なタスク（あいさつとか）に関しては、CoTを学習データに追加しません。\n    - paragraph [ref=s1e1114]: こうすることで、合計200kのトレーニングサンプルを最終的に収集したそうです。\n    - paragraph [ref=s1e1115]: 上記の合計800kサンプルの学習データを利用して、DeepSeek-V3-Base（初回強化学習済み）を2epoch分SFTを実施しています。\n    - heading "最後の強化学習" [level=3] [ref=s1e1116]\n    - paragraph [ref=s1e1117]: 最後に、モデルの思考能力をさらに磨くと同時に、モデルの無害性などを向上させることを目的に、強化学習を実施します。\n    - paragraph [ref=s1e1118]: 思考タスクの強化学習に関しては、これまで説明したのと同様に、数学、コード、論理的推論の領域でルールベース報酬を利用して強化学習をします。\n        一方で、思考タスク以外の一般的なデータに対しては、ルールベースで回答の良し悪しを判断することはできないため、報酬モデルを別途DeepSeek-V3パイプラインをベースに作成しています。\n    - heading "DeepSeek-R1のまとめ" [level=2] [ref=s1e1120]\n    - paragraph [ref=s1e1121]: 以上より、DeepSeek-R1では、DeepSeek-V3-Baseに対して、SFTと強化学習を2回実施することにより構築されています。\n    - paragraph [ref=s1e1122]: SFTにより、強化学習のコールドスタート問題の解消と、モデル出力のユーザフレンドリー性の向上、一般的なタスクに対する性能向上に貢献しています。\n    - heading "結果" [level=1] [ref=s1e1123]\n    - paragraph [ref=s1e1124]: 結果に関しては、他の方がさまざま検証していたり、話題になっているので詳しくは書きません。\n        ただ、DeepSeek-R1は、現在のオープンソースモデルの中では、最も高い性能と言ってよく、また、Open\n        AIのo1モデルにも匹敵する性能だと言われています。\n    - heading "蒸留" [level=1] [ref=s1e1126]\n    - paragraph [ref=s1e1127]: 最後に、蒸留モデルについての解説します。（論文でも言及されています）\n        DeepSeek-R1は671Bものモデルパラメータを持つため、一般的な家庭用PCで動作させるのは難しいです。\n    - paragraph [ref=s1e1129]: そこで、より小さなモデルパラメータのモデルに、DeepSeek-R1のような思考能力を与えるために、蒸留を実施しています。\n        （公式が、個人の利用者のことも考えて、家庭用PCで動作する思考モデルを開発してくれるのは非常にありがたいですね）\n    - paragraph [ref=s1e1131]: 具体的には、Qwen2.5-Math-1.5B、Qwen2.5-Math-7B、Qwen2.5-14B、Qwen2.5-32B、Llama-3.1-8B、Llama-3.3-70B-Instructに対して、DeepSeek-R1から取得した800kのトレーニングサンプルを利用して、直接FineTuningする形で、蒸留を実施しています。\n    - paragraph [ref=s1e1132]: 蒸留モデルに対しては、SFTのみを実施し、後段の強化学習は実施しておらず、蒸留モデルに対する追加の強化学習の探究は、著者らが実施する予定はないそうです。\n        （著者らは、強化学習を追加で実施することにより性能が上がると思っているが、その探究は、別のコミュニティに任せると記述しています）\n    - heading "蒸留 vs 強化学習" [level=2] [ref=s1e1134]\n    - paragraph [ref=s1e1135]: 蒸留後の強化学習に関しては、著者らは実験していない、と記述していました。\n        では、小型モデルに対して、蒸留を実施する前に、強化学習を実施したらどうなるのでしょうか？\n        DeepSeek-V3-Baseのように、大幅に思考能力が強化されて、性能向上が期待できるのでしょうか？\n    - paragraph [ref=s1e1138]: それに関しては、著者らが実験してくれています。\n    - paragraph [ref=s1e1139]: ここでは、「Qwen-32B-Base」モデルを利用して、10kステップ以上の大規模強化学習を実施し、「DeepSeek-R1-Zero-Qwen-32B」を構築しました。\n        一方で、「Qwen-32B-Base」モデルに対して、蒸留を実施したモデルを、「DeepSeek-R1Distill-Qwen-32B」モデルとして、この二つのモデルを比較します。\n    - paragraph [ref=s1e1141]: その結果を比較すると、小型モデルに対する強化学習を適用した「DeepSeek-R1-Zero-Qwen-32B」よりも、小型モデルに蒸留を適用した「DeepSeek-R1Distill-Qwen-32B」モデルの方がはるかに高い性能でした。\n    - paragraph [ref=s1e1142]:\n        - strong [ref=s1e1143]: 以下は私の主観ですが、\n        - text: 以上の結果から、\n        - strong [ref=s1e1146]: 強化学習による思考能力の向上は、安易に小型モデルに試していい戦略でない\n        - text: ということが示唆されていると思います。\n    - paragraph [ref=s1e1148]: 強化学習による思考能力の大幅な強化は、おそらくですが「DeepSeek-V3」という元々大規模かつ性能の高いモデルであったからこそ、達成されたのだと思います。\n    - paragraph [ref=s1e1149]: DeepSeek-R1-Zeroの課題でも説明した通り、今回のルールベース報酬では、正しい数学の解答を生成できないと永遠に報酬が獲得できません。\n        小型モデルでは、このコールドスタートの問題が、より致命的なレベルで発生するのだと思います。\n    - paragraph [ref=s1e1151]: したがって、思考モデルを作りたいからといって、小型モデルに対して、安易にルールベース強化学習を試すのはおすすめしません。\n        しかしながら、コールドスタート問題を解決できる戦略があるのであれば、有効な手段になると思われます。\n        （例えば、DeepSeek-R1で蒸留済みの小型モデルを利用する、最初から数学に強い小型モデルで強化学習を試すなどです。重要なのは強化学習のコールドスタートを解消できるかどうかだと思われます。）\n    - paragraph [ref=s1e1154]:\n        - text: （追記） また、同様の実験を小型モデルに対して試したグループもいるようです。 そちらの実験レポートが下記になります。\n        - iframe [ref=s1e1158]: <could not take iframe snapshot>\n        - text: （DeepSeek-R1-Zeroと同様に、「Aha Moment」が見られることもわかっており、非常に興味深いです）\n    - paragraph [ref=s1e1159]: ただ、こちらのレポートにおいても、ベースモデルとしては数学に特化したモデルを利用しています。\n        このことからも、ある程度の数学能力は必要なのだなと思います。\n    - heading "今後の展望" [level=1] [ref=s1e1161]\n    - paragraph [ref=s1e1162]: DeepSeek-R1は、さまざまなタスクにおいて、OpenAI\n        o1に匹敵する性能を達成した、驚異的なオープンソースモデルですが、苦手としている部分もあります。 それが今後の展望に記載されています。\n        （今後の展望を見れば、DeepSeek-R1が苦手な部分を知ることができます）\n    - paragraph [ref=s1e1165]: それは以下の項目です。\n    - list [ref=s1e1166]:\n        - listitem [ref=s1e1167]:\n            - strong [ref=s1e1168]: 一般的な能力\n            - list [ref=s1e1169]:\n                - listitem [ref=s1e1170]: 関数呼び出しやマルチターン対話、複雑なロールプレイング。、json出力において、DeepSeek-V3に及ばない\n        - listitem [ref=s1e1171]:\n            - strong [ref=s1e1172]: 言語の混在\n            - list [ref=s1e1173]:\n                - listitem [ref=s1e1174]: 現在、DeepSeek-R1は英語と中国語に最適化されているため、そのほかの言語を利用した場合、言語の混在が発生する可能性がある\n        - listitem [ref=s1e1175]:\n            - strong [ref=s1e1176]: プロンプトエンジニアリング\n            - list [ref=s1e1177]:\n                - listitem [ref=s1e1178]: DeepSeek-R1はプロンプトに非常に敏感で、特にFewーshotプロンプトを利用すると、パフォーマンスが低下する。\n                - listitem [ref=s1e1179]: 基本的には、問題を直接シンプルに記述し、例を提示しない（Zero-shotプロンプト）を利用することを推奨している\n        - listitem [ref=s1e1180]:\n            - strong [ref=s1e1181]: ソフトウェアエンジニアリングタスク\n            - list [ref=s1e1182]:\n                - listitem [ref=s1e1183]: コード生成やコード保管などのソフトウェアエンジニアリングタスクは、評価に時間がかかるため、強化学習の効率が悪くなることから、現時点では強化学習において、このようなタスクは学習していない。\n                    大々的には学習しておらず、一部のテスト可能な範囲でしか学習できていない。\n                - listitem [ref=s1e1185]: 結果として、このタスクの性能は、DeepSeek-V3からほとんど改善していない。\n    - complementary [ref=s1e1186]:\n        - text: "!"\n        - paragraph [ref=s1e1189]: （追記） masaishiさんのコメントに基づき修正させていただきました。 私の読み間違いで誤解を与えてしまい申し訳ございません。\n        - blockquote [ref=s1e1192]:\n            - paragraph [ref=s1e1193]: 2.2.2. Reward Modeling ・・・ ・Accuracy\n                rewards:・・・Similarly, for LeetCode problems, a compiler can be\n                used to generate feedback based on predefined test cases.\n        - paragraph [ref=s1e1196]: と記載のあるように、全くコード生成などを学習していないわけではなく、一部の、簡単にテストができる問題のみ学習しているようです。\n            ただ、そういう簡易的なタスクだけしか学習できていないため、性能向上は小さめということだと思われます。\n    - paragraph [ref=s1e1198]: 上記の内容が、現時点での「DeepSeek-R1」の課題です。\n        ですが、今後のアップデートで改善していくとも記載されているため、現時点で不満があったとしても、上記に記載されている項目であれば、今後改善する可能性が高いです。\n    - heading "まとめ" [level=1] [ref=s1e1200]\n    - paragraph [ref=s1e1201]: ここまで読んでくださった方、本当にお疲れ様でした。\n        （これでも説明はだいぶ短くした方なんです・・・本当は、もっと強化学習の部分とかを数式をもとに説明したかったです）\n    - paragraph [ref=s1e1203]: DeepSeek-R1の論文いかがでしたか？\n        私にとっては、非常に勉強になるような示唆や知見に溢れており、こんな論文を公開してくれて、かつオープンソースのモデルとしても公開してくれる「DeepSeek-AI」には感謝しかありません。\n    - paragraph [ref=s1e1205]: 本記事が、皆様の理解の助けになり、一人でも多くの方が原論文に興味を持って、読んでくれましたら幸いです。\n        ここまで読んでくださってありがとうございました！\n    - heading "参考文献" [level=1] [ref=s1e1207]\n    - paragraph [ref=s1e1208]: 本論文は、論文の中では読みやすい方だとは思いますが、強化学習に関しては、一定程度の基礎知識は要求されます。\n        そこで、本論文を読む上で必要な知識を収集できるおすすめの書籍に関して記載します。 （少なくとも私はこの書籍で学習しました）\n    - paragraph [ref=s1e1211]: （書籍のリンクはamazonアフィリエイトリンクになります）\n    - heading "強化学習" [level=4] [ref=s1e1212]\n    - paragraph [ref=s1e1213]: 強化学習に関しておすすめの本は、下記の本たちです。\n    - paragraph [ref=s1e1214]:\n        - link "ゼロから作るDeep Learning ❹ ―強化学習編" [ref=s1e1215]:\n            - /url: https://amzn.to/40qFuNg\n        - text: まずは、定番の本です。\n            強化学習に関して、本当に最初の部分（マルコフ決定過程とは何か）から、非常に詳細かつわかりやすく解説されており、深層学習を利用して、どのように強化学習をすることできるのかまで、しっかり書かれているためおすすめです。\n            また、コードと理論のバランスもちょうどよく、理系大学生とかであれば問題なく読み進められると思います。\n            特に、強化学習の基礎の部分が、詳細かつ分かりやすく書かれているため、一冊目におすすめです。\n    - paragraph [ref=s1e1220]:\n        - link "機械学習スタートアップシリーズ Ｐｙｔｈｏｎで学ぶ強化学習 ［改訂第２版］ 入門から実践まで (ＫＳ情報科学専門書)" [ref=s1e1221]:\n            - /url: https://amzn.to/4joKbjg\n    - paragraph [ref=s1e1223]: 普段からpythonを使っており、数式よりも実装に興味がある方にとっては、こちらの書籍もおすすめです。\n        こちらの書籍は、強化学習の基礎よりは、少し応用というか、実用を意識した書籍になっており、実務でよく使われるモデルなどに関しても、詳しく説明されています。\n        強化学習の基礎はわかっている人や、もっと実装をベースに学びたいというかたにおすすめです。\n    - paragraph [ref=s1e1226]:\n        - link "強化学習 (機械学習プロフェッショナルシリーズ)" [ref=s1e1227]:\n            - /url: https://amzn.to/40NX5QM\n        - link "強化学習から信頼できる意思決定へ (AI/データサイエンスライブラリ“基礎から応用へ” 5)" [ref=s1e1229]:\n            - /url: https://amzn.to/4jtFq8d\n    - paragraph [ref=s1e1230]: 名著です。\n        どちらも、理論説明の部分が非常に面白く、正直難易度は高いですが、強化学習の理論をちゃんと理解したいという方におすすめの書籍です。\n        下の本に関しては、2024年の本ということもあり、近年の強化学習の実世界での応用と、理論的な定式化の架け橋になっているような本になっており、非常に面白いです。\n    - heading "AlphaGO Zero" [level=4] [ref=s1e1233]\n    - paragraph [ref=s1e1234]:\n        - link "AlphaZero 深層学習・強化学習・探索 人工知能プログラミング実践入門" [ref=s1e1235]:\n            - /url: https://amzn.to/3Cdnuy1\n    - paragraph [ref=s1e1236]: だいぶ昔に購入させていただいた本ですが、まさか2025年に活躍することがあろうとは。\n        という感じですが、こちらの書籍も参考にさせていただきました。 Alpha\n        Zeroを作る上で核となる技術を余すところなく解説されており、私は非常に勉強になりました。 Alpha\n        Zeroに興味がある方は、ぜひ一度ご一読ください。\n    - heading "LLM、RLHF, DPO" [level=4] [ref=s1e1240]\n    - paragraph [ref=s1e1241]:\n        - link "大規模言語モデル入門Ⅱ〜生成型LLMの実装と評価" [ref=s1e1242]:\n            - /url: https://amzn.to/3Cs3Uhn\n        - text: よく紹介させていただいておりますが、こちらの書籍は、LLMのファインチューニングから、RLHF、RAG、分散学習にかけて、本当に幅広く解説されており、いつも参考にさせていただいております。\n            今回のDPOやRLHFの解説部分でも、とても活躍してくれました。\n            LLMを取り扱っている方は、とりあえず買っておいても損はないと思います。\n            さまざまな章、ページで読めば読むほど新しい発見が得られる、スルメのような本だなといつも思っております。\n    - button "いいね" [ref=s1e1249]:\n        - img [ref=s1e1250]\n    - text: "921"\n    - button "ブックマーク" [ref=s1e1266]:\n        - img [ref=s1e1267]\n    - text: "344"\n    - button "その他の操作" [ref=s1e1272]:\n        - img [ref=s1e1273]\n    - tooltip "X(Twitter)にポスト" [ref=s1e1276]:\n        - img [ref=s1e1277]\n        - text: X(Twitter)にポスト\n    - tooltip "Facebookに投稿" [ref=s1e1283]:\n        - img [ref=s1e1284]\n        - text: Facebookに投稿\n    - tooltip "はてなブックマークに登録" [ref=s1e1290]:\n        - img [ref=s1e1291]\n        - text: はてなブックマークに登録\n    - complementary [ref=s1e1294]:\n        - link "asap" [ref=s1e1296]:\n            - /url: /asap\n            - img "asap" [ref=s1e1297]\n        - link "asap" [ref=s1e1299]:\n            - /url: /asap\n        - paragraph [ref=s1e1301]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n        - button "フォロー" [ref=s1e1305]\n        - tooltip "@personabb" [ref=s1e1306]:\n            - img "GitHub" [ref=s1e1307]\n            - text: "@personabb"\n        - tooltip "@asap2650" [ref=s1e1309]:\n            - img "X(Twitter)" [ref=s1e1310]\n            - text: "@asap2650"\n        - tooltip "yard.tips" [ref=s1e1316]:\n            - img "リンク" [ref=s1e1317]\n            - text: yard.tips\n    - complementary [ref=s1e1320]:\n        - text: バッジを贈って著者を応援しよう\n        - paragraph [ref=s1e1324]: バッジを受け取った著者にはZennから現金やAmazonギフトカードが還元されます。\n        - button "バッジを贈る" [ref=s1e1326]\n        - img [ref=s1e1327]\n    - text: この記事に贈られたバッジ\n    - img "Thank you" [ref=s1e1338]\n    - img [ref=s1e1340]\n    - img "参考になった" [ref=s1e1352]\n    - img [ref=s1e1354]\n    - img "もっと読みたい" [ref=s1e1363]\n    - img [ref=s1e1365]\n    - img "参考になった" [ref=s1e1375]\n    - img [ref=s1e1377]\n    - img "もっと読みたい" [ref=s1e1386]\n    - img [ref=s1e1388]\n    - heading "Discussion" [level=3] [ref=s1e1400]\n    - article [ref=s1e1403]:\n        - link "はまなすなぎさ" [ref=s1e1405]:\n            - /url: /nhamanasu\n            - img "はまなすなぎさ" [ref=s1e1406]\n        - link "はまなすなぎさ" [ref=s1e1407]:\n            - /url: /nhamanasu\n        - link "2025年1月28日 8時33分" [ref=s1e1408]:\n            - /url: "#comment-f230c9d295b3f5"\n            - tooltip "2025年1月28日 8時33分" [ref=s1e1409]: 2ヶ月前 2025年1月28日 8時33分\n        - button "メニューを開く" [ref=s1e1411]:\n            - img [ref=s1e1412]\n        - paragraph [ref=s1e1417]: 非常にためになる骨太な記事、ありがとうございます！とても勉強になります。\n        - paragraph [ref=s1e1418]: ちょっとした誤字かも？の箇所なのですが、冒頭の「強化学習手法「DRPO」すごい！」の部分は「GRPO」ではないかなと思いました。ご報告まで。\n        - button "いいね" [ref=s1e1421]:\n            - img [ref=s1e1422]\n        - text: "1"\n        - button "コメント 1" [ref=s1e1437]:\n            - img "コメント" [ref=s1e1438]\n            - text: "1"\n        - article [ref=s1e1442]:\n            - link "asap" [ref=s1e1444]:\n                - /url: /asap\n                - img "asap" [ref=s1e1445]\n            - link "asap" [ref=s1e1446]:\n                - /url: /asap\n            - link "2025年1月28日 10時23分" [ref=s1e1447]:\n                - /url: "#comment-41cdebd92d8d44"\n                - tooltip "2025年1月28日 10時23分" [ref=s1e1448]: 2ヶ月前 2025年1月28日 10時23分\n            - button "メニューを開く" [ref=s1e1450]:\n                - img [ref=s1e1451]\n            - paragraph [ref=s1e1456]: 読んでくださりありがとうございます！ おっしゃる通りです！修正させていただきました！\n            - button "いいね" [ref=s1e1460]:\n                - img [ref=s1e1461]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1477]\n    - article [ref=s1e1479]:\n        - link "bang" [ref=s1e1481]:\n            - /url: /zenmnm\n            - img "bang" [ref=s1e1482]\n        - link "bang" [ref=s1e1483]:\n            - /url: /zenmnm\n        - link "2025年1月28日 13時46分" [ref=s1e1484]:\n            - /url: "#comment-d0aac9375fbbf7"\n            - tooltip "2025年1月28日 13時46分" [ref=s1e1485]: 2ヶ月前 2025年1月28日 13時46分\n        - button "メニューを開く" [ref=s1e1487]:\n            - img [ref=s1e1488]\n        - paragraph [ref=s1e1493]: AIについてはまだ詳しくありませんが、興味はあります。同じように古いモデルを使って新しいモデルを作るのですが、蒸留方法の代わりにモデルを組み合わせることは可能でしょうか?sakana\n            aiのようにです\n        - button "いいね" [ref=s1e1496]:\n            - img [ref=s1e1497]\n        - text: "1"\n        - button "コメント 2" [ref=s1e1512]:\n            - img "コメント" [ref=s1e1513]\n            - text: "2"\n        - article [ref=s1e1517]:\n            - link "asap" [ref=s1e1519]:\n                - /url: /asap\n                - img "asap" [ref=s1e1520]\n            - link "asap" [ref=s1e1521]:\n                - /url: /asap\n            - link "2025年1月28日 15時33分" [ref=s1e1522]:\n                - /url: "#comment-27e79c6ee51f2a"\n                - tooltip "2025年1月28日 15時33分" [ref=s1e1523]: 2ヶ月前 2025年1月28日 15時33分\n            - button "メニューを開く" [ref=s1e1525]:\n                - img [ref=s1e1526]\n            - paragraph [ref=s1e1531]: モデルマージに関してはあまり詳しくないですが、基本的には同じアーキテクチャのモデルを利用することになると思います。したがってDeepSeek-R1によって蒸留されたLlamaモデルとかであれば、他のLlamaモデル（同じモデルアーキテクチャ）とモデルマージすることで、推論機能を付け加えたり、日本語機能を強化したりすることが期待できると思います。\n            - button "いいね" [ref=s1e1534]:\n                - img [ref=s1e1535]\n        - article [ref=s1e1549]:\n            - link "bang" [ref=s1e1551]:\n                - /url: /zenmnm\n                - img "bang" [ref=s1e1552]\n            - link "bang" [ref=s1e1553]:\n                - /url: /zenmnm\n            - link "2025年1月28日 16時42分" [ref=s1e1554]:\n                - /url: "#comment-0221309240798f"\n                - tooltip "2025年1月28日 16時42分" [ref=s1e1555]: 2ヶ月前 2025年1月28日 16時42分\n            - button "メニューを開く" [ref=s1e1557]:\n                - img [ref=s1e1558]\n            - paragraph [ref=s1e1563]: お返事ありがとうございます。私はまだ初心者なので、これは私の軽い考えです。今では研究のプロセスごとにいくつかの方法があり、それらを組み合わせたり微調整したりすれば、たくさんのモデルができるようになっているでしょう。\n            - button "いいね" [ref=s1e1566]:\n                - img [ref=s1e1567]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1583]\n    - article [ref=s1e1585]:\n        - link "HWSeungwoo" [ref=s1e1587]:\n            - /url: /syoyuu96\n            - img "HWSeungwoo" [ref=s1e1588]\n        - link "HWSeungwoo" [ref=s1e1589]:\n            - /url: /syoyuu96\n        - link "2025年1月28日 19時28分" [ref=s1e1590]:\n            - /url: "#comment-a07230ddbdaebe"\n            - tooltip "2025年1月28日 19時28分" [ref=s1e1591]: 2ヶ月前 2025年1月28日 19時28分\n        - button "メニューを開く" [ref=s1e1593]:\n            - img [ref=s1e1594]\n        - paragraph [ref=s1e1599]: Deep\n            Seekに関する記事やニュースはたくさん目にしますが、ここまで深掘りされた内容には驚きました。とても勉強になりました。ありがとうございます！\n        - button "いいね" [ref=s1e1602]:\n            - img [ref=s1e1603]\n        - text: "1"\n        - button "コメント 1" [ref=s1e1618]:\n            - img "コメント" [ref=s1e1619]\n            - text: "1"\n        - article [ref=s1e1623]:\n            - link "asap" [ref=s1e1625]:\n                - /url: /asap\n                - img "asap" [ref=s1e1626]\n            - link "asap" [ref=s1e1627]:\n                - /url: /asap\n            - link "2025年1月28日 19時32分" [ref=s1e1628]:\n                - /url: "#comment-1e1e3abc86df00"\n                - tooltip "2025年1月28日 19時32分" [ref=s1e1629]: 2ヶ月前 2025年1月28日 19時32分\n            - button "メニューを開く" [ref=s1e1631]:\n                - img [ref=s1e1632]\n            - paragraph [ref=s1e1637]: 読んでくださってありがとうございます！ コメント嬉しいです！ありがとうございます！\n            - button "いいね" [ref=s1e1641]:\n                - img [ref=s1e1642]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1658]\n    - article [ref=s1e1660]:\n        - link "masaishi" [ref=s1e1662]:\n            - /url: /masaishi\n            - img "masaishi" [ref=s1e1663]\n        - link "masaishi" [ref=s1e1664]:\n            - /url: /masaishi\n        - link "2025年1月29日 16時14分" [ref=s1e1665]:\n            - /url: "#comment-a0d75bd96cadfc"\n            - tooltip "2025年1月29日 16時14分" [ref=s1e1666]: 2ヶ月前 2025年1月29日 16時14分\n        - button "メニューを開く" [ref=s1e1668]:\n            - img [ref=s1e1669]\n        - paragraph [ref=s1e1674]: 素晴らしい記事をありがとうございます！\n            GRPOなどの強化学習手法に関する解説も大変勉強になりました。この記事をきっかけに、DeepSeek-R1の論文にも興味を持ち、読んでみようと思えました。\n        - paragraph [ref=s1e1676]: 一点だけ気になったことがあり、もし私の理解が誤っていたら申し訳ありません。\n            "コード生成やコード保管などのソフトウェアエンジニアリングタスク"の強化学習に用いられていないとありますが、論文の\n            2.2.2（Reward Modeling）の\n        - blockquote [ref=s1e1678]:\n            - paragraph [ref=s1e1679]: For LeetCode problems, a compiler can be used to\n                generate feedback based on predefined test cases.\n        - paragraph [ref=s1e1680]: という記述を見ると、少なくともアルゴリズム問題のコード生成には強化学習が用いられているのかなと思います。\n        - paragraph [ref=s1e1681]: ソフトウェアエンジニアリングタスクは強化学習していませんが、コード生成自体は行っているような気がするので、注釈などで少し補足していただけると、読者としては誤解が減って助かるかもしれません。しかし、かなり細かい点ですので、最終的には著者さまの判断にお任せいたします。\n        - button "いいね" [ref=s1e1684]:\n            - img [ref=s1e1685]\n        - text: "2"\n        - button "コメント 2" [ref=s1e1700]:\n            - img "コメント" [ref=s1e1701]\n            - text: "2"\n        - article [ref=s1e1705]:\n            - link "asap" [ref=s1e1707]:\n                - /url: /asap\n                - img "asap" [ref=s1e1708]\n            - link "asap" [ref=s1e1709]:\n                - /url: /asap\n            - link "2025年1月29日 16時37分" [ref=s1e1710]:\n                - /url: "#comment-afb254c15106b7"\n                - tooltip "2025年1月29日 16時37分" [ref=s1e1711]: 2ヶ月前 2025年1月29日 16時37分\n            - button "メニューを開く" [ref=s1e1713]:\n                - img [ref=s1e1714]\n            - paragraph [ref=s1e1719]: コメントありがとうございます！ おっしゃる通りでした！読み間違いです。ご指摘助かります。\n            - blockquote [ref=s1e1721]:\n                - paragraph [ref=s1e1722]: Due to the long evaluation times, which impact the\n                    efficiency of the RL process, large-scale RL has not been\n                    applied extensively in software engineering tasks.\n            - paragraph [ref=s1e1723]: この部分の文章をもとに記載していましたが、大々的には適用されていないだけで、一部は学習に入っているっぽいですね。\n            - paragraph [ref=s1e1724]: 助かります！修正もこれからさせていただきます\n            - button "いいね" [ref=s1e1727]:\n                - img [ref=s1e1728]\n            - text: "2"\n        - article [ref=s1e1743]:\n            - link "masaishi" [ref=s1e1745]:\n                - /url: /masaishi\n                - img "masaishi" [ref=s1e1746]\n            - link "masaishi" [ref=s1e1747]:\n                - /url: /masaishi\n            - link "2025年1月29日 19時5分" [ref=s1e1748]:\n                - /url: "#comment-debff7de1aad74"\n                - tooltip "2025年1月29日 19時5分" [ref=s1e1749]: 2ヶ月前 2025年1月29日 19時5分\n            - button "メニューを開く" [ref=s1e1751]:\n                - img [ref=s1e1752]\n            - paragraph [ref=s1e1757]: ご丁寧にありがとうございます！\n            - button "いいね" [ref=s1e1760]:\n                - img [ref=s1e1761]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1777]\n    - article [ref=s1e1779]:\n        - link "RyotoTanaka" [ref=s1e1781]:\n            - /url: /ryototanaka\n            - img "RyotoTanaka" [ref=s1e1782]\n        - link "RyotoTanaka" [ref=s1e1783]:\n            - /url: /ryototanaka\n        - link "2025年1月30日 8時0分" [ref=s1e1784]:\n            - /url: "#comment-4ae639fe8d1892"\n            - tooltip "2025年1月30日 8時0分" [ref=s1e1785]: 2ヶ月前 2025年1月30日 8時0分\n        - button "メニューを開く" [ref=s1e1787]:\n            - img [ref=s1e1788]\n        - paragraph [ref=s1e1793]: 読み応えがありました。素晴らしい記事をありがとうございます！ 些細な事なのですが、DPOは Direct\n            Preference Optimization ではないでしょうか。\n        - button "いいね" [ref=s1e1797]:\n            - img [ref=s1e1798]\n        - text: "1"\n        - button "コメント 1" [ref=s1e1813]:\n            - img "コメント" [ref=s1e1814]\n            - text: "1"\n        - article [ref=s1e1818]:\n            - link "asap" [ref=s1e1820]:\n                - /url: /asap\n                - img "asap" [ref=s1e1821]\n            - link "asap" [ref=s1e1822]:\n                - /url: /asap\n            - link "2025年1月30日 13時15分" [ref=s1e1823]:\n                - /url: "#comment-ec7e829404f7bc"\n                - tooltip "2025年1月30日 13時15分" [ref=s1e1824]: 2ヶ月前 2025年1月30日 13時15分\n            - button "メニューを開く" [ref=s1e1826]:\n                - img [ref=s1e1827]\n            - paragraph [ref=s1e1832]: ほんとですね！間違えて入力してました ありがとうございます！\n            - button "いいね" [ref=s1e1836]:\n                - img [ref=s1e1837]\n            - text: "1"\n        - button "返信を追加" [ref=s1e1853]\n    - text: ログインするとコメントできます\n    - button "Login" [ref=s1e1857]\n    - complementary [ref=s1e1858]:\n        - link "asap" [ref=s1e1863]:\n            - /url: /asap\n            - img "asap" [ref=s1e1864]\n        - link "asap" [ref=s1e1866]:\n            - /url: /asap\n        - button "フォロー" [ref=s1e1868]\n        - link "@personabb" [ref=s1e1869]:\n            - /url: https://github.com/personabb\n            - tooltip "@personabb" [ref=s1e1870]:\n                - img "GitHub" [ref=s1e1871]\n                - text: "@personabb"\n        - link "@asap2650" [ref=s1e1873]:\n            - /url: https://twitter.com/asap2650\n            - tooltip "@asap2650" [ref=s1e1874]:\n                - img "X(Twitter)" [ref=s1e1875]\n                - text: "@asap2650"\n        - link "yard.tips" [ref=s1e1881]:\n            - /url: https://yard.tips/asap2650\n            - tooltip "yard.tips" [ref=s1e1882]:\n                - img "リンク" [ref=s1e1883]\n                - text: yard.tips\n        - paragraph [ref=s1e1886]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n        - button "バッジを贈る" [ref=s1e1889]\n        - link "バッジを贈るとは" [ref=s1e1891]:\n            - /url: /faq#badges\n            - text: バッジを贈るとは\n            - img [ref=s1e1892]\n        - text: 目次\n        - list [ref=s1e1901]:\n            - listitem [ref=s1e1902]:\n                - link "はじめに" [ref=s1e1903]:\n                    - /url: "#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB"\n            - listitem [ref=s1e1904]:\n                - link "本論文の興味深いところ" [ref=s1e1905]:\n                    - /url: "#%E6%9C%AC%E8%AB%96%E6%96%87%E3%81%AE%E8%88%88%E5%91%B3%E6%B7%B1%E3%81%8\\\n                        4%E3%81%A8%E3%81%93%E3%82%8D"\n            - listitem [ref=s1e1906]:\n                - link "本論文の流れ" [ref=s1e1907]:\n                    - /url: "#%E6%9C%AC%E8%AB%96%E6%96%87%E3%81%AE%E6%B5%81%E3%82%8C"\n                - list [ref=s1e1908]:\n                    - listitem [ref=s1e1909]:\n                        - link "前提のモデル構造" [ref=s1e1910]:\n                            - /url: "#%E5%89%8D%E6%8F%90%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E6%A7%8B%E9%80%A\\\n                                0"\n                    - listitem [ref=s1e1911]:\n                        - link "DeepSeek-R1-Zeroを強化学習を用いて作る" [ref=s1e1912]:\n                            - /url: "#deepseek-r1-zero%E3%82%92%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E3%82%92%\\\n                                E7%94%A8%E3%81%84%E3%81%A6%E4%BD%9C%E3%82%8B"\n                    - listitem [ref=s1e1913]:\n                        - link "DeepSeek-R1をSFT+強化学習を用いて作る" [ref=s1e1914]:\n                            - /url: "#deepseek-r1%E3%82%92sft%2B%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E3%82%92\\\n                                %E7%94%A8%E3%81%84%E3%81%A6%E4%BD%9C%E3%82%8B"\n                    - listitem [ref=s1e1915]:\n                        - link "小型モデルへの蒸留を試す" [ref=s1e1916]:\n                            - /url: "#%E5%B0%8F%E5%9E%8B%E3%83%A2%E3%83%87%E3%83%AB%E3%81%B8%E3%81%AE%E8%92%B\\\n                                8%E7%95%99%E3%82%92%E8%A9%A6%E3%81%99"\n            - listitem [ref=s1e1917]:\n                - link "DeepSeek-R1-Zeroを作るまで" [ref=s1e1918]:\n                    - /url: "#deepseek-r1-zero%E3%82%92%E4%BD%9C%E3%82%8B%E3%81%BE%E3%81%A7"\n                - list [ref=s1e1919]:\n                    - listitem [ref=s1e1920]:\n                        - link "元々のアーキテクチャ" [ref=s1e1921]:\n                            - /url: "#%E5%85%83%E3%80%85%E3%81%AE%E3%82%A2%E3%83%BC%E3%82%AD%E3%83%86%E3%82%A\\\n                                F%E3%83%81%E3%83%A3"\n                    - listitem [ref=s1e1922]:\n                        - link "すごいぞ！強化学習" [ref=s1e1923]:\n                            - /url: "#%E3%81%99%E3%81%94%E3%81%84%E3%81%9E%EF%BC%81%E5%BC%B7%E5%8C%96%E5%AD%A\\\n                                6%E7%BF%92"\n                    - listitem [ref=s1e1924]:\n                        - link "そもそもLLMで強化学習とは？" [ref=s1e1925]:\n                            - /url: "#%E3%81%9D%E3%82%82%E3%81%9D%E3%82%82llm%E3%81%A7%E5%BC%B7%E5%8C%96%E5%A\\\n                                D%A6%E7%BF%92%E3%81%A8%E3%81%AF%EF%BC%9F"\n                    - listitem [ref=s1e1926]:\n                        - link "ZeroのSystemプロンプト" [ref=s1e1927]:\n                            - /url: "#zero%E3%81%AEsystem%E3%83%97%E3%83%AD%E3%83%B3%E3%83%97%E3%83%88"\n                    - listitem [ref=s1e1928]:\n                        - link "Zeroの「Aha Moment」" [ref=s1e1929]:\n                            - /url: "#zero%E3%81%AE%E3%80%8Caha-moment%E3%80%8D"\n                    - listitem [ref=s1e1930]:\n                        - link "DeepSeek-R1-Zeroのまとめ" [ref=s1e1931]:\n                            - /url: "#deepseek-r1-zero%E3%81%AE%E3%81%BE%E3%81%A8%E3%82%81"\n            - listitem [ref=s1e1932]:\n                - link "DeepSeek-R1への道" [ref=s1e1933]:\n                    - /url: "#deepseek-r1%E3%81%B8%E3%81%AE%E9%81%93"\n                - list [ref=s1e1934]:\n                    - listitem [ref=s1e1935]:\n                        - link "DeepSeek-R1-Zeroの課題" [ref=s1e1936]:\n                            - /url: "#deepseek-r1-zero%E3%81%AE%E8%AA%B2%E9%A1%8C"\n                    - listitem [ref=s1e1937]:\n                        - link "課題対処への道" [ref=s1e1938]:\n                            - /url: "#%E8%AA%B2%E9%A1%8C%E5%AF%BE%E5%87%A6%E3%81%B8%E3%81%AE%E9%81%93"\n                    - listitem [ref=s1e1939]:\n                        - link "DeepSeek-R1のまとめ" [ref=s1e1940]:\n                            - /url: "#deepseek-r1%E3%81%AE%E3%81%BE%E3%81%A8%E3%82%81"\n            - listitem [ref=s1e1941]:\n                - link "結果" [ref=s1e1942]:\n                    - /url: "#%E7%B5%90%E6%9E%9C"\n            - listitem [ref=s1e1943]:\n                - link "蒸留" [ref=s1e1944]:\n                    - /url: "#%E8%92%B8%E7%95%99"\n                - list [ref=s1e1945]:\n                    - listitem [ref=s1e1946]:\n                        - link "蒸留 vs 強化学習" [ref=s1e1947]:\n                            - /url: "#%E8%92%B8%E7%95%99-vs-%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92"\n            - listitem [ref=s1e1948]:\n                - link "今後の展望" [ref=s1e1949]:\n                    - /url: "#%E4%BB%8A%E5%BE%8C%E3%81%AE%E5%B1%95%E6%9C%9B"\n            - listitem [ref=s1e1950]:\n                - link "まとめ" [ref=s1e1951]:\n                    - /url: "#%E3%81%BE%E3%81%A8%E3%82%81"\n            - listitem [ref=s1e1952]:\n                - link "参考文献" [ref=s1e1953]:\n                    - /url: "#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"\n        - paragraph [ref=s1e1955]: Zennからのお知らせ\n        - article [ref=s1e1958]:\n            - link "第２回 AI Agent Hackathon with Google Cloud 第２回 AI Agent Hackathon with Google Cloud 4/14登録受付開始！" [ref=s1e1959]:\n                - /url: /hackathons/google-cloud-japan-ai-hackathon-vol2\n                - img "第２回 AI Agent Hackathon with Google Cloud" [ref=s1e1961]\n                - paragraph [ref=s1e1962]: 第２回 AI Agent Hackathon with Google Cloud 4/14登録受付開始！\n        - article [ref=s1e1963]:\n            - link "Zenncafe日比谷#2 生成AIで変わる仕事 Zenncafe日比谷#2 生成AIで変わる仕事" [ref=s1e1964]:\n                - /url: https://zenn.connpass.com/event/351131/\n                - img "Zenncafe日比谷#2 生成AIで変わる仕事" [ref=s1e1966]\n                - paragraph [ref=s1e1967]: Zenncafe日比谷#2 生成AIで変わる仕事\n- contentinfo [ref=s1e1969]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1973]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1974]\n    - paragraph [ref=s1e1982]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e1984]:\n        - heading "About" [level=4] [ref=s1e1985]\n        - list [ref=s1e1986]:\n            - listitem [ref=s1e1987]:\n                - link "Zennについて" [ref=s1e1989]:\n                    - /url: /about\n            - listitem [ref=s1e1990]:\n                - link "運営会社" [ref=s1e1991]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e1992]:\n                - link "お知らせ・リリース" [ref=s1e1993]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e1994]:\n                - link "イベント" [ref=s1e1996]:\n                    - /url: /events\n    - navigation [ref=s1e1997]:\n        - heading "Guides" [level=4] [ref=s1e1998]\n        - list [ref=s1e1999]:\n            - listitem [ref=s1e2000]:\n                - link "使い方" [ref=s1e2002]:\n                    - /url: /manual\n            - listitem [ref=s1e2003]:\n                - link "法人向けメニュー" [ref=s1e2005]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e2007]:\n                - link "Publication / Pro" [ref=s1e2009]:\n                    - /url: /publications\n            - listitem [ref=s1e2010]:\n                - link "よくある質問" [ref=s1e2012]:\n                    - /url: /faq\n    - navigation [ref=s1e2013]:\n        - heading "Links" [level=4] [ref=s1e2014]\n        - list [ref=s1e2015]:\n            - listitem [ref=s1e2016]:\n                - link "X(Twitter)" [ref=s1e2017]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e2018]:\n                - link "GitHub" [ref=s1e2019]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e2020]:\n                - link "メディアキット" [ref=s1e2022]:\n                    - /url: /mediakit\n    - navigation [ref=s1e2023]:\n        - heading "Legal" [level=4] [ref=s1e2024]\n        - list [ref=s1e2025]:\n            - listitem [ref=s1e2026]:\n                - link "利用規約" [ref=s1e2028]:\n                    - /url: /terms\n            - listitem [ref=s1e2029]:\n                - link "プライバシーポリシー" [ref=s1e2031]:\n                    - /url: /privacy\n            - listitem [ref=s1e2032]:\n                - link "特商法表記" [ref=s1e2034]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e2037]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e2038]\n- alert [ref=s1e2057]\n```\n', name='browser_navigate', tool_call_id='d85f8505-9c2a-4960-b17f-9ef88d983d9d'), AIMessage(content='まず、指定されたZennの著者のページにアクセスし、最もいいねが多い記事を特定しました。その結果、「DeepSeek-R1の論文読んだ？【勉強になるよ】」という記事が最も評価されていることがわかりました。\n\n次に、その記事の内容を詳しく調べました。このブログ記事は、DeepSeek-R1という大規模言語モデルに関する論文を解説したものです。記事は、論文の内容をわかりやすく説明することに重点を置いており、特に強化学習の概念や、DeepSeek-R1を開発する上で用いられたGRPO（Group Relative Policy Optimization）という手法について詳しく解説しています。\n\n記事が評価されている理由として、以下の点が考えられます。\n\n1.  **技術的な深さと分かりやすさの両立:** 大規模言語モデルや強化学習といった複雑なトピックを扱いながらも、専門知識がない読者にも理解できるように、かみ砕いた説明がなされています。\n2.  **論文の内容の網羅性:** DeepSeek-R1の論文の重要なポイントを漏れなくカバーしており、論文を読むための良い入門となっています。\n3.  **著者自身の考察:** 論文の内容を解説するだけでなく、著者自身の考察や解釈が加えられており、読者に新たな視点を提供しています。\n4.  **読者への配慮:** 記事の冒頭で、誤りがあるかもしれないことを断り、コメントでの指摘を歓迎するなど、読者とのコミュニケーションを重視する姿勢が見られます。\n5.  **参考文献の紹介:** 記事の最後に、強化学習や関連技術を学ぶための参考文献が紹介されており、読者の学習をサポートする姿勢が評価されていると考えられます。\n\nこれらの要素が組み合わさることで、この記事はDeepSeek-R1に関心のあるエンジニアにとって非常に価値のある情報源となり、高い評価を得ていると考えられます。', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-d5ac6129-cca3-4759-ac6a-eafe68db505f-0', usage_metadata={'input_tokens': 47987, 'output_tokens': 387, 'total_tokens': 48374, 'input_token_details': {'cache_read': 0}}), HumanMessage(content=[{'type': 'text', 'text': 'LLMに強化学習ってどういうこと？GRPOって何？'}], additional_kwargs={}, response_metadata={}), AIMessage(content='LLM（大規模言語モデル）に対する強化学習と、GRPO（Group Relative Policy Optimization）について説明します。\n\n### LLMに対する強化学習\n\nLLMに対する強化学習は、LLMがより良いテキストを生成するように学習させるための手法です。通常のLLMの学習では、大量のテキストデータを用いて、与えられたテキストの続きを予測するように学習します。これに対して、強化学習では、LLMが生成したテキストに対して「報酬」を与え、その報酬を最大化するように学習します。\n\n**なぜ強化学習が必要なのか？**\n\nLLMは、大量のテキストデータから学習することで、自然な文章を生成する能力を獲得します。しかし、必ずしも人間が望むような文章を生成するとは限りません。例えば、以下のような課題があります。\n\n*   **目的の達成:** 特定のタスク（質問応答、要約など）において、必ずしも最適な回答を生成するとは限らない。\n*   **安全性:** 有害な情報や偏見のある情報を生成する可能性がある。\n*   **創造性:** 新しいアイデアや表現を生み出す能力が限られている。\n\n強化学習は、これらの課題を克服するために用いられます。\n\n**強化学習の基本的な流れ**\n\n1.  **LLM（エージェント）:** テキストを生成する主体。\n2.  **環境:** LLMがテキストを生成する対象となるタスクや状況。\n3.  **報酬:** LLMが生成したテキストの良し悪しを評価する指標。\n4.  **学習:** LLMが報酬を最大化するように、テキスト生成の方針（方策）を調整する。\n\n具体的には、以下のような手順で学習が進みます。\n\n1.  LLMがプロンプト（指示）を受け取り、テキストを生成します。\n2.  生成されたテキストに対して、報酬関数が報酬を与えます。\n    *   例えば、質問応答タスクであれば、正解との一致度合いが高いほど高い報酬を与えます。\n    *   安全性に関する報酬であれば、有害な情報を生成した場合に低い報酬を与えます。\n3.  LLMは、その報酬に基づいて、自身のテキスト生成の方針を調整します。\n    *   具体的には、報酬が高くなるような単語やフレーズを選択する確率を高めます。\n4.  このプロセスを繰り返すことで、LLMはより高い報酬を得られるようなテキストを生成するように学習していきます。\n\n### GRPO（Group Relative Policy Optimization）とは？\n\nGRPOは、LLMに対する強化学習の一種で、特に大規模なLLMを効率的に学習させるために開発された手法です。\n\n**PPO（Proximal Policy Optimization）との比較**\n\nLLMの強化学習でよく用いられる手法として、PPO（Proximal Policy Optimization）があります。PPOでは、報酬に加えて、LLMの出力が大きく変化しないように制約を加えながら学習を進めます。しかし、PPOでは、状態価値モデルという追加のモデルを学習する必要があり、計算コストが高くなるという課題がありました。\n\nGRPOは、この状態価値モデルを不要にし、より効率的に学習を進めることを目指します。\n\n**GRPOの仕組み**\n\nGRPOでは、LLMに複数のテキストを生成させ、それらのテキストに対する報酬を比較することで、相対的な評価を行います。具体的には、以下の手順で学習が進みます。\n\n1.  LLMが同じプロンプトに対して、複数のテキスト（例えば4つ）を生成します。\n2.  それぞれのテキストに対して報酬関数が報酬を与えます。\n3.  それぞれのテキストの報酬を比較し、最も高い報酬を得たテキストを「良いテキスト」、それ以外のテキストを「悪いテキスト」とします。\n4.  LLMは、「良いテキスト」を生成する確率を高め、「悪いテキスト」を生成する確率を下げるように、自身のテキスト生成の方針を調整します。\n\nこのように、GRPOでは、テキスト間の相対的な評価を用いることで、状態価値モデルを学習する必要がなくなり、計算コストを削減することができます。\n\n**GRPOの利点**\n\n*   **計算効率:** 状態価値モデルが不要なため、PPOよりも計算コストが低い。\n*   **大規模LLMへの適用:** 大規模なLLMでも効率的に学習できる。\n\n**まとめ**\n\nLLMに対する強化学習は、LLMがより良いテキストを生成するように学習させるための手法であり、GRPOはその効率的な学習を可能にする手法の一つです。GRPOは、テキスト間の相対的な評価を用いることで、状態価値モデルを不要にし、計算コストを削減することができます。', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-4d11a554-e7d5-4420-9ac9-b8efaccf8b11-0', usage_metadata={'input_tokens': 48388, 'output_tokens': 996, 'total_tokens': 49384, 'input_token_details': {'cache_read': 0}}), HumanMessage(content=[{'type': 'text', 'text': '2番目にいいねが多い記事は何？'}], additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-aae69546-e8ee-4891-9040-daee70739fa4-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap'}, 'id': '61c67f9f-6525-4601-af0d-36cd5f4631a2', 'type': 'tool_call'}], usage_metadata={'input_tokens': 49392, 'output_tokens': 13, 'total_tokens': 49405, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap\n\n- Page URL: https://zenn.dev/asap\n- Page Title: asapさんの記事一覧 | Zenn\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- banner [ref=s1e32]:\n    - img "asap" [ref=s1e36]\n    - heading "asap" [level=1] [ref=s1e39]\n    - button "フォロー" [ref=s1e41]\n    - paragraph [ref=s1e43]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n    - button "2,214 Likes" [ref=s1e46]\n    - button "310 Followers" [ref=s1e48]\n    - link "@personabb" [ref=s1e51]:\n        - /url: https://github.com/personabb\n        - tooltip "@personabb" [ref=s1e52]:\n            - img "GitHub" [ref=s1e53]\n            - text: "@personabb"\n    - link "@asap2650" [ref=s1e55]:\n        - /url: https://twitter.com/asap2650\n        - tooltip "@asap2650" [ref=s1e56]:\n            - img "X(Twitter)" [ref=s1e57]\n            - text: "@asap2650"\n    - link "yard.tips" [ref=s1e63]:\n        - /url: https://yard.tips/asap2650\n        - tooltip "yard.tips" [ref=s1e64]:\n            - img [ref=s1e65]\n            - text: yard.tips\n    - link "RSS" [ref=s1e67]:\n        - /url: /asap/feed\n        - tooltip "RSS" [ref=s1e68]:\n            - img [ref=s1e69]\n            - text: RSS\n- link "Articles 72" [ref=s1e80]:\n    - /url: /asap\n- link "Scraps 5" [ref=s1e82]:\n    - /url: /asap?tab=scraps\n- link "Comments" [ref=s1e84]:\n    - /url: /asap?tab=comments\n- contentinfo [ref=s1e88]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e92]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e93]\n    - paragraph [ref=s1e101]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e103]:\n        - heading "About" [level=4] [ref=s1e104]\n        - list [ref=s1e105]:\n            - listitem [ref=s1e106]:\n                - link "Zennについて" [ref=s1e108]:\n                    - /url: /about\n            - listitem [ref=s1e109]:\n                - link "運営会社" [ref=s1e110]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e111]:\n                - link "お知らせ・リリース" [ref=s1e112]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e113]:\n                - link "イベント" [ref=s1e115]:\n                    - /url: /events\n    - navigation [ref=s1e116]:\n        - heading "Guides" [level=4] [ref=s1e117]\n        - list [ref=s1e118]:\n            - listitem [ref=s1e119]:\n                - link "使い方" [ref=s1e121]:\n                    - /url: /manual\n            - listitem [ref=s1e122]:\n                - link "法人向けメニュー" [ref=s1e124]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e126]:\n                - link "Publication / Pro" [ref=s1e128]:\n                    - /url: /publications\n            - listitem [ref=s1e129]:\n                - link "よくある質問" [ref=s1e131]:\n                    - /url: /faq\n    - navigation [ref=s1e132]:\n        - heading "Links" [level=4] [ref=s1e133]\n        - list [ref=s1e134]:\n            - listitem [ref=s1e135]:\n                - link "X(Twitter)" [ref=s1e136]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e137]:\n                - link "GitHub" [ref=s1e138]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e139]:\n                - link "メディアキット" [ref=s1e141]:\n                    - /url: /mediakit\n    - navigation [ref=s1e142]:\n        - heading "Legal" [level=4] [ref=s1e143]\n        - list [ref=s1e144]:\n            - listitem [ref=s1e145]:\n                - link "利用規約" [ref=s1e147]:\n                    - /url: /terms\n            - listitem [ref=s1e148]:\n                - link "プライバシーポリシー" [ref=s1e150]:\n                    - /url: /privacy\n            - listitem [ref=s1e151]:\n                - link "特商法表記" [ref=s1e153]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e156]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e157]\n- alert [ref=s1e176]\n```\n', name='browser_navigate', tool_call_id='61c67f9f-6525-4601-af0d-36cd5f4631a2'), AIMessage(content='asapさんのZennの記事の中で2番目にいいねが多い記事は、「VAEって結局何者なの？」で、いいね数は104です。', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-7c7bf5a2-7650-41d4-9cc5-c1fb3e0de2f1-0', usage_metadata={'input_tokens': 51212, 'output_tokens': 32, 'total_tokens': 51244, 'input_token_details': {'cache_read': 0}}), HumanMessage(content=[{'type': 'text', 'text': 'どんな記事ですか？解説してください。'}], additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap/articles/6caa9043276424"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-093f5ac0-5315-41a0-aaf8-f4fef2243382-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap/articles/6caa9043276424'}, 'id': '45a87836-eb09-4499-bf44-a2bcdbd3ec2b', 'type': 'tool_call'}], usage_metadata={'input_tokens': 51251, 'output_tokens': 28, 'total_tokens': 51279, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap/articles/6caa9043276424\n\n- Page URL: https://zenn.dev/asap/articles/6caa9043276424\n- Page Title: VAEって結局何者なの？\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- article [ref=s1e32]:\n    - text: 📖\n    - heading "VAEって結局何者なの？" [level=1] [ref=s1e38]\n    - text: 2025/01/20に公開\n    - link "ディープラーニング" [ref=s1e52]:\n        - /url: /topics/deeplearning\n        - img [ref=s1e54]\n        - text: ディープラーニング\n    - link "VAE" [ref=s1e56]:\n        - /url: /topics/vae\n        - img [ref=s1e58]\n        - text: VAE\n    - link "Stable Diffusion" [ref=s1e60]:\n        - /url: /topics/stablediffusion\n        - img [ref=s1e62]\n        - text: Stable Diffusion\n    - link "生成 AI" [ref=s1e64]:\n        - /url: /topics/%E7%94%9F%E6%88%90ai\n        - img [ref=s1e66]\n        - text: 生成 AI\n    - link "elbo" [ref=s1e68]:\n        - /url: /topics/elbo\n        - img [ref=s1e70]\n        - text: elbo\n    - link "tech" [ref=s1e72]:\n        - /url: /tech-or-idea\n        - img [ref=s1e74]\n        - text: tech\n    - heading "はじめに" [level=1] [ref=s1e78]\n    - paragraph [ref=s1e79]: 今回は、満を持してVAE（Variational Auto Encoder）をちゃんと理解していこうと思います。\n    - paragraph [ref=s1e80]: VAEに関しては、だいたい知っていますが、MusicGenという音楽生成AIを理解しようと思った時に、関連してRVQ-GANが出てきたので、再勉強をしています。\n    - paragraph [ref=s1e81]: 今後、下記の流れで記事を書いていく予定です。\n    - paragraph [ref=s1e82]: VAE（今回） ↓ VQ-VAE ↓ RQ-VAE,RVQ-GAN ↓ MusicGen\n    - paragraph [ref=s1e89]: 今回は、流れの最初であり、現在でも非常に重要な概念であるVAEに関して記事を書きます。\n    - paragraph [ref=s1e90]: VAEは、Stable\n        Diffusionの中でも中核として使われていたりと、比較的古い概念でありながら、まだまだ活躍が期待できる汎用ネットワークです。\n        本記事が、みなさまの理解の手助けになれば、これほど嬉しいことはございません。\n    - heading "参考文献" [level=1] [ref=s1e92]\n    - paragraph [ref=s1e93]:\n        - iframe [ref=s1e95]:\n            - link "Auto-Encoding Variational Bayes How can we perform efficient inference and learning in directed probabilistic models, in the presenc arxiv.org favicon image arxiv.org arxiv.org thumbnail image" [ref=f1s1e6]:\n                - /url: https://arxiv.org/abs/1312.6114\n                - heading "Auto-Encoding Variational Bayes" [level=1]\n                  [ref=f1s1e8]\n                - text: How can we perform efficient inference and learning in directed\n                    probabilistic models, in the presenc\n                - img "arxiv.org favicon image" [ref=f1s1e11]\n                - text: arxiv.org\n                - img "arxiv.org thumbnail image" [ref=f1s1e13]\n            - alert [ref=f1s1e15]\n        - text: VAEの論文です。\n    - paragraph [ref=s1e96]:\n        - link "ゼロから作るDeep Learning ❺ ―生成モデル編" [ref=s1e97]:\n            - /url: https://amzn.to/3Aw9vCt\n    - paragraph [ref=s1e98]: 圧倒的な名著シリーズである「ゼロから作るDeep Learning」シリーズの5作目である、生成モデル編です。\n        この本では、最終的に拡散モデルを理解することを目的に、VAEや混合ガウスモデル、正規分布まで遡り、本当にゼロから中身を理解しながら実装をすることができる、大変素晴らしい書籍になっています。\n    - paragraph [ref=s1e100]:\n        - link "妥協しないデータ分析のための 微積分+線形代数入門" [ref=s1e101]:\n            - /url: https://amzn.to/4hE7ftx\n    - paragraph [ref=s1e102]: 本書の後半にもVAEやELBOについての簡単な記載があり、本記事を書くうえで表現を参考にさせていただきました。\n        確率分布のどの部分が計算可能で、どの部分が計算不可能なのかが常に明示されながら進んでいくので、初学者には非常に分かりやすい書籍でした！\n    - paragraph [ref=s1e104]: 特に、VAEなどの生成AIをただ使うだけでなく、ちゃんと理論を理解したいと思った場合、論文や理論書を理解するために「線形代数」や「微積分学」の事前知識が必要になります。\n        本書は、初学者が一番最初に読んで必要な基礎知識が習得できる良本になります。\n    - paragraph [ref=s1e106]:\n        - link "生成 Deep Learning 第2版 ―絵を描き、物語や音楽を作り、ゲームをプレイする" [ref=s1e107]:\n            - /url: https://amzn.to/4g8hjJg\n    - paragraph [ref=s1e108]: 本書の序盤で、Auto\n        EncoderとVAEの違いを説明してくれています。しかもそれぞれの潜在空間の分布を可視化してくれているので、違いが分かりやすく、理解もしやすいです。\n        さらに、本書はVAEだけでなくGANや画像生成、音楽生成、テキスト生成など、ありとあらゆる生成AIの紹介と解説をしてくれており、必読の一冊となっております。\n    - paragraph [ref=s1e110]: （書籍のリンクはAmazonアフィリエイトリンクになっております）\n    - heading "関連記事" [level=1] [ref=s1e111]\n    - paragraph [ref=s1e112]: VAEに関連した記事は過去のも書いております。 こちらの記事もご覧いただけると嬉しいです！\n    - paragraph [ref=s1e114]:\n        - iframe [ref=s1e116]:\n            - link "【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】 asapさんによる記事 zenn.dev favicon image zenn.dev zenn.dev thumbnail image" [ref=f2s1e6]:\n                - /url: https://zenn.dev/asap/articles/ea9f42edff6573\n                - heading\n                  "【生成AI】なぜ「対数尤度の最大化」ではなく「ELBOの最大化」が目的関数になるのか【VAE・拡散モデル】"\n                  [level=1] [ref=f2s1e8]\n                - text: asapさんによる記事\n                - img "zenn.dev favicon image" [ref=f2s1e11]\n                - text: zenn.dev\n                - img "zenn.dev thumbnail image" [ref=f2s1e13]\n            - alert [ref=f2s1e15]\n    - heading "Auto Encoder" [level=1] [ref=s1e117]\n    - heading "Auto Encoderの紹介" [level=2] [ref=s1e118]\n    - paragraph [ref=s1e119]: VAEを語る上で欠かせないのは、Auto Encoderの存在です。 Auto\n        Encoderというのは、ニューラルネットワークにより構築されたネットワーク構造の一つで、入力と出力を一致させるように学習させるネットワークです。\n    - paragraph [ref=s1e121]:\n        - img [ref=s1e122]\n    - paragraph [ref=s1e123]: 最も単純なAuto Encoderは上記のような構造をしています。\n        前段は、入力データから、それより次元の小さな潜在表現を生成するEncoderです。\n        上の図では、入力データを重みデータによる線形変換（+活性化層における非線形変換）で成り立っています。\n        Encoderでは、次元の大きな入力データを、次元の小さい潜在表現に圧縮する処理を行います。\n    - paragraph [ref=s1e127]: 後段は、潜在表現から、それより次元の大きな出力データを生成するDecoderです。\n        出力データは入力と一致するように学習がなされています。\n        このとき、Decoderでは、次元の小さな潜在表現から、出力データ（入力データ）を復元する処理を行います。\n        この「圧縮(符号化)」と「復元(複合化)」の構造から、「Encoder」「Decoder」と名付けられており、この圧縮・復元をデータを与えれば自動で実施するニューラルネットワークなので、「Auto\n        Encoder」と呼ばれます。\n    - heading "Auto Encoderの本質" [level=2] [ref=s1e131]\n    - paragraph [ref=s1e132]: ここでの本質は、次元の大きい入力データを、潜在表現に圧縮し、その潜在表現から、入力データを再構成することです。\n    - heading "小さな潜在表現から元データを復元できる理由" [level=3] [ref=s1e133]\n    - paragraph [ref=s1e134]: これが達成される理由は2つあります。\n    - paragraph [ref=s1e135]: 1つ目は、世の中にあるデータは冗長であることです。\n        世の中に存在するデータを冗長なデータが多いです。例えば画像データであれば、生の01でデータを保持するよりも、pngなどの圧縮形式を利用してデータを保存した方が、データ量を抑えることができます。\n        これは、画像中の「隣接する画素は、近い値が多い」という事前知識を用いることで、データを効率的に圧縮することができるからです。\n    - complementary [ref=s1e138]:\n        - text: "!"\n        - paragraph [ref=s1e141]: 話が脱線しますが、pngは可逆圧縮と呼ばれる手法であるため、この手法を利用してデータ量を削減した場合であっても、元の画像データを「完全に」復元することができます。これはzipなども同様です。\n        - paragraph [ref=s1e142]: 一方、画像でよく使われている、jpgやHEIFなどの規格は、非可逆圧縮と呼ばれる手法です。\n            この手法でデータ量を削減した場合は、元の画像から劣化することになります。\n            ただし、その劣化具合はみなさんご存知の通り非常に小さく、かつデータ削減量も可逆圧縮手法と比較して大幅に削減できます。\n    - paragraph [ref=s1e145]: 2つ目は、Auto Encoderの重みデータに、入力データを復元するための情報を学習しているからです。\n    - paragraph [ref=s1e146]: 例えば画像であれば、Auto\n        Encoderを利用する際に、事前に大規模画像データセットを利用して学習が行われます。その学習を通して、Decoderは「どうやったら少ない潜在表現から画像データを品質高く復元できるのか」、Encoderは「大量の画像データのうち、どの情報を潜在表現に埋め込めば、Decoderが元の画像を復元できるのか」を学習しています。\n        その結果、ちょうどpngで説明した通り、「隣接する画素は、近い値が多い」のような自然画像全般に通ずる事前知識を獲得し、その情報を用いることで、少ないデータ量から、元の入力データを復元するようなネットワークを構築することができるというわけです。\n    - heading "構造" [level=3] [ref=s1e148]\n    - paragraph [ref=s1e149]: 先ほど記載した通り、Auto\n        Encoderの本質は、次元の大きい入力データを、潜在表現に圧縮し、小さな潜在表現から、入力データを再構成することです。\n        そして、これが達成できるのであれば、どんなEncoder,Decoderの構造をしていたとしても、それはAuto\n        Encoderの一種であると考えることができます。\n    - paragraph [ref=s1e151]: 例えば画像を処理する際には、しばしば畳み込みニューラルネットワークやTransformerなどが使われます。\n        当然、これらを利用したAuto Encoderも考えることができます。\n    - paragraph [ref=s1e153]: 潜在表現も単純な1次元データだけではなく、画像のような3次元（縦, 横,\n        チャネル）データを考えることができます。 （ResNetなどのGAP前の層の出力結果をEncoderの出力として利用することもできます）\n    - paragraph [ref=s1e155]: したがって、以降ではEncoder Decoder構造を記載する際には、下記のように記載することにします。\n    - paragraph [ref=s1e156]:\n        - img [ref=s1e157]\n    - heading "Auto Encoerの課題" [level=2] [ref=s1e158]\n    - paragraph [ref=s1e159]: Auto Encoerはそれ単体では、それなりに優れた情報圧縮器ではありますが、それ以外での活用可能性が難しいネットワークではありました。\n    - paragraph [ref=s1e160]: 入力データと形状は同じな、出力データを出力する際には、ネットワーク構造が使われることも多いです（例えばセマンティックセグメンテーションタスクでU-Netが利用されるなど）が、同じ入出力になるように学習することというのは、情報圧縮以外の文脈で利用するのは難しいです。\n    - paragraph [ref=s1e161]: そこで、生成モデルとしての活用可能性が考えられました、\n        例えば、潜在表現を何らかの形で得ることができれば、その潜在表現をDecoderに入力することで、対応した出力データ（画像など）を得ることができます。\n        これが達成されれば、AIに絵を描かせたり、音楽を生成させたりすることが可能です。\n    - paragraph [ref=s1e164]: ただし、Auto Encoderでは、その潜在表現をEncoderを利用すること以外で取得することは困難です。\n        ここで、Auto Encoderによる画像生成の夢は途絶えたかのように見えました。\n    - heading "VAE（Variational Auto Encoder）" [level=1] [ref=s1e166]\n    - heading "VAEと生成モデル" [level=2] [ref=s1e167]\n    - heading "生成モデルの観点から見たVAE" [level=3] [ref=s1e168]\n    - paragraph [ref=s1e169]:\n        - text: VAEというのは、生成モデルとして重要な「対数尤度の最大化」問題を解くために必要な構造を用意したら、\n        - strong [ref=s1e171]: 結果的にAuto Encoderのような形になったというモデルです。\n    - paragraph [ref=s1e172]: すなわち、Auto\n        Encoderの構造があって、それをこうしたら生成モデルとして使えるようになった。というよりは、生成モデルを作る上で重要な「対数尤度を最大化する」という方向性で、理論的に式変形を繰り返し、計算不可能な部分を取り除き、計算が難しい部分をニューラルネットで近似することで、結果としてAuto\n        Encoderの構造が現れたという解釈があっているような気がします。\n    - heading "対数尤度の最大化からモデル構造を考える" [level=3] [ref=s1e173]\n    - heading "対数尤度の最大化に関しての軽い復習" [level=4] [ref=s1e174]\n    - paragraph [ref=s1e175]:\n        - text: 「対数尤度の最大化」に関しては下記の記事で解説しておりますので、そちらをご覧ください。\n        - iframe [ref=s1e177]: <could not take iframe snapshot>\n    - paragraph [ref=s1e178]:\n        - text: 上記の記事では詳細に記載していますが、簡単に解説すると、生成モデルにおいて重要なのは、対数尤度\n        - math [ref=s1e182]: p θ ( x )\n        - text: を最大化することです。 ここで、 x は実際のデータで、 \\theta は生成モデルのパラメータです。 したがって、 p_\\theta(x)\n            は対数尤度の確率分布になります。\n    - paragraph [ref=s1e199]:\n        - text: 生成モデルのパラメータ自体にはランダム性がないため、様々なバリエーションのデータを生成するためには、入力にランダムにサンプリングされたデータや定数を入力し、生成モデルがデータを再構成します。\n            イメージとしては下記のイメージです。\n        - img [ref=s1e202]\n    - complementary [ref=s1e203]:\n        - text: "!"\n        - paragraph [ref=s1e206]:\n            - text: ネットワークが確率分布を生成するというのは違和感があるかと思います。 その辺りの詳細は、下記の記事でも解説しているので参考にしてみてください。\n            - link "https://zenn.dev/asap/articles/4092ab60570b05" [ref=s1e209]:\n                - /url: https://zenn.dev/asap/articles/4092ab60570b05\n        - paragraph [ref=s1e210]: "ここでは、ネットワークが出力したデータ y を「平均」、単位行列 I を「分散」とする多変数正規分布\n            \\\\mathcal{N}(y, I) を考えることで、Decoderの出力を確率分布として計算しています。\n            実用上は、ネットワークが出力した平均 y の値を、そのまま生成したデータ x\'=y として扱います。"\n    - paragraph [ref=s1e222]: "しかし、上記で計算しているのは、尤度 p_{\\\\theta}(x|z) です。\n        （なぜなら、Decoderはパラメータ \\\\theta と潜在表現 z によって条件付けられたネットワークであるためです。）"\n    - paragraph [ref=s1e230]: これを用いて対数尤度 p_\\theta(x) を計算するには、下記の式を最適化する必要があります。\n    - text: "\\\\log p_{\\\\theta}(x) = \\\\mathbb{E}_{q_{\\\\psi}(z|x)}[\\\\log\n        p_{\\\\theta}(x|z)] - \\\\mathrm{KL}\\\\left(q_{\\\\psi}(z|x) \\\\parallel\n        p(z)\\\\right) + \\\\mathrm{KL}\\\\left(q_{\\\\psi}(z|x) \\\\parallel\n        p_{\\\\theta}(z|x)\\\\right)"\n    - paragraph [ref=s1e236]: "この中で、事後分布 p_{\\\\theta}(z|x) は計算不可能なため、近似事後分布\n        q_{\\\\psi}(z|x) を用意して、KLダイバージェンスに押し込みます。\n        そうすることで、第三項のKLダイバージェンスは非負であるため、この項を除いた部分(ELBO)を最大化することで、間接的に対数尤度を最大化するこ\\\n        とができます。"\n    - paragraph [ref=s1e242]: "このとき必要な、近似事後分布 q_{\\\\psi}(z|x) を計算するために、パラメータ \\\\psi\n        を持ち、データ x を入力して、潜在表現 z の確率分布を出力するようなネットワークとして、Encoderが必要になります。"\n    - paragraph [ref=s1e251]:\n        - text: 詳しくは\n        - link "上述した記事" [ref=s1e252]:\n            - /url: https://zenn.dev/asap/articles/ea9f42edff6573\n        - text: をご覧ください。\n    - heading "Encoderが平均と分散を出力する理由" [level=4] [ref=s1e253]\n    - paragraph [ref=s1e254]:\n        - text: ここまでの議論を考慮すると、下記のようなモデル構造になります。\n        - img [ref=s1e256]\n    - paragraph [ref=s1e257]: "上述した通り、Decoderの場合は、Decoderが出力するデータ y を「平均」、単位行列 I\n        を「分散」とする多変数正規分布 \\\\mathcal{N}(y, I) を確率分布として持ちますが、実用上は、Decoderが出力した平均 y\n        の値を、そのまま生成データ x\'=y と考えることができます。"\n    - paragraph [ref=s1e268]: 一方で、Encoderは確率分布を生成する必要があります。\n        さらにいうと、ELBOの式の中で、Encoderが出力する確率分布を用いて下記の式を計算する必要があります。\n    - text: "\\\\mathrm{KL}\\\\left(q_{\\\\psi}(z|x) \\\\parallel p(z)\\\\right)"\n    - paragraph [ref=s1e273]: ここで、 p(z) は潜在表現 z の事前分布なので、ベイズ推定等と同様に、こちらが設定することができます。\n        ネットワークは、こちらが設定した事前分布が成立するように学習します。\n        今回の場合、真の事前分布がどんな形状になるのかの事前情報を持っていないので、あまりバイアス情報が入らないような分布を設定することが望ましいです。\n    - paragraph [ref=s1e280]: "問題は、 q_{\\\\psi}(z|x) です。"\n    - paragraph [ref=s1e283]: この分布は特殊な分布になってはいけません。 なぜなら、KLダイバージェンスを計算できる分布の種類はあまり多くないからです。\n    - complementary [ref=s1e285]:\n        - text: "!"\n        - paragraph [ref=s1e288]: 一般的な連続確率分布同士のKLダイバージェンスを計算するのは、積分計算が入るため困難です。\n            しかしながら、指数型分布のような確率分布であれば計算できる余地があります。\n        - paragraph [ref=s1e290]: 例えば、正規分布同士のKLダイバージェンスであれば、比較的簡単に計算をすることができます。\n    - paragraph [ref=s1e291]: したがって、Encoderが生成する確率分布に、何らかの制限を加える必要があります。 今回は、確率分布が正規分布になるように制限をかけます。\n    - paragraph [ref=s1e293]: "その場合、Encoderは、平均 \\\\mu と分散 \\\\sigma^2\n        のパラメータを出力するような構造を考えることで、Encoderが正規分布を表現することができます。 そして、Encoderが出力した、平均\n        \\\\mu ・分散 \\\\sigma^2 パラメータから得られる正規分布 \\\\mathcal{N}(\\\\mu, \\\\sigma)\n        からサンプリングされた値を潜在表現 z とすることで、ELBOを計算することができます。 この場合、計算効率化のため、事前分布 p(z)\n        には平均 0 、分散 I の標準正規分布 \\\\mathcal{N}(0, I) を設定します。 標準正規分布 \\\\mathcal{N}(0,\n        I) は、正規分布とのKLダイバージェンスの計算が簡単であることと、平均 0 、分散 I\n        の条件下におけるエントロピーが最大になる分布（つまり、そのほかの事前情報を含まない分布）であるため、採用されています。"\n    - paragraph [ref=s1e323]: これを達成するためのモデル構造は下記になります。\n    - paragraph [ref=s1e324]:\n        - img [ref=s1e325]\n    - paragraph [ref=s1e326]:\n        - text: これでVAEを構築することができました。 上の図をよく見ると、\n        - strong [ref=s1e329]: まるでAuto Encoderのような構造になっていると思います。\n    - heading "VAEの挙動・振る舞い" [level=2] [ref=s1e330]\n    - heading "VAEの目的関数" [level=3] [ref=s1e331]\n    - paragraph [ref=s1e332]: 改めてVAEの目的関数は下記になります。\n    - complementary [ref=s1e333]:\n        - text: "!"\n        - paragraph [ref=s1e336]: ELBO\n        - text: "\\\\mathrm{ELBO} = \\\\mathbb{E}_{q_{\\\\psi}(z|x)}[\\\\log p_{\\\\theta}(x|z)] -\n            \\\\mathrm{KL}\\\\left(q_{\\\\psi}(z|x) \\\\parallel p(z)\\\\right)"\n    - paragraph [ref=s1e340]: このELBOを最大化するようにVAEのネットワークパラメータ( \\theta 、 \\psi )が最適化されます。\n    - paragraph [ref=s1e345]:\n        - text: ここで、\n        - link "先日書いた記事" [ref=s1e346]:\n            - /url: https://zenn.dev/asap/articles/ea9f42edff6573#%E7%AC%AC%E4%B8%80%E9%A0%85%E3%82%92%E8%80%83%E3%81%88%E3%82%8B\n        - text: にも記載しましたが、第1項は「再構成誤差」、第2項は「正則化項」になります。\n    - paragraph [ref=s1e347]: 式変形を行うと、第1項はDecoderの出力 y と、正解データ x との2乗誤差の負数になります。\n        したがってELBOを最大化する際には、2乗誤差を最小化する方向にVAEのネットワークパラメータ( \\theta 、 \\psi\n        )が最適化されます。\n    - complementary [ref=s1e357]:\n        - text: "!"\n        - paragraph [ref=s1e360]: なお、ここで再構成誤差が2乗誤差となるのは、VAEの出力の確率分布を正規分布と仮定したためです。\n            もし、出力の分布を「ベルヌーイ分布」と仮定した場合、再構成誤差項は下記のようにクロスエントロピー損失と等価になります。\n        - paragraph [ref=s1e362]: ベルヌーイ分布は、データ内のあるサンプル i を考えると、下記で表せます。\n        - text: "p_{\\\\theta}(x_i|z_i) = y_i^{x_i} (1 - y_i)^{1 - x_i}"\n        - paragraph [ref=s1e368]: "この時、ELBOの第1項は、期待値計算において、テンプルサイズ1で近似すると、 \\\\log\n            p_{\\\\theta}(x|z) に近似できるため、これを計算すると、"\n        - text: "\\\\log p_{\\\\theta}(x_i|z_i) = x_i \\\\log y_i + (1 - x_i) \\\\log (1 - y_i)"\n        - paragraph [ref=s1e374]: となるため、まさしくクロスエントロピー項となります。\n        - paragraph [ref=s1e375]: ただし、ベルヌーイ分布と仮定するためには、生成するデータ x が[0,1]である必要があります。\n    - paragraph [ref=s1e378]: "第2項は、Encoderが出力する q_{\\\\psi}(z|x) と平均 0 、分散 I の標準正規分布\n        \\\\mathcal{N}(0, I) とのKLダイバージェンスの負数になります。\n        したがってELBOを最大化する際には、KLダイバージェンス自体を最小化する方向にVAEのEncoderパラメータ( \\\\psi\n        )が最適化されます。 KLダイバージェンスは、2つの分布が一致しているときに最小値 0 をとります。したがって、Encoderが出力する確率分布\n        q_{\\\\psi}(z|x) が標準正規分布 \\\\mathcal{N}(0, I) に近づくように最適化されます。"\n    - heading "ELBOの第2項" [level=3] [ref=s1e397]\n    - paragraph [ref=s1e398]: 第1項は再構成誤差のため、分かりやすいと思いますが、第2項がどう言う意味を含むのかが分かりにくいと思います。\n    - complementary [ref=s1e399]:\n        - text: "!"\n        - paragraph [ref=s1e402]: ELBOの第2項\n        - text: "- \\\\mathrm{KL}\\\\left(q_{\\\\psi}(z|x) \\\\parallel p(z)\\\\right)"\n    - paragraph [ref=s1e406]: "ここでは、潜在表現 z の事前分布が標準正規分布 \\\\mathcal{N}(0, I) に従うと仮定していました。"\n    - paragraph [ref=s1e411]:\n        - text: "また、実は、Encoderが出力する近似事後分布 q_{\\\\psi}(z|x) は、ELBOに関係なく、実際の計算不能な真の事後分布\n            p_{\\\\theta}(z|x) に近づくように学習されています。 この詳細は、"\n        - link "以前書いた記事" [ref=s1e417]:\n            - /url: https://zenn.dev/asap/articles/ea9f42edff6573#elbo%E3%81%8C%E5%AF%BE%E6%95%B0%E5%B0%A4%E5%BA%A6%E3%81%AB%E8%BF%91%E3%81%A5%E3%81%8F%E6%9D%A1%E4%BB%B6\n        - text: "をご覧ください。 これは主に、Encoderが出力する近似事後分布 q_{\\\\psi}(z|x) が、真の事後分布\n            p_{\\\\theta}(z|x) に近づくことで、Decoderの再構成誤差が小さくなることに由来します。"\n    - paragraph [ref=s1e423]: "その上で、ELBOの第2項では、Encoderが出力する近似事後分布 q_{\\\\psi}(z|x)\n        は、標準正規分布 \\\\mathcal{N}(0, I) に近づくようにも学習されることを示しています。"\n    - paragraph [ref=s1e428]: "したがって、上記の議論をまとめると、Encoderが出力する近似事後分布 q_{\\\\psi}(z|x)\n        は、標準正規分布 \\\\mathcal{N}(0, I)\n        に近づくようにも学習されますが、その上で、Decoderの再構成誤差とバランスをとりながら、真の事後分布 p_{\\\\theta}(z|x)\n        にも近づくように学習されます。"\n    - paragraph [ref=s1e435]: "さらに言うと、真の事後分布 p_{\\\\theta}(z|x) は、Decoderパラメータ \\\\theta\n        依存して変化します。（計算できないですが、必ず存在し、 \\\\theta と x に依存します） この場合、真の事後分布\n        p_{\\\\theta}(z|x) もEncoderが出力する近似事後分布 q_{\\\\psi}(z|x)\n        に近づくような力学が多少なりとも働くことになります。"\n    - paragraph [ref=s1e449]: したがってDecoder側は、過度に複雑な事後分布をEncoder側に近似することを要求せず、ある程度、真の事後分布が標準正規分布に近づくように、（すなわち標準正規分布からデータを再構成できるように）学習が協力して進んでいく挙動をします。\n    - paragraph [ref=s1e450]: "以上を考慮すると、Decoderは、Encoderがないときには、事前分布として仮定した、標準正規分布\n        \\\\mathcal{N}(0, I) からサンプリングされた潜在表現 z を利用することで、そこそこ意味のあるデータを生成することができます。\n        この特性から、VAEは「生成モデル」として利用できることがわかります。"\n    - complementary [ref=s1e456]:\n        - text: "!"\n        - paragraph [ref=s1e459]: ただし、この関係は非常に難しいバランスの上で成立しています。\n        - paragraph [ref=s1e460]: 潜在表現の次元が増えたり、目的関数の項ごとの比率が変わるなどして、再構成誤差項と正則化項のバランスが崩れた際には、上記が成立しなくなります。\n            それについては後述します。\n        - paragraph [ref=s1e462]: しかし、その場合であっても、上記のような力学はある程度働きながら、学習が進むことになります。\n    - heading "Decoderが学習する潜在表現のランダム性" [level=3] [ref=s1e463]\n    - paragraph [ref=s1e464]: VAEの全体の構造を改めて下記に提示します。\n    - paragraph [ref=s1e465]:\n        - img [ref=s1e466]\n    - paragraph [ref=s1e467]: "こちらを見ると、Decoderに入力される潜在表現 z というのは、平均 \\\\mu ・分散 \\\\sigma^2\n        パラメータから得られる正規分布 \\\\mathcal{N}(\\\\mu, \\\\sigma) からサンプリングされた値を用いていることがわかります。\n        すなわち、学習時にDecoderが見る潜在表現 z には多少のランダム性がございます。\n        これは、Encoderに入力されるデータが同じデータであっても、decoderはランダムに異なる潜在表現 z から入力データ x\n        を再構成することを示します。"\n    - paragraph [ref=s1e484]: "そしてこのサンプリングは、平均ベクトル \\\\mu を中心として、正規分布 \\\\mathcal{N}(\\\\mu,\n        \\\\sigma) としてサンプリングされるため、中心から若干だけズレた値がサンプリングされることになります。"\n    - paragraph [ref=s1e489]: このことを考えると、Decoderは毎回若干異なる潜在表現 z から、入力データ x\n        を再構成することになります。 したがって、Decoderが入力データ x を再構成する際の潜在表現 z の許容範囲が広くなるように学習されます。\n        これを大量の入力 x に対して学習を実施すると、似たようなデータ同士では、似たような潜在表現 z\n        をEncoderがサンプリングするようになり、大きく異なるデータ同士では、異なる潜在表現 z をEncoderがサンプリングするようになります。\n    - paragraph [ref=s1e506]: これが達成されると、潜在表現 z が少し変わると、再構成データ y も少しだけ変わるという滑らかさをDecoderは得ることができます。\n    - paragraph [ref=s1e511]: この特性のおかげで、VAEは、2種類のデータ（例えば、男性の顔写真と女性の顔写真）を構成する潜在表現 z\n        の間を動かすことで、出力データ y は男性の顔から女性の顔に滑らかに変化するような挙動をするようになります。\n        これはよくVAEの解説サイトなどで見ることのできる挙動だと思います。\n    - heading "VAEはどうやって学習しているのか" [level=3] [ref=s1e517]\n    - paragraph [ref=s1e518]: もう一度VAEの構成図を表示します。\n    - paragraph [ref=s1e519]:\n        - img [ref=s1e520]\n    - paragraph [ref=s1e521]: ここまで読んで、勘のいい読者は、下記のように考えるかもしれません。\n        「確率分布からのSampling処理は微分不可能なため、学習できないのでないか」と\n    - paragraph [ref=s1e523]: その疑問はおっしゃる通りです。 ではここから、VAEの構造を損なわない形で、上記の図中からSampling構造を取り除くことを考えます。\n    - heading "Decoder出力部分" [level=4] [ref=s1e525]\n    - paragraph [ref=s1e526]: "図を見ると、Decoderの出力部分にてSampling構造があります。\n        生成モデルは、「対数尤度を最大化する」という目的のため、確率分布を出力することが求められますが、Decoderの出力はベクトルです。したがって\\\n        、出力ベクトル y を平均、単位行列 I を分散とする正規分布 \\\\mathcal{N}(y, I)\n        を確率分布として考え、そこからサンプリングされた値 x\' を生成データだと考えます。"\n    - paragraph [ref=s1e536]: この形で、目的関数ELBOを計算すると、出力ベクトル y と、入力データ x\n        の2乗誤差を最小化することが最終的な目的関数になることがわかります。\n        したがって、Sampling処理をなくすため、Decoderが出力するベクトル y 自体を、生成したデータ x\'=y と見なすことが可能です。\n    - complementary [ref=s1e546]:\n        - text: "!"\n        - paragraph [ref=s1e549]: 可能と言っているのは、 x\'=y と解釈したところで、目的関数は変わらず、パラメータを更新する微分値が変わらないことを指しています。\n    - paragraph [ref=s1e552]:\n        - text: したがって、これを考慮すると下記のような図になります。\n        - img [ref=s1e554]\n    - heading "Encoder出力部分" [level=4] [ref=s1e555]\n    - paragraph [ref=s1e556]: "続いて、Samplingが残っている箇所は、Encoderの出力する平均 \\\\mu ・分散 \\\\sigma^2\n        パラメータから得られる正規分布 \\\\mathcal{N}(\\\\mu, \\\\sigma) からサンプリングして、潜在表現 z\n        を得る部分になります。"\n    - paragraph [ref=s1e565]: この部分では、微分可能な計算グラフにするために、「Reparameterization\n        Trick」という手法を利用しています。 統計学に詳しい方には、「標準化の逆」という説明がしっくりくるかもしれません。\n    - complementary [ref=s1e567]:\n        - text: "!"\n        - paragraph [ref=s1e570]: 標準化とは、平均 \\mu 、分散 \\sigma^2 の複数データ z に対して、下記の処理を施した後のデータ\n            \\epsilon は、平均 0 、分散 1 となる変換です。\n        - text: "\\\\epsilon = \\\\dfrac{z - \\\\mu}{\\\\sigma}"\n    - paragraph [ref=s1e586]: "VAEでは、Encoderが平均 \\\\mu ・分散 \\\\sigma^2 を出力し、そこから得られる正規分布\n        \\\\mathcal{N}(\\\\mu, \\\\sigma) から潜在表現 z をサンプリングしています。"\n    - paragraph [ref=s1e595]: "このようなサンプリングの操作を、計算グラフの外に出すことで計算可能にする手法をReparameterizet\\\n        ion Trickと呼びます。 具体的には、計算グラフの外であらかじめ、平均 0 ・分散 1 の標準正規分布 \\\\mathcal{N}(0,\n        1) から乱数 \\\\epsilon を取得しておきます。"\n    - paragraph [ref=s1e605]: "その後、潜在表現 z は下記の計算を行うことで、まるで「標準化の逆」を行ったように、平均 \\\\mu ・分散\n        \\\\sigma^2 の正規分布 \\\\mathcal{N}(\\\\mu, \\\\sigma) からサンプリングされたような値に変換できます。"\n    - text: z = \\mu + \\sigma \\epsilon\n    - paragraph [ref=s1e617]: この処理を反映させると下記のような図になります。\n    - paragraph [ref=s1e618]:\n        - img [ref=s1e619]\n    - paragraph [ref=s1e620]: この処理により、DeocderとEncoderを繋ぐ部分にはSampling処理が現れなくなったため、Decoder側からEncoder側に勾配が正しく伝播するようになるため、学習が可能なネットワークになりました。\n    - heading "VAEを生成モデルとして利用する" [level=2] [ref=s1e621]\n    - heading "VAEの特徴" [level=3] [ref=s1e622]\n    - paragraph [ref=s1e623]: さて、ここまでの議論からVAEには下記の特徴を持つことが言えます。\n    - list [ref=s1e624]:\n        - listitem [ref=s1e625]:\n            - text: VAEの潜在表現 z には滑らかさが存在し、 z を少し変化させると生成されるデータ x\' が少しだけ変化する\n            - list [ref=s1e632]:\n                - listitem [ref=s1e633]:\n                    - text: この特徴より、潜在空間内において、2地点を行き来することで、滑らかに画像を変化させることができる\n                    - list [ref=s1e634]:\n                        - listitem [ref=s1e635]: 例えば、男性の顔写真から女性の顔写真へと滑らかに生成データを変化させることができる\n        - listitem [ref=s1e636]:\n            - text: "VAEの潜在表現の事前分布 p(z) は標準正規分布 \\\\mathcal{N}(0, 1) を仮定している"\n            - list [ref=s1e641]:\n                - listitem [ref=s1e642]: "標準正規分布 \\\\mathcal{N}(0, 1) から潜在表現 z\n                    をサンプリングすることができ、それを用いて「ある程度意味のある」データ x\' を生成できる"\n    - paragraph [ref=s1e649]: したがって、新しいデータを生成できるという観点で、生成モデルと呼ぶことができます。\n    - heading "落とし穴" [level=3] [ref=s1e650]\n    - paragraph [ref=s1e651]: と、私も昔は思っておりました。 いや、完全に間違っている訳ではないのですが、時には理論通りに進まないものです。\n    - paragraph [ref=s1e653]: 正確には、VAEの特徴は場合によっては正しくて、場合によっては正しくないです。\n        「目的関数の再構成誤差と正則化項のバランス」が崩れた場合に、VAEの特徴は機能しなくなります。\n        （正確には、ある程度その特徴は持っているが、だんだん機能しなくなってくるというイメージ）\n    - paragraph [ref=s1e656]: これから、説明させていただきます。 ここで、VAEの目的関数であるELBOを再提示します。\n    - complementary [ref=s1e658]: "! \\\\mathrm{ELBO} =\n        \\\\mathbb{E}_{q_{\\\\psi}(z|x)}[\\\\log p_{\\\\theta}(x|z)] -\n        \\\\mathrm{KL}\\\\left(q_{\\\\psi}(z|x) \\\\parallel p(z)\\\\right)"\n    - paragraph [ref=s1e664]: この式は、第1項の再構成誤差項と、第2項の正則化項の「バランス」をとって学習が進むことがわかっています。\n        このバランスは、例えば、第2項に対してハイパーパラメータを導入し、その寄与割合を変化させることによっても変わります。 （これを\n        β-VAE（ベータVAE）と呼びます）\n    - paragraph [ref=s1e667]:\n        - strong [ref=s1e668]: そしてもう一つ、潜在表現の次元数によっても変わります。\n    - paragraph [ref=s1e669]: よく見るVAEの解説記事では、MNISTなどの簡単な画像に対して、潜在表現の次元を2次元に設定して検証している記事が多いと思います。\n        これは、VAEの潜在表現の分布と、手描き文字の数字を対応付けて可視化するために非常に優れた例示の仕方だと思っております。\n        一方で、「VAEであれば、必ず、潜在表現の事前分布が正規分布に落ち着く」というようなミスリードを読者に引き起こす危険性もはらんでいると思っております。\n    - heading "潜在表現の次元が小さい場合" [level=3] [ref=s1e672]\n    - paragraph [ref=s1e673]: 他記事のように、VAEの潜在表現 z が2次元の場合、潜在表現 z はたった二つの値しか持たないことになります。\n        2次元の潜在表現 z では、画像の情報を保持することは困難です。\n    - paragraph [ref=s1e681]: この場合、VAEは、潜在空間 z に情報を埋め込むのは諦めます。 そして、潜在空間 z\n        はあくまで「乱数のサンプリング+おおよそのラベル情報」と割り切り、画像の再構成情報をDecoderに埋め込むように学習します。\n    - paragraph [ref=s1e687]: "すると、潜在表現 z は画像情報などの追加情報をほとんど持たないため、事前分布\n        p(z)=\\\\mathcal{N}(0, 1) に従って分布されるようになります。"\n    - paragraph [ref=s1e692]: この場合は、上記の「VAEの特徴」で記載した生成モデルとしての特徴を強く持つことになります。\n    - paragraph [ref=s1e693]: これは、ELBOの正則化項の寄与率を大きくしても発生します。\n    - heading "潜在変数の次元が多い場合" [level=3] [ref=s1e694]\n    - paragraph [ref=s1e695]: 一方で、潜在変数の次元数が多い場合はどうか。\n        潜在変数の次元数が多い場合は、あまり解説に向かないため、解説記事は少ないです。\n        しかし、潜在変数の次元数が多いVAEが使われており、比較的解析しやすい技術はわかります。\n    - paragraph [ref=s1e698]:\n        - strong [ref=s1e699]: みなさんご存知、Stable Diffusionです。\n    - heading "Stable Diffusionのモデル構造" [level=4] [ref=s1e700]\n    - paragraph [ref=s1e701]:\n        - img [ref=s1e702]\n        - text: （引用：\n        - link "https://github.com/CompVis/latent-diffusion/blob/main/assets/modelfigure.png）" [ref=s1e703]:\n            - /url: https://github.com/CompVis/latent-diffusion/blob/main/assets/modelfigure.png%EF%BC%89\n    - paragraph [ref=s1e704]: Stable Diffusionのモデル構造は上記のようになっております。\n    - paragraph [ref=s1e705]:\n        - strong [ref=s1e706]: Conditioningブロック\n    - paragraph [ref=s1e707]: 一番右の「Conditioning」ブロックは、画像の条件付けに使われる部分です。 例えば Stable\n        Diffusionにおける、Text to Imageモデルの場合、プロンプトと呼ばれる呪文で、生成される画像のテーマを制御することができます。\n    - paragraph [ref=s1e709]: なぜかというと、まず、「Conditioning」ブロックで使われいてる、CLIPやT5\n        Encoderと呼ばれるtext Encoderにより、プロンプトを特徴量に変換しています。\n        このとき、CLIPは、「画像とテキストの特徴量が一致」するように学習されているため、プロンプトからCLIPを通じて得られた特徴量は、画像の特徴量と同様に取り扱うことができます。\n        そのため、プロンプトの特徴量を、画像特徴量として、後述する「クロスアテンション」にて、計算することができるようになるからです。\n    - paragraph [ref=s1e712]:\n        - strong [ref=s1e713]: Latent Spaceブロック\n        - text: 真ん中の「Latent Space」ブロックでは、潜在表現 z を生成するブロックになります。\n            ここでは、「拡散モデル」というモデルを利用して、潜在表現 z を生成します。 （拡散モデルに関しては\n        - link "こちらの記事も" [ref=s1e721]:\n            - /url: https://zenn.dev/asap/articles/4092ab60570b05\n        - text: ご覧ください）\n    - paragraph [ref=s1e722]: また、拡散モデルでは、TransformerやU-netというモデルが利用されていますが、そのどちらにおいても、CLIPから得られたプロンプト特徴量を「クロスアテンション」により注入しています。\n    - paragraph [ref=s1e723]:\n        - text: クロスアテンションとは、処理中の画像特徴量と、プロンプト特徴量の類似度を計算し、類似度が高い画像特徴量を増幅させるような処理が行われており、この処理により、プロンプトに適した画像になるように潜在表現\n            z が生成されていきます。 （アテンションに関しては、\n        - link "こちらの記事" [ref=s1e727]:\n            - /url: https://zenn.dev/asap/articles/6bdf89d8e964ec#self-attension%E3%81%A8%E3%81%AF\n        - text: でも解説しております。）\n    - paragraph [ref=s1e728]:\n        - strong [ref=s1e729]: Pixel Spaceブロック\n        - text: 最後に、一番右側の「Pixel Space」ブロックが、VAEが担当する部分です。 上の段の x は生成したい入力データで、 \\varepsilon\n            と書かれている部分がEncoderです。\n    - paragraph [ref=s1e736]: "そして、下の段の \\\\tilde{x} が生成データで、 D と書かれているのが、Decoderになります。"\n    - paragraph [ref=s1e741]: "すなわち、拡散モデルが生成した潜在表現 z から、VAE decoderが出力画像 \\\\tilde{x} を生成していることになります。"\n    - complementary [ref=s1e746]:\n        - text: "!"\n        - paragraph [ref=s1e749]: ここでは、VAEは完全畳み込み層になっており、入力が3次元の画像（縦x横xチャネル）データの場合、潜在表現 z\n            は(縦/8 x 横/8 x 4)の形になっています。\n    - paragraph [ref=s1e752]: "さて、ここで重要なのは、Stable\n        Diffusionでは、VAEが使われてるはずですが、画像を生成する際に標準正規分布 \\\\mathcal{N}(0, 1)\n        からサンプリングした潜在表現 z を利用していないことです。"\n    - paragraph [ref=s1e757]: ちなみに、(詳しい方向けに補足しておくと)Stable Diffusion\n        XLで使われているのは、VQ-VAEではなく、VAEです。\n        （これはDiffusersライブラリでSDXLの部分を見ればわかります。Diffusersライブラリにて実装されているのは、通常のVAEです。）\n    - group [ref=s1e759]: 該当コード\n    - paragraph [ref=s1e761]: "Stable Diffusionでは何をやっているのかというと、標準正規分布 \\\\mathcal{N}(0,\n        1) から得られた潜在表現を、拡散モデルにより分布を変換し、変換後の潜在表現 z\n        をVAEのdecoderに入力することで、画像を生成しています。"\n    - paragraph [ref=s1e766]: "すなわち、上述した「VAEの特徴：標準正規分布 \\\\mathcal{N}(0, 1) から潜在表現 z\n        をサンプリングすることができ、それを用いて「ある程度意味のある」データ x\' を生成できる」が崩壊していることがわかります。"\n    - heading "Stable Diffusion内部のVAEの特徴" [level=4] [ref=s1e773]\n    - paragraph [ref=s1e774]:\n        - text: "実際、単純な標準正規分布 \\\\mathcal{N}(0, 1) から得られた潜在表現を、そのままVAE\n            decoderに入力すると、ノイズ画像が生成されます。 そのあたりは下記の記事などもご参照ください。"\n        - iframe [ref=s1e779]: <could not take iframe snapshot>\n    - paragraph [ref=s1e780]: 上記の記事中のgifを見ると、最初の方は砂嵐のようなノイズ画像が表示されており、だんだん拡散モデルにより潜在表現\n        z の分布が変化していくと、decoderが画像として出力できるようになっているのがわかると思います。\n    - heading "そのときVAEに何が起きているのか！？" [level=4] [ref=s1e783]\n    - paragraph [ref=s1e784]: "これは何が起きているのかというと、第一に、VAEの潜在表現 z の事前分布 p(z) が標準正規分布\n        \\\\mathcal{N}(0, 1) から大きくずれてしまっています。"\n    - paragraph [ref=s1e791]: これは、学習時に、ELBO第2項の正則化項が入っているのにも関わらず・・・です。\n    - paragraph [ref=s1e792]:\n        - text: 正則化項は、下記のコードの\n        - code [ref=s1e793]: DiagonalGaussianDistribution.kl\n        - text: メソッドにて計算がされています。\n        - iframe [ref=s1e795]: <could not take iframe snapshot>\n    - paragraph [ref=s1e796]: "では、なぜVAEの潜在表現 z の事前分布 p(z) が標準正規分布 \\\\mathcal{N}(0, 1)\n        から大きくずれるのかというと、VAEの潜在表現 z の次元数の多さが原因です。"\n    - paragraph [ref=s1e805]: 上述した通り、Stable Diffusionの潜在表現 z のサイズは「縦/8 x 横/8 x\n        4」になっています。 Stable Diffusion XLでは、学習される画像のサイズは「1024x1024」画素が一般的です。\n        このとき、潜在表現 z のサイズは、「128 x 128 x 4」となり、潜在表現 z の要素数は65536となります。\n        65536という数字は、MNISTの実験で使っていた2次元の潜在表現の次元数と比較して、非常に大きな次元数であることがわかります。\n    - paragraph [ref=s1e815]: このレベルで潜在表現 z の次元が大きくなると、潜在表現 z 自体にも画像の情報を埋め込めるようになります。\n        2次元しか潜在表現 z\n        がない場合、潜在表現に情報を埋め込むのは困難なため、画像の再構成情報はDecoderに全て埋め込まれるような挙動になると、記載しました。\n        しかし、今回の場合は、潜在表現 z とDecoderの協業で、画像を再構成することになります。\n    - paragraph [ref=s1e826]: これは、すなわち正則化項よりも、再構成損失項の方が強いバランスになります。\n        潜在表現に画像情報を埋め込むことによって、正則化項の損失が大きくなったとしても、その分大幅に再構成損失項の損失を減少させることができるからです。\n    - paragraph [ref=s1e828]: "その結果、潜在表現 z の事前分布 p(z) が、標準正規分布 \\\\mathcal{N}(0, 1)\n        から大きく離れるようになります。 その代わり、Stable Diffusionでは、より高品質な画像が生成できるようになっているわけです。"\n    - heading "だから拡散モデルがいる" [level=4] [ref=s1e836]\n    - paragraph [ref=s1e837]: ただし、VAEが正則化項を無視し始めると、画質が良くなる代わりに、潜在表現 z の事前分布 p(z)\n        は意味がわからない分布になります。 この意味のわからない分布を、人間は把握することはできず、その分布からサンプリングした潜在表現 z\n        をEncoderなしでは取得できません。\n    - paragraph [ref=s1e845]: そこで、その潜在表現の事前分布 p(z) を、拡散モデルに学習してもらいます\n        そして、その学習ずみ拡散モデルが、VAEの潜在表現 z\n        を生成することで、高品質なVAEの生成画像を維持しながら、生成モデルとして0から新しいデータを生み出しつつ、さらにプロンプトによる条件付けまで達成しています。\n        （だから、これだけ騒がれて、めちゃくちゃ使われているわけですね）\n    - complementary [ref=s1e852]:\n        - text: "!"\n        - paragraph [ref=s1e855]: この場合、VAEの特徴はかなり少なくなっており、どちらかというと Auto Encoderに近い挙動をします。\n            しかし、正則化項が完全になくなったわけではないため、ある程度、潜在表現の滑らかさなどは残っており、結果として、Auto\n            Encoderの潜在表現 z よりも、VAEの潜在表現 z の方が、拡散モデルが学習しやすくなっていると思われます。\n    - heading "最後にVAEの課題" [level=1] [ref=s1e861]\n    - paragraph [ref=s1e862]: ここまでVAEのいいところを説明してきましたが、VAEを単体で利用する場合は、まだまだ課題があります。\n    - heading "Posterior Collapse" [level=2] [ref=s1e863]\n    - paragraph [ref=s1e864]: "一つは「Posterior Collapse（事後崩壊）」の問題です。\n        これは、VAEの目的関数ELBOの正則化項が強すぎる場合、Encoderが出力する近似事後分布 q_{\\\\psi}(z|x) と、事前分布（\n        p(z)=\\\\mathcal{N}(0, 1) )が完全に一致してしまうことで発生します。\n        この場合、画像の再構成情報が完全にDecoderに入ってしまい、潜在表現 z\n        を無視した再構成が行われてしまいます。なまじDecoderの性能が高いと発生しやすい問題です。"\n    - paragraph [ref=s1e873]: posterior collapseの対策として、学習初期は正則化項の寄与率を下げて学習させ、だんだん寄与率を大きくしていくことが挙げられます。\n    - heading "画像がボケやすい" [level=2] [ref=s1e874]\n    - paragraph [ref=s1e875]: もう一つは、生成される画像がボケやすい点です。 Stable\n        Diffusionに搭載されているVAEのように、事前分布 p(z) の形状を完全に気にしないように学習する（つまり潜在表現 z\n        の次元数が非常に大きい）場合は起こりにくい問題ではありますが、ある程度正則化項による正則化を入れていきたい場合（つまり、事前分布 p(z)\n        に正規分布を仮定したい状況）では、この問題が発生しやすいです。\n    - paragraph [ref=s1e883]: これは、上述した通り、Decoderに入力される潜在表現 z\n        は、乱数のサンプリング後の値であるため、例え全く同じ画像を学習していたとしても、潜在表現 z の値は学習のたびに異なります。\n    - paragraph [ref=s1e888]: すると、似たような画像 y を生成する潜在表現 z は、潜在表現空間において、近づきやすくなります。\n        一方で、decoder側は、潜在表現 z が密集している空間の場合、ある潜在表現 z が、どの生成画像 x\n        に対応しているのか、そのランダム性のために判断することができなくなってしまうため、なるべく最終的な損失を小さくするために、密集している潜在表現\n        z から生成されうる画像 x の平均のような画像を学習してしまいます。 これは、潜在表現 z\n        の次元が小さい方が、潜在表現空間における密集度が高まるため、発生しやすくなります。（一般に低次元空間の方が2点間の距離は小さく、高次元空間の方が2点間の距離が広くなる）\n    - paragraph [ref=s1e907]: "これの対処法は、VAEの潜在表現の次元を増やすことか、第2項の寄与率を下げるくらいしかありませんでした。\n        しかし、これを行うと、どうしても生成モデルとしての特徴である、「潜在表現zの事前分布 p(z) が標準正規分布 \\\\mathcal{N}(0,\n        1) である」という仮定が消失しやすくなります。"\n    - paragraph [ref=s1e913]: したがって、バランス調整が難しい問題で、VAEが持つ悪い特徴の一つでしたが、後日記事を書くVQ-VAEでは、この問題を解消しています。\n    - heading "まとめ" [level=1] [ref=s1e914]\n    - paragraph [ref=s1e915]: ここまで読んでくださってありがとうございました！\n        生成モデルと言えば、Transformerや拡散モデルが主流ですが、非常に長い系列長（音声など）を生成する場合には、まだまだVAEが使われいてるように見えます。\n    - paragraph [ref=s1e917]: VAEは一度に全てのデータを生成できるため、速度が必要なアプリケーションにも適していますし、圧縮器としての機能もあるため、大きなデータを潜在表現に圧縮し、その潜在表現をTransformerや拡散モデルによって再構成するといった使い方がよくなされており、古い概念ではありますが、まだまだVAEには価値があるなと感じております。\n    - paragraph [ref=s1e918]: わかった気になっていましたが、VAEに関して詳細に色々調べていると、まだまだ理解があやふやな部分もあり、記事を書きながら理解度を高めることができました！\n    - paragraph [ref=s1e919]: この記事が、何らかの形でみなさまの手助けになれば幸いです！\n    - paragraph [ref=s1e920]: ここまで読んでくださり、ありがとうございました！\n    - heading "参考文献" [level=1] [ref=s1e921]\n    - paragraph [ref=s1e922]:\n        - iframe [ref=s1e924]: <could not take iframe snapshot>\n        - text: VAEの論文です。\n    - paragraph [ref=s1e925]:\n        - link "ゼロから作るDeep Learning ❺ ―生成モデル編" [ref=s1e926]:\n            - /url: https://amzn.to/3Aw9vCt\n    - paragraph [ref=s1e927]: 圧倒的な名著シリーズである「ゼロから作るDeep Learning」シリーズの5作目である、生成モデル編です。\n        この本では、最終的に拡散モデルを理解することを目的に、VAEや混合ガウスモデル、正規分布まで遡り、本当にゼロから中身を理解しながら実装をすることができる、大変素晴らしい書籍になっています。\n    - paragraph [ref=s1e929]:\n        - link "妥協しないデータ分析のための 微積分+線形代数入門" [ref=s1e930]:\n            - /url: https://amzn.to/4hE7ftx\n    - paragraph [ref=s1e931]: 本書の後半にもVAEやELBOについての簡単な記載があり、本記事を書くうえで表現を参考にさせていただきました。\n        確率分布のどの部分が計算可能で、どの部分が計算不可能なのかが常に明示されながら進んでいくので、初学者には非常に分かりやすい書籍でした！\n    - paragraph [ref=s1e933]: 特に、VAEなどの生成AIをただ使うだけでなく、ちゃんと理論を理解したいと思った場合、論文や理論書を理解するために「線形代数」や「微積分学」の事前知識が必要になります。\n        本書は、初学者が一番最初に読んで必要な基礎知識が習得できる良本になります。\n    - paragraph [ref=s1e935]:\n        - link "生成 Deep Learning 第2版 ―絵を描き、物語や音楽を作り、ゲームをプレイする" [ref=s1e936]:\n            - /url: https://amzn.to/4g8hjJg\n    - paragraph [ref=s1e937]: 本書の序盤で、Auto\n        EncoderとVAEの違いを説明してくれています。しかもそれぞれの潜在空間の分布を可視化してくれているので、違いが分かりやすく、理解もしやすいです。\n        さらに、本書はVAEだけでなくGANや画像生成、音楽生成、テキスト生成など、ありとあらゆる生成AIの紹介と解説をしてくれており、必読の一冊となっております。\n    - paragraph [ref=s1e939]: （書籍のリンクはAmazonアフィリエイトリンクになっております）\n    - button "いいね" [ref=s1e942]:\n        - img [ref=s1e943]\n    - text: "104"\n    - button "ブックマーク" [ref=s1e959]:\n        - img [ref=s1e960]\n    - text: "43"\n    - button "その他の操作" [ref=s1e965]:\n        - img [ref=s1e966]\n    - tooltip "X(Twitter)にポスト" [ref=s1e969]:\n        - img [ref=s1e970]\n        - text: X(Twitter)にポスト\n    - tooltip "Facebookに投稿" [ref=s1e976]:\n        - img [ref=s1e977]\n        - text: Facebookに投稿\n    - tooltip "はてなブックマークに登録" [ref=s1e983]:\n        - img [ref=s1e984]\n        - text: はてなブックマークに登録\n    - complementary [ref=s1e987]:\n        - link "asap" [ref=s1e989]:\n            - /url: /asap\n            - img "asap" [ref=s1e990]\n        - link "asap" [ref=s1e992]:\n            - /url: /asap\n        - paragraph [ref=s1e994]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n        - button "フォロー" [ref=s1e998]\n        - tooltip "@personabb" [ref=s1e999]:\n            - img "GitHub" [ref=s1e1000]\n            - text: "@personabb"\n        - tooltip "@asap2650" [ref=s1e1002]:\n            - img "X(Twitter)" [ref=s1e1003]\n            - text: "@asap2650"\n        - tooltip "yard.tips" [ref=s1e1009]:\n            - img "リンク" [ref=s1e1010]\n            - text: yard.tips\n    - complementary [ref=s1e1013]:\n        - text: バッジを贈って著者を応援しよう\n        - paragraph [ref=s1e1017]: バッジを受け取った著者にはZennから現金やAmazonギフトカードが還元されます。\n        - button "バッジを贈る" [ref=s1e1019]\n        - img [ref=s1e1020]\n    - heading "Discussion" [level=3] [ref=s1e1029]\n    - img [ref=s1e1030]\n    - text: ログインするとコメントできます\n    - button "Login" [ref=s1e1034]\n    - complementary [ref=s1e1035]:\n        - link "asap" [ref=s1e1040]:\n            - /url: /asap\n            - img "asap" [ref=s1e1041]\n        - link "asap" [ref=s1e1043]:\n            - /url: /asap\n        - button "フォロー" [ref=s1e1045]\n        - link "@personabb" [ref=s1e1046]:\n            - /url: https://github.com/personabb\n            - tooltip "@personabb" [ref=s1e1047]:\n                - img "GitHub" [ref=s1e1048]\n                - text: "@personabb"\n        - link "@asap2650" [ref=s1e1050]:\n            - /url: https://twitter.com/asap2650\n            - tooltip "@asap2650" [ref=s1e1051]:\n                - img "X(Twitter)" [ref=s1e1052]\n                - text: "@asap2650"\n        - link "yard.tips" [ref=s1e1058]:\n            - /url: https://yard.tips/asap2650\n            - tooltip "yard.tips" [ref=s1e1059]:\n                - img "リンク" [ref=s1e1060]\n                - text: yard.tips\n        - paragraph [ref=s1e1063]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n        - button "バッジを贈る" [ref=s1e1066]\n        - link "バッジを贈るとは" [ref=s1e1068]:\n            - /url: /faq#badges\n            - text: バッジを贈るとは\n            - img [ref=s1e1069]\n        - text: 目次\n        - list [ref=s1e1078]:\n            - listitem [ref=s1e1079]:\n                - link "はじめに" [ref=s1e1080]:\n                    - /url: "#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB"\n            - listitem [ref=s1e1081]:\n                - link "参考文献" [ref=s1e1082]:\n                    - /url: "#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"\n            - listitem [ref=s1e1083]:\n                - link "関連記事" [ref=s1e1084]:\n                    - /url: "#%E9%96%A2%E9%80%A3%E8%A8%98%E4%BA%8B"\n            - listitem [ref=s1e1085]:\n                - link "Auto Encoder" [ref=s1e1086]:\n                    - /url: "#auto-encoder"\n                - list [ref=s1e1087]:\n                    - listitem [ref=s1e1088]:\n                        - link "Auto Encoderの紹介" [ref=s1e1089]:\n                            - /url: "#auto-encoder%E3%81%AE%E7%B4%B9%E4%BB%8B"\n                    - listitem [ref=s1e1090]:\n                        - link "Auto Encoderの本質" [ref=s1e1091]:\n                            - /url: "#auto-encoder%E3%81%AE%E6%9C%AC%E8%B3%AA"\n                    - listitem [ref=s1e1092]:\n                        - link "Auto Encoerの課題" [ref=s1e1093]:\n                            - /url: "#auto-encoer%E3%81%AE%E8%AA%B2%E9%A1%8C"\n            - listitem [ref=s1e1094]:\n                - link "VAE（Variational Auto Encoder）" [ref=s1e1095]:\n                    - /url: "#vae%EF%BC%88variational-auto-encoder%EF%BC%89"\n                - list [ref=s1e1096]:\n                    - listitem [ref=s1e1097]:\n                        - link "VAEと生成モデル" [ref=s1e1098]:\n                            - /url: "#vae%E3%81%A8%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB"\n                    - listitem [ref=s1e1099]:\n                        - link "VAEの挙動・振る舞い" [ref=s1e1100]:\n                            - /url: "#vae%E3%81%AE%E6%8C%99%E5%8B%95%E3%83%BB%E6%8C%AF%E3%82%8B%E8%88%9E%E3%8\\\n                                1%84"\n                    - listitem [ref=s1e1101]:\n                        - link "VAEを生成モデルとして利用する" [ref=s1e1102]:\n                            - /url: "#vae%E3%82%92%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB%E3%81%A8%E3%8\\\n                                1%97%E3%81%A6%E5%88%A9%E7%94%A8%E3%81%99%E3%82%\\\n                                8B"\n            - listitem [ref=s1e1103]:\n                - link "最後にVAEの課題" [ref=s1e1104]:\n                    - /url: "#%E6%9C%80%E5%BE%8C%E3%81%ABvae%E3%81%AE%E8%AA%B2%E9%A1%8C"\n                - list [ref=s1e1105]:\n                    - listitem [ref=s1e1106]:\n                        - link "Posterior Collapse" [ref=s1e1107]:\n                            - /url: "#posterior-collapse"\n                    - listitem [ref=s1e1108]:\n                        - link "画像がボケやすい" [ref=s1e1109]:\n                            - /url: "#%E7%94%BB%E5%83%8F%E3%81%8C%E3%83%9C%E3%82%B1%E3%82%84%E3%81%99%E3%81%8\\\n                                4"\n            - listitem [ref=s1e1110]:\n                - link "まとめ" [ref=s1e1111]:\n                    - /url: "#%E3%81%BE%E3%81%A8%E3%82%81"\n            - listitem [ref=s1e1112]:\n                - link "参考文献" [ref=s1e1113]:\n                    - /url: "#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE-1"\n        - paragraph [ref=s1e1115]: Zennからのお知らせ\n        - article [ref=s1e1118]:\n            - link "第２回 AI Agent Hackathon with Google Cloud 第２回 AI Agent Hackathon with Google Cloud 4/14登録受付開始！" [ref=s1e1119]:\n                - /url: /hackathons/google-cloud-japan-ai-hackathon-vol2\n                - img "第２回 AI Agent Hackathon with Google Cloud" [ref=s1e1121]\n                - paragraph [ref=s1e1122]: 第２回 AI Agent Hackathon with Google Cloud 4/14登録受付開始！\n        - article [ref=s1e1123]:\n            - link "Zenncafe日比谷#2 生成AIで変わる仕事 Zenncafe日比谷#2 生成AIで変わる仕事" [ref=s1e1124]:\n                - /url: https://zenn.connpass.com/event/351131/\n                - img "Zenncafe日比谷#2 生成AIで変わる仕事" [ref=s1e1126]\n                - paragraph [ref=s1e1127]: Zenncafe日比谷#2 生成AIで変わる仕事\n- contentinfo [ref=s1e1129]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1133]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1134]\n    - paragraph [ref=s1e1142]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e1144]:\n        - heading "About" [level=4] [ref=s1e1145]\n        - list [ref=s1e1146]:\n            - listitem [ref=s1e1147]:\n                - link "Zennについて" [ref=s1e1149]:\n                    - /url: /about\n            - listitem [ref=s1e1150]:\n                - link "運営会社" [ref=s1e1151]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e1152]:\n                - link "お知らせ・リリース" [ref=s1e1153]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e1154]:\n                - link "イベント" [ref=s1e1156]:\n                    - /url: /events\n    - navigation [ref=s1e1157]:\n        - heading "Guides" [level=4] [ref=s1e1158]\n        - list [ref=s1e1159]:\n            - listitem [ref=s1e1160]:\n                - link "使い方" [ref=s1e1162]:\n                    - /url: /manual\n            - listitem [ref=s1e1163]:\n                - link "法人向けメニュー" [ref=s1e1165]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e1167]:\n                - link "Publication / Pro" [ref=s1e1169]:\n                    - /url: /publications\n            - listitem [ref=s1e1170]:\n                - link "よくある質問" [ref=s1e1172]:\n                    - /url: /faq\n    - navigation [ref=s1e1173]:\n        - heading "Links" [level=4] [ref=s1e1174]\n        - list [ref=s1e1175]:\n            - listitem [ref=s1e1176]:\n                - link "X(Twitter)" [ref=s1e1177]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e1178]:\n                - link "GitHub" [ref=s1e1179]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e1180]:\n                - link "メディアキット" [ref=s1e1182]:\n                    - /url: /mediakit\n    - navigation [ref=s1e1183]:\n        - heading "Legal" [level=4] [ref=s1e1184]\n        - list [ref=s1e1185]:\n            - listitem [ref=s1e1186]:\n                - link "利用規約" [ref=s1e1188]:\n                    - /url: /terms\n            - listitem [ref=s1e1189]:\n                - link "プライバシーポリシー" [ref=s1e1191]:\n                    - /url: /privacy\n            - listitem [ref=s1e1192]:\n                - link "特商法表記" [ref=s1e1194]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e1197]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e1198]\n- alert [ref=s1e1217]\n```\n', name='browser_navigate', tool_call_id='45a87836-eb09-4499-bf44-a2bcdbd3ec2b'), AIMessage(content='「VAEって結局何者なの？」という記事は、生成モデルの一つであるVAE（Variational Autoencoder）について解説した記事です。\n\n記事の内容をまとめると、以下のようになります。\n\n*   **VAEとは:** VAEは、生成モデルとして重要な「対数尤度の最大化」問題を解くために必要な構造を用意したら、結果的にAuto Encoderのような形になったというモデルです。\n*   **Auto Encoderとの違い:** VAEを理解するためには、まずAuto Encoderを理解する必要があります。Auto Encoderは、入力データと出力を一致させるように学習させるネットワークですが、それ自体では生成モデルとして活用することが難しいという課題があります。\n*   **VAEの仕組み:** VAEは、Encoderで入力データを潜在表現と呼ばれる低次元のデータに圧縮し、Decoderでその潜在表現から元のデータを復元します。この際、Encoderは潜在表現の平均と分散を出力し、その分布からサンプリングされた値をDecoderに入力します。これにより、VAEは潜在空間に滑らかさを持ち、生成モデルとして機能することができます。\n*   **VAEの学習:** VAEは、ELBO（Evidence Lower Bound）と呼ばれる目的関数を最大化するように学習されます。ELBOは、再構成誤差と正則化項の和で表され、VAEはこのバランスをとりながら学習を進めます。\n*   **VAEの課題:** VAEには、Posterior Collapse（事後崩壊）や生成される画像がボケやすいといった課題があります。これらの課題は、潜在表現の次元数や正則化項の強さを調整することで、ある程度緩和することができます。\n*   **Stable DiffusionにおけるVAE:** Stable Diffusionでは、VAEが画像のエンコード・デコードを担当しています。ただし、Stable Diffusionでは、VAEの潜在表現の事前分布が標準正規分布から大きくずれており、VAEはAuto Encoderに近い挙動をしています。\n\nこの記事は、VAEの基本的な仕組みから、学習方法、課題、そしてStable DiffusionにおけるVAEの役割まで、幅広く解説しています。特に、VAEの潜在空間の性質や、Stable DiffusionにおけるVAEの挙動について詳しく解説している点が特徴です。', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-bf93ed43-18df-4eb1-bd5a-f1423de25a27-0', usage_metadata={'input_tokens': 70378, 'output_tokens': 467, 'total_tokens': 70845, 'input_token_details': {'cache_read': 0}}), HumanMessage(content=[{'type': 'text', 'text': '3番目のいいねが多い記事は？'}], additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-ed215635-eb93-42e2-8508-9905e26caa2a-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap'}, 'id': 'fb8928e6-1802-4cb0-a1b1-235173be5629', 'type': 'tool_call'}], usage_metadata={'input_tokens': 70853, 'output_tokens': 13, 'total_tokens': 70866, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap\n\n- Page URL: https://zenn.dev/asap\n- Page Title: asapさんの記事一覧 | Zenn\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- banner [ref=s1e32]:\n    - img "asap" [ref=s1e36]\n    - heading "asap" [level=1] [ref=s1e39]\n    - button "フォロー" [ref=s1e41]\n    - paragraph [ref=s1e43]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n    - button "2,214 Likes" [ref=s1e46]\n    - button "310 Followers" [ref=s1e48]\n    - link "@personabb" [ref=s1e51]:\n        - /url: https://github.com/personabb\n        - tooltip "@personabb" [ref=s1e52]:\n            - img "GitHub" [ref=s1e53]\n            - text: "@personabb"\n    - link "@asap2650" [ref=s1e55]:\n        - /url: https://twitter.com/asap2650\n        - tooltip "@asap2650" [ref=s1e56]:\n            - img "X(Twitter)" [ref=s1e57]\n            - text: "@asap2650"\n    - link "yard.tips" [ref=s1e63]:\n        - /url: https://yard.tips/asap2650\n        - tooltip "yard.tips" [ref=s1e64]:\n            - img [ref=s1e65]\n            - text: yard.tips\n    - link "RSS" [ref=s1e67]:\n        - /url: /asap/feed\n        - tooltip "RSS" [ref=s1e68]:\n            - img [ref=s1e69]\n            - text: RSS\n- link "Articles 72" [ref=s1e80]:\n    - /url: /asap\n- link "Scraps 5" [ref=s1e82]:\n    - /url: /asap?tab=scraps\n- link "Comments" [ref=s1e84]:\n    - /url: /asap?tab=comments\n- contentinfo [ref=s1e88]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e92]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e93]\n    - paragraph [ref=s1e101]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e103]:\n        - heading "About" [level=4] [ref=s1e104]\n        - list [ref=s1e105]:\n            - listitem [ref=s1e106]:\n                - link "Zennについて" [ref=s1e108]:\n                    - /url: /about\n            - listitem [ref=s1e109]:\n                - link "運営会社" [ref=s1e110]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e111]:\n                - link "お知らせ・リリース" [ref=s1e112]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e113]:\n                - link "イベント" [ref=s1e115]:\n                    - /url: /events\n    - navigation [ref=s1e116]:\n        - heading "Guides" [level=4] [ref=s1e117]\n        - list [ref=s1e118]:\n            - listitem [ref=s1e119]:\n                - link "使い方" [ref=s1e121]:\n                    - /url: /manual\n            - listitem [ref=s1e122]:\n                - link "法人向けメニュー" [ref=s1e124]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e126]:\n                - link "Publication / Pro" [ref=s1e128]:\n                    - /url: /publications\n            - listitem [ref=s1e129]:\n                - link "よくある質問" [ref=s1e131]:\n                    - /url: /faq\n    - navigation [ref=s1e132]:\n        - heading "Links" [level=4] [ref=s1e133]\n        - list [ref=s1e134]:\n            - listitem [ref=s1e135]:\n                - link "X(Twitter)" [ref=s1e136]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e137]:\n                - link "GitHub" [ref=s1e138]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e139]:\n                - link "メディアキット" [ref=s1e141]:\n                    - /url: /mediakit\n    - navigation [ref=s1e142]:\n        - heading "Legal" [level=4] [ref=s1e143]\n        - list [ref=s1e144]:\n            - listitem [ref=s1e145]:\n                - link "利用規約" [ref=s1e147]:\n                    - /url: /terms\n            - listitem [ref=s1e148]:\n                - link "プライバシーポリシー" [ref=s1e150]:\n                    - /url: /privacy\n            - listitem [ref=s1e151]:\n                - link "特商法表記" [ref=s1e153]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e156]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e157]\n- alert [ref=s1e176]\n```\n', name='browser_navigate', tool_call_id='fb8928e6-1802-4cb0-a1b1-235173be5629'), AIMessage(content='asapさんのZennの記事の中で3番目にいいねが多い記事は、「💯 拡散モデルの理論って面白いよね」で、いいね数は87です。', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-db79d227-62d3-4b10-ab28-7edd9f261c16-0', usage_metadata={'input_tokens': 72673, 'output_tokens': 35, 'total_tokens': 72708, 'input_token_details': {'cache_read': 0}}), HumanMessage(content=[{'type': 'text', 'text': '4番目にいいねが多い記事は？中身も解説して'}], additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-7d87d69a-e022-497f-be64-80b1380f371b-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap'}, 'id': 'd37ce32f-80cf-457d-a5c2-214147053825', 'type': 'tool_call'}], usage_metadata={'input_tokens': 72719, 'output_tokens': 13, 'total_tokens': 72732, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap\n\n- Page URL: https://zenn.dev/asap\n- Page Title: asapさんの記事一覧 | Zenn\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- banner [ref=s1e32]:\n    - img "asap" [ref=s1e36]\n    - heading "asap" [level=1] [ref=s1e39]\n    - button "フォロー" [ref=s1e41]\n    - paragraph [ref=s1e43]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n    - button "2,214 Likes" [ref=s1e46]\n    - button "310 Followers" [ref=s1e48]\n    - link "@personabb" [ref=s1e51]:\n        - /url: https://github.com/personabb\n        - tooltip "@personabb" [ref=s1e52]:\n            - img "GitHub" [ref=s1e53]\n            - text: "@personabb"\n    - link "@asap2650" [ref=s1e55]:\n        - /url: https://twitter.com/asap2650\n        - tooltip "@asap2650" [ref=s1e56]:\n            - img "X(Twitter)" [ref=s1e57]\n            - text: "@asap2650"\n    - link "yard.tips" [ref=s1e63]:\n        - /url: https://yard.tips/asap2650\n        - tooltip "yard.tips" [ref=s1e64]:\n            - img [ref=s1e65]\n            - text: yard.tips\n    - link "RSS" [ref=s1e67]:\n        - /url: /asap/feed\n        - tooltip "RSS" [ref=s1e68]:\n            - img [ref=s1e69]\n            - text: RSS\n- link "Articles 72" [ref=s1e80]:\n    - /url: /asap\n- link "Scraps 5" [ref=s1e82]:\n    - /url: /asap?tab=scraps\n- link "Comments" [ref=s1e84]:\n    - /url: /asap?tab=comments\n- contentinfo [ref=s1e88]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e92]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e93]\n    - paragraph [ref=s1e101]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e103]:\n        - heading "About" [level=4] [ref=s1e104]\n        - list [ref=s1e105]:\n            - listitem [ref=s1e106]:\n                - link "Zennについて" [ref=s1e108]:\n                    - /url: /about\n            - listitem [ref=s1e109]:\n                - link "運営会社" [ref=s1e110]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e111]:\n                - link "お知らせ・リリース" [ref=s1e112]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e113]:\n                - link "イベント" [ref=s1e115]:\n                    - /url: /events\n    - navigation [ref=s1e116]:\n        - heading "Guides" [level=4] [ref=s1e117]\n        - list [ref=s1e118]:\n            - listitem [ref=s1e119]:\n                - link "使い方" [ref=s1e121]:\n                    - /url: /manual\n            - listitem [ref=s1e122]:\n                - link "法人向けメニュー" [ref=s1e124]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e126]:\n                - link "Publication / Pro" [ref=s1e128]:\n                    - /url: /publications\n            - listitem [ref=s1e129]:\n                - link "よくある質問" [ref=s1e131]:\n                    - /url: /faq\n    - navigation [ref=s1e132]:\n        - heading "Links" [level=4] [ref=s1e133]\n        - list [ref=s1e134]:\n            - listitem [ref=s1e135]:\n                - link "X(Twitter)" [ref=s1e136]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e137]:\n                - link "GitHub" [ref=s1e138]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e139]:\n                - link "メディアキット" [ref=s1e141]:\n                    - /url: /mediakit\n    - navigation [ref=s1e142]:\n        - heading "Legal" [level=4] [ref=s1e143]\n        - list [ref=s1e144]:\n            - listitem [ref=s1e145]:\n                - link "利用規約" [ref=s1e147]:\n                    - /url: /terms\n            - listitem [ref=s1e148]:\n                - link "プライバシーポリシー" [ref=s1e150]:\n                    - /url: /privacy\n            - listitem [ref=s1e151]:\n                - link "特商法表記" [ref=s1e153]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e156]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e157]\n- alert [ref=s1e176]\n```\n', name='browser_navigate', tool_call_id='d37ce32f-80cf-457d-a5c2-214147053825'), AIMessage(content='asapさんのZennの記事の中で4番目にいいねが多い記事は、「🥰 Open AI Realtime APIのPythonサンプルコードを作成」で、いいね数は79です。\n記事の中身を解説します。', additional_kwargs={'function_call': {'name': 'browser_navigate', 'arguments': '{"url": "https://zenn.dev/asap/articles/4368fd306b592a"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-db44b5c9-f50b-4af3-8ec1-e436036de4f1-0', tool_calls=[{'name': 'browser_navigate', 'args': {'url': 'https://zenn.dev/asap/articles/4368fd306b592a'}, 'id': '7cb428c3-7d50-4f34-819d-e06432672aa4', 'type': 'tool_call'}], usage_metadata={'input_tokens': 74539, 'output_tokens': 72, 'total_tokens': 74611, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Navigated to https://zenn.dev/asap/articles/4368fd306b592a\n\n- Page URL: https://zenn.dev/asap/articles/4368fd306b592a\n- Page Title: Open AI Realtime APIのPythonサンプルコードを作成\n- Page Snapshot\n```yaml\n- banner [ref=s1e4]:\n    - link "🔔 あなたへのおすすめ記事「For you（β）」をリリースしました" [ref=s1e6]:\n        - /url: https://info.zenn.dev/2025-04-02-beta-release-for-you\n        - text: 🔔 あなたへのおすすめ記事「For you（β）」をリリースしました\n        - img [ref=s1e7]\n    - button "閉じる" [ref=s1e11]:\n        - img [ref=s1e12]\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e17]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e18]\n    - link "検索" [ref=s1e28]:\n        - /url: /search\n        - img [ref=s1e29]\n    - button "Log in" [ref=s1e31]\n- article [ref=s1e32]:\n    - text: 🥰\n    - heading "Open AI Realtime APIのPythonサンプルコードを作成" [level=1] [ref=s1e38]\n    - text: 2024/10/04に公開\n    - tooltip "更新日" [ref=s1e45]:\n        - img [ref=s1e46]\n        - text: 2024/12/23 更新日\n    - link "OpenAI" [ref=s1e59]:\n        - /url: /topics/openai\n        - img [ref=s1e61]\n        - text: OpenAI\n    - link "ChatGPT" [ref=s1e63]:\n        - /url: /topics/chatgpt\n        - img [ref=s1e65]\n        - text: ChatGPT\n    - link "生成 AI" [ref=s1e67]:\n        - /url: /topics/%E7%94%9F%E6%88%90ai\n        - img [ref=s1e69]\n        - text: 生成 AI\n    - link "realtimeapi" [ref=s1e71]:\n        - /url: /topics/realtimeapi\n        - img [ref=s1e73]\n        - text: realtimeapi\n    - link "記事投稿コンテスト「今年の最も大きなチャレンジ」" [ref=s1e75]:\n        - /url: /topics/contest2024\n        - img [ref=s1e77]\n        - text: 記事投稿コンテスト「今年の最も大きなチャレンジ」\n    - link "tech" [ref=s1e79]:\n        - /url: /tech-or-idea\n        - img [ref=s1e81]\n        - text: tech\n    - heading "はじめに" [level=1] [ref=s1e85]\n    - complementary [ref=s1e86]:\n        - text: "!"\n        - paragraph [ref=s1e89]: （2024年12月22日 追記） 3点追記させてください。\n        - paragraph [ref=s1e91]:\n            - strong [ref=s1e92]: (1)\n            - text: 12月に新しいモデルが出ているようです。\n            - code [ref=s1e95]: gpt-4o-realtime-preview-2024-12-17\n        - paragraph [ref=s1e96]: こちらを利用すると、10月に出たモデルの60%OFFの料金で利用できるそうです。\n            本日、記事の中身のモデルを上記の最新のものに変更しました。\n        - paragraph [ref=s1e98]: 以前、本記事をご覧いただいた方は、モデル指定の箇所を変更いただけますと幸いです\n            （残念ながら2024年12月22日現在では、Azure版では公開されていないようです。今後に期待です）\n        - code [ref=s1e103]: WS_URL =\n            "wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-12-17"\n        - button "クリップボードにコピー" [ref=s1e106]:\n            - img [ref=s1e107]\n        - button "右端で折り返す" [ref=s1e108]:\n            - img [ref=s1e109]\n        - paragraph [ref=s1e110]:\n            - strong [ref=s1e111]: (2)\n            - text: websocketにおいて、新しいバージョンは引数の名前が変更されたようです。 これまでは、ヘッダー情報を与える際に、\n            - code [ref=s1e114]: extra_headers\n            - text: の引数を利用していましたが、新しいバーションでは\n            - code [ref=s1e115]: additional_headers\n            - text: になるそうです。\n            - link "https://stackoverflow.com/questions/65230690/send-headers-in-websockets-connection-request-from-python-client" [ref=s1e117]:\n                - /url: https://stackoverflow.com/questions/65230690/send-headers-in-websockets-connection-request-from-python-client\n        - paragraph [ref=s1e118]: 記事は最新版に対応できるように変更しました。\n        - paragraph [ref=s1e119]:\n            - text: 今回の件を反省して、バージョン指定込みの\n            - code [ref=s1e120]: requirements.txt\n            - text: も用意しました。\n        - text: requirements.txt\n        - code [ref=s1e126]: cffi==1.17.1 numpy==2.2.1 PyAudio==0.2.14 pycparser==2.22\n            soundfile==0.12.1 websockets==14.1\n        - button "クリップボードにコピー" [ref=s1e143]:\n            - img [ref=s1e144]\n        - button "右端で折り返す" [ref=s1e145]:\n            - img [ref=s1e146]\n        - paragraph [ref=s1e147]:\n            - strong [ref=s1e148]: (3)\n            - text: gpt-4o-miniを利用したRealtime APIが新しくリリースされています。 こちら、WebSocket\n                URLのモデルをgpt-4o-miniのものに変更するだけで、そのまま利用できることがわかりました。\n        - paragraph [ref=s1e151]: 下記のように設定してみてください。（その他箇所は変更不要です）\n        - code [ref=s1e155]: WS_URL =\n            "wss://api.openai.com/v1/realtime?model=gpt-4o-mini-realtime-preview-2024-12-17"\n        - button "クリップボードにコピー" [ref=s1e158]:\n            - img [ref=s1e159]\n        - button "右端で折り返す" [ref=s1e160]:\n            - img [ref=s1e161]\n        - paragraph [ref=s1e162]: （残念ながら2024年12月22日現在では、Azure版では公開されていないようです。今後に期待です）\n        - paragraph [ref=s1e163]: 長々と失礼いたしました。では以下本文です。\n    - paragraph [ref=s1e164]: タイトルの通りです。\n        AzureのAPIは先行で公開しておりましたが、ついにOpenAIからも、APIが解放されたので、取り急ぎpythonでサンプルコードを作成してみました。\n        （公式からはPythonのサンプルコードが公開されていないので、java\n        scriptのコードを参考にしてpythonに書き換える形で作成しました）\n    - paragraph [ref=s1e167]: サンプルコードなので、できるだけ1ファイルかつ、最小限の機能で実装しました。 なのでわかりやすいとは思います。\n        （その代わり、システムプロンプトなどはベタ書きしてます）\n    - paragraph [ref=s1e170]: リアルタイム音声対話ができるRealtime\n        APIをpythonで動かしたいけど、pythonのサンプルコードがなくて困っているという人向けに参考になれば幸いです。\n    - paragraph [ref=s1e171]: （追記） サンプルコードを少し変更することで、Azure版にも接続できることを確認しました。 サンプルコードの章にて変更箇所を紹介します。\n    - paragraph [ref=s1e174]: （2024年10月6日 AM11:52 追記）\n        AI発話中にこちらが発話しても、発話を止めてくれなかった問題ですが、解決方法を見つけましたので、改めて記事にする予定です。（今日は時間がなく・・・申し訳ございません）\n        （ちょっとサンプルコードからは離れると思ったので、別記事の予定です） おそらく10月7日になると思います。\n        記事書きましたら、こちらにもリンクを貼ります。\n    - paragraph [ref=s1e180]:\n        - text: 下記にて取り急ぎコードだけ共有します。コードの解説は後日更新します。 続編の記事は下記になります。\n        - iframe [ref=s1e184]:\n            - link "Open AI Realtime APIのPythonサンプルコードで、AI発話にカットイン（割り込み）する方法 asapさんによる記事 zenn.dev favicon image zenn.dev zenn.dev thumbnail image" [ref=f1s1e6]:\n                - /url: https://zenn.dev/asap/articles/563500af4649da\n                - heading "Open AI Realtime\n                  APIのPythonサンプルコードで、AI発話にカットイン（割り込み）する方法" [level=1]\n                  [ref=f1s1e8]\n                - text: asapさんによる記事\n                - img "zenn.dev favicon image" [ref=f1s1e11]\n                - text: zenn.dev\n                - img "zenn.dev thumbnail image" [ref=f1s1e13]\n            - alert [ref=f1s1e15]\n        - iframe [ref=s1e186]:\n            - link "Open AI Realtime APIで、会話履歴を削除して高額請求を回避したい！【Pythonサンプルコード】 asapさんによる記事 zenn.dev favicon image zenn.dev zenn.dev thumbnail image" [ref=f2s1e6]:\n                - /url: https://zenn.dev/asap/articles/af07fcedbbef61\n                - heading "Open AI Realtime\n                  APIで、会話履歴を削除して高額請求を回避したい！【Pythonサンプルコード】" [level=1]\n                  [ref=f2s1e8]\n                - text: asapさんによる記事\n                - img "zenn.dev favicon image" [ref=f2s1e11]\n                - text: zenn.dev\n                - img "zenn.dev thumbnail image" [ref=f2s1e13]\n            - alert [ref=f2s1e15]\n    - paragraph [ref=s1e187]: なお、実践上は上記で紹介するコードを利用するのをお勧めしますが、Realtime\n        APIをpythonで利用するにあたっての仕様などの理解は、本記事で紹介するサンプルコードを利用する方が理解しやすいと思います。\n    - heading "参考" [level=1] [ref=s1e188]\n    - paragraph [ref=s1e189]: 下記の公式ドキュメントを参考にしながら組みました。（というかそれ以外に情報がない・・・）\n    - paragraph [ref=s1e190]:\n        - iframe [ref=s1e192]:\n            - link "OpenAI Platform Explore developer resources, tutorials, API docs, and dynamic examples to get the most out of OpenAI platform.openai.com favicon image platform.openai.com platform.openai.com thumbnail image" [ref=f3s1e6]:\n                - /url: https://platform.openai.com/docs/guides/realtime?text-generation-quickstart-example=stream\n                - heading "OpenAI Platform" [level=1] [ref=f3s1e8]\n                - text: Explore developer resources, tutorials, API docs, and dynamic examples to\n                    get the most out of OpenAI\n                - img "platform.openai.com favicon image" [ref=f3s1e11]\n                - text: platform.openai.com\n                - img "platform.openai.com thumbnail image" [ref=f3s1e13]\n            - alert [ref=f3s1e15]\n        - iframe [ref=s1e194]: <could not take iframe snapshot>\n    - heading "サンプルコード" [level=1] [ref=s1e195]\n    - paragraph [ref=s1e196]:\n        - text: 取り急ぎ下記がサンプルコードです。 下記コマンドで実行できるとおもいます。 （\n        - code [ref=s1e199]: ~/.zshrc\n        - text: や\n        - code [ref=s1e200]: ~/.bashrc\n        - text: にOPENAI_API_KEYが記録されていることを前提にしていますが、わからなければ直接コード内にAPIKEYを書き下してもよいです。推奨はしません。）\n    - code [ref=s1e204]: pip install websockets soundfile numpy pyaudio\n    - button "クリップボードにコピー" [ref=s1e205]:\n        - img [ref=s1e206]\n    - button "右端で折り返す" [ref=s1e207]:\n        - img [ref=s1e208]\n    - complementary [ref=s1e209]:\n        - text: "!"\n        - paragraph [ref=s1e212]:\n            - text: バーション指定込みの\n            - code [ref=s1e213]: requirements.txt\n            - text: を用意したので、もしコードが動かない場合は、下記を試してみてください。\n        - text: requirements.txt\n        - code [ref=s1e219]: cffi==1.17.1 numpy==2.2.1 PyAudio==0.2.14 pycparser==2.22\n            soundfile==0.12.1 websockets==14.1\n        - button "クリップボードにコピー" [ref=s1e236]:\n            - img [ref=s1e237]\n        - button "右端で折り返す" [ref=s1e238]:\n            - img [ref=s1e239]\n        - code [ref=s1e243]: pip install -r requirements.txt\n        - button "クリップボードにコピー" [ref=s1e244]:\n            - img [ref=s1e245]\n        - button "右端で折り返す" [ref=s1e246]:\n            - img [ref=s1e247]\n    - code [ref=s1e251]: python main.py\n    - button "クリップボードにコピー" [ref=s1e252]:\n        - img [ref=s1e253]\n    - button "右端で折り返す" [ref=s1e254]:\n        - img [ref=s1e255]\n    - complementary [ref=s1e256]:\n        - text: "!"\n        - paragraph [ref=s1e259]:\n            - text: （追記） Macで実行する場合は、\n            - code [ref=s1e261]: pyaudio\n            - text: のインストールでエラーが出てしまうようです。 その場合は先に下記を実行してから\n            - code [ref=s1e263]: pip install\n            - text: をお試しください\n        - code [ref=s1e267]: brew install portaudio\n        - button "クリップボードにコピー" [ref=s1e268]:\n            - img [ref=s1e269]\n        - button "右端で折り返す" [ref=s1e270]:\n            - img [ref=s1e271]\n    - paragraph [ref=s1e272]: 実行したら、AIが発話するので、その後こちらはマイクに向かって発話すれば、会話を始めることができます。\n    - text: main.py\n    - code [ref=s1e278]: "import asyncio import websockets import pyaudio import numpy\n        as np import base64 import json import wave import io import os API_KEY\n        = os.environ.get(\'OPENAI_API_KEY\') #わからない人は、上の行をコメントアウトして、下記のように直接API\n        KEYを書き下してもよい #API_KEY = \\"sk-xxxxx\\" # WebSocket URLとヘッダー情報 WS_URL =\n        \\"wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-1\\\n        2-17\\" HEADERS = { \\"Authorization\\": \\"Bearer \\"+ API_KEY,\n        \\"OpenAI-Beta\\": \\"realtime=v1\\" } # PCM16形式に変換する関数 def\n        base64_to_pcm16(base64_audio): audio_data =\n        base64.b64decode(base64_audio) return audio_data # 音声を送信する非同期関数 async\n        def send_audio(websocket, stream, CHUNK): def read_audio_block():\n        \\"\\"\\"同期的に音声データを読み取る関数\\"\\"\\" try: return stream.read(CHUNK,\n        exception_on_overflow=False) except Exception as e: print(f\\"音声読み取りエラー:\n        {e}\\") return None print(\\"マイクから音声を取得して送信中...\\") while True: #\n        マイクから音声を取得 audio_data = await\n        asyncio.get_event_loop().run_in_executor(None, read_audio_block) if\n        audio_data is None: continue # 読み取りに失敗した場合はスキップ # PCM16データをBase64にエンコード\n        base64_audio = base64.b64encode(audio_data).decode(\\"utf-8\\")\n        audio_event = { \\"type\\": \\"input_audio_buffer.append\\", \\"audio\\":\n        base64_audio } # WebSocketで音声データを送信 await\n        websocket.send(json.dumps(audio_event)) await asyncio.sleep(0) #\n        サーバーから音声を受信して再生する非同期関数 async def receive_audio(websocket,\n        output_stream): print(\\"assistant: \\", end = \\"\\", flush = True) loop =\n        asyncio.get_event_loop() while True: # サーバーからの応答を受信 response = await\n        websocket.recv() response_data = json.loads(response) #\n        サーバーからの応答をリアルタイム（ストリーム）で表示 if \\"type\\" in response_data and\n        response_data[\\"type\\"] == \\"response.audio_transcript.delta\\":\n        print(response_data[\\"delta\\"], end = \\"\\", flush = True) #\n        サーバからの応答が完了したことを取得 elif \\"type\\" in response_data and\n        response_data[\\"type\\"] == \\"response.audio_transcript.done\\":\n        print(\\"\\\\nassistant: \\", end = \\"\\", flush = True) #\n        サーバーからの応答に音声データが含まれているか確認 if \\"delta\\" in response_data: if\n        response_data[\\"type\\"] == \\"response.audio.delta\\":\n        base64_audio_response = response_data[\\"delta\\"] if\n        base64_audio_response: pcm16_audio =\n        base64_to_pcm16(base64_audio_response) #音声データがある場合は、出力ストリームから再生 await\n        loop.run_in_executor(None, output_stream.write, pcm16_audio) #\n        マイクからの音声を取得し、WebSocketで送信しながらサーバーからの音声応答を再生する非同期関数 async def\n        stream_audio_and_receive_response(): # WebSocketに接続 async with\n        websockets.connect(WS_URL, additional_headers=HEADERS) as websocket:\n        print(\\"WebSocketに接続しました。\\") # 初期リクエスト (モダリティ設定) init_request = {\n        \\"type\\": \\"response.create\\", \\"response\\": { \\"modalities\\":\n        [\\"audio\\", \\"text\\"], \\"instructions\\": \\"ユーザーをサポートしてください。\\",\n        \\"voice\\": \\"echo\\" #\\"alloy\\", \\"echo\\", \\"shimmer\\", \\"ash\\",\n        \\"ballad\\",\\"coral\\",\\"sage\\",\\"verse\\" } } await\n        websocket.send(json.dumps(init_request)) print(\\"初期リクエストを送信しました。\\") #\n        PyAudioの設定 CHUNK = 2048 # マイクからの入力データのチャンクサイズ FORMAT = pyaudio.paInt16 #\n        PCM16形式 CHANNELS = 1 # モノラル RATE = 24000 # サンプリングレート（24kHz） #\n        PyAudioインスタンス p = pyaudio.PyAudio() # マイクストリームの初期化 stream =\n        p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True,\n        frames_per_buffer=CHUNK) # サーバーからの応答音声を再生するためのストリームを初期化 output_stream =\n        p.open(format=FORMAT, channels=CHANNELS, rate=RATE, output=True,\n        frames_per_buffer=CHUNK) print(\\"マイク入力およびサーバーからの音声再生を開始...\\") try: #\n        音声送信タスクと音声受信タスクを非同期で並行実行 send_task =\n        asyncio.create_task(send_audio(websocket, stream, CHUNK)) receive_task =\n        asyncio.create_task(receive_audio(websocket, output_stream)) #\n        タスクが終了するまで待機 await asyncio.gather(send_task, receive_task) except\n        KeyboardInterrupt: # キーボードの割り込みで終了 print(\\"終了します...\\") finally: #\n        ストリームを閉じる if stream.is_active(): stream.stop_stream() stream.close()\n        output_stream.stop_stream() output_stream.close() p.terminate() if\n        __name__ == \\"__main__\\":\n        asyncio.get_event_loop().run_until_complete(stream_audio_and_receive_re\\\n        sponse())"\n    - button "クリップボードにコピー" [ref=s1e707]:\n        - img [ref=s1e708]\n    - button "右端で折り返す" [ref=s1e709]:\n        - img [ref=s1e710]\n    - group [ref=s1e711]: Azure版に接続する\n    - heading "実行結果" [level=1] [ref=s1e713]\n    - paragraph [ref=s1e714]: 実行結果を表示します。（音量小さめなので、静かな場所でのご視聴をお勧めします）\n    - paragraph [ref=s1e715]:\n        - iframe [ref=s1e717]: <could not take iframe snapshot>\n    - paragraph [ref=s1e718]: 相手の発話開始までの応答時間はめちゃくちゃ早かったです。1.5秒かかってないんじゃないかな\n        こんなすごいのが出てくると、今まで、音声認識やらLLMやら合成音声を繋げて頑張ってた自分の自信が完全になくなりますね。\n    - paragraph [ref=s1e720]: ちなみに1ドルちょっと使ったら、すぐにRate Limitになりました。\n        tier1だからというのもあると思いますが・・・ →一瞬Rate\n        Limitになりましたが、記事書き終わった頃には解消されていました。結構早かったです。\n    - heading "詰まった点" [level=1] [ref=s1e723]\n    - paragraph [ref=s1e724]:\n        - text: 基本的には下記の記事をよく読めばわかる内容ですが、私はつまづいたので忘備録も兼ねて\n        - iframe [ref=s1e726]: <could not take iframe snapshot>\n    - paragraph [ref=s1e727]: とにかく重要なのは下記の部分\n    - code [ref=s1e731]: "audio_event = { \\"type\\": \\"input_audio_buffer.append\\",\n        \\"audio\\": base64_audio }"\n    - button "クリップボードにコピー" [ref=s1e741]:\n        - img [ref=s1e742]\n    - button "右端で折り返す" [ref=s1e743]:\n        - img [ref=s1e744]\n    - paragraph [ref=s1e745]:\n        - text: 今回のように、入出力をストリーミングで処理する場合は、\n        - code [ref=s1e746]: "\\"type\\": \\"input_audio_buffer.append"\n        - text: を指定する必要があります。\n    - paragraph [ref=s1e747]: この部分を下記のように書いていると一生うまくいきません（1敗）\n    - code [ref=s1e751]: "event = { type: \'conversation.item.create\', item: { type:\n        \'message\', role: \'user\', content: [ { type: \'input_audio\', audio:\n        base64AudioData } ] }"\n    - button "クリップボードにコピー" [ref=s1e778]:\n        - img [ref=s1e779]\n    - button "右端で折り返す" [ref=s1e780]:\n        - img [ref=s1e781]\n    - paragraph [ref=s1e782]: あとは単純にpythonの実装の話ですが、pyaudioを利用する場合は、入出力で同じスレッドを利用するとぶつぶつした音声になってしまうので、ちゃんとスレッドを分けつつ、時間がかかる処理は非同期で実施することをお勧めします（1敗）\n    - heading "コードの解説" [level=1] [ref=s1e783]\n    - paragraph [ref=s1e784]: 簡単にコードの解説をしていきます。\n        上記のコードは、OpenAIのモデルにWebsocketで接続する部分と、PyAudioによる入出力音声の制御の主に二つの処理によって構成されていますが、今回は、WebsocketでOpenAIのサーバと通信する部分を主に説明します。\n    - paragraph [ref=s1e786]: （解説のために順番などはバラバラになっています。実際の処理は実際のコードをご覧ください）\n    - heading "Websocketへの接続" [level=2] [ref=s1e787]\n    - paragraph [ref=s1e788]: 最初の接続は下記のようなコードで達成されます。\n    - code [ref=s1e792]: "API_KEY = os.environ.get(\'OPENAI_API_KEY\') # WebSocket\n        URLとヘッダー情報 WS_URL =\n        \\"wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-1\\\n        0-01\\" HEADERS = { \\"Authorization\\": \\"Bearer \\" + API_KEY,\n        \\"OpenAI-Beta\\": \\"realtime=v1\\" } # WebSocketに接続 async with\n        websockets.connect(WS_URL, additional_headers=HEADERS) as websocket:\n        print(\\"WebSocketに接続しました。\\")"\n    - button "クリップボードにコピー" [ref=s1e827]:\n        - img [ref=s1e828]\n    - button "右端で折り返す" [ref=s1e829]:\n        - img [ref=s1e830]\n    - paragraph [ref=s1e831]: API_KEYを環境変数から取得し、\n        OpenAIが指定するサーバURLと認証情報を付与したHEADERSを利用して、Websocket通信で接続しています。\n    - heading "最初のリクエスト" [level=2] [ref=s1e832]\n    - paragraph [ref=s1e833]: 接続後、最初のリクエストは下記のように実施します。\n    - code [ref=s1e837]: "async with websockets.connect(WS_URL,\n        additional_headers=HEADERS) as websocket: print(\\"WebSocketに接続しました。\\") #\n        初期リクエスト (モダリティ設定) init_request = { \\"type\\": \\"response.create\\",\n        \\"response\\": { \\"modalities\\": [\\"audio\\", \\"text\\"], \\"instructions\\":\n        \\"関西弁で回答してください。\\", \\"voice\\": \\"echo\\" #\\"alloy\\", \\"echo\\",\n        \\"shimmer\\", \\"ash\\", \\"ballad\\",\\"coral\\",\\"sage\\",\\"verse\\" } } await\n        websocket.send(json.dumps(init_request))"\n    - button "クリップボードにコピー" [ref=s1e886]:\n        - img [ref=s1e887]\n    - button "右端で折り返す" [ref=s1e888]:\n        - img [ref=s1e889]\n    - paragraph [ref=s1e890]: 最初の2行は前節と共通で、Websocket通信が接続している範囲内で処理をしてくださいという意味です。\n    - paragraph [ref=s1e891]:\n        - text: その上で、最初は\n        - code [ref=s1e892]: "\\"type\\": \\"response.create\\""\n        - text: のtypeを持つJSONを作成します。 詳細か下記をご覧ください。\n        - iframe [ref=s1e895]: <could not take iframe snapshot>\n    - paragraph [ref=s1e896]: 公式のドキュメントにも記載されていますが、下記の要素を指定できます・\n    - code [ref=s1e900]: "{ \\"event_id\\": \\"event_234\\", \\"type\\":\n        \\"response.create\\", \\"response\\": { \\"modalities\\": [\\"text\\",\n        \\"audio\\"], \\"instructions\\": \\"Please assist the user.\\", \\"voice\\":\n        \\"alloy\\", \\"output_audio_format\\": \\"pcm16\\", \\"tools\\": [ { \\"type\\":\n        \\"function\\", \\"name\\": \\"calculate_sum\\", \\"description\\": \\"Calculates\n        the sum of two numbers.\\", \\"parameters\\": { \\"type\\": \\"object\\",\n        \\"properties\\": { \\"a\\": { \\"type\\": \\"number\\" }, \\"b\\": { \\"type\\":\n        \\"number\\" } }, \\"required\\": [\\"a\\", \\"b\\"] } } ], \\"tool_choice\\":\n        \\"auto\\", \\"temperature\\": 0.7, \\"max_output_tokens\\": 150 } }"\n    - button "クリップボードにコピー" [ref=s1e1000]:\n        - img [ref=s1e1001]\n    - button "右端で折り返す" [ref=s1e1002]:\n        - img [ref=s1e1003]\n    - paragraph [ref=s1e1004]:\n        - text: 今回はFunction Callingなどは利用しないため、最低限の指定に収めていますが、\n        - code [ref=s1e1005]: temperature\n        - text: や\n        - code [ref=s1e1006]: max_output_tokens\n        - text: などのよく見る要素も、最初に指定できます。\n    - paragraph [ref=s1e1007]:\n        - text: 本記事では\n        - code [ref=s1e1008]: modalities\n        - text: と\n        - code [ref=s1e1009]: instructions\n        - text: と\n        - code [ref=s1e1010]: voice\n        - text: を主に設定しており、そのほかはDefaultの値（つまり上に記載した全文の情報）を利用しています。\n    - paragraph [ref=s1e1011]:\n        - code [ref=s1e1012]: modalities\n        - text: は、何の種類のモダリティを利用するかを指定しており、今回はSystem\n            PromptにはTextデータを、会話にはAudioデータを利用するため、両方指定しています。\n    - paragraph [ref=s1e1013]:\n        - code [ref=s1e1014]: instructions\n        - text: は俗にいうSystem Promptです。ここでAIにどのように振る舞ってほしいかを指定することができます。\n            今回はサンプルなので「関西弁で回答してください。」の一行だけですが、普通に長いPromptを投入することも可能です。\n    - paragraph [ref=s1e1016]:\n        - code [ref=s1e1017]: voice\n        - text: は対話でモデルが出す声のキャラクターです。具体的に現在どのモデルが利用できるのかが調べてもわからなかったため、PlayGroundで選択できるキャラクター（3名）のうち一人を選んでいます。\n            PlayGroundでは名前の先頭が大文字で書かれていますが、JSONでは小文字で書く必要があることに注意してください。\n    - complementary [ref=s1e1019]:\n        - text: "!"\n        - paragraph [ref=s1e1022]: （追記） 2024年12月22日現在、利用可能な声の種類はどんどん増えています。 現時点で確認できたのは、\n        - code [ref=s1e1028]: "#\\"alloy\\", \\"echo\\", \\"shimmer\\", \\"ash\\",\n            \\"ballad\\",\\"coral\\",\\"sage\\",\\"verse\\""\n        - button "クリップボードにコピー" [ref=s1e1030]:\n            - img [ref=s1e1031]\n        - button "右端で折り返す" [ref=s1e1032]:\n            - img [ref=s1e1033]\n        - paragraph [ref=s1e1034]: の8名です\n    - group [ref=s1e1035]: 最初に発話させたくなかったら\n    - heading "自分が発話した音声を送信する" [level=2] [ref=s1e1037]\n    - paragraph [ref=s1e1038]: 発話した音声を送信する方法はいくつかありますが、音声対話を行う場合は、ストリーミング（音声を細かいCHUNK単位で区切って、リアルタイムに送信する）で音声を送信する必要があります。\n        公式のドキュメントではあまり触れられていないので、私も沼りましたが、結論、下記のように送信すれば良いです。\n    - code [ref=s1e1043]: "# PCM16データをBase64にエンコード base64_audio =\n        base64.b64encode(audio_data).decode(\\"utf-8\\") audio_event = { \\"type\\":\n        \\"input_audio_buffer.append\\", \\"audio\\": base64_audio } #\n        WebSocketで音声データを送信 await websocket.send(json.dumps(audio_event))"\n    - button "クリップボードにコピー" [ref=s1e1070]:\n        - img [ref=s1e1071]\n    - button "右端で折り返す" [ref=s1e1072]:\n        - img [ref=s1e1073]\n    - paragraph [ref=s1e1074]: 当然、上記もWebsocketが接続している中で実行してください。\n    - paragraph [ref=s1e1075]:\n        - text: まず、前提として\n        - code [ref=s1e1076]: audio_data\n        - text: はPCM16というフォーマットの音声データのチャンクで、サンプリングレートは24,000Hzであることとします。\n            （そのほかの設定でも上手くいくのかもしれないですが、私はよく知らないです）\n            今回、私は音声のチャンクサイズを2048としています。おおよそ0.085秒程度の音声データです。\n    - paragraph [ref=s1e1079]: 今回は音声データをそのままWebsocketで送信できるわけではなく、一度、Base64形式にエンコードしたのちにJSONの要素として送る必要があるため、上記のような実装コードになっています。\n    - paragraph [ref=s1e1080]:\n        - text: また、Websocketで送るJSONの\n        - code [ref=s1e1081]: type\n        - text: も\n        - code [ref=s1e1082]: "\\"type\\": \\"input_audio_buffer.append\\""\n        - text: を利用する必要があります。 こちらは下記の公式ドキュメントをご覧ください。\n        - iframe [ref=s1e1085]: <could not take iframe snapshot>\n    - paragraph [ref=s1e1086]:\n        - text: ここで、\n        - code [ref=s1e1087]: audio\n        - text: には、エンコードされた音声のチャンクが入ります。 これをループして、マイク入力音声を少しずつ、全てWebsocketで送信していることになります。\n    - heading "AIが発話した音声を受信する" [level=2] [ref=s1e1089]\n    - paragraph [ref=s1e1090]: 受信に関しては下記のように取得できます。\n    - code [ref=s1e1094]: response = await websocket.recv() response_data = json.loads(response)\n    - button "クリップボードにコピー" [ref=s1e1104]:\n        - img [ref=s1e1105]\n    - button "右端で折り返す" [ref=s1e1106]:\n        - img [ref=s1e1107]\n    - paragraph [ref=s1e1108]: もちろん、Websocketが接続されている状態で実行してください。\n        OpenAIからはJSON形式でデータが返ってくるので、それを辞書型としてPythonで処理できるようにロードしています。\n    - paragraph [ref=s1e1110]: 取得後のデータの中身の処理は、実装するアプリケーションに依存しますが、音声対話では、最低限取得した音声を再生できることが求められます。\n    - paragraph [ref=s1e1111]: それは、下記で実行されます。\n    - code [ref=s1e1115]: "if \\"delta\\" in response_data: if response_data[\\"type\\"]\n        == \\"response.audio.delta\\": base64_audio_response =\n        response_data[\\"delta\\"] if base64_audio_response: pcm16_audio =\n        base64.b64decode(base64_audio_response) #音声データがある場合は、出力ストリームから再生 await\n        loop.run_in_executor(None, output_stream.write, pcm16_audio)"\n    - button "クリップボードにコピー" [ref=s1e1146]:\n        - img [ref=s1e1147]\n    - button "右端で折り返す" [ref=s1e1148]:\n        - img [ref=s1e1149]\n    - paragraph [ref=s1e1150]:\n        - text: 音声自体も、一度に全ての発話音声データが送られてくるわけではなく、一定のCHUNKに分割して返却されます。 返却されるJSONの\n        - code [ref=s1e1152]: type\n        - text: は\n        - code [ref=s1e1153]: "\\"response.audio.delta\\""\n        - text: となります。 公式のドキュメントは下記になります。\n        - iframe [ref=s1e1156]: <could not take iframe snapshot>\n    - paragraph [ref=s1e1157]:\n        - text: このJSONの\n        - code [ref=s1e1158]: delta\n        - text: にBase64でEncodeされた音声データが格納されています。\n            したがって、デコードすれば音声データになるため、デコード後、PyAudioで非同期に再生しています。\n    - heading "AIが発話した音声の文字起こしをストリーミングで取得する" [level=2] [ref=s1e1160]\n    - paragraph [ref=s1e1161]: 音声対話システムを実装するなら、AIが発話した内容の音声だけではなく、テキストでも表示がしたいです。 テキストを取得するには下記を実行します。\n    - code [ref=s1e1166]: "print(\\"assistant: \\", end = \\"\\", flush = True) ・・・ while\n        True: ・・・ # サーバーからの応答をリアルタイムに表示 if \\"type\\" in response_data and\n        response_data[\\"type\\"] == \\"response.audio_transcript.delta\\":\n        print(response_data[\\"delta\\"], end = \\"\\", flush = True) #\n        サーバからの応答が完了したことを取得 elif \\"type\\" in response_data and\n        response_data[\\"type\\"] == \\"response.audio_transcript.done\\":\n        print(\\"\\\\nassistant: \\", end = \\"\\", flush = True)"\n    - button "クリップボードにコピー" [ref=s1e1224]:\n        - img [ref=s1e1225]\n    - button "右端で折り返す" [ref=s1e1226]:\n        - img [ref=s1e1227]\n    - paragraph [ref=s1e1228]:\n        - text: ストリーミングでテキストを取得するには、\n        - code [ref=s1e1229]: response.audio_transcript.delta\n        - text: の\n        - code [ref=s1e1230]: type\n        - text: のJSONの中身を取得して、それをコンソールに表示します。 また、ストリーミングで取得するテキストが一段落ついたら\n        - code [ref=s1e1232]: response.audio_transcript.done\n        - text: の\n        - code [ref=s1e1233]: type\n        - text: のJSONが発行されるため、それを取得したら、次のテキストを表示する準備をしています。\n    - heading "実装できなかった部分" [level=1] [ref=s1e1234]\n    - paragraph [ref=s1e1235]: AIが話しているときに、こちらが話し始めても、うまく音声を止めてくれませんでした。\n        (止めてくれることもあれば、そのまま話し続けることもある） 上記のサンプルコードだけでは、その部分が弱いみたいです。\n        上手な実装を知っている方がいたら教えてください。\n    - paragraph [ref=s1e1240]:\n        - text: 上記に関しても実装できました！ 下記にて取り急ぎコードだけ共有します。コードの解説は後日更新します。\n        - iframe [ref=s1e1243]: <could not take iframe snapshot>\n    - paragraph [ref=s1e1244]: なお、実践上は上記で紹介するコードを利用するのをお勧めしますが、Realtime\n        APIをpythonで利用するにあたっての仕様などの理解は、本記事で紹介するサンプルコードを利用する方が理解しやすいと思います。\n    - paragraph [ref=s1e1245]: あと、キャラクターの音声を変える部分は記載してないですので、これもRate Limitが解除されたら記載します。 こちらは実装して、更新しました！\n    - code [ref=s1e1251]: "init_request = { \\"type\\": \\"response.create\\",\n        \\"response\\": { \\"modalities\\": [\\"audio\\", \\"text\\"], \\"instructions\\":\n        \\"ユーザーをサポートしてください。\\", \\"voice\\": \\"echo\\" #\\"alloy\\", \\"echo\\",\n        \\"shimmer\\", \\"ash\\", \\"ballad\\",\\"coral\\",\\"sage\\",\\"verse\\" } }"\n    - button "クリップボードにコピー" [ref=s1e1279]:\n        - img [ref=s1e1280]\n    - button "右端で折り返す" [ref=s1e1281]:\n        - img [ref=s1e1282]\n    - paragraph [ref=s1e1283]:\n        - text: 上記の\n        - code [ref=s1e1284]: instructions\n        - text: の部分が、最初にAIに与えるプロンプトになります。system promptのようなものです。 また\n        - code [ref=s1e1286]: voice\n        - text: のところを変更すると、声を変えることができます。 誰の声が使えるのかよくわからなかったですが、\n        - code [ref=s1e1288]: PlayGround\n        - text: にて利用できる名前は全て動作確認できました。 もっと増えるといいですね。\n    - heading "まとめ" [level=1] [ref=s1e1290]\n    - paragraph [ref=s1e1291]: 取り急ぎ、pythonで動かしたいけどサンプルコードがなくて困っているという方向けに、実装したものを公開します。\n        Rate Limitが解除されたら、もっといろいろ試して機能追加できたらいいなと思っています。\n    - paragraph [ref=s1e1293]: 今回の記事の本題とは異なるため紹介しませんでしたが、実はユーザが発話した音声の文字起こしを取得する機能もあります。したがってRAGのQueryに利用すれば、Realtime\n        APIとRAGを組み合わせることもできそうです。（試していない＋文字起こしの処理速度は遅いので、応答時間が長くなると思うが）\n    - paragraph [ref=s1e1294]:\n        - text: 下記に置いているコードは、ユーザが発話した音声の文字起こしも取得しています。（取得しただけ）\n        - iframe [ref=s1e1296]: <could not take iframe snapshot>\n    - paragraph [ref=s1e1297]:\n        - text: 続編として、下記二つの記事も書いています。 両方ともRealtime APIを利用する上で意識しないといけないことが記載されています。\n            お役に立てれば幸いです。\n        - iframe [ref=s1e1301]: <could not take iframe snapshot>\n        - iframe [ref=s1e1303]: <could not take iframe snapshot>\n    - paragraph [ref=s1e1304]: ありがとうございました！\n    - button "いいね" [ref=s1e1307]:\n        - img [ref=s1e1308]\n    - text: "79"\n    - button "ブックマーク" [ref=s1e1324]:\n        - img [ref=s1e1325]\n    - button "その他の操作" [ref=s1e1329]:\n        - img [ref=s1e1330]\n    - tooltip "X(Twitter)にポスト" [ref=s1e1333]:\n        - img [ref=s1e1334]\n        - text: X(Twitter)にポスト\n    - tooltip "Facebookに投稿" [ref=s1e1340]:\n        - img [ref=s1e1341]\n        - text: Facebookに投稿\n    - tooltip "はてなブックマークに登録" [ref=s1e1347]:\n        - img [ref=s1e1348]\n        - text: はてなブックマークに登録\n    - complementary [ref=s1e1351]:\n        - link "asap" [ref=s1e1353]:\n            - /url: /asap\n            - img "asap" [ref=s1e1354]\n        - link "asap" [ref=s1e1356]:\n            - /url: /asap\n        - paragraph [ref=s1e1358]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n        - button "フォロー" [ref=s1e1362]\n        - tooltip "@personabb" [ref=s1e1363]:\n            - img "GitHub" [ref=s1e1364]\n            - text: "@personabb"\n        - tooltip "@asap2650" [ref=s1e1366]:\n            - img "X(Twitter)" [ref=s1e1367]\n            - text: "@asap2650"\n        - tooltip "yard.tips" [ref=s1e1373]:\n            - img "リンク" [ref=s1e1374]\n            - text: yard.tips\n    - complementary [ref=s1e1377]:\n        - text: バッジを贈って著者を応援しよう\n        - paragraph [ref=s1e1381]: バッジを受け取った著者にはZennから現金やAmazonギフトカードが還元されます。\n        - button "バッジを贈る" [ref=s1e1383]\n        - img [ref=s1e1384]\n    - heading "Discussion" [level=3] [ref=s1e1393]\n    - img [ref=s1e1394]\n    - text: ログインするとコメントできます\n    - button "Login" [ref=s1e1398]\n    - complementary [ref=s1e1399]:\n        - link "asap" [ref=s1e1404]:\n            - /url: /asap\n            - img "asap" [ref=s1e1405]\n        - link "asap" [ref=s1e1407]:\n            - /url: /asap\n        - button "フォロー" [ref=s1e1409]\n        - link "@personabb" [ref=s1e1410]:\n            - /url: https://github.com/personabb\n            - tooltip "@personabb" [ref=s1e1411]:\n                - img "GitHub" [ref=s1e1412]\n                - text: "@personabb"\n        - link "@asap2650" [ref=s1e1414]:\n            - /url: https://twitter.com/asap2650\n            - tooltip "@asap2650" [ref=s1e1415]:\n                - img "X(Twitter)" [ref=s1e1416]\n                - text: "@asap2650"\n        - link "yard.tips" [ref=s1e1422]:\n            - /url: https://yard.tips/asap2650\n            - tooltip "yard.tips" [ref=s1e1423]:\n                - img "リンク" [ref=s1e1424]\n                - text: yard.tips\n        - paragraph [ref=s1e1427]: 記事は趣味、備忘録。理論が好き。 2024年5月〜 執筆継続中。\n        - button "バッジを贈る" [ref=s1e1430]\n        - link "バッジを贈るとは" [ref=s1e1432]:\n            - /url: /faq#badges\n            - text: バッジを贈るとは\n            - img [ref=s1e1433]\n        - text: 目次\n        - list [ref=s1e1442]:\n            - listitem [ref=s1e1443]:\n                - link "はじめに" [ref=s1e1444]:\n                    - /url: "#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB"\n            - listitem [ref=s1e1445]:\n                - link "参考" [ref=s1e1446]:\n                    - /url: "#%E5%8F%82%E8%80%83"\n            - listitem [ref=s1e1447]:\n                - link "サンプルコード" [ref=s1e1448]:\n                    - /url: "#%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%82%B3%E3%83%BC%E3%83%89"\n            - listitem [ref=s1e1449]:\n                - link "実行結果" [ref=s1e1450]:\n                    - /url: "#%E5%AE%9F%E8%A1%8C%E7%B5%90%E6%9E%9C"\n            - listitem [ref=s1e1451]:\n                - link "詰まった点" [ref=s1e1452]:\n                    - /url: "#%E8%A9%B0%E3%81%BE%E3%81%A3%E3%81%9F%E7%82%B9"\n            - listitem [ref=s1e1453]:\n                - link "コードの解説" [ref=s1e1454]:\n                    - /url: "#%E3%82%B3%E3%83%BC%E3%83%89%E3%81%AE%E8%A7%A3%E8%AA%AC"\n                - list [ref=s1e1455]:\n                    - listitem [ref=s1e1456]:\n                        - link "Websocketへの接続" [ref=s1e1457]:\n                            - /url: "#websocket%E3%81%B8%E3%81%AE%E6%8E%A5%E7%B6%9A"\n                    - listitem [ref=s1e1458]:\n                        - link "最初のリクエスト" [ref=s1e1459]:\n                            - /url: "#%E6%9C%80%E5%88%9D%E3%81%AE%E3%83%AA%E3%82%AF%E3%82%A8%E3%82%B9%E3%83%8\\\n                                8"\n                    - listitem [ref=s1e1460]:\n                        - link "自分が発話した音声を送信する" [ref=s1e1461]:\n                            - /url: "#%E8%87%AA%E5%88%86%E3%81%8C%E7%99%BA%E8%A9%B1%E3%81%97%E3%81%9F%E9%9F%B\\\n                                3%E5%A3%B0%E3%82%92%E9%80%81%E4%BF%A1%E3%81%99%\\\n                                E3%82%8B"\n                    - listitem [ref=s1e1462]:\n                        - link "AIが発話した音声を受信する" [ref=s1e1463]:\n                            - /url: "#ai%E3%81%8C%E7%99%BA%E8%A9%B1%E3%81%97%E3%81%9F%E9%9F%B3%E5%A3%B0%E3%82\\\n                                %92%E5%8F%97%E4%BF%A1%E3%81%99%E3%82%8B"\n                    - listitem [ref=s1e1464]:\n                        - link "AIが発話した音声の文字起こしをストリーミングで取得する" [ref=s1e1465]:\n                            - /url: "#ai%E3%81%8C%E7%99%BA%E8%A9%B1%E3%81%97%E3%81%9F%E9%9F%B3%E5%A3%B0%E3%81\\\n                                %AE%E6%96%87%E5%AD%97%E8%B5%B7%E3%81%93%E3%81%9\\\n                                7%E3%82%92%E3%82%B9%E3%83%88%E3%83%AA%E3%83%BC%\\\n                                E3%83%9F%E3%83%B3%E3%82%B0%E3%81%A7%E5%8F%96%E5\\\n                                %BE%97%E3%81%99%E3%82%8B"\n            - listitem [ref=s1e1466]:\n                - link "実装できなかった部分" [ref=s1e1467]:\n                    - /url: "#%E5%AE%9F%E8%A3%85%E3%81%A7%E3%81%8D%E3%81%AA%E3%81%8B%E3%81%A3%E3%81%9\\\n                        F%E9%83%A8%E5%88%86"\n            - listitem [ref=s1e1468]:\n                - link "まとめ" [ref=s1e1469]:\n                    - /url: "#%E3%81%BE%E3%81%A8%E3%82%81"\n        - paragraph [ref=s1e1471]: Zennからのお知らせ\n        - article [ref=s1e1474]:\n            - link "第２回 AI Agent Hackathon with Google Cloud 第２回 AI Agent Hackathon with Google Cloud 4/14登録受付開始！" [ref=s1e1475]:\n                - /url: /hackathons/google-cloud-japan-ai-hackathon-vol2\n                - img "第２回 AI Agent Hackathon with Google Cloud" [ref=s1e1477]\n                - paragraph [ref=s1e1478]: 第２回 AI Agent Hackathon with Google Cloud 4/14登録受付開始！\n        - article [ref=s1e1479]:\n            - link "Zenncafe日比谷#2 生成AIで変わる仕事 Zenncafe日比谷#2 生成AIで変わる仕事" [ref=s1e1480]:\n                - /url: https://zenn.connpass.com/event/351131/\n                - img "Zenncafe日比谷#2 生成AIで変わる仕事" [ref=s1e1482]\n                - paragraph [ref=s1e1483]: Zenncafe日比谷#2 生成AIで変わる仕事\n- contentinfo [ref=s1e1485]:\n    - link "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1489]:\n        - /url: /\n        - img "Zenn | エンジニアのための情報共有コミュニティ" [ref=s1e1490]\n    - paragraph [ref=s1e1498]: エンジニアのための情報共有コミュニティ\n    - navigation [ref=s1e1500]:\n        - heading "About" [level=4] [ref=s1e1501]\n        - list [ref=s1e1502]:\n            - listitem [ref=s1e1503]:\n                - link "Zennについて" [ref=s1e1505]:\n                    - /url: /about\n            - listitem [ref=s1e1506]:\n                - link "運営会社" [ref=s1e1507]:\n                    - /url: https://classmethod.jp\n            - listitem [ref=s1e1508]:\n                - link "お知らせ・リリース" [ref=s1e1509]:\n                    - /url: https://info.zenn.dev\n            - listitem [ref=s1e1510]:\n                - link "イベント" [ref=s1e1512]:\n                    - /url: /events\n    - navigation [ref=s1e1513]:\n        - heading "Guides" [level=4] [ref=s1e1514]\n        - list [ref=s1e1515]:\n            - listitem [ref=s1e1516]:\n                - link "使い方" [ref=s1e1518]:\n                    - /url: /manual\n            - listitem [ref=s1e1519]:\n                - link "法人向けメニュー" [ref=s1e1521]:\n                    - /url: /biz-lp\n                - text: New\n            - listitem [ref=s1e1523]:\n                - link "Publication / Pro" [ref=s1e1525]:\n                    - /url: /publications\n            - listitem [ref=s1e1526]:\n                - link "よくある質問" [ref=s1e1528]:\n                    - /url: /faq\n    - navigation [ref=s1e1529]:\n        - heading "Links" [level=4] [ref=s1e1530]\n        - list [ref=s1e1531]:\n            - listitem [ref=s1e1532]:\n                - link "X(Twitter)" [ref=s1e1533]:\n                    - /url: https://twitter.com/zenn_dev\n            - listitem [ref=s1e1534]:\n                - link "GitHub" [ref=s1e1535]:\n                    - /url: https://github.com/zenn-dev\n            - listitem [ref=s1e1536]:\n                - link "メディアキット" [ref=s1e1538]:\n                    - /url: /mediakit\n    - navigation [ref=s1e1539]:\n        - heading "Legal" [level=4] [ref=s1e1540]\n        - list [ref=s1e1541]:\n            - listitem [ref=s1e1542]:\n                - link "利用規約" [ref=s1e1544]:\n                    - /url: /terms\n            - listitem [ref=s1e1545]:\n                - link "プライバシーポリシー" [ref=s1e1547]:\n                    - /url: /privacy\n            - listitem [ref=s1e1548]:\n                - link "特商法表記" [ref=s1e1550]:\n                    - /url: /terms/transaction-law\n    - link [ref=s1e1553]:\n        - /url: https://classmethod.jp/\n        - img [ref=s1e1554]\n- alert [ref=s1e1573]\n```\n', name='browser_navigate', tool_call_id='7cb428c3-7d50-4f34-819d-e06432672aa4'), AIMessage(content='asapさんのZennの記事の中で4番目にいいねが多い記事は、「Open AI Realtime APIのPythonサンプルコードを作成」で、いいね数は79です。\n\nこの記事は、OpenAIが提供するRealtime APIをPythonで利用するためのサンプルコードを紹介したものです。\n\n**記事の内容**\n\n*   **はじめに:** OpenAIからRealtime APIが公開されたことを受け、Pythonでのサンプルコードを作成した経緯を説明しています。\n*   **参考:** OpenAIの公式ドキュメントを参考にコードを作成したことを述べています。\n*   **サンプルコード:** 実際に動作するPythonのサンプルコードが掲載されています。コードは、WebsocketでOpenAIのサーバーに接続し、音声データを送受信することでリアルタイムな音声対話を実現するものです。\n*   **実行結果:** 実際にコードを実行した結果のデモ動画が掲載されています。\n*   **詰まった点:** コードを作成する上で著者が遭遇した問題点や、解決策が紹介されています。\n*   **コードの解説:** サンプルコードの各部分について、詳細な解説が加えられています。Websocketへの接続方法、音声データの送受信方法、AIが発話した音声の文字起こしを取得する方法などが説明されています。\n*   **実装できなかった部分:** AIが話しているときに、ユーザーが話し始めても、AIの音声をうまく止められないという問題が残っていることが述べられています。\n\n**記事の特徴**\n\n*   **実践的な内容:** OpenAIのRealtime APIを実際にPythonで利用するための具体的な手順が、サンプルコードとともに解説されています。\n*   **初心者向け:** コードの解説が丁寧で、Realtime APIやPythonのWebsocket通信に詳しくない読者でも理解しやすいように書かれています。\n*   **問題点と解決策の提示:** 著者がコードを作成する上で遭遇した問題点や、解決策が共有されており、読者が同様の問題に遭遇した場合に役立つ情報が提供されています。\n\nこの記事は、OpenAIのRealtime APIをPythonで手軽に試してみたいエンジニアにとって、非常に役立つ情報源となっています。', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-2804f27a-3af5-491b-8aaa-b16c04f1559c-0', usage_metadata={'input_tokens': 89632, 'output_tokens': 432, 'total_tokens': 90064, 'input_token_details': {'cache_read': 0}}), HumanMessage(content=[{'type': 'text', 'text': 'GRPOを導入するにあたり、著者はどういう目的があって導入しましたか？著者の意図を教えてください。'}], additional_kwargs={}, response_metadata={}), AIMessage(content='DeepSeek-R1の論文に関する記事の中で、著者がGRPOを導入した目的について解説している箇所を参考に、その意図を説明します。\n\n著者がGRPO（Group Relative Policy Optimization）を導入した主な目的は、LLM（特にDeepSeek-R1-Zero）を強化学習で学習させる際の計算量を削減することです。\n\nPPO（Proximal Policy Optimization）という強化学習の手法は高い性能を発揮しますが、LLMに適用することを考えると、状態価値モデルを別途学習する必要があり、計算量的に大変です。\n\nGRPOは、PPOの状態価値モデルがやっていたこと（今回出力したトークンが、平均よりも良いのか悪いのかを判断すること）を、状態価値モデルなしで実現しようとするものです。具体的には、LLMに複数の文章を生成させ、それぞれの報酬を平均することで、Advantageを計算します。\n\nこれにより、状態価値モデルなしでAdvantageを計算できるため、計算量を大幅に削減できるというメリットがあります。\n\nまとめると、著者の意図は以下のようになります。\n\n*   強化学習によるLLMの性能向上を目指す。\n*   大規模なLLMでも効率的に学習できるよう、計算量を削減する。\n*   PPOの性能を維持しつつ、計算コストを削減できるGRPOを活用する。\n\n著者は、計算量を徹底的に削減することで、高性能かつ圧倒的に格安なAPI料金の両立を達成しようという意図があったと考えられます。', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-3b1515c2-f887-400d-b4ca-d307d1565773-0', usage_metadata={'input_tokens': 90087, 'output_tokens': 320, 'total_tokens': 90407, 'input_token_details': {'cache_read': 0}})]}
=================================
DeepSeek-R1の論文に関する記事の中で、著者がGRPOを導入した目的について解説している箇所を参考に、その意図を説明します。

著者がGRPO（Group Relative Policy Optimization）を導入した主な目的は、LLM（特にDeepSeek-R1-Zero）を強化学習で学習させる際の計算量を削減することです。

PPO（Proximal Policy Optimization）という強化学習の手法は高い性能を発揮しますが、LLMに適用することを考えると、状態価値モデルを別途学習する必要があり、計算量的に大変です。

GRPOは、PPOの状態価値モデルがやっていたこと（今回出力したトークンが、平均よりも良いのか悪いのかを判断すること）を、状態価値モデルなしで実現しようとするものです。具体的には、LLMに複数の文章を生成させ、それぞれの報酬を平均することで、Advantageを計算します。

これにより、状態価値モデルなしでAdvantageを計算できるため、計算量を大幅に削減できるというメリットがあります。

まとめると、著者の意図は以下のようになります。

*   強化学習によるLLMの性能向上を目指す。
*   大規模なLLMでも効率的に学習できるよう、計算量を削減する。
*   PPOの性能を維持しつつ、計算コストを削減できるGRPOを活用する。

著者は、計算量を徹底的に削減することで、高性能かつ圧倒的に格安なAPI料金の両立を達成しようという意図があったと考えられます。
入力してください: exit
終了します。